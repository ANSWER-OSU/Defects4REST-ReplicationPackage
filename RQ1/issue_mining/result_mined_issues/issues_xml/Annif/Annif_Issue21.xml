<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>21</ISSUENO>
  <ISSUEURL>https://github.com/NatLibFi/Annif/issues/21</ISSUEURL>
  <TITLE>Evaluation against a gold standard</TITLE>
  <DESCRIPTION>We could support a command for evaluating how well Annif performs when compared against a gold standard (one or more manually created subject sets per document). The CLI command for a single document could be: annif eval &lt;projectid&gt; &lt;subjectfile&gt; &lt;document.txt and the corresponding REST API operation would be POST /projects/&lt;projectid&gt;/evaluate with the subjects and document text passed in the body. The result should be metrics including precision, recall, F-measure and Rolling similarity. A batch operation for directories could be annif evaldir &lt;projectid&gt; &lt;directory&gt; --maxdocs 10 The maxdocs parameter would limit the evaluation to a random sample of the given number of documents. The reported metrics would be averaged over the sampled documents.</DESCRIPTION>
  <REPONAME>Annif</REPONAME>
  <TIMEDIFFERENCEDAYS>161</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>evaluation metric functions</MESSAGE>
    <SHA>66330fc72de7e867295f258fb60c667e4422f3ed</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>Basic evaluation support (one document at a time via CLI). Part of #21</MESSAGE>
      <SHA>727a4592640e24fce4b0e38dfcf02261525781c4</SHA>
      <PATCHEDFILES>
        <FILE>annif/cli.py</FILE>
        <FILE>annif/corpus/__init__.py</FILE>
        <FILE>tests/test_cli.py</FILE>
        <FILE>tests/test_corpus.py</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
