<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>5649</ISSUENO>
  <ISSUEURL>https://github.com/seaweedfs/seaweedfs/issues/5649</ISSUEURL>
  <TITLE>[Mount] File changes are not propagated to other mounts if they are executing the file</TITLE>
  <DESCRIPTION>**Describe the bug** I had this bug for a while, but only now I was able to isolate and replicate it Consider 3 servers, each one has access to an executable file **script.sh** on SeaweedFS. The script is a long running process. Server 1 and Server 2 are running the script.sh Server 3 is not running the script.sh I want to update script.sh, and for testing I update a comment with a timestamp at the top of file. Step 1) I update the script from Server 3. Reading the same script from Server 1 or Server 2 show still the old version. - Server 1: `# 2024-06-05 02:07:25` - Server 2: `# 2024-06-05 02:07:25` - Server 3: `# 2024-06-05 02:18:05` Step 2) I update the script from Server 1. Server 3 sees the update, Server 2 is using the old version - Server 1: `# 2024-06-05 02:19:58` - Server 2: `# 2024-06-05 02:07:25` - Server 3: `# 2024-06-05 02:19:58` Similar effect is if I update from Server 2, then Server 2 and Server 3 match, but server 1 has the old version Step 3) I update again from Server 3. Now all 3 servers have 3 different versions of the same file - Server 1: `# 2024-06-05 02:19:58` - Server 2: `# 2024-06-05 02:07:25` - Server 3: `# 2024-06-05 02:25:23` However, only the last one updated is the real script.sh. I verified with weed sheel `fs.meta.cat script.sh` and `fs.cat script.sh`, connecting from both Server 1 and Server 2, that at every change, the fileId, timestamps etc are updated, and the data is the latest. Thus filer has the latest version always no matter which server updates the file. When a vacuum happens, the mounts with the older versions lose access to the file because they try to get a chunk that does not exists: ``` Jun 05 01:09:17.836133 reader_at.go:159 fetching chunk &amp;{FileId:4,18f373f609487000a5da75fb OffsetInChunk:0 ViewSize:18130 ViewOffset:0 ChunkSize:18130 CipherKey:[] IsGzipped:true ModifiedTsNs:1717489687525686323}: http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb?readDeleted=true: 404 Not Found Jun 05 01:09:17.836236 filehandle_read.go:65 file handle read /script.sh: http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb?readDeleted=true: 404 Not Found Jun 05 01:09:17.836268 weedfs_file_read.go:51 file handle read /script.sh 0: http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb?readDeleted=true: 404 Not Found Jun 05 01:09:17.837876 http_util.go:482 read http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb failed, err: http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb?readDeleted=true: 404 Not Found Jun 05 01:09:17.837919 http_util.go:488 retry reading in 1s Jun 05 01:09:18.840220 http_util.go:482 read http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb failed, err: http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb?readDeleted=true: 404 Not Found Jun 05 01:09:18.840263 http_util.go:488 retry reading in 1.5s Jun 05 01:09:20.341886 http_util.go:482 read http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb failed, err: http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb?readDeleted=true: 404 Not Found Jun 05 01:09:20.341911 http_util.go:488 retry reading in 2.25s Jun 05 01:09:22.594019 http_util.go:482 read http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb failed, err: http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb?readDeleted=true: 404 Not Found Jun 05 01:09:22.594053 http_util.go:488 retry reading in 3.375s Jun 05 01:09:25.971267 http_util.go:482 read http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb failed, err: http://vpn.data01.nash01.usa.katapy.io:8080/4,18f373f609487000a5da75fb?readDeleted=true: 404 Not Found Jun 05 01:09:25.971304 http_util.go:488 retry reading in 5.0625s ``` The file is not available to the mounts on Server 1 and Server 2 until the mount is restarted. When the mount is restarted, the latest version becomes visibile, but once the script is executed, the file is not updated again if steps 1-3 are repeated. In my example I was using a bash script, but the same is true if a binary file is on SeaweedFS, being executed and then replaced with an updated version. When a file that is not being executed, like test.txt is updated in any server, the others see the latest version. I usually test that the mount sync works by executing a command like `date &gt;&gt; /mnt/seaweedfs/test.txt` on one server, and `less /mnt/seaweedfs/test.txt` on the others always shows the latest version. Also if I have a script which is not directly executed, like by using `python /mnt/seaweedfs/script.py` the updates to `/mnt/seaweedfs/script.py` are propagated correctly **Summary** Long story short, updates to scripts that are in a weed mount are not applied to other mounts if the file in these mounts is currently being **executed**. If the mount is not executing the file, it has always access to the latest version of the file **System Setup** - Command: /opt/weed/bin/weed-large-disk -v 0 mount -filer=vpn.katapy.io:8888,lh1.vpn.katapy.io:8888,lh2.vpn.katapy.io:8888,lh3.vpn.katapy.io:8888 -dir=/mnt/seaweedfs/cloud -cacheDir=/mnt/seaweedfs/.docker/cloud_edge -cacheCapacityMB=134995 -chunkSizeLimitMB=250 -concurrentWriters=128 -volumeServerAccess=publicUrl -dirAutoCreate=true - OS version: Ubuntu 22 LTS, kernel 5.15.0-76-generic x86_64 - output of `weed version`: version 8000GB 3.65 - if using filer, show the content of `filer.toml`: ``` [filer.options] recursive_delete = true [leveldb2] enabled = true dir = &quot;./filerldb2&quot; # directory to store level db files ``` **Expected behavior** The file should always be the latest version, even if it is executed. Linux for example supports modify/replate/delete executable files that are running, without impacting the running processes, because the version they are using is in memory. **Additional context** Minimal script.sh to reproduce the bug: ``` #!/usr/bin/env bash # -*- coding: utf-8 -*- # 2024-06-05 03:08:45 while(true); do sleep 10 done ``` - Create the script on a seaweedfs mount shared between 3 servers - Execute with `./script.sh &amp;` on 2 Servers, do not execute on Server 3 - Execute step 1-3 and see how the files diverge - Kill the script on server 1 - Check now on server 1, the data was updated with latest version, but not on server 2 - Kill on server 2 and verify the data was updated after the script was not executing anymore</DESCRIPTION>
  <REPONAME>seaweedfs</REPONAME>
  <TIMEDIFFERENCEDAYS>1</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Set the capacity of clientChan to 10000 (#5647)</MESSAGE>
    <SHA>d8da4bbaa713db935514b74f44b5a498bbe5ad7e</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>Fix Issue #5649 (#5652)</MESSAGE>
      <SHA>fce8fc1e16978e398f4d55177328ea50ac8210b9</SHA>
      <PATCHEDFILES>
        <FILE>weed/mount/inode_to_path.go</FILE>
        <FILE>weed/mount/weedfs.go</FILE>
        <FILE>weed/mount/weedfs_file_sync.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
