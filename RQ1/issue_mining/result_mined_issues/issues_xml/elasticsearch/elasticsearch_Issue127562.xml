<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>127562</ISSUENO>
  <ISSUEURL>https://github.com/elastic/elasticsearch/issues/127562</ISSUEURL>
  <TITLE>[CI] StoredFieldsSequentialIT class failing</TITLE>
  <DESCRIPTION>**Build Scans:** - [elasticsearch-periodic #7934 / single-processor-node-tests](https://gradle-enterprise.elastic.co/s/jcezc6hfjfqtk) - [elasticsearch-periodic-platform-support #7955 / almalinux-8_platform-support-unix](https://gradle-enterprise.elastic.co/s/zc5whlq7l7tng) - [elasticsearch-pull-request #69975 / part-3](https://gradle-enterprise.elastic.co/s/lzlrcpa67twke) - [elasticsearch-intake #22476 / part3](https://gradle-enterprise.elastic.co/s/7rhrxikiad2qs) - [elasticsearch-pull-request #69962 / part-3](https://gradle-enterprise.elastic.co/s/dpql47fxvzvyg) - [elasticsearch-pull-request #69936 / part-3](https://gradle-enterprise.elastic.co/s/ugwsauz2sisaa) - [elasticsearch-intake #22452 / part3](https://gradle-enterprise.elastic.co/s/tpfl5hmajtkz2) **Reproduction Line:** ``` ./gradlew &quot;:x-pack:plugin:esql:qa:server:single-node:javaRestTest&quot; --tests &quot;org.elasticsearch.xpack.esql.qa.single_node.StoredFieldsSequentialIT.testAggTenPercentDefault&quot; -Dtests.seed=BD008EF4955BAD95 -Dtests.configure_test_clusters_with_one_processor=true -Dtests.locale=fa-Arab-IR -Dtests.timezone=Europe/Bratislava -Druntime.java=24 ``` **Applicable branches:** main **Reproduces locally?:** N/A **Failure History:** [See dashboard](https://es-delivery-stats.elastic.dev/app/dashboards#/view/dcec9e60-72ac-11ee-8f39-55975ded9e63?_g=(refreshInterval:(pause:!t,value:60000),time:(from:now-7d%2Fd,to:now))&amp;_a=(controlGroupState:(initialChildControlState:('0c0c9cb8-ccd2-45c6-9b13-96bac4abc542':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:task.keyword,order:0,selectedOptions:!(),title:'GradleTask',type:optionsListControl),'4e6ad9d6-6fdc-4fcc-bf1a-aa6ca79e0850':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:className.keyword,order:1,selectedOptions:!(org.elasticsearch.xpack.esql.qa.single_node.StoredFieldsSequentialIT),title:'Suite',type:optionsListControl),'144933da-5c1b-4257-a969-7f43455a7901':(dataViewId:fbbdc689-be23-4b3d-8057-aa402e9ed0c5,fieldName:name.keyword,order:2,selectedOptions:!(org.elasticsearch.xpack.esql.qa.single_node.StoredFieldsSequentialIT),title:'Test',type:optionsListControl))))) **Failure Message:** ``` org.elasticsearch.client.ResponseException: method [POST], host [http://[::1]:38811], URI [/_query?pretty=true&amp;error_trace=true], status line [HTTP/1.1 400 Bad Request] Warnings: [No limit defined, adding default limit of [1000]] --- error: root_cause: - type: &quot;verification_exception&quot; reason: &quot;Found 1 problem\nline 2:21: Unknown column [test.keyword]&quot; stack_trace: &quot;org.elasticsearch.xpack.esql.VerificationException: Found 1 problem\n\ line 2:21: Unknown column [test.keyword]\n\tat org.elasticsearch.xpack.esql.analysis.Analyzer.verify(Analyzer.java:203)\n\ \tat org.elasticsearch.xpack.esql.analysis.Analyzer.analyze(Analyzer.java:197)\n\ \tat org.elasticsearch.xpack.esql.session.EsqlSession.lambda$analyzedPlan$7(EsqlSession.java:322)\n\ \tat org.elasticsearch.xpack.esql.session.EsqlSession.analyzeAndMaybeRetry(EsqlSession.java:521)\n\ \tat org.elasticsearch.xpack.esql.session.EsqlSession.lambda$analyzedPlan$15(EsqlSession.java:364)\n\ \tat [truncated] ``` **Issue Reasons:** - [main] 7 failures in class org.elasticsearch.xpack.esql.qa.single_node.StoredFieldsSequentialIT (6.7% fail rate in 104 executions) - [main] 3 failures in step part-3 (7.5% fail rate in 40 executions) - [main] 2 failures in step part3 (10.5% fail rate in 19 executions) - [main] 3 failures in pipeline elasticsearch-pull-request (7.5% fail rate in 40 executions) - [main] 2 failures in pipeline elasticsearch-intake (10.5% fail rate in 19 executions) **Note:** This issue was created using new test triage automation. Please report issues or feedback to es-delivery.</DESCRIPTION>
  <REPONAME>elasticsearch</REPONAME>
  <TIMEDIFFERENCEDAYS>0</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>[ML] Use INTERNAL_INGEST for Inference (#127522) Inference now inspects InputType and validates it before sending it to the model. INTERNAL_INGEST has special privileges for model validation that will help pass during ingestion.</MESSAGE>
    <SHA>55fb5f3dafa66e23c3b6283be8d7e31180c79c10</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>ESQL: Fix test locale Was formatting a string and didn't include `Locale.ROOT` so sometimes the string would use the Arabic ٩ instead of 9. And JSON doesn't parse those. Closes #127562</MESSAGE>
      <SHA>de5530dec9924096cee1544f0ac290350f37d0e6</SHA>
      <PATCHEDFILES>
        <FILE>x-pack/plugin/esql/qa/server/single-node/src/javaRestTest/java/org/elasticsearch/xpack/esql/qa/single_node/StoredFieldsSequentialIT.java</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>ESQL: Fix test locale (#127566) Was formatting a string and didn't include `Locale.ROOT` so sometimes the string would use the Arabic ٩ instead of 9. And JSON doesn't parse those. Closes #127562</MESSAGE>
      <SHA>6075c3dff15844421094023290ad18991a4fd9fa</SHA>
      <PATCHEDFILES>
        <FILE>x-pack/plugin/esql/qa/server/single-node/src/javaRestTest/java/org/elasticsearch/xpack/esql/qa/single_node/StoredFieldsSequentialIT.java</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>ESQL: Speed loading stored fields (#127348) This speeds up loading from stored fields by opting more blocks into the &quot;sequential&quot; strategy. This really kicks in when loading stored fields like `text`. And when you need less than 100% of documents, but more than, say, 10%. This is most useful when you need 99.9% of field documents. That sort of thing. Here's the perf numbers: ``` %100.0 {&quot;took&quot;: 403 -&gt; 401,&quot;documents_found&quot;:1000000} %099.9 {&quot;took&quot;:3990 -&gt; 436,&quot;documents_found&quot;: 999000} %099.0 {&quot;took&quot;:4069 -&gt; 440,&quot;documents_found&quot;: 990000} %090.0 {&quot;took&quot;:3468 -&gt; 421,&quot;documents_found&quot;: 900000} %030.0 {&quot;took&quot;:1213 -&gt; 152,&quot;documents_found&quot;: 300000} %020.0 {&quot;took&quot;: 766 -&gt; 104,&quot;documents_found&quot;: 200000} %010.0 {&quot;took&quot;: 397 -&gt; 55,&quot;documents_found&quot;: 100000} %009.0 {&quot;took&quot;: 352 -&gt; 375,&quot;documents_found&quot;: 90000} %008.0 {&quot;took&quot;: 304 -&gt; 317,&quot;documents_found&quot;: 80000} %007.0 {&quot;took&quot;: 273 -&gt; 287,&quot;documents_found&quot;: 70000} %005.0 {&quot;took&quot;: 199 -&gt; 204,&quot;documents_found&quot;: 50000} %001.0 {&quot;took&quot;: 46 -&gt; 46,&quot;documents_found&quot;: 10000} ``` Let's explain this with an example. First, jump to `main` and load a million documents: ``` rm -f /tmp/bulk for a in {1..1000}; do echo '{&quot;index&quot;:{}}' &gt;&gt; /tmp/bulk echo '{&quot;text&quot;:&quot;text '$(printf %04d $a)'&quot;}' &gt;&gt; /tmp/bulk done curl -s -uelastic:password -HContent-Type:application/json -XDELETE localhost:9200/test for a in {1..1000}; do echo -n $a: curl -s -uelastic:password -HContent-Type:application/json -XPOST localhost:9200/test/_bulk?pretty --data-binary @/tmp/bulk | grep errors done curl -s -uelastic:password -HContent-Type:application/json -XPOST localhost:9200/test/_forcemerge?max_num_segments=1 curl -s -uelastic:password -HContent-Type:application/json -XPOST localhost:9200/test/_refresh echo ``` Now query them all. Run this a few times until it's stable: ``` echo -n &quot;%100.0 &quot; curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ &quot;query&quot;: &quot;FROM test | STATS SUM(LENGTH(text))&quot;, &quot;pragma&quot;: { &quot;data_partitioning&quot;: &quot;shard&quot; } }' | jq -c '{took, documents_found}' ``` Now fetch 99.9% of documents: ``` echo -n &quot;%099.9 &quot; curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ &quot;query&quot;: &quot;FROM test | WHERE NOT text.keyword IN (\&quot;text 0998\&quot;) | STATS SUM(LENGTH(text))&quot;, &quot;pragma&quot;: { &quot;data_partitioning&quot;: &quot;shard&quot; } }' | jq -c '{took, documents_found}' ``` This should spit out something like: ``` %100.0 { &quot;took&quot;:403,&quot;documents_found&quot;:1000000} %099.9 {&quot;took&quot;:4098, &quot;documents_found&quot;:999000} ``` We're loading *fewer* documents but it's slower! What in the world?! If you dig into the profile you'll see that it's value loading: ``` $ curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ &quot;query&quot;: &quot;FROM test | STATS SUM(LENGTH(text))&quot;, &quot;pragma&quot;: { &quot;data_partitioning&quot;: &quot;shard&quot; }, &quot;profile&quot;: true }' | jq '.profile.drivers[].operators[] | select(.operator | contains(&quot;ValuesSourceReaderOperator&quot;))' { &quot;operator&quot;: &quot;ValuesSourceReaderOperator[fields = [text]]&quot;, &quot;status&quot;: { &quot;readers_built&quot;: { &quot;stored_fields[requires_source:true, fields:0, sequential: true]&quot;: 222, &quot;text:column_at_a_time:null&quot;: 222, &quot;text:row_stride:BlockSourceReader.Bytes&quot;: 1 }, &quot;values_loaded&quot;: 1000000, &quot;process_nanos&quot;: 370687157, &quot;pages_processed&quot;: 222, &quot;rows_received&quot;: 1000000, &quot;rows_emitted&quot;: 1000000 } } $ curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ &quot;query&quot;: &quot;FROM test | WHERE NOT text.keyword IN (\&quot;text 0998\&quot;) | STATS SUM(LENGTH(text))&quot;, &quot;pragma&quot;: { &quot;data_partitioning&quot;: &quot;shard&quot; }, &quot;profile&quot;: true }' | jq '.profile.drivers[].operators[] | select(.operator | contains(&quot;ValuesSourceReaderOperator&quot;))' { &quot;operator&quot;: &quot;ValuesSourceReaderOperator[fields = [text]]&quot;, &quot;status&quot;: { &quot;readers_built&quot;: { &quot;stored_fields[requires_source:true, fields:0, sequential: false]&quot;: 222, &quot;text:column_at_a_time:null&quot;: 222, &quot;text:row_stride:BlockSourceReader.Bytes&quot;: 1 }, &quot;values_loaded&quot;: 999000, &quot;process_nanos&quot;: 3965803793, &quot;pages_processed&quot;: 222, &quot;rows_received&quot;: 999000, &quot;rows_emitted&quot;: 999000 } } ``` It jumps from 370ms to almost four seconds! Loading fewer values! The second big difference is in the `stored_fields` marker. In the second on it's `sequential: false` and in the first `sequential: true`. `sequential: true` uses Lucene's &quot;merge&quot; stored fields reader instead of the default one. It's much more optimized at decoding sequences of documents. Previously we only enabled this reader when loading compact sequences of documents - when the entire block looks like ``` 1, 2, 3, 4, 5, ... 1230, 1231 ``` If there are any gaps we wouldn't enable it. That was a very conservative thing we did long ago without doing any experiments. We knew it was faster without any gaps, but not otherwise. It turns out it's a lot faster in a lot more cases. I've measured it as faster for 99% gaps, at least on simple documents. I'm a bit worried that this is too aggressive, so I've set made it configurable and made the default being to use the &quot;merge&quot; loader with 10% gaps. So we'd use the merge loader with a block like: ``` 1, 11, 21, 31, ..., 1231, 1241 ``` ESQL: Fix test locale (#127566) Was formatting a string and didn't include `Locale.ROOT` so sometimes the string would use the Arabic ٩ instead of 9. And JSON doesn't parse those. Closes #127562</MESSAGE>
      <SHA>0e8fae41b851106598b46cb8dc4ccae5b189e1e7</SHA>
      <PATCHEDFILES>
        <FILE>benchmarks/src/main/java/org/elasticsearch/benchmark/compute/operator/ValuesSourceReaderBenchmark.java</FILE>
        <FILE>docs/changelog/127348.yaml</FILE>
        <FILE>docs/reference/esql/functions/functionNamedParams/qstr.asciidoc</FILE>
        <FILE>docs/reference/esql/functions/kibana/definition/qstr.json</FILE>
        <FILE>x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/lucene/ValuesSourceReaderOperator.java</FILE>
        <FILE>x-pack/plugin/esql/compute/src/test/java/org/elasticsearch/compute/OperatorTests.java</FILE>
        <FILE>x-pack/plugin/esql/compute/src/test/java/org/elasticsearch/compute/lucene/LuceneQueryEvaluatorTests.java</FILE>
        <FILE>x-pack/plugin/esql/compute/src/test/java/org/elasticsearch/compute/lucene/ValueSourceReaderTypeConversionTests.java</FILE>
        <FILE>x-pack/plugin/esql/compute/src/test/java/org/elasticsearch/compute/lucene/ValuesSourceReaderOperatorTests.java</FILE>
        <FILE>x-pack/plugin/esql/qa/server/single-node/src/javaRestTest/java/org/elasticsearch/xpack/esql/qa/single_node/RestEsqlIT.java</FILE>
        <FILE>x-pack/plugin/esql/qa/server/single-node/src/javaRestTest/java/org/elasticsearch/xpack/esql/qa/single_node/StoredFieldsSequentialIT.java</FILE>
        <FILE>x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/RestEsqlTestCase.java</FILE>
        <FILE>x-pack/plugin/esql/qa/testFixtures/src/main/java/org/elasticsearch/xpack/esql/EsqlTestUtils.java</FILE>
        <FILE>x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/action/LookupFromIndexIT.java</FILE>
        <FILE>x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/enrich/AbstractLookupService.java</FILE>
        <FILE>x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java</FILE>
        <FILE>x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/plugin/ComputeService.java</FILE>
        <FILE>x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/plugin/EsqlPlugin.java</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>ESQL: Speed loading stored fields (#127348) (#127721) This speeds up loading from stored fields by opting more blocks into the &quot;sequential&quot; strategy. This really kicks in when loading stored fields like `text`. And when you need less than 100% of documents, but more than, say, 10%. This is most useful when you need 99.9% of field documents. That sort of thing. Here's the perf numbers: ``` %100.0 {&quot;took&quot;: 403 -&gt; 401,&quot;documents_found&quot;:1000000} %099.9 {&quot;took&quot;:3990 -&gt; 436,&quot;documents_found&quot;: 999000} %099.0 {&quot;took&quot;:4069 -&gt; 440,&quot;documents_found&quot;: 990000} %090.0 {&quot;took&quot;:3468 -&gt; 421,&quot;documents_found&quot;: 900000} %030.0 {&quot;took&quot;:1213 -&gt; 152,&quot;documents_found&quot;: 300000} %020.0 {&quot;took&quot;: 766 -&gt; 104,&quot;documents_found&quot;: 200000} %010.0 {&quot;took&quot;: 397 -&gt; 55,&quot;documents_found&quot;: 100000} %009.0 {&quot;took&quot;: 352 -&gt; 375,&quot;documents_found&quot;: 90000} %008.0 {&quot;took&quot;: 304 -&gt; 317,&quot;documents_found&quot;: 80000} %007.0 {&quot;took&quot;: 273 -&gt; 287,&quot;documents_found&quot;: 70000} %005.0 {&quot;took&quot;: 199 -&gt; 204,&quot;documents_found&quot;: 50000} %001.0 {&quot;took&quot;: 46 -&gt; 46,&quot;documents_found&quot;: 10000} ``` Let's explain this with an example. First, jump to `main` and load a million documents: ``` rm -f /tmp/bulk for a in {1..1000}; do echo '{&quot;index&quot;:{}}' &gt;&gt; /tmp/bulk echo '{&quot;text&quot;:&quot;text '$(printf %04d $a)'&quot;}' &gt;&gt; /tmp/bulk done curl -s -uelastic:password -HContent-Type:application/json -XDELETE localhost:9200/test for a in {1..1000}; do echo -n $a: curl -s -uelastic:password -HContent-Type:application/json -XPOST localhost:9200/test/_bulk?pretty --data-binary @/tmp/bulk | grep errors done curl -s -uelastic:password -HContent-Type:application/json -XPOST localhost:9200/test/_forcemerge?max_num_segments=1 curl -s -uelastic:password -HContent-Type:application/json -XPOST localhost:9200/test/_refresh echo ``` Now query them all. Run this a few times until it's stable: ``` echo -n &quot;%100.0 &quot; curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ &quot;query&quot;: &quot;FROM test | STATS SUM(LENGTH(text))&quot;, &quot;pragma&quot;: { &quot;data_partitioning&quot;: &quot;shard&quot; } }' | jq -c '{took, documents_found}' ``` Now fetch 99.9% of documents: ``` echo -n &quot;%099.9 &quot; curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ &quot;query&quot;: &quot;FROM test | WHERE NOT text.keyword IN (\&quot;text 0998\&quot;) | STATS SUM(LENGTH(text))&quot;, &quot;pragma&quot;: { &quot;data_partitioning&quot;: &quot;shard&quot; } }' | jq -c '{took, documents_found}' ``` This should spit out something like: ``` %100.0 { &quot;took&quot;:403,&quot;documents_found&quot;:1000000} %099.9 {&quot;took&quot;:4098, &quot;documents_found&quot;:999000} ``` We're loading *fewer* documents but it's slower! What in the world?! If you dig into the profile you'll see that it's value loading: ``` $ curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ &quot;query&quot;: &quot;FROM test | STATS SUM(LENGTH(text))&quot;, &quot;pragma&quot;: { &quot;data_partitioning&quot;: &quot;shard&quot; }, &quot;profile&quot;: true }' | jq '.profile.drivers[].operators[] | select(.operator | contains(&quot;ValuesSourceReaderOperator&quot;))' { &quot;operator&quot;: &quot;ValuesSourceReaderOperator[fields = [text]]&quot;, &quot;status&quot;: { &quot;readers_built&quot;: { &quot;stored_fields[requires_source:true, fields:0, sequential: true]&quot;: 222, &quot;text:column_at_a_time:null&quot;: 222, &quot;text:row_stride:BlockSourceReader.Bytes&quot;: 1 }, &quot;values_loaded&quot;: 1000000, &quot;process_nanos&quot;: 370687157, &quot;pages_processed&quot;: 222, &quot;rows_received&quot;: 1000000, &quot;rows_emitted&quot;: 1000000 } } $ curl -s -uelastic:password -HContent-Type:application/json -XPOST 'localhost:9200/_query?pretty' -d'{ &quot;query&quot;: &quot;FROM test | WHERE NOT text.keyword IN (\&quot;text 0998\&quot;) | STATS SUM(LENGTH(text))&quot;, &quot;pragma&quot;: { &quot;data_partitioning&quot;: &quot;shard&quot; }, &quot;profile&quot;: true }' | jq '.profile.drivers[].operators[] | select(.operator | contains(&quot;ValuesSourceReaderOperator&quot;))' { &quot;operator&quot;: &quot;ValuesSourceReaderOperator[fields = [text]]&quot;, &quot;status&quot;: { &quot;readers_built&quot;: { &quot;stored_fields[requires_source:true, fields:0, sequential: false]&quot;: 222, &quot;text:column_at_a_time:null&quot;: 222, &quot;text:row_stride:BlockSourceReader.Bytes&quot;: 1 }, &quot;values_loaded&quot;: 999000, &quot;process_nanos&quot;: 3965803793, &quot;pages_processed&quot;: 222, &quot;rows_received&quot;: 999000, &quot;rows_emitted&quot;: 999000 } } ``` It jumps from 370ms to almost four seconds! Loading fewer values! The second big difference is in the `stored_fields` marker. In the second on it's `sequential: false` and in the first `sequential: true`. `sequential: true` uses Lucene's &quot;merge&quot; stored fields reader instead of the default one. It's much more optimized at decoding sequences of documents. Previously we only enabled this reader when loading compact sequences of documents - when the entire block looks like ``` 1, 2, 3, 4, 5, ... 1230, 1231 ``` If there are any gaps we wouldn't enable it. That was a very conservative thing we did long ago without doing any experiments. We knew it was faster without any gaps, but not otherwise. It turns out it's a lot faster in a lot more cases. I've measured it as faster for 99% gaps, at least on simple documents. I'm a bit worried that this is too aggressive, so I've set made it configurable and made the default being to use the &quot;merge&quot; loader with 10% gaps. So we'd use the merge loader with a block like: ``` 1, 11, 21, 31, ..., 1231, 1241 ``` ESQL: Fix test locale (#127566) Was formatting a string and didn't include `Locale.ROOT` so sometimes the string would use the Arabic ٩ instead of 9. And JSON doesn't parse those. Closes #127562</MESSAGE>
      <SHA>0201ce63f4aa6a9fa954cd1f94ec273cbc5257fd</SHA>
      <PATCHEDFILES>
        <FILE>benchmarks/src/main/java/org/elasticsearch/benchmark/compute/operator/ValuesSourceReaderBenchmark.java</FILE>
        <FILE>docs/changelog/127348.yaml</FILE>
        <FILE>docs/reference/esql/functions/functionNamedParams/qstr.asciidoc</FILE>
        <FILE>docs/reference/esql/functions/kibana/definition/qstr.json</FILE>
        <FILE>x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/lucene/ValuesSourceReaderOperator.java</FILE>
        <FILE>x-pack/plugin/esql/compute/src/test/java/org/elasticsearch/compute/OperatorTests.java</FILE>
        <FILE>x-pack/plugin/esql/compute/src/test/java/org/elasticsearch/compute/lucene/LuceneQueryEvaluatorTests.java</FILE>
        <FILE>x-pack/plugin/esql/compute/src/test/java/org/elasticsearch/compute/lucene/ValueSourceReaderTypeConversionTests.java</FILE>
        <FILE>x-pack/plugin/esql/compute/src/test/java/org/elasticsearch/compute/lucene/ValuesSourceReaderOperatorTests.java</FILE>
        <FILE>x-pack/plugin/esql/qa/server/single-node/src/javaRestTest/java/org/elasticsearch/xpack/esql/qa/single_node/RestEsqlIT.java</FILE>
        <FILE>x-pack/plugin/esql/qa/server/single-node/src/javaRestTest/java/org/elasticsearch/xpack/esql/qa/single_node/StoredFieldsSequentialIT.java</FILE>
        <FILE>x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/RestEsqlTestCase.java</FILE>
        <FILE>x-pack/plugin/esql/qa/testFixtures/src/main/java/org/elasticsearch/xpack/esql/EsqlTestUtils.java</FILE>
        <FILE>x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/action/LookupFromIndexIT.java</FILE>
        <FILE>x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/enrich/AbstractLookupService.java</FILE>
        <FILE>x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/planner/EsPhysicalOperationProviders.java</FILE>
        <FILE>x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/plugin/ComputeService.java</FILE>
        <FILE>x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/plugin/EsqlPlugin.java</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
