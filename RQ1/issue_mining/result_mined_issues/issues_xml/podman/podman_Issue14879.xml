<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>14879</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/14879</ISSUEURL>
  <TITLE>Periodical log API call cause high CPU/memory usage</TITLE>
  <DESCRIPTION>&lt;!-- --------------------------------------------------- BUG REPORT INFORMATION --------------------------------------------------- Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --&gt; **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** The periodical, every 3 seconds for example, API call of getting log will cause the high CPU/memory usage in podman API service. **Steps to reproduce the issue:** 1. Install podman Note: I tried Podman 3.0.2, 3.3.1 and 4.0.2, but the result does not change. 2. Run a &quot;do-nothing&quot; container 3. Call log acquition API every 3 seconds like below. ``` while : do curl -m 3 --output - --unix-socket /var/run/podman/podman.sock -v http://localhost/containers/&lt;Container ID&gt;/logs?follow=1&amp;since=0&amp;stderr=1&amp;stdout=1&amp;tail=all &gt; /dev/null done ``` **Describe the results you received:** Podman CPU/Memory usage became high after several hours later and lead resource shortage and OOM on the Podman's host. | # | Podman version | CPU usage(%) | Memory usage(%) | Used Memory(KiB) | Duration | |:---|:---:|:---:|-----:|-----:|-----:| | 1 | 3.0.1 | 2.6 -&gt; 75.4 | 1.3 -&gt; 36.8 | 55,288 -&gt; 1,483,168 | 6h | | 2 | 3.3.2 | 2 -&gt; 15.4 | 1.4 -&gt; 28.8 | 59,848 -&gt; 1,160,136 | 6h | | 3 | 4.0.1 | 0.3 -&gt; 27.6 | 0.3 -&gt; 19.7 | 62,112 -&gt; 3,176,820 | 17h | **Describe the results you expected:** Podman API service runs in a limited resource usage. **Additional information you deem important (e.g. issue happens only occasionally):** None. **Output of `podman version`:** ``` [root@localhost ~]# podman version Client: Podman Engine Version: 4.0.2 API Version: 4.0.2 Go Version: go1.17.7 Built: Tue Apr 19 19:16:32 2022 OS/Arch: linux/amd64 ``` **Output of `podman info --debug`:** ``` [root@localhost ~]# podman info --debug host: arch: amd64 buildahVersion: 1.24.1 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma cgroupManager: systemd cgroupVersion: v1 conmon: package: conmon-2.1.0-1.module+el8.6.0+14877+f643d2d6.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: 87b7a9037cbd1d81288bdf2d6705dfda889f7cf9' cpus: 4 distribution: distribution: '&quot;rhel&quot;' version: &quot;8.4&quot; eventLogger: file hostname: localhost.localdomain idMappings: gidmap: null uidmap: null kernel: 4.18.0-305.el8.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 12447719424 memTotal: 16512086016 networkBackend: cni ociRuntime: name: runc package: runc-1.0.3-2.module+el8.6.0+14877+f643d2d6.x86_64 path: /usr/bin/runc version: |- runc version 1.0.3 spec: 1.0.2-dev go: go1.17.7 libseccomp: 2.5.2 os: linux remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.1.8-2.module+el8.6.0+14877+f643d2d6.x86_64 version: |- slirp4netns version 1.1.8 commit: d361001f495417b880f20329121e3aa431a8f90f libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 8472489984 swapTotal: 8472489984 uptime: 17h 1m 4.81s (Approximately 0.71 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: rdocker:6000: Blocked: false Insecure: true Location: rdocker:6000 MirrorByDigestOnly: false Mirrors: null Prefix: rdocker:6000 search: - [registry.fedoraproject.org](http://registry.fedoraproject.org/) - [registry.access.redhat.com](http://registry.access.redhat.com/) - [registry.centos.org](http://registry.centos.org/) - [docker.io](http://docker.io/) store: configFile: /etc/containers/storage.conf containerStore: number: 1 paused: 0 running: 1 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphStatus: Backing Filesystem: xfs Native Overlay Diff: &quot;false&quot; Supports d_type: &quot;true&quot; Using metacopy: &quot;true&quot; imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.0.2 Built: 1650363392 BuiltTime: Tue Apr 19 19:16:32 2022 GitCommit: &quot;&quot; GoVersion: go1.17.7 OsArch: linux/amd64 Version: 4.0.2 ``` **Package info (e.g. output of `rpm -q podman` or `apt list podman`):** ``` [root@localhost ~]# rpm -q podman podman-4.0.2-6.module+el8.6.0+14877+f643d2d6.x86_64 ``` **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** None.</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>12</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #14932 from jakecorrenti/pull-all-tags-shorthand Podman pull --all-tags shorthand option</MESSAGE>
    <SHA>98b22e29c9a769020de1ae8d2bf9a7630e930e78</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>fix goroutine leaks in events and logs backend When running a single podman logs this is not really important since we will exit when we finish reading the logs. However for the system service this is very important. Leaking goroutines will cause an increased memory and CPU ussage over time. Both the the event and log backend have goroutine leaks with both the file and journald drivers. The journald backend has the problem that journal.Wait(IndefiniteWait) will block until we get a new journald event. So when a client closes the connection the goroutine would still wait until there is a new journal entry. To fix this we just wait for a maximum of 5 seconds, after that we can check if the client connection was closed and exit correctly in this case. For the file backend we can fix this by waiting for either the log line or context cancel at the same time. Currently it would block waiting for new log lines and only check afterwards if the client closed the connection and thus hang forever if there are no new log lines. [NO NEW TESTS NEEDED] I am open to ideas how we can test memory leaks in CI. To test manually run a container like this: `podman run --log-driver $driver --name test -d alpine sh -c 'i=1; while [ &quot;$i&quot; -ne 1000 ]; do echo &quot;line $i&quot;; i=$((i + 1)); done; sleep inf'` where `$driver` can be either `journald` or `k8s-file`. Then start the podman system service and use: `curl -m 1 --output - --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock -v 'http://d/containers/test/logs?follow=1&amp;since=0&amp;stderr=1&amp;stdout=1' &amp;&gt;/dev/null` to get the logs from the API and then it closes the connection after 1 second. Now run the curl command several times and check the memory usage of the service. Fixes #14879 Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>4e72aa58604f9384c3d2913b16baa41f00d0a9e1</SHA>
      <PATCHEDFILES>
        <FILE>libpod/container_log.go</FILE>
        <FILE>libpod/container_log_linux.go</FILE>
        <FILE>libpod/events/journal_linux.go</FILE>
        <FILE>libpod/events/logfile.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>fix goroutine leaks in events and logs backend When running a single podman logs this is not really important since we will exit when we finish reading the logs. However for the system service this is very important. Leaking goroutines will cause an increased memory and CPU ussage over time. Both the the event and log backend have goroutine leaks with both the file and journald drivers. The journald backend has the problem that journal.Wait(IndefiniteWait) will block until we get a new journald event. So when a client closes the connection the goroutine would still wait until there is a new journal entry. To fix this we just wait for a maximum of 5 seconds, after that we can check if the client connection was closed and exit correctly in this case. For the file backend we can fix this by waiting for either the log line or context cancel at the same time. Currently it would block waiting for new log lines and only check afterwards if the client closed the connection and thus hang forever if there are no new log lines. [NO NEW TESTS NEEDED] I am open to ideas how we can test memory leaks in CI. To test manually run a container like this: `podman run --log-driver $driver --name test -d alpine sh -c 'i=1; while [ &quot;$i&quot; -ne 1000 ]; do echo &quot;line $i&quot;; i=$((i + 1)); done; sleep inf'` where `$driver` can be either `journald` or `k8s-file`. Then start the podman system service and use: `curl -m 1 --output - --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock -v 'http://d/containers/test/logs?follow=1&amp;since=0&amp;stderr=1&amp;stdout=1' &amp;&gt;/dev/null` to get the logs from the API and then it closes the connection after 1 second. Now run the curl command several times and check the memory usage of the service. Fixes #14879 Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>f6d18ed41cb4ce0fd59090d2114f728d557638db</SHA>
      <PATCHEDFILES>
        <FILE>libpod/container_log.go</FILE>
        <FILE>libpod/container_log_linux.go</FILE>
        <FILE>libpod/events/journal_linux.go</FILE>
        <FILE>libpod/events/logfile.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
