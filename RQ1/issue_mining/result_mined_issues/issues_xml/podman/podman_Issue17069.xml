<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>17069</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/17069</ISSUEURL>
  <TITLE>[Bug]: podman stop results in panic: runtime error: invalid memory address or nil pointer dereference</TITLE>
  <DESCRIPTION>### Issue Description Trying to use traefik with podman (more specifically podman-compose) not sure what specifically is the issue or if it is even related to traefik. &lt;details&gt; &lt;summary&gt;Here is what the compose file looks like spoiler&lt;/summary&gt; ```yaml version: &quot;3.3&quot; services: # IMPORTANT # Run commands with keep-id to make volume permissions correct and all truly rootless # podman-compose --podman-run-args=&quot;--userns=keep-id&quot; [...] # # Forward traffic to right port with # iptables -A PREROUTING -t nat -p tcp --dport 80 -j REDIRECT --to-port 1024 # iptables -A OUTPUT -t nat -p tcp --dport 80 -j REDIRECT --to-port 1024 ########################################################################### # PROXY ########################################################################### traefik: user: &quot;1000:1001&quot; image: &quot;docker.io/library/traefik&quot; labels: - &quot;io.containers.autoupdate=registry&quot; restart: always command: #- &quot;--log.level=DEBUG&quot; - &quot;--api.insecure=true&quot; - &quot;--providers.docker=true&quot; - &quot;--providers.docker.exposedbydefault=false&quot; - &quot;--entrypoints.web.address=:1024&quot; # HTTP - &quot;--entrypoints.ssh.address=:1025&quot; # GIT SSH ports: - &quot;1024:1024&quot; - &quot;1025:1025&quot; - &quot;1026:8080&quot; volumes: - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro - &quot;/run/user/1000/podman/podman.sock:/var/run/docker.sock:ro&quot; # NOTE # Sometimes when shutting down the rootlessport process will hang for some reason # sudo lsof -i -P -n | grep $port # sudo kill $process_number # whoami: # user: &quot;1000:1001&quot; # image: &quot;docker.io/traefik/whoami&quot; # labels: # - &quot;io.containers.autoupdate=registry&quot; # - &quot;traefik.enable=true&quot; # - &quot;traefik.http.routers.whoami.rule=Host(`whoami.localhost`)&quot; # - &quot;traefik.http.routers.whoami.entrypoints=web&quot; # - &quot;traefik.http.services.whoami-juke.loadbalancer.server.port=1024&quot; # command: # - &quot;--port=1024&quot; # restart: always ########################################################################### # NEXTCLOUD ########################################################################### # user # password # database # cloud.localhost nextcloud_database: user: &quot;1000:1001&quot; image: &quot;docker.io/library/postgres:alpine&quot; labels: - &quot;io.containers.autoupdate=registry&quot; restart: always volumes: - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro - ./resources/postgres_alpine_passwd:/etc/passwd:ro - ./volumes/nextcloud_database:/var/lib/postgresql/data:Z environment: - POSTGRES_DB=database - POSTGRES_USER=user - POSTGRES_PASSWORD=password nextcloud_server: user: &quot;1000:1001&quot; depends_on: - traefik - nextcloud_database image: &quot;docker.io/library/nextcloud&quot; labels: - &quot;io.containers.autoupdate=registry&quot; - &quot;traefik.enable=true&quot; - &quot;traefik.http.routers.nextcloud_server.rule=Host(`cloud.localhost`)&quot; - &quot;traefik.http.routers.nextcloud_server.entrypoints=web&quot; - &quot;traefik.http.services.nextcloud_server-juke.loadbalancer.server.port=1024&quot; restart: always volumes: - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro - ./resources/nextcloud_server_passwd:/etc/passwd:ro - ./resources/nextcloud_server_ports.conf:/etc/apache2/ports.conf:ro - ./volumes/nextcloud_server:/var/www/html:Z hostname: cloud.localhost environment: - POSTGRES_PASSWORD=password - POSTGRES_DB=database - POSTGRES_USER=user - POSTGRES_HOST=nextcloud_database - NEXTCLOUD_TRUSTED_DOMAINS=cloud.localhost [...] ``` &lt;/details&gt; Everything seems to work fine when I ```shell podman-compose --podman-run-args=&quot;--userns=keep-id&quot; up -d ``` However when I ```shell podman-compose --podman-run-args=&quot;--userns=keep-id&quot; down -v ``` I get the following error ```shell ['podman', '--version', ''] using podman version: 4.3.1 ** excluding: set() podman stop -t 10 juke_uptime_kuma_server_1 juke_uptime_kuma_server_1 exit code: 0 podman stop -t 10 juke_gitea_server_1 juke_gitea_server_1 exit code: 0 podman stop -t 10 juke_nextcloud_server_1 juke_nextcloud_server_1 exit code: 0 podman stop -t 10 juke_element_server_1 juke_element_server_1 exit code: 0 podman stop -t 10 juke_gitea_database_1 juke_gitea_database_1 exit code: 0 podman stop -t 10 juke_nextcloud_database_1 juke_nextcloud_database_1 exit code: 0 podman stop -t 10 juke_traefik_1 panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x5564a2c4495b] goroutine 50 [running]: os.(*File).Name(...) os/file.go:56 github.com/containers/podman/v4/pkg/errorhandling.CloseQuiet(0xc0002bc9a0?) github.com/containers/podman/v4/pkg/errorhandling/errorhandling.go:74 +0x5b github.com/containers/podman/v4/libpod.(*Runtime).setupRootlessPortMappingViaRLK(0xc00027e380, 0xc0003d3b00, {0xc00060f700, 0x3f}, 0xc000604f01?) github.com/containers/podman/v4/libpod/networking_slirp4netns.go:581 +0x105d github.com/containers/podman/v4/libpod.(*Container).setupRootlessNetwork(0xc0003d3b00) github.com/containers/podman/v4/libpod/container_internal_linux.go:414 +0x13c github.com/containers/podman/v4/libpod.(*Container).handleRestartPolicy(0xc0003d3b00, {0x5564a3cc7510, 0xc0000460d0}) github.com/containers/podman/v4/libpod/container_internal.go:296 +0x445 github.com/containers/podman/v4/libpod.(*Container).Cleanup(0xc0003d3b00, {0x5564a3cc7510, 0xc0000460d0}) github.com/containers/podman/v4/libpod/container_api.go:726 +0x3dd github.com/containers/podman/v4/pkg/domain/infra/abi.(*ContainerEngine).ContainerStop.func1(0xc0003d3b00) github.com/containers/podman/v4/pkg/domain/infra/abi/containers.go:248 +0x24e github.com/containers/podman/v4/pkg/parallel/ctr.ContainerOp.func1() github.com/containers/podman/v4/pkg/parallel/ctr/ctr.go:28 +0x22 github.com/containers/podman/v4/pkg/parallel.Enqueue.func1() github.com/containers/podman/v4/pkg/parallel/parallel.go:67 +0x1ac created by github.com/containers/podman/v4/pkg/parallel.Enqueue github.com/containers/podman/v4/pkg/parallel/parallel.go:56 +0xbe exit code: 2 podman rm juke_uptime_kuma_server_1 juke_uptime_kuma_server_1 exit code: 0 podman rm juke_gitea_server_1 juke_gitea_server_1 exit code: 0 podman rm juke_nextcloud_server_1 juke_nextcloud_server_1 exit code: 0 podman rm juke_element_server_1 juke_element_server_1 exit code: 0 podman rm juke_gitea_database_1 juke_gitea_database_1 exit code: 0 podman rm juke_nextcloud_database_1 juke_nextcloud_database_1 exit code: 0 podman rm juke_traefik_1 ERRO[0000] Unable to clean up network for container 56582b9b79c1eaf581d92fde3ae67d1e02e2ab8b894cd83c708f6c0820ad4bfc: &quot;unmounting network namespace for container 56582b9b79c1eaf581d92fde3ae67d1e02e2ab8b894cd83c708f6c0820ad4bfc: failed to remove ns path /run/user/1000/netns/netns-8528016b-18be-bf00-27e5-7c4f38f8fe90: remove /run/user/1000/netns/netns-8528016b-18be-bf00-27e5-7c4f38f8fe90: device or resource busy&quot; juke_traefik_1 exit code: 0 ['podman', 'volume', 'inspect', '--all'] ``` Sometimes it will look like this however: ```shell [user@demovm juke]$ podman-compose --podman-run-args=&quot;--userns=keep-id&quot; down -v ['podman', '--version', ''] using podman version: 4.3.1 ** excluding: set() podman stop -t 10 juke_uptime_kuma_server_1 juke_uptime_kuma_server_1 exit code: 0 podman stop -t 10 juke_gitea_server_1 juke_gitea_server_1 exit code: 0 podman stop -t 10 juke_nextcloud_server_1 juke_nextcloud_server_1 exit code: 0 podman stop -t 10 juke_gitea_database_1 juke_gitea_database_1 exit code: 0 podman stop -t 10 juke_nextcloud_database_1 juke_nextcloud_database_1 exit code: 0 podman stop -t 10 juke_traefik_1 ERRO[0001] Unable to clean up network for container 9520a833f7e2230bfa4eafe785d0320efb80f593bdeaf8e62ab78424dbd7e8b3: &quot;unmounting network namespace for container 9520a833f7e2230bfa4eafe785d0320efb80f593bdeaf8e62ab78424dbd7e8b3: failed to remove ns path /run/user/1000/netns/netns-ac16b2fe-694e-4255-bbc6-5810f553715c: remove /run/user/1000/netns/netns-ac16b2fe-694e-4255-bbc6-5810f553715c: device or resource busy&quot; juke_traefik_1 exit code: 0 podman rm juke_uptime_kuma_server_1 juke_uptime_kuma_server_1 exit code: 0 podman rm juke_gitea_server_1 juke_gitea_server_1 exit code: 0 podman rm juke_nextcloud_server_1 juke_nextcloud_server_1 exit code: 0 podman rm juke_gitea_database_1 juke_gitea_database_1 exit code: 0 podman rm juke_nextcloud_database_1 juke_nextcloud_database_1 exit code: 0 podman rm juke_traefik_1 juke_traefik_1 exit code: 0 ['podman', 'volume', 'inspect', '--all'] ``` At this point if I lsof I see a process that I can kill ```shell ~ sudo lsof -i -P -n | grep 1024 rootlessp 36533 user 10u IPv6 137699 0t0 TCP *:1024 (LISTEN) ~ sudo kill 36533 ``` But doing so still apparently leaves the system thinking that IP addresses are allocated when they shouldnt be because trying to spin up the services again with ```shell podman-compose --podman-run-args=&quot;--userns=keep-id&quot; up -d ``` results in the following ```shell ['podman', '--version', ''] using podman version: 4.3.1 ** excluding: set() ['podman', 'network', 'exists', 'juke_default'] podman run --userns=keep-id --name=juke_traefik_1 -d --label io.containers.autoupdate=registry --label io.podman.compose.config-hash=123 --label io.podman.compose.project=juke --label io.podman.compose.version=0.0.1 --label com.docker.compose.project=juke --label com.docker.compose.project.working_dir=/home/user/Documents/juke --label com.docker.compose.project.config_files=docker-compose.yml --label com.docker.compose.container-number=1 --label com.docker.compose.service=traefik -v /etc/timezone:/etc/timezone:ro -v /usr/share/zoneinfo/Europe/Paris:/etc/localtime:ro -v /run/user/1000/podman/podman.sock:/var/run/docker.sock:ro --net juke_default --network-alias traefik -p 1024:1024 -p 1025:1025 -p 1026:8080 -u 1000:1001 --restart always docker.io/library/traefik --api.insecure=true --providers.docker=true --providers.docker.exposedbydefault=false --entrypoints.web.address=:1024 --entrypoints.ssh.address=:1025 9e257843121f1294bfbeec12548cf198c93f8cb032f491d1aec79d7843117b7d exit code: 0 ['podman', 'network', 'exists', 'juke_default'] podman run --userns=keep-id --name=juke_nextcloud_database_1 -d --label io.containers.autoupdate=registry --label io.podman.compose.config-hash=123 --label io.podman.compose.project=juke --label io.podman.compose.version=0.0.1 --label com.docker.compose.project=juke --label com.docker.compose.project.working_dir=/home/user/Documents/juke --label com.docker.compose.project.config_files=docker-compose.yml --label com.docker.compose.container-number=1 --label com.docker.compose.service=nextcloud_database -e POSTGRES_DB=database -e POSTGRES_USER=user -e POSTGRES_PASSWORD=password -v /etc/timezone:/etc/timezone:ro -v /usr/share/zoneinfo/Europe/Paris:/etc/localtime:ro -v /home/user/Documents/juke/resources/postgres_alpine_passwd:/etc/passwd:ro -v /home/user/Documents/juke/volumes/nextcloud_database:/var/lib/postgresql/data:Z --net juke_default --network-alias nextcloud_database -u 1000:1001 --restart always docker.io/library/postgres:alpine f0b6ec0f62e2d492a404ffc90f5ce0ff5a31aaa7fb2e139e79caf4399f177e0d exit code: 0 ['podman', 'network', 'exists', 'juke_default'] podman run --userns=keep-id --name=juke_gitea_database_1 -d --label io.containers.autoupdate=registry --label io.podman.compose.config-hash=123 --label io.podman.compose.project=juke --label io.podman.compose.version=0.0.1 --label com.docker.compose.project=juke --label com.docker.compose.project.working_dir=/home/user/Documents/juke --label com.docker.compose.project.config_files=docker-compose.yml --label com.docker.compose.container-number=1 --label com.docker.compose.service=gitea_database -e POSTGRES_DB=database -e POSTGRES_USER=user -e POSTGRES_PASSWORD=password -v /etc/timezone:/etc/timezone:ro -v /usr/share/zoneinfo/Europe/Paris:/etc/localtime:ro -v /home/user/Documents/juke/resources/postgres_alpine_passwd:/etc/passwd:ro -v /home/user/Documents/juke/volumes/gitea_database:/var/lib/postgresql/data:Z --net juke_default --network-alias gitea_database -u 1000:1001 --restart always docker.io/library/postgres:alpine 72354b4dcf75071a511f109e98ea954954e83622a9881a14a6aa29224caf0810 exit code: 0 ['podman', 'network', 'exists', 'juke_default'] podman run --userns=keep-id --name=juke_element_server_1 -d --label io.containers.autoupdate=registry --label traefik.enable=true --label traefik.http.routers.element_server.rule=Host(`chat.localhost`) --label traefik.http.routers.element_server.entrypoints=web --label traefik.http.services.element_server-juke.loadbalancer.server.port=1024 --label io.podman.compose.config-hash=123 --label io.podman.compose.project=juke --label io.podman.compose.version=0.0.1 --label com.docker.compose.project=juke --label com.docker.compose.project.working_dir=/home/user/Documents/juke --label com.docker.compose.project.config_files=docker-compose.yml --label com.docker.compose.container-number=1 --label com.docker.compose.service=element_server -v /etc/timezone:/etc/timezone:ro -v /usr/share/zoneinfo/Europe/Paris:/etc/localtime:ro -v /home/user/Documents/juke/resources/element_server_passwd:/etc/passwd:ro -v /home/user/Documents/juke/resources/element_server_nginx.conf:/etc/nginx/conf.d/default.conf:ro -v /home/user/Documents/juke/resources/element_server_config.json:/app/config.json:ro --net juke_default --network-alias element_server -u 1000:1001 --restart always docker.io/vectorim/element-web 7e074a3a9ae23003fdfa6642a48e05507585f6b1fcf45f3e92c91e543ced7e45 exit code: 0 ['podman', 'network', 'exists', 'juke_default'] podman run --userns=keep-id --name=juke_nextcloud_server_1 -d --label io.containers.autoupdate=registry --label traefik.enable=true --label traefik.http.routers.nextcloud_server.rule=Host(`cloud.localhost`) --label traefik.http.routers.nextcloud_server.entrypoints=web --label traefik.http.services.nextcloud_server-juke.loadbalancer.server.port=1024 --label io.podman.compose.config-hash=123 --label io.podman.compose.project=juke --label io.podman.compose.version=0.0.1 --label com.docker.compose.project=juke --label com.docker.compose.project.working_dir=/home/user/Documents/juke --label com.docker.compose.project.config_files=docker-compose.yml --label com.docker.compose.container-number=1 --label com.docker.compose.service=nextcloud_server -e POSTGRES_PASSWORD=password -e POSTGRES_DB=database -e POSTGRES_USER=user -e POSTGRES_HOST=nextcloud_database -e NEXTCLOUD_TRUSTED_DOMAINS=cloud.localhost -v /etc/timezone:/etc/timezone:ro -v /usr/share/zoneinfo/Europe/Paris:/etc/localtime:ro -v /home/user/Documents/juke/resources/nextcloud_server_passwd:/etc/passwd:ro -v /home/user/Documents/juke/resources/nextcloud_server_ports.conf:/etc/apache2/ports.conf:ro -v /home/user/Documents/juke/volumes/nextcloud_server:/var/www/html:Z --net juke_default --network-alias nextcloud_server -u 1000:1001 --hostname cloud.localhost --restart always docker.io/library/nextcloud 687ad8936a6aa6427bcdccd1d02c05acb2a0733229b564ebadcbe1cb0ee0bc5f exit code: 0 ['podman', 'network', 'exists', 'juke_default'] podman run --userns=keep-id --name=juke_gitea_server_1 -d --label io.containers.autoupdate=registry --label traefik.enable=true --label traefik.http.routers.gitea_server.rule=Host(`code.localhost`) --label traefik.http.routers.gitea_server.entrypoints=web --label traefik.http.services.gitea_server-juke.loadbalancer.server.port=1024 --label traefik.tcp.routers.gitea_server_ssh.rule=HostSNI(`*`) --label traefik.tcp.routers.gitea_server_ssh.entrypoints=ssh --label traefik.tcp.services.girea_server_ssh-juke.loadbalancer.server.port=1025 --label io.podman.compose.config-hash=123 --label io.podman.compose.project=juke --label io.podman.compose.version=0.0.1 --label com.docker.compose.project=juke --label com.docker.compose.project.working_dir=/home/user/Documents/juke --label com.docker.compose.project.config_files=docker-compose.yml --label com.docker.compose.container-number=1 --label com.docker.compose.service=gitea_server -e HTTP_PORT=1024 -e DEFAULT_BRANCH=main -e RUN_MODE=prod -e DISABLE_SSH=false -e START_SSH_SERVER=true -e SSH_PORT=1025 -e SSH_LISTEN_PORT=1025 -e ROOT_URL=http://code.localhost -e GITEA__database__DB_TYPE=postgres -e GITEA__database__HOST=gitea_database:5432 -e GITEA__database__NAME=database -e GITEA__database__USER=user -e GITEA__database__PASSWD=password -e GITEA__service__DISABLE_REGISTRATION=true -v /etc/timezone:/etc/timezone:ro -v /usr/share/zoneinfo/Europe/Paris:/etc/localtime:ro -v /home/user/Documents/juke/resources/gitea_server_passwd:/etc/passwd:ro -v /home/user/Documents/juke/volumes/gitea_server:/data:Z --net juke_default --network-alias gitea_server -u 1000:1001 --restart always docker.io/gitea/gitea:latest-rootless ERRO[0000] IPAM error: failed to get ips for container ID 49493f4c0bc56fe1bc48ab5cdc113ca5d56a5fb369835d144e0a413b041e5024 on network juke_default [ERROR netavark::network::bridge] failed to parse ipam options: no static ips provided ERRO[0000] IPAM error: failed to find ip for subnet 10.89.0.0/24 on network juke_default ERRO[0000] Unable to clean up network for container 49493f4c0bc56fe1bc48ab5cdc113ca5d56a5fb369835d144e0a413b041e5024: &quot;tearing down network namespace configuration for container 49493f4c0bc56fe1bc48ab5cdc113ca5d56a5fb369835d144e0a413b041e5024: netavark: failed to delete container veth eth0: Netlink error: No such device (os error 19)&quot; Error: IPAM error: failed to find free IP in range: 10.89.0.1 - 10.89.0.254 exit code: 126 podman start juke_gitea_server_1 ERRO[0000] IPAM error: failed to get ips for container ID 49493f4c0bc56fe1bc48ab5cdc113ca5d56a5fb369835d144e0a413b041e5024 on network juke_default [ERROR netavark::network::bridge] failed to parse ipam options: no static ips provided ERRO[0000] IPAM error: failed to find ip for subnet 10.89.0.0/24 on network juke_default ERRO[0000] Unable to clean up network for container 49493f4c0bc56fe1bc48ab5cdc113ca5d56a5fb369835d144e0a413b041e5024: &quot;tearing down network namespace configuration for container 49493f4c0bc56fe1bc48ab5cdc113ca5d56a5fb369835d144e0a413b041e5024: netavark: failed to delete container veth eth0: Netlink error: No such device (os error 19)&quot; Error: unable to start container &quot;49493f4c0bc56fe1bc48ab5cdc113ca5d56a5fb369835d144e0a413b041e5024&quot;: IPAM error: failed to find free IP in range: 10.89.0.1 - 10.89.0.254 exit code: 125 ['podman', 'network', 'exists', 'juke_default'] podman run --userns=keep-id --name=juke_uptime_kuma_server_1 -d --label io.containers.autoupdate=registry --label traefik.enable=true --label traefik.http.routers.uptime_kuma_server.rule=Host(`status.localhost`) --label traefik.http.routers.uptime_kuma_server.entrypoints=web --label traefik.http.services.uptime_kuma_server-juke.loadbalancer.server.port=1024 --label io.podman.compose.config-hash=123 --label io.podman.compose.project=juke --label io.podman.compose.version=0.0.1 --label com.docker.compose.project=juke --label com.docker.compose.project.working_dir=/home/user/Documents/juke --label com.docker.compose.project.config_files=docker-compose.yml --label com.docker.compose.container-number=1 --label com.docker.compose.service=uptime_kuma_server -e PUID=1000 -e PGID=1001 -e PORT=1024 -v /etc/timezone:/etc/timezone:ro -v /usr/share/zoneinfo/Europe/Paris:/etc/localtime:ro -v /home/user/Documents/juke/resources/uptime_kuma_server_passwd:/etc/passwd:ro -v /home/user/Documents/juke/volumes/uptime_kuma_server:/app/data:Z --net juke_default --network-alias uptime_kuma_server -u 1000:1001 --restart always --entrypoint [&quot;node&quot;, &quot;/app/server/server.js&quot;] docker.io/louislam/uptime-kuma ERRO[0000] IPAM error: failed to get ips for container ID 1c7c25f5e7c6816dc644b93402010e5c85bbed88d38e74b97f5f347e125a2ab6 on network juke_default [ERROR netavark::network::bridge] failed to parse ipam options: no static ips provided ERRO[0000] IPAM error: failed to find ip for subnet 10.89.0.0/24 on network juke_default ERRO[0000] Unable to clean up network for container 1c7c25f5e7c6816dc644b93402010e5c85bbed88d38e74b97f5f347e125a2ab6: &quot;tearing down network namespace configuration for container 1c7c25f5e7c6816dc644b93402010e5c85bbed88d38e74b97f5f347e125a2ab6: netavark: failed to delete container veth eth0: Netlink error: No such device (os error 19)&quot; Error: IPAM error: failed to find free IP in range: 10.89.0.1 - 10.89.0.254 exit code: 126 podman start juke_uptime_kuma_server_1 ERRO[0000] IPAM error: failed to get ips for container ID 1c7c25f5e7c6816dc644b93402010e5c85bbed88d38e74b97f5f347e125a2ab6 on network juke_default [ERROR netavark::network::bridge] failed to parse ipam options: no static ips provided ERRO[0000] IPAM error: failed to find ip for subnet 10.89.0.0/24 on network juke_default ERRO[0000] Unable to clean up network for container 1c7c25f5e7c6816dc644b93402010e5c85bbed88d38e74b97f5f347e125a2ab6: &quot;tearing down network namespace configuration for container 1c7c25f5e7c6816dc644b93402010e5c85bbed88d38e74b97f5f347e125a2ab6: netavark: failed to delete container veth eth0: Netlink error: No such device (os error 19)&quot; Error: unable to start container &quot;1c7c25f5e7c6816dc644b93402010e5c85bbed88d38e74b97f5f347e125a2ab6&quot;: IPAM error: failed to find free IP in range: 10.89.0.1 - 10.89.0.254 exit code: 125 ``` Saying that its failing to find any free IPs ### Steps to reproduce the issue Steps to reproduce the issue ```shell 0 clean arch install inside of a VM (endeavouros in this specific case) 1 yay -S podman podman-compose cni-plugins aardvark-dns wget 3 systemctl --user enable --now podman 4 wget &quot;https://github.com/containers/podman/files/10389570/files.zip&quot; 5 unzip files.zip 6 mv files juke 7 cd juke 9 ./create_dirs.sh 10 podman-compose --podman-run-args=&quot;--userns=keep-id&quot; up ``` press CTRL-C, error happens alteratively, start the services in daemon mode `up -d` and destroy them and their volumes in another step `down -v`, same error happens ### Describe the results you received Stopping is not clean and not leaving hung processes and IP addresses stay unavailable. The only way I found to fix it properly is to reboot the entire host. ### Describe the results you expected Not having hung processes that make it impossible to restart the pods because no more IPs are available and needing to reboot to get it to work. ### podman info output &lt;details&gt; &lt;summary&gt;podman information output log spoiler&lt;/summary&gt; ```shell ~ podman version Client: Podman Engine Version: 4.3.1 API Version: 4.3.1 Go Version: go1.19.3 Git Commit: 814b7b003cc630bf6ab188274706c383f9fb9915-dirty Built: Sun Nov 20 23:32:45 2022 OS/Arch: linux/amd64 ~ podman info host: arch: amd64 buildahVersion: 1.28.0 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: /usr/bin/conmon is owned by conmon 1:2.1.5-1 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: c9f7f19eb82d5b8151fc3ba7fbbccf03fdcd0325' cpuUtilization: idlePercent: 90.28 systemPercent: 1.51 userPercent: 8.2 cpus: 8 distribution: distribution: endeavouros version: unknown eventLogger: journald hostname: user-standardpcq35ich92009 idMappings: gidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.1.4-arch1-1 linkmode: dynamic logDriver: journald memFree: 4336590848 memTotal: 8333340672 networkBackend: netavark ociRuntime: name: crun package: /usr/bin/crun is owned by crun 1.7.2-1 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: /usr/bin/slirp4netns is owned by slirp4netns 1.2.0-1 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 0 swapTotal: 0 uptime: 0h 20m 17.00s plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: {} store: configFile: /home/user/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/user/.local/share/containers/storage graphRootAllocated: 31523282944 graphRootUsed: 12035514368 graphStatus: Backing Filesystem: extfs Native Overlay Diff: &quot;true&quot; Supports d_type: &quot;true&quot; Using metacopy: &quot;false&quot; imageCopyTmpDir: /var/tmp imageStore: number: 6 runRoot: /run/user/1000/containers volumePath: /home/user/.local/share/containers/storage/volumes version: APIVersion: 4.3.1 Built: 1668983565 BuiltTime: Sun Nov 20 23:32:45 2022 GitCommit: 814b7b003cc630bf6ab188274706c383f9fb9915-dirty GoVersion: go1.19.3 Os: linux OsArch: linux/amd64 Version: 4.3.1 ``` &lt;/details&gt; ### Podman in a container No ### Privileged Or Rootless Rootless ### Upstream Latest Release Yes ### Additional environment details Happens both locally and inside a fresh VM. ### Additional information Ask if anything unclear None of the info in here is sensitive and mostly placeholders for password and such not to worry</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>1</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #16732 from flouthoc/network-update network: add support for `podman network update` and `--network-dns-server`</MESSAGE>
    <SHA>b107d7720a4a62fcf11a83953aa5ae985c78daed</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>Set StoppedByUser earlier in the process of stopping The StoppedByUser variable indicates that the container was requested to stop by a user. It's used to prevent restart policy from firing (so that a restart=always container won't restart if the user does a `podman stop`. The problem is we were setting it *very* late in the stop() function. Originally, this was fine, but after the changes to add the new Stopping state, the logic that triggered restart policy was firing before StoppedByUser was even set - so the container would still restart. Setting it earlier shouldn't hurt anything and guarantees that checks will see that the container was stopped manually. Fixes #17069 Signed-off-by: Matthew Heon &lt;matthew.heon@pm.me&gt;</MESSAGE>
      <SHA>1ab833fb73e3ac27c8bd2d9fcec11d39ca04b490</SHA>
      <PATCHEDFILES>
        <FILE>libpod/container_internal.go</FILE>
        <FILE>test/e2e/run_test.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
