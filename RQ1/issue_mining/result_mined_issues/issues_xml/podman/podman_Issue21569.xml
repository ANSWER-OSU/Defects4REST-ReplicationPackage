<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>21569</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/21569</ISSUEURL>
  <TITLE>podman stop: rootless netns ref counter out of sync, counter is at -1, resetting it back to 0</TITLE>
  <DESCRIPTION>``` &lt;+012ms&gt; # $ podman pod rm test_pod &lt;+328ms&gt; # time=&quot;2024-02-08T08:28:16-06:00&quot; level=error msg=&quot;rootless netns ref counter out of sync, counter is at -1, resetting it back to 0&quot; # 2ef9c5c99f00484285a5a7394e9d276d97c436311f197e7d2a0fa05ea78e69b5 ``` Seen mostly in system tests, **BUT ALSO IN E2E TESTS WHERE IT IS IMPOSSIBLE TO DETECT**. Because it happens in test cleanup, where `rm` does not trigger a test failure. So I caught all these by accident. **THIS IS HAPPENING A LOT MORE THAN WE ARE AWARE OF**. * fedora-38 : sys podman fedora-38 rootless host boltdb * PR #21535 * [02-08 09:30](https://api.cirrus-ci.com/v1/artifact/task/6664771953491968/html/sys-podman-fedora-38-rootless-host-boltdb.log.html#t--00621) in [sys] [700] podman play --service-container * fedora-39 : int podman fedora-39 rootless host sqlite * [02-06 08:21](https://api.cirrus-ci.com/v1/artifact/task/6507554541404160/html/int-podman-fedora-39-rootless-host-sqlite.log.html#t--Podman-kube-play-test-with-reserved-init-annotation-in-yaml--1) in Podman kube play test with reserved init annotation in yaml * [01-30 09:16](https://api.cirrus-ci.com/v1/artifact/task/5196874873831424/html/int-podman-fedora-39-rootless-host-sqlite.log.html#t--Podman-kube-play-override-with-tcp-should-keep-udp-from-YAML-file--1) in Podman kube play override with tcp should keep udp from YAML file * [01-18 08:40](https://api.cirrus-ci.com/v1/artifact/task/6165486266744832/html/int-podman-fedora-39-rootless-host-sqlite.log.html#t--Podman-kube-play-secret-as-volume-support-multiple-volumes--1) in Podman kube play secret as volume support - multiple volumes * fedora-39β : sys podman fedora-39β rootless host sqlite * PR #20161 * [10-17-2023 15:39](https://api.cirrus-ci.com/v1/artifact/task/4824180621836288/html/sys-podman-fedora-39β-rootless-host-sqlite.log.html#t--00592) in [sys] podman play --service-container * rawhide : int podman rawhide rootless host sqlite * [02-06 11:21](https://api.cirrus-ci.com/v1/artifact/task/6252698094272512/html/int-podman-rawhide-rootless-host-sqlite.log.html#t--Podman-kube-play-secret-as-volume-support-multiple-volumes--1) in Podman kube play secret as volume support - multiple volumes * [02-04 12:39](https://api.cirrus-ci.com/v1/artifact/task/5821207593877504/html/int-podman-rawhide-rootless-host-sqlite.log.html#t--Podman-kube-play-test-restartPolicy--1) in Podman kube play test restartPolicy * rawhide : sys podman rawhide rootless host sqlite * PR #21246 * [01-12 13:11](https://api.cirrus-ci.com/v1/artifact/task/6754094187020288/html/sys-podman-rawhide-rootless-host-sqlite.log.html#t--00616) in [sys] [700] podman play | x | x | x | x | x | x | | ---: | ---: | ---: | ---: | ---: | ---: | | int(5) | podman(8) | rawhide(3) | rootless(8) | host(8) | sqlite(7) | | sys(3) | | fedora-39(3) | | | boltdb(1) | | | | fedora-39β(1) | | | | | | | fedora-38(1) | | | |</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>185</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #23442 from legobeat/compose-warning-logs-env-conf fix: disable compose_warning_logs if PODMAN_COMPOSE_WARNING_LOGS=false</MESSAGE>
    <SHA>d38268062ace913b1bc3b1e00563867173cdec67</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>test/system: podman play --service-container slog to syslog In theory when syslog is set the cleanup process should log its errors to syslog (journald) so we can have a look at the errors in CI. Without it podman container cleanup errors will never be logged anywhere. In order to rey to debug #21569 Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>13786a1fa008d7259f5746a77a67aa568548c16b</SHA>
      <PATCHEDFILES>
        <FILE>test/system/700-play.bats</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>fix network cleanup flake in play kube When using service containers and play kube we create a complicated set of dependencies. First in a pod all conmon/container cgroups are part of one slice, that slice will be removed when the entire pod is stopped resulting in systemd killing all processes that were part in it. Now the issue here is around the working of stopPodIfNeeded() and stopIfOnlyInfraRemains(), once a container is cleaned up it will check if the pod should be stopped depending on the pod ExitPolicy. If this is the case it wil stop all containers in that pod. However in our flaky test we calle podman pod kill which logically killed all containers already. Thus the logic now thinks on cleanup it must stop the pod and calls into pod.stopWithTimeout(). Then there we try to stop but because all containers are already stopped it just throws errors and never gets to the point were it would call Cleanup(). So the code does not do cleanup and eventually calls removePodCgroup() which will cause all conmon and other podman cleanup processes of this pod to be killed. Thus the podman container cleanup process was likely killed while actually trying to the the proper cleanup which leaves us in a bad state. Following commands such as podman pod rm will try to the cleanup again as they see it was not completed but then fail as they are unable to recover from the partial cleanup state. Long term network cleanup needs to be more robust and ideally should be idempotent to handle cases were cleanup was killed in the middle. Fixes #21569 Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>4c3531a1a4dd23faeb41da5fd9f9b33dee6d9746</SHA>
      <PATCHEDFILES>
        <FILE>libpod/pod_api.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>fix network cleanup flake in play kube When using service containers and play kube we create a complicated set of dependencies. First in a pod all conmon/container cgroups are part of one slice, that slice will be removed when the entire pod is stopped resulting in systemd killing all processes that were part in it. Now the issue here is around the working of stopPodIfNeeded() and stopIfOnlyInfraRemains(), once a container is cleaned up it will check if the pod should be stopped depending on the pod ExitPolicy. If this is the case it wil stop all containers in that pod. However in our flaky test we calle podman pod kill which logically killed all containers already. Thus the logic now thinks on cleanup it must stop the pod and calls into pod.stopWithTimeout(). Then there we try to stop but because all containers are already stopped it just throws errors and never gets to the point were it would call Cleanup(). So the code does not do cleanup and eventually calls removePodCgroup() which will cause all conmon and other podman cleanup processes of this pod to be killed. Thus the podman container cleanup process was likely killed while actually trying to the the proper cleanup which leaves us in a bad state. Following commands such as podman pod rm will try to the cleanup again as they see it was not completed but then fail as they are unable to recover from the partial cleanup state. Long term network cleanup needs to be more robust and ideally should be idempotent to handle cases were cleanup was killed in the middle. Fixes #21569 Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>d6ae7427f738f7cac71496d3aa7970b81dcd1c7a</SHA>
      <PATCHEDFILES>
        <FILE>libpod/pod_api.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
