<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>23913</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/23913</ISSUEURL>
  <TITLE>CI: system tests: something is eating volumes</TITLE>
  <DESCRIPTION>Seeing flakes in `|600| podman shell completion test` when run in parallel: ``` $ podman __completeNoDesc something ... # #| expected: '.*v-t361-kugfvnq4 (but no, it is not there!) ``` The trick is finding the culprit. There are very few `run_podman volume rm` commands in system tests, it's easy to check them individually, and I don't see any way for any of them to be responsible. Journal shows nothing useful. * debian-13 : sys podman debian-13 root host sqlite * PR #23275 * [08-21 19:49](https://api.cirrus-ci.com/v1/artifact/task/4719582198366208/html/sys-podman-debian-13-root-host-sqlite.log.html#t--00326p) in [sys] |600| podman shell completion test * rawhide : sys podman rawhide rootless host sqlite * PR #23275 * [09-10 08:08](https://api.cirrus-ci.com/v1/artifact/task/6657454413447168/html/sys-podman-rawhide-rootless-host-sqlite.log.html#t--00361p) in [sys] |600| podman shell completion test | x | x | x | x | x | x | | ---: | ---: | ---: | ---: | ---: | ---: | | sys(2) | podman(2) | rawhide(1) | root(1) | host(2) | sqlite(2) | | | | debian-13(1) | rootless(1) | | |</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>93</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #24407 from Luap99/readthedocs readthedocs: build extra formats</MESSAGE>
    <SHA>f139bc17b3cc34c010c9cc7bd993367bf3ffb01c</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>Instrument cleanup tracer to log weird volume removal flake Debug for #23913, I though if we have no idea which process is nuking the volume then we need to figure this out. As there is no reproducer we can (ab)use the cleanup tracer. Simply trace all unlink syscalls to see which process deletes our special named volume. Given the volume name is used as path on the fs and is deleted on volume rm we should know exactly which process deleted it the next time hoopefully. Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>c817d1faf9fdec711c283cceb33dbd65544e98bb</SHA>
      <PATCHEDFILES>
        <FILE>hack/podman_cleanup_tracer.bt</FILE>
        <FILE>test/system/600-completion.bats</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>Instrument cleanup tracer to log weird volume removal flake Debug for #23913, I though if we have no idea which process is nuking the volume then we need to figure this out. As there is no reproducer we can (ab)use the cleanup tracer. Simply trace all unlink syscalls to see which process deletes our special named volume. Given the volume name is used as path on the fs and is deleted on volume rm we should know exactly which process deleted it the next time hopefully. Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>d633824a9527b9ec937cdfc8aacc890ec3249127</SHA>
      <PATCHEDFILES>
        <FILE>hack/podman_cleanup_tracer.bt</FILE>
        <FILE>test/system/600-completion.bats</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>volume ls: fix race that caused it to fail If volume ls was called while another volume was removed at the right time it could have failed with &quot;no such volume&quot; as we did not ignore such error during listing. As we list things and this no longer exists the correct thing is to ignore the error and continue like we do with containers, pods, etc... I have a slight feeling that this might solve #23913 but I am not to sure there so I am not adding a Fixes here. Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>b1a49be4e6bb7a32d7575340e5465b19029ddb81</SHA>
      <PATCHEDFILES>
        <FILE>pkg/api/handlers/compat/volumes.go</FILE>
        <FILE>pkg/api/handlers/libpod/volumes.go</FILE>
        <FILE>pkg/domain/infra/abi/volumes.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>volume ls: fix race that caused it to fail If volume ls was called while another volume was removed at the right time it could have failed with &quot;no such volume&quot; as we did not ignore such error during listing. As we list things and this no longer exists the correct thing is to ignore the error and continue like we do with containers, pods, etc... This was pretty easy to reproduce with these two commands running in differernt terminals: while :; do bin/podman volume create test &amp;&amp; bin/podman volume rm test || break; done while :; do bin/podman volume ls || break ; done I have a slight feeling that this might solve #23913 but I am not to sure there so I am not adding a Fixes here. Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>a686a6c9a9f0d64ec7eb6bf7f9bede8b7d9bd9dc</SHA>
      <PATCHEDFILES>
        <FILE>pkg/api/handlers/compat/volumes.go</FILE>
        <FILE>pkg/api/handlers/libpod/volumes.go</FILE>
        <FILE>pkg/domain/infra/abi/volumes.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>volume ls: fix race that caused it to fail If volume ls was called while another volume was removed at the right time it could have failed with &quot;no such volume&quot; as we did not ignore such error during listing. As we list things and this no longer exists the correct thing is to ignore the error and continue like we do with containers, pods, etc... This was pretty easy to reproduce with these two commands running in different terminals: while :; do bin/podman volume create test &amp;&amp; bin/podman volume rm test || break; done while :; do bin/podman volume ls || break ; done I have a slight feeling that this might solve #23913 but I am not to sure there so I am not adding a Fixes here. Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>9a0c0b2eef962bdb63372ed0ccd2bb6b1e5de3b8</SHA>
      <PATCHEDFILES>
        <FILE>pkg/api/handlers/compat/volumes.go</FILE>
        <FILE>pkg/api/handlers/libpod/volumes.go</FILE>
        <FILE>pkg/domain/infra/abi/volumes.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>volume ls: fix race that caused it to fail If volume ls was called while another volume was removed at the right time it could have failed with &quot;no such volume&quot; as we did not ignore such error during listing. As we list things and this no longer exists the correct thing is to ignore the error and continue like we do with containers, pods, etc... This was pretty easy to reproduce with these two commands running in different terminals: while :; do bin/podman volume create test &amp;&amp; bin/podman volume rm test || break; done while :; do bin/podman volume ls || break ; done I have a slight feeling that this might solve #23913 but I am not to sure there so I am not adding a Fixes here. Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt; (cherry picked from commit 9a0c0b2eef962bdb63372ed0ccd2bb6b1e5de3b8) Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>9e38b455e4575e19ecb7a22d7f629d418633a2a0</SHA>
      <PATCHEDFILES>
        <FILE>pkg/api/handlers/compat/volumes.go</FILE>
        <FILE>pkg/api/handlers/libpod/volumes.go</FILE>
        <FILE>pkg/domain/infra/abi/volumes.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
