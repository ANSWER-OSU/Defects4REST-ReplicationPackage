<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>16515</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/16515</ISSUEURL>
  <TITLE>Various issues in systemd.NotifyProxy</TITLE>
  <DESCRIPTION>I've been reviewing this code, hoping to find the cause of #16076. I'm not 100% sure I've found the underlying reason, but I have found several issues. First of all, there seems to be a general misconception of how datagram sockets work. Any single write is never split, and is delivered as a separate read, never combined with any other write. This means that the loop in waitForReady() is wrong when combining reads. The receiving end should read one single read at a time (which will be the equivalent of one packet that was sent) and then immediately handle it, then handling further reads separately. If the sending side side sends a larger message than the buffer you passed, then you will get a truncated read, and the remaining data is thrown away. If the receive buffer in the kernel is full for the socket, further incoming packets will be dropped. Here is how systemd handles incoming messages, which we should emulate: * setsockopt is used on the socket, to set SO_RCVBUF to 8\*1024\*1024, which means less risk of packets being dropped. * The recvmsg() has a buffer size of PIPE_BUF == 4096 bytes * Truncated reads (MSG_TRUNC) are detected and handled as an error * Each non-truncated read is treated on its own as one full packet * reads are done with recvmsg() which allows passing of file descriptors from the client Secondly, sd-notify handles something called barriers. What happens then is that the client sends a packet containing only &quot;BARRIER=1&quot;, which includes a file descriptor. The client side sends this, then waits for the file descriptor to be closed. So, when the server gets a message like this it needs to extract and close all the passed file descriptors. The test case is using `systemd-notify --ready`, and that has this code: ``` r = sd_pid_notify(source_pid, false, n); // main notify ... if (!arg_no_block) { r = sd_notify_barrier(0, 5 * USEC_PER_SEC); ``` In other words, unless you passed `--no-block`, the client will first send the notify message, then a separate barrier message and then wait until the barrier is handled. So, issues here: * we must handle barrier messages correctly * we must continue to handle messages after the notify message I think the best way to handle the barrier messages are to just close the fd:s in the message and ignore the message. Alternatively we could send them on to systemd and have it close them, but that seems unnecessary. I think the point of the barrier is to ensure that the remote side has seen the previous messages and we're sure they are not lost in the send buffer of the client or something. Maybe we need to have the notify proxy similarly use a barrier when reporting the notify to systemd though. For the continued handling of messages, I'm not sure what the best approach is. There is not real way to tell that the peer side of a datagram socket has exited or closed the fd. The way systemd handles this is that it has only a single notify receive socket in the entire pid 1, and then it looks at the kernel reported peer pid on received messages to see what pid sent the message. Then it keeps the socket open forever. Thirdly, It feels like there is a race condition between errorChan and readyChan when we deal with a timeout. If the timeout races with the readyChan, then both could be sent. I don't think this actually is an issue right now, because WaitAndClose() will just randomly pick one of them and ignore the other. However, long-term if we handle multiple messages, etc it may be an issue. Other minor issues: * Systemd accepts (but warns) about the message text having a single trailing nul For details of how systemd works, see manager_dispatch_notify_fd() https://github.com/systemd/systemd/blob/main/src/core/manager.c#L2444 this is called from the mainloop everytime poll/select returns the notify socket fd as readable.</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>22</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>notify k8s system test: move sending message into exec The flake in #16076 is likely related to the notify message not being delivered/read correctly. Move sending the message into an exec session such that flakes will reveal an error message. Signed-off-by: Valentin Rothberg &lt;vrothberg@redhat.com&gt;</MESSAGE>
    <SHA>8c3af71862cbe078a69feba2906cc6349c88c38b</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>kube sdnotify: run proxies for the lifespan of the service As outlined in #16076, a subsequent BARRIER *may* follow the READY message sent by a container. To correctly imitate the behavior of systemd's NOTIFY_SOCKET, the notify proxies span up by `kube play` must hence process messages for the entirety of the workload. We know that the workload is done and that all containers and pods have exited when the service container exits. Hence, all proxies are closed at that time. The above changes imply that Podman runs for the entirety of the workload and will henceforth act as the MAINPID when running inside of systemd. Prior to this change, the service container acted as the MAINPID which is now not possible anymore; Podman would be killed immediately on exit of the service container and could not clean up. The kube template now correctly transitions to in-active instead of failed in systemd. Fixes: #16076 Fixes: #16515 Signed-off-by: Valentin Rothberg &lt;vrothberg@redhat.com&gt;</MESSAGE>
      <SHA>4fa307f149234ed27a4e07da8065e1495a1962c2</SHA>
      <PATCHEDFILES>
        <FILE>pkg/domain/infra/abi/play.go</FILE>
        <FILE>pkg/systemd/notifyproxy/notifyproxy.go</FILE>
        <FILE>pkg/systemd/notifyproxy/notifyproxy_test.go</FILE>
        <FILE>test/system/250-systemd.bats</FILE>
        <FILE>test/system/260-sdnotify.bats</FILE>
        <FILE>test/system/700-play.bats</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
