<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>2893</ISSUENO>
  <ISSUEURL>https://github.com/harness/harness/issues/2893</ISSUEURL>
  <TITLE>Error &quot;connect: cannot assign requested address&quot; due to goroutine / socket leak</TITLE>
  <DESCRIPTION>We recently upgraded to Drone 1.6.1 running in Kubernetes and just had an issue where all builds were failing with the following error: &gt; Post http://localhost:3001: dial tcp 127.0.0.1:3001: connect: cannot assign requested address which is the `DRONE_CONVERT_PLUGIN_ENDPOINT` and in our case is the [drone-convert-starlark](https://github.com/drone/drone-convert-starlark) extension running as a sidecar in the pod spec. Restarting the drone-ci server pod fixed our issue. But having seen that error before (usually indicating exhaustion of ephemeral port range) I started looking into metrics and found what looks like a resource leak related to how the server calls the converter. `process_open_fds{kubernetes_namespace=&quot;drone&quot;}` grew to 28267 over ~22 days `go_goroutines{kubernetes_namespace=&quot;drone&quot;}` grew to 56547 indicating a potential issue with goroutines not returning/finishing Investigating the open connections inside the new pod we saw the connections from localhost to the converter service on 127.0.0.1:3001 were often (but not always) staying in the ESTABLISHED state. Established connections to the secret endpoint (drone-vault in our case) also may be leaking but at a much slower pace. Here you can see 4 samples each taken 10 minutes apart, showing src IPs and the local listening port they are connected to. In this data all the sockets are in the ESTABLISHED state, and you can also see the go_goroutines gauge metric is steadily increasing. ``` / # for run in `seq 1 10`; do uptime;netstat -nat &gt; /dev/shm/netstat; echo &quot;Totals: $(curl -s 127.0.0.1:80/metrics|grep '^go_goroutines') Established $(grep 'ESTABLISHED' /dev/shm/netstat | wc -l)&quot;; gawk 'BEGIN {OFS=&quot;\t&quot;} $1 ~ /tcp/ &amp;&amp; $6 !~ /LISTEN/ &amp;&amp; $4 ~ /:(3000|3001)$/{ndst=split($4,dst,&quot;:&quot;);nsrc=split($5,src,&quot;:&quot;);groups[(src[nsrc-1] &quot;-&gt; :&quot; dst[ndst])]++;states[(src[nsrc-1] &quot;-&gt; :&quot; dst[ndst])][$6]++}END{PROCINFO[&quot;sorted_in&quot;] = &quot;@val_num_desc&quot;;for (g in groups) if (groups[g]&gt;5) {statelist=&quot; &quot;;for (s in states[g]) statelist=statelist s &quot;=&quot; states[g][s] &quot; &quot;;print groups[g],g,statelist}}' /dev/shm/netstat; sleep 600;done; 23:50:40 up 6 days, 6:07, load average: 0.04, 0.75, 2.24 Totals: go_goroutines 2295 Established 2545 1088 127.0.0.1-&gt; :3001 ESTABLISHED=1088 75 10.97.82.95-&gt; :3000 ESTABLISHED=75 69 10.102.153.135-&gt; :3000 ESTABLISHED=69 57 10.101.239.149-&gt; :3000 ESTABLISHED=57 48 10.96.21.19-&gt; :3000 ESTABLISHED=48 37 10.97.82.93-&gt; :3000 ESTABLISHED=37 17 10.97.111.6-&gt; :3000 ESTABLISHED=17 16 10.107.202.127-&gt; :3000 ESTABLISHED=16 7 10.102.229.196-&gt; :3000 ESTABLISHED=7 00:00:41 up 6 days, 6:17, load average: 0.00, 0.08, 1.15 Totals: go_goroutines 2350 Established 2637 1136 127.0.0.1-&gt; :3001 ESTABLISHED=1136 77 10.97.82.95-&gt; :3000 ESTABLISHED=77 70 10.102.153.135-&gt; :3000 ESTABLISHED=70 55 10.101.239.149-&gt; :3000 ESTABLISHED=55 51 10.96.21.19-&gt; :3000 ESTABLISHED=51 38 10.97.82.93-&gt; :3000 ESTABLISHED=38 22 10.97.111.6-&gt; :3000 ESTABLISHED=22 18 10.107.202.127-&gt; :3000 ESTABLISHED=18 9 10.102.229.196-&gt; :3000 ESTABLISHED=9 00:10:41 up 6 days, 6:27, load average: 0.20, 0.10, 0.63 Totals: go_goroutines 2459 Established 2746 1167 127.0.0.1-&gt; :3001 ESTABLISHED=1167 76 10.97.82.95-&gt; :3000 ESTABLISHED=76 69 10.102.153.135-&gt; :3000 ESTABLISHED=69 59 10.101.239.149-&gt; :3000 ESTABLISHED=59 52 10.96.21.19-&gt; :3000 ESTABLISHED=52 42 10.97.82.93-&gt; :3000 ESTABLISHED=42 24 10.97.111.6-&gt; :3000 ESTABLISHED=24 21 10.107.202.127-&gt; :3000 ESTABLISHED=21 13 10.102.229.196-&gt; :3000 ESTABLISHED=13 00:20:42 up 6 days, 6:37, load average: 0.19, 0.32, 0.47 Totals: go_goroutines 2490 Established 2785 1184 127.0.0.1-&gt; :3001 ESTABLISHED=1184 75 10.97.82.95-&gt; :3000 ESTABLISHED=75 69 10.102.153.135-&gt; :3000 ESTABLISHED=69 62 10.101.239.149-&gt; :3000 ESTABLISHED=62 55 10.96.21.19-&gt; :3000 ESTABLISHED=55 42 10.97.82.93-&gt; :3000 ESTABLISHED=42 24 10.97.111.6-&gt; :3000 ESTABLISHED=24 23 10.107.202.127-&gt; :3000 ESTABLISHED=23 15 10.102.229.196-&gt; :3000 ESTABLISHED=15 ``` Checking the code I do see a 60 second timeout in a few places ([remote converter method](https://github.com/drone/drone/blob/9c9b99bd70339af1d9272c2c34ae4539055aba96/plugin/converter/remote.go#L50), [http client Do method](https://github.com/drone/drone-go/blob/1d2e07e87e79109e32b9d7a1291751571433283a/plugin/internal/client/client.go#L95)) so I'd expect things to get cleaned up rather quickly, not continued growth like we're seeing. I don't see any pproff endpoints in the 1.6 server, but I hope to be able to get some stack traces via triggering a core dump using `pkill -SIGABRT $(pidof drone-server)`. Hopefully that will help track down what all the extra goroutines and sockets are doing.</DESCRIPTION>
  <REPONAME>harness</REPONAME>
  <TIMEDIFFERENCEDAYS>1848</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>fix: [AH-492]: add default size (#2898) * fix: [AH-492]: add default size</MESSAGE>
    <SHA>9227b297bb1985e8e0d821994233bfbcaa876422</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>fix: [AH-492]: show size coming from BE in layers table (#2893) * fix: [AH-492]: show size coming from BE in layers table</MESSAGE>
      <SHA>34bee25c56bf8fb743ad9388f528aba684cc3a95</SHA>
      <PATCHEDFILES>
        <FILE>web/src/ar/pages/version-details/DockerVersion/components/LayersTable/LayersTable.tsx</FILE>
        <FILE>web/src/ar/pages/version-details/utils.ts</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
