<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>91</ISSUENO>
  <ISSUEURL>https://github.com/confluentinc/kafka-rest/issues/91</ISSUEURL>
  <TITLE>Handle large number of schemas</TITLE>
  <DESCRIPTION>See related issue: confluentinc/schema-registry#172 When many requests come in for the same topic with the same schema and do not use the schema ID (e.g., the first request from many different clients), each one is registered via the serializer (see AvroRestProducer). However, because the serializer uses an IdentityHashMap, any cached schemas that are equivalent are not used, which results in a) an unnecessary HTTP request and b) a _different_ entry in the IdentityHashMap for the same schema. This is a) slower than necessary and b) eventually exhausts the number of entries allowed in the cache (`max.schemas.per.subject`). One solution to this problem if the schema-registry implementation isn't going to detect these identical schemas would be to do deduplication at the kafka-rest level. This is a case where deduplication via the more expensive equals() check is actually valid since the REST proxy is expected to end up with many different instances of the same schema.</DESCRIPTION>
  <REPONAME>kafka-rest</REPONAME>
  <TIMEDIFFERENCEDAYS>438</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #230 from kevinconaway/issue-229 Issue #229: Add blurb on Jetty jmx metrics</MESSAGE>
    <SHA>755d0f1b72903dbe01a423e768ce72d1ec574792</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>Merge pull request #222 from jdehrlich/master fixing issue #91</MESSAGE>
      <SHA>e87400c1a3f72dfe3163af7f35e37299c154a625</SHA>
      <PATCHEDFILES>
        <FILE>src/main/java/io/confluent/kafkarest/AvroRestProducer.java</FILE>
        <FILE>src/test/java/io/confluent/kafkarest/unit/AvroRestProducerTest.java</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
