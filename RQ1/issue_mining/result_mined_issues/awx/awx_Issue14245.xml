<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>14245</ISSUENO>
  <ISSUEURL>https://github.com/ansible/awx/issues/14245</ISSUEURL>
  <TITLE>Dispatcher shutdown deadlock when redis is unavailable</TITLE>
  <DESCRIPTION>### Please confirm the following - [X] I agree to follow this project's [code of conduct](https://docs.ansible.com/ansible/latest/community/code_of_conduct.html). - [X] I have checked the [current issues](https://github.com/ansible/awx/issues) for duplicates. - [X] I understand that AWX is open source software provided for free and that I might not receive a timely response. - [X] I am **NOT** reporting a (potential) security vulnerability. (These should be emailed to `security@ansible.com` instead.) ### Bug Summary We have identified a set of circumstances and the mechanic for how `awx-manage run_dispatcher` can hit an error but fail to shut down. Instead of exiting, it remains in a deadlocked state where it cannot process new work. Because the process is known by supervisord to be healthy (although it's not) this state is not recovered from without manual intervention. ### AWX version devel ### Select the relevant components - [ ] UI - [ ] UI (tech preview) - [X] API - [ ] Docs - [ ] Collection - [ ] CLI - [ ] Other ### Installation method N/A ### Modifications no ### Ansible version N/A ### Operating system RHEL9 ### Web browser Chrome ### Steps to reproduce There is a test scenario that will cause the deadlock every time, and that is: - shut down redis - wait until a control node reports a new heatbeat timestamp - start redis back up Redis is shut down for &lt; 20s through the process (although it may vary). The heatbeat may not actually run, but it does obtain the deadlocked state. ### Expected results No deadlocks ### Actual results Dispatcher main process hits an error and tries to exit, shown by the `Encountered unhandled error in dispatcher main loop` log: ``` Traceback (most recent call last): File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/awx/main/dispatch/worker/base.py&quot;, line 203, in run self.run_periodic_tasks() File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/awx/main/dispatch/worker/base.py&quot;, line 180, in run_periodic_tasks self.subsystem_metrics.pipe_execute() ``` The state of the dispatcher service is that some workers and the periodic scheduler thread exit, but several workers (1 to 3 seen in practice) are still running. ``` awx 73763 0.0 0.9 832528 146988 ? S Jul15 0:02 \_ /var/lib/awx/venv/awx/bin/python /usr/bin/awx-manage run_dispatcher awx 73961 0.0 0.0 0 0 ? Z Jul15 0:00 | \_ [awx-manage] &lt;defunct&gt; awx 73962 0.0 0.8 604868 131944 ? Sl Jul15 0:03 | \_ /var/lib/awx/venv/awx/bin/python /usr/bin/awx-manage run_dispatcher awx 73963 0.0 0.8 605004 135184 ? Sl Jul15 0:03 | \_ /var/lib/awx/venv/awx/bin/python /usr/bin/awx-manage run_dispatcher awx 73966 0.0 0.8 605056 135152 ? Sl Jul15 0:03 | \_ /var/lib/awx/venv/awx/bin/python /usr/bin/awx-manage run_dispatcher ``` At this point, gdb / py-bt information shows the main process is waiting for its forks to close: ``` Traceback (most recent call first): &lt;built-in method waitpid of module object at remote 0x7fa931941e50&gt; File &quot;/usr/lib64/python3.9/multiprocessing/popen_fork.py&quot;, line 27, in poll pid, sts = os.waitpid(self.pid, flag) File &quot;/usr/lib64/python3.9/multiprocessing/popen_fork.py&quot;, line 43, in wait return self.poll(os.WNOHANG if timeout == 0.0 else 0) File &quot;/usr/lib64/python3.9/multiprocessing/process.py&quot;, line 149, in join res = self._popen.wait(timeout) File &quot;/usr/lib64/python3.9/multiprocessing/util.py&quot;, line 357, in _exit_function p.join() ``` At this same time, and with the same tool, those forks themselves are in their normal main loop, as if nothing had happened. In other words, they are reading from the queue that the main process sends signals through. Critically, we have found this error in the workers right before obtaining this state: ``` Traceback (most recent call last): File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/awx/main/dispatch/worker/base.py&quot;, line 248, in work_loop body = self.read(queue) File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/awx/main/dispatch/worker/base.py&quot;, line 236, in read return queue.get(block=True, timeout=1) File &quot;/usr/lib64/python3.9/multiprocessing/queues.py&quot;, line 113, in get if not self._poll(timeout): File &quot;/usr/lib64/python3.9/multiprocessing/connection.py&quot;, line 266, in poll return self._poll(timeout) File &quot;/usr/lib64/python3.9/multiprocessing/connection.py&quot;, line 433, in _poll r = wait([self], timeout) File &quot;/usr/lib64/python3.9/multiprocessing/connection.py&quot;, line 940, in wait ready = selector.select(timeout) File &quot;/usr/lib64/python3.9/selectors.py&quot;, line 416, in select fd_event_list = self._selector.poll(timeout) File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/awx/main/scheduler/task_manager.py&quot;, line 122, in record_aggregate_metrics_and_exit self.record_aggregate_metrics() File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/awx/main/scheduler/task_manager.py&quot;, line 107, in record_aggregate_metrics s_metrics.Metrics(auto_pipe_execute=True).inc(f&quot;{self.prefix}__schedule_calls&quot;, 1) File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/awx/main/analytics/subsystem_metrics.py&quot;, line 241, in inc self.pipe_execute() File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/awx/main/analytics/subsystem_metrics.py&quot;, line 285, in pipe_execute self.pipe.execute() File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/redis/client.py&quot;, line 2072, in execute conn = self.connection_pool.get_connection(&quot;MULTI&quot;, self.shard_hint) File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/redis/connection.py&quot;, line 1387, in get_connection connection.connect() File &quot;/var/lib/awx/venv/awx/lib64/python3.9/site-packages/redis/connection.py&quot;, line 617, in connect raise ConnectionError(self._error_message(e)) redis.exceptions.ConnectionError: Error 2 connecting to unix socket: /var/run/redis/redis.sock. No such file or directory. ``` How does one go from `queue.get` to creating a redis connection? This is because this particular method is registered as the callback when receiving a SIGTERM signal. https://github.com/ansible/awx/blob/b8ba2feecd71c5e33eaead004b89807d27661461/awx/main/scheduler/task_manager.py#L135 ### Additional information The worker receives a SIGTERM and fires off the `record_aggregate_metrics_and_exit` method which _was registered by the task manger_ when the task manager task ran. However, this worker _was not running the task manager_, it was waiting for new work, listening on its multiprocessing queue. https://github.com/ansible/awx/blob/b8ba2feecd71c5e33eaead004b89807d27661461/awx/main/scheduler/task_manager.py#L135 The expectation is that, while in the main loop, an idle worker has _this_ signal handler connected: https://github.com/ansible/awx/blob/b8ba2feecd71c5e33eaead004b89807d27661461/awx/main/dispatch/worker/base.py#L38-L41 From the debugging, we're pretty sure that `self.kill_now` fails to get set due to this task-specific custom signal handler. Although this issue was introduced when &quot;task manager metrics&quot; were added, in https://github.com/ansible/awx/pull/12235, the problem of deadlocks on closing were not observed (to my knowledge) right away. When metrics for the _dispatcher_ were introduced in https://github.com/ansible/awx/pull/13989, now a trigger was created, since it added a task to the dispatcher main loop which could error when redis was unavailable. Still, we don't know the _exact_ thing that caused us to start seeing this, because tests were completing successfully for a significant time after the dispatcher metrics were added. This cause might be more mundane, like a change in testing strategy.</DESCRIPTION>
  <REPONAME>awx</REPONAME>
  <TIMEDIFFERENCEDAYS>1</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Allow `job_template` collection module to set verbosity to 5 (#14244)</MESSAGE>
    <SHA>b021ad7b2879887c8caaef04d3c0cf2882962064</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>Make dispatcher timeout use SIGUSR1, not SIGTERM Refs #14245 Signed-off-by: Rick Elrod &lt;rick@elrod.me&gt;</MESSAGE>
      <SHA>6d5f2ccd637527fe829a1295c7476e67532714fe</SHA>
      <PATCHEDFILES>
        <FILE>awx/main/dispatch/pool.py</FILE>
        <FILE>awx/main/scheduler/task_manager.py</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>Make dispatcher timeout use SIGUSR1, not SIGTERM Refs #14245 Signed-off-by: Rick Elrod &lt;rick@elrod.me&gt;</MESSAGE>
      <SHA>5b7fc3402598e0f5e317caf7dc4d6c36e1efc9d1</SHA>
      <PATCHEDFILES>
        <FILE>awx/main/dispatch/pool.py</FILE>
        <FILE>awx/main/scheduler/task_manager.py</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>Prevent Dispatcher deadlock when Redis disappears (#14249) This fixes https://github.com/ansible/awx/issues/14245 which has more information about this issue. This change addresses both: - A clashing signal handler (registering a callback to fire when the task manager times out, and hitting that callback in cases where we didn't expect to). Make dispatcher timeout use SIGUSR1, not SIGTERM. - Metrics not being reported should not make us crash, so that is now fixed as well. Signed-off-by: Rick Elrod &lt;rick@elrod.me&gt; Co-authored-by: Alan Rominger &lt;arominge@redhat.com&gt;</MESSAGE>
      <SHA>48edb15a03bf3cae94405559693ea14a0ee6f2fd</SHA>
      <PATCHEDFILES>
        <FILE>awx/main/dispatch/pool.py</FILE>
        <FILE>awx/main/dispatch/worker/base.py</FILE>
        <FILE>awx/main/scheduler/task_manager.py</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>Prevent Dispatcher deadlock when Redis disappears (#14249) This fixes https://github.com/ansible/awx/issues/14245 which has more information about this issue. This change addresses both: - A clashing signal handler (registering a callback to fire when the task manager times out, and hitting that callback in cases where we didn't expect to). Make dispatcher timeout use SIGUSR1, not SIGTERM. - Metrics not being reported should not make us crash, so that is now fixed as well. Signed-off-by: Rick Elrod &lt;rick@elrod.me&gt; Co-authored-by: Alan Rominger &lt;arominge@redhat.com&gt;</MESSAGE>
      <SHA>30b084858c9eabb4ef90fd32e67b38867a43dc81</SHA>
      <PATCHEDFILES>
        <FILE>awx/main/dispatch/pool.py</FILE>
        <FILE>awx/main/dispatch/worker/base.py</FILE>
        <FILE>awx/main/scheduler/task_manager.py</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>Prevent Dispatcher deadlock when Redis disappears (#14249) This fixes https://github.com/ansible/awx/issues/14245 which has more information about this issue. This change addresses both: - A clashing signal handler (registering a callback to fire when the task manager times out, and hitting that callback in cases where we didn't expect to). Make dispatcher timeout use SIGUSR1, not SIGTERM. - Metrics not being reported should not make us crash, so that is now fixed as well. Signed-off-by: Rick Elrod &lt;rick@elrod.me&gt; Co-authored-by: Alan Rominger &lt;arominge@redhat.com&gt;</MESSAGE>
      <SHA>1d3151ff4a4b6aaaf38e768fdf4d6c40a4a2afe4</SHA>
      <PATCHEDFILES>
        <FILE>awx/main/dispatch/pool.py</FILE>
        <FILE>awx/main/dispatch/worker/base.py</FILE>
        <FILE>awx/main/scheduler/task_manager.py</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
