<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>5465</ISSUENO>
  <ISSUEURL>https://github.com/seaweedfs/seaweedfs/issues/5465</ISSUEURL>
  <TITLE>Internal Server Error and EIO on fuse mount for remote EC shard read failure.</TITLE>
  <DESCRIPTION>**Describe the bug** When a EC shard is compromised on disk, the servers did not try to recover from other shards but gives an 500 internal error. ``` Apr 03 16:13:02 aries-b02 seaweedfs-volume-0[501258]: I0403 16:13:02.645677 store_ec.go:288 read remote ec shard 1201.5 from 10.8.150.91:8080 Apr 03 16:13:02 aries-b02 seaweedfs-volume-0[501258]: I0403 16:13:02.647847 volume_server_handlers_read.go:160 read /1201,0a4a115622d1530d isNormalVolume false error: readbytes: entry not found: offset 27663713352 found id 34313936356436363739353636653636 size -2007146114, expected size 168275 ``` The problem happens 100% with the file id and the shard at `https://s3-haosu.nrp-nautilus.io/ruoxi-bucket/1201.tar`. **System Setup** - `/usr/local/bin/weed server -volume=0 -filer -dir=/weedfs` - `/usr/local/bin/weed volume -max=400 -dir=/weedfs -mserver=10.8.149.13:9333` on 8 machines different from the master - These commands are done by systemctl services with setups exactly as the wiki page. - OS version: Ubuntu 22.04 - output of `weed version`: version 8000GB 3.63 54d7748a4a54d94a31ce04d05db801faeff4f690 linux amd64 - if using filer, show the content of `filer.toml`: no `filer.toml` - security: ``` [access] ui = false [grpc] ca = &quot;/etc/ariesdockerd/certs/Aries_SeaweedFS_CA.crt&quot; [grpc.volume] cert = &quot;/etc/ariesdockerd/certs/volume01.crt&quot; key = &quot;/etc/ariesdockerd/certs/volume01.key&quot; [grpc.master] cert = &quot;/etc/ariesdockerd/certs/master01.crt&quot; key = &quot;/etc/ariesdockerd/certs/master01.key&quot; [grpc.filer] cert = &quot;/etc/ariesdockerd/certs/filer01.crt&quot; key = &quot;/etc/ariesdockerd/certs/filer01.key&quot; [grpc.client] cert = &quot;/etc/ariesdockerd/certs/client01.crt&quot; key = &quot;/etc/ariesdockerd/certs/client01.key&quot; ``` - master: ``` # Put this file to one of the location, with descending priority # ./master.toml # $HOME/.seaweedfs/master.toml # /etc/seaweedfs/master.toml # this file is read by master [master.maintenance] # periodically run these scripts are the same as running them from 'weed shell' scripts = &quot;&quot;&quot; lock ec.encode -fullPercent=95 -quietFor=1h ec.balance -force volume.deleteEmpty -quietFor=24h -force volume.balance -force unlock &quot;&quot;&quot; sleep_minutes = 17 # sleep minutes between each script execution [master.sequencer] type = &quot;raft&quot; # Choose [raft|snowflake] type for storing the file id sequence # when sequencer.type = snowflake, the snowflake id must be different from other masters sequencer_snowflake_id = 0 # any number between 1~1023 # configurations for tiered cloud storage # old volumes are transparently moved to cloud for cost efficiency [storage.backend] [storage.backend.s3.default] enabled = false aws_access_key_id = &quot;&quot; # if empty, loads from the shared credentials file (~/.aws/credentials). aws_secret_access_key = &quot;&quot; # if empty, loads from the shared credentials file (~/.aws/credentials). region = &quot;us-east-2&quot; bucket = &quot;your_bucket_name&quot; # an existing bucket endpoint = &quot;&quot; storage_class = &quot;STANDARD_IA&quot; # create this number of logical volumes if no more writable volumes # count_x means how many copies of data. # e.g.: # 000 has only one copy, copy_1 # 010 and 001 has two copies, copy_2 # 011 has only 3 copies, copy_3 [master.volume_growth] copy_1 = 7 # create 1 x 7 = 7 actual volumes copy_2 = 6 # create 2 x 6 = 12 actual volumes copy_3 = 3 # create 3 x 3 = 9 actual volumes copy_other = 1 # create n x 1 = n actual volumes # configuration flags for replication [master.replication] # any replication counts should be considered minimums. If you specify 010 and # have 3 different racks, that's still considered writable. Writes will still # try to replicate to all available volumes. You should only use this option # if you are doing your own replication or periodic sync of volumes. treat_replication_as_minimums = false ``` **Expected behavior** When the EC shard fails, the volume server should try to recover the shard from other shards.</DESCRIPTION>
  <REPONAME>seaweedfs</REPONAME>
  <TIMEDIFFERENCEDAYS>132</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>update java client to 3.71, also adjust the groupId</MESSAGE>
    <SHA>915f9f50547daae9c046ef0dab38ef768ce3fc0c</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>fix EC reading on nLargeBlockRows fix https://github.com/seaweedfs/seaweedfs/issues/5465</MESSAGE>
      <SHA>3a2e21fee78f932eba19a4d20d0914176cc9f5c8</SHA>
      <PATCHEDFILES>
        <FILE>weed/Makefile</FILE>
        <FILE>weed/storage/erasure_coding/ec_locate.go</FILE>
        <FILE>weed/storage/erasure_coding/ec_test.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
