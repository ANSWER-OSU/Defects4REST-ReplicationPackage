<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>5082</ISSUENO>
  <ISSUEURL>https://github.com/seaweedfs/seaweedfs/issues/5082</ISSUEURL>
  <TITLE>[mount] Increasing RSS memory usage on long term run until killed by OOM ( Possible memory leak )</TITLE>
  <DESCRIPTION>**Describe the bug** I have long-term running weed mount server-wide. The mount has a high intensity usage of creating and deleting files, and the total filesystem size is not big ( 12 GB ) Last month I had a server memory problem, tracked down to the weed mount occupying over 70GB of resident memory, then being killed by the OOM The more a server has a turnover of files, the faster resident memory increases. Currently, on server (A) I restarted the mount @ 2023-11-28 16:54:08 UTC and it reached at the moment of posting 17.8 GB of resident memory, still increasing. The same command, running on another server (B) with no files turnover, restarted @ 2023-11-28 16:54:10 UTC is currently occupying 82 MB Both are connected to the same filer cluster, receiving thus the same metadata updates, which means (B) sees the files that (A) writes Seems that when files are added and deleted, the weed mount process of the server that is writing/deleting data keeps some reference that increases over time Another main difference between (A) and (B) is that the writing server (A) has a continuous loading of .ldb cache: ``` Dec 06 15:53:05 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:53:05.845692 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_1.ldb, watermark 0, num of entries:0 Dec 06 15:53:05 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:53:05.846792 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_1.ldb... , watermark: 0 Dec 06 15:54:06 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:54:06.653019 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_1.ldb, watermark 0, num of entries:0 Dec 06 15:54:06 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:54:06.654767 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_1.ldb... , watermark: 0 Dec 06 15:55:55 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:55:55.970502 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_2.ldb, watermark 0, num of entries:0 Dec 06 15:55:55 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:55:55.971889 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_2.ldb... , watermark: 0 Dec 06 15:56:11 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:56:11.819742 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_0.ldb, watermark 0, num of entries:0 Dec 06 15:56:11 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:56:11.822162 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_0.ldb... , watermark: 0 Dec 06 15:57:47 gpu05.nash01.usa.katapy.io bash[736103]: read old data1 2021617 ns Dec 06 15:58:29 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:58:29.477750 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_0.ldb, watermark 0, num of entries:0 Dec 06 15:58:29 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:58:29.479641 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_0.ldb... , watermark: 0 Dec 06 15:58:50 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:58:50.064610 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_1.ldb, watermark 0, num of entries:0 Dec 06 15:58:50 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:58:50.065699 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_1.ldb... , watermark: 0 Dec 06 15:59:55 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:59:55.289314 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_1.ldb, watermark 0, num of entries:0 Dec 06 15:59:55 gpu05.nash01.usa.katapy.io bash[736103]: I1206 15:59:55.291024 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_1.ldb... , watermark: 0 Dec 06 16:01:54 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:01:54.130863 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_0.ldb, watermark 0, num of entries:0 Dec 06 16:01:54 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:01:54.133372 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_0.ldb... , watermark: 0 Dec 06 16:01:56 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:01:56.315398 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_2.ldb, watermark 0, num of entries:0 Dec 06 16:01:56 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:01:56.319372 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_2.ldb... , watermark: 0 Dec 06 16:03:57 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:03:57.908344 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_0.ldb, watermark 0, num of entries:0 Dec 06 16:03:57 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:03:57.909444 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_0.ldb... , watermark: 0 Dec 06 16:05:02 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:05:02.483789 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_1.ldb, watermark 0, num of entries:0 Dec 06 16:05:02 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:05:02.485513 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_1.ldb... , watermark: 0 Dec 06 16:05:44 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:05:44.096002 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_1.ldb, watermark 0, num of entries:0 Dec 06 16:05:44 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:05:44.097322 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_1.ldb... , watermark: 0 Dec 06 16:07:30 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:07:30.590859 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_2.ldb, watermark 0, num of entries:0 Dec 06 16:07:30 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:07:30.592525 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_2.ldb... , watermark: 0 Dec 06 16:08:00 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:08:00.332572 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_0.ldb, watermark 0, num of entries:0 Dec 06 16:08:00 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:08:00.333664 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c0_2_0.ldb... , watermark: 0 Dec 06 16:09:18 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:09:18.080407 needle_map_leveldb.go:122 generateLevelDbFile /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_0.ldb, watermark 0, num of entries:0 Dec 06 16:09:18 gpu05.nash01.usa.katapy.io bash[736103]: I1206 16:09:18.082657 needle_map_leveldb.go:66 Loading /mnt/KTCACHE/.docker/cache_edge/bc0fc5c1/c1_3_0.ldb... , watermark: 0 ``` **System Setup** - Command: /opt/weed/bin/weed mount -filer=katapy.io:8889,ns1.katapy.io:8889,ns2.katapy.io:8889,ns3.katapy.io:8889 -dir=/mnt/seaweedfs/cache -cacheDir=/mnt/KTCACHE/.docker/cache_edge -cacheCapacityMB=9625 -chunkSizeLimitMB=30 -concurrentWriters=128 -volumeServerAccess=publicUrl -dirAutoCreate=true - OS version: Ubuntu 22 LTS, kernel 5.15.0-50-generic x86_64 - output of `weed version`: version 30GB 3.59 - if using filer, show the content of `filer.toml`: ``` [filer.options] recursive_delete = true [leveldb2] enabled = true dir = &quot;./filerldb2&quot; # directory to store level db files ``` **Expected behavior** Weed mount RSS memory should not increase over time without boundaries until crashing **Screenshots** If applicable, add screenshots to help explain your problem. Server (A) ![image](https://github.com/seaweedfs/seaweedfs/assets/3657228/a7a8f261-a06e-453e-ada8-f95c1dd979d9) Server (B) ![image](https://github.com/seaweedfs/seaweedfs/assets/3657228/1cc468ee-8d8a-459a-b285-ba5a4efb375e) **Additional context** Add any other context about the problem here.</DESCRIPTION>
  <REPONAME>seaweedfs</REPONAME>
  <TIMEDIFFERENCEDAYS>36</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>adjust options</MESSAGE>
    <SHA>c4badf73962f70072cdbb3a83756de7bc5da4b1e</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>correct lock accounting possibly fix https://github.com/seaweedfs/seaweedfs/issues/5082</MESSAGE>
      <SHA>03c4b2e9880765746bd47520d613515336a3f679</SHA>
      <PATCHEDFILES>
        <FILE>weed/util/lock_table.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
