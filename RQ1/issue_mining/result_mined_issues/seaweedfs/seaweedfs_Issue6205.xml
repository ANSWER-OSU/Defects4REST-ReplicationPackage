<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>6205</ISSUENO>
  <ISSUEURL>https://github.com/seaweedfs/seaweedfs/issues/6205</ISSUEURL>
  <TITLE>&quot;no space left&quot; when spreading ec shards leaves volume both sharded and normal</TITLE>
  <DESCRIPTION>**Describe the bug** Sharding sometimes works and sometimes does not. I often see a single volume create the 14 shards and then try to spread them with a single volume server claiming there is no space even though the volume server has plenty of disk space. This one particular volume server is set to a max of 1 because I am planning on removing it. It currently has a single shard on it so maybe that makes it &quot;full&quot; but in that case why is another volume server trying to spread to it? ``` I1104 11:43:10.145247 master_server.go:321 executing: lock [] I1104 11:43:10.145705 master_server.go:321 executing: ec.encode [-fullPercent=95 -quietFor=1h] I1104 11:43:10.396301 volume_layout.go:232 volume 152 are not all writable I1104 11:43:10.396313 volume_layout.go:237 volume 152 remove from writable I1104 15:22:10.778653 command_ec_common.go:98 weedvh8.bm:9334 ec volume 152 deletes shards [6] I1104 15:22:26.611916 command_ec_common.go:98 weedvh8.bm:9334 ec volume 152 deletes shards [7] I1104 15:48:34.764194 command_ec_common.go:98 weedvh8.bm:9334 ec volume 152 deletes shards [5 13] I1104 15:48:35.749174 command_ec_common.go:98 weedvh8.bm:9334 ec volume 152 deletes shards [0 8] I1104 15:48:40.565509 command_ec_common.go:98 weedvh8.bm:9334 ec volume 152 deletes shards [1 9] I1104 15:49:57.006111 command_ec_common.go:98 weedvh8.bm:9334 ec volume 152 deletes shards [4 12] I1104 15:49:57.006668 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:16 DiskType: DestroyTime:0} I1104 15:49:57.021059 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:4096 DiskType: DestroyTime:0} I1104 15:49:57.022428 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:32 DiskType: DestroyTime:0} I1104 15:49:57.022588 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:8192 DiskType: DestroyTime:0} I1104 15:49:57.024654 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:64 DiskType: DestroyTime:0} I1104 15:49:57.026605 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:128 DiskType: DestroyTime:0} I1104 15:49:57.027818 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:1 DiskType: DestroyTime:0} I1104 15:49:57.028437 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:256 DiskType: DestroyTime:0} I1104 15:49:57.055166 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:2 DiskType: DestroyTime:0} I1104 15:49:57.055791 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:512 DiskType: DestroyTime:0} I1104 15:49:57.079995 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:8 DiskType: DestroyTime:0} I1104 15:49:57.080075 topology_ec.go:118 removing ec shard info:&amp;{VolumeId:152 Collection: ShardBits:2048 DiskType: DestroyTime:0} I1104 15:49:57.108112 master_server.go:323 error: spread ec shards for volume 152 from weedvh8.bm:9334: copy 152.[2 10] weedvh8.bm:9334 =&gt; weedvd2.bm:9334 : rpc error: code = Unknown desc = no space left I1104 15:49:57.108146 master_server.go:321 executing: ec.rebuild [-force] I1104 15:49:57.109320 master_server.go:321 executing: ec.balance [-force] I1104 15:49:57.110767 master_server.go:321 executing: volume.deleteEmpty [-quietFor=24h -force] I1104 15:49:57.111379 master_server.go:321 executing: volume.balance [-force] I1104 15:50:12.113367 master_server.go:321 executing: volume.fix.replication [] I1104 15:50:27.119722 master_server.go:321 executing: s3.clean.uploads [-timeAgo=24h] I1104 15:50:27.121091 master_server.go:321 executing: unlock [] ``` **System Setup** - List the command line to start &quot;weed master&quot;, &quot;weed volume&quot;, &quot;weed filer&quot;, &quot;weed s3&quot;, &quot;weed mount&quot;. `weed master -mdir=/root/weedmaster -ip=weedmb.bm -ip.bind=0.0.0.0 -port=9333 -metricsPort=9331 -volumeSizeLimitMB=250000 -volumePreallocate` `weed filer -master=weedmb.bm:9333 -ip=weedmb.bm -ip.bind=0.0.0.0 -port=9335 -metricsPort=9332 -defaultStoreDir=/root -ui.deleteDir=false` `weed volume -ip=weedvd2.bm -ip.bind=0.0.0.0 -mserver=weedmb.bm:9333 -port=9334 -metricsPort=9331 -disk hdd -dir=/mnt/weed -dir.idx=/root/weedvol -max 1` - OS version `6.8.12-3-pve #1 SMP PREEMPT_DYNAMIC PMX 6.8.12-3 (2024-10-23T11:41Z) x86_64 Linux` - output of weed version `version 8000GB 3.79 228946369cad29ee8edc07a42a2e0d218ba16d0b linux amd64` - if using filer, show the content of `filer.toml` ``` [filer.options] master = &quot;weedmb.bm:9333&quot; recursive_delete = false port = 9335 [filer.ui] deleteDir = false [leveldb2] enabled = true dir = &quot;/root/filerldb2&quot; ```</DESCRIPTION>
  <REPONAME>seaweedfs</REPONAME>
  <TIMEDIFFERENCEDAYS>4</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>remove the direct_io flag, as it is not well-supported on macOS</MESSAGE>
    <SHA>113c9ce6a83e0554e73aa881a6eb94690a05a0a7</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>delete aborted ec shards from both source and target servers fix https://github.com/seaweedfs/seaweedfs/issues/6205#issuecomment-2465004586</MESSAGE>
      <SHA>7d04ee1beeb63135f0e814d782c5c4c587b790ef</SHA>
      <PATCHEDFILES>
        <FILE>weed/shell/command_ec_encode.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>delete aborted ec shards from both source and target servers (#6221) fix https://github.com/seaweedfs/seaweedfs/issues/6205#issuecomment-2465004586</MESSAGE>
      <SHA>72b14a451ec44ad700f3e4b35103f7fd2676d8ef</SHA>
      <PATCHEDFILES>
        <FILE>weed/shell/command_ec_encode.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
