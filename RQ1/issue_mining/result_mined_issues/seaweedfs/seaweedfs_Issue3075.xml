<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>3075</ISSUENO>
  <ISSUEURL>https://github.com/seaweedfs/seaweedfs/issues/3075</ISSUEURL>
  <TITLE>Records after ttl continue to be stored in the filer database</TITLE>
  <DESCRIPTION>Filer database: MySql fs.configure ```{ &quot;locations&quot;: [ { &quot;locationPrefix&quot;: &quot;/buckets/s3-test_ttl_10m&quot;, &quot;ttl&quot;: &quot;10m&quot; } ] } ``` - Added file s3-test_ttl_10m/12345 via s3 - Volume deleted itself after 10+ minutes - Now when I try to get the file I get a 50x error and error in the filler logs: ``` failed to stream content /buckets/s3-test_ttl_10m/12345: volume 17 not found``` And the filer's database continues to store information about this file. So, is this functionality not working? Because the database will forever accumulate garbage</DESCRIPTION>
  <REPONAME>seaweedfs</REPONAME>
  <TIMEDIFFERENCEDAYS>20</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge branch 'master' of https://github.com/chrislusf/seaweedfs</MESSAGE>
    <SHA>8902fa6ff653aa40249d2d6da49a9227b63415bb</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>use final destination to resolve fs configuration related to https://github.com/chrislusf/seaweedfs/issues/3075</MESSAGE>
      <SHA>596c3860cac83a75ae9ce728c8a043133c03d098</SHA>
      <PATCHEDFILES>
        <FILE>weed/s3api/s3_constants/s3_actions.go</FILE>
        <FILE>weed/s3api/s3api_object_copy_handlers.go</FILE>
        <FILE>weed/s3api/s3api_object_handlers.go</FILE>
        <FILE>weed/s3api/s3api_object_handlers_postpolicy.go</FILE>
        <FILE>weed/s3api/s3api_object_multipart_handlers.go</FILE>
        <FILE>weed/server/filer_server_handlers_write.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
