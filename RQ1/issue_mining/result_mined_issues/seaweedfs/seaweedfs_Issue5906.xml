<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>5906</ISSUENO>
  <ISSUEURL>https://github.com/seaweedfs/seaweedfs/issues/5906</ISSUEURL>
  <TITLE>volume.vacuum needs warning if replication isn't met</TITLE>
  <DESCRIPTION>**Describe the bug** If you run volume.vacuum, it doesn't do anything to volumes that aren't replicated to the spec (currently only 2 copies when you have 002 set). It just skips them silently **System Setup** - version 30GB 3.67 N/A linux amd64 **Expected behavior** I still think it shouldn't do anything other then let the user know that it would have done those volumes if they were replicated properly # Additional context OK, Story time... I recently started using seaweedfs and migrated from a large 10 drive BTRFS raid1c3 array (3x 14TB and 7x 6TB) that was fairly full, I have about 19TB of data to migrate. BTRFS has the function to remove drives and move their data to other drives. So I started with a couple 6TB that weren't in the array and started removing a couple others while I started coping data to seaweedfs with replication 001. Was still using all the seaweedfs parts manually with tmux windows (I had 17 windows open at one point) , and I had problems with remembering the -encryption flag half way through the migration. Ok, I thought maybe I should actually use the collections feature instead just a huge big vault. so I will just migrate it again later, lets just get it off of the BTRFS array asap. After the migration completed, I used all the 6TB drives I have for this array, for a total of 14 drives, and saved the 14TB drives for a 2nd server in the future. I then change the default and all the volumes from 001 to 002 via master startup config and volume.configure.replication command. I then started to copy the 15TB of media I have into a new seaweedfs mount specifying -collection &quot;Media&quot; with replication 002. I realized I wasn't gonna have enough storage for all of it with the old 2 copies and 3 copies for the new, so I started moving media collection to ensure encoding as I was coping it. Starting to run really tight on storage (in fact, ensure encoding failed because one drive had only 5gb available), and I was finished with a large ~8.5TiB folder of the media so I deleted it on the &quot;&quot; collection and I could not figure out how to crop the drives down, I must have had over 150 volumes with 99% deleted files. I spent several days searching around and found that you need to run volume.vacuum, but I couldn't get any output and it ran almost instantly, I could not for the life of me figure out what was wrong. Turns out volume.vacuum only started to work when I did ~0.01 or less because it was doing volumes from the media collection that were caught up on replication and had a couple deleted files from when I was migrating media and had to stop midway, this was the first time I saw volume.vacuum actually work, so now that I know it wasn't some weird edge case that with vacuum.enable/disable breaking a scanning service or something. (keep in mind this was my first time playing with seaweedfs) I was like it worked with -collection &quot;Media&quot;, maybe it doesn't work with -collection &quot;&quot; or something, then I remember I haven't run volume.fix.replication yet because I planned to migrate all the data off into different pools. so I changed replication for collection “” back to 001 and did a vacuum for .99 and got like 6+tb freed in about 2 minutes, did it again for .9 and 10 minutes another tb or so was freed</DESCRIPTION>
  <REPONAME>seaweedfs</REPONAME>
  <TIMEDIFFERENCEDAYS>1</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Allow using a PVC to store filer and master logs - changes in values.yaml (#5918)</MESSAGE>
    <SHA>ad5a62578147605e27efc282efef39153b8d1794</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>add warning for not enough copies when skipping vacuuming volumes fix https://github.com/seaweedfs/seaweedfs/issues/5906</MESSAGE>
      <SHA>b3696024d118fd13aff0fc5e94010ee3d17d4dc9</SHA>
      <PATCHEDFILES>
        <FILE>weed/topology/topology_vacuum.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
