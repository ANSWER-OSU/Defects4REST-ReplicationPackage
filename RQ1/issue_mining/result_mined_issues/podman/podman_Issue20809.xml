<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>20809</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/20809</ISSUEURL>
  <TITLE>`save transaction: database is locked` on `podman exec`</TITLE>
  <DESCRIPTION>### Issue Description Upgraded to `podman 4.8.0`, did a `system reset` and took `sqlite` db for a spin. Started creation of 3 independent containers in parallel followed by some setup steps done with `podman exec` and one such invocation ended up with: ```console ERRO[0008] Container 68e95ee6822e1975764367770b5f9def2717de18a0624d71f1c857809303343d exec session cda744d36c8c72ce7f701cad851001f22e32432855dae12f1cda745bf4372aea error: saving container 68e95ee6822e1975764367770b5f9def2717de18a0624d71f1c857809303343d state: beginning container 68e95ee6822e1975764367770b5f9def2717de18a0624d71f1c857809303343d save transaction: database is locked ``` ### Steps to reproduce the issue No reproducible steps I'm afraid. I assume it's a timing issue. ### Describe the results you received `podman exec` failed once with: ```console ERRO[0008] Container 68e95ee6822e1975764367770b5f9def2717de18a0624d71f1c857809303343d exec session cda744d36c8c72ce7f701cad851001f22e32432855dae12f1cda745bf4372aea error: saving container 68e95ee6822e1975764367770b5f9def2717de18a0624d71f1c857809303343d state: beginning container 68e95ee6822e1975764367770b5f9def2717de18a0624d71f1c857809303343d save transaction: database is locked ``` ### Describe the results you expected No error should be present. ### podman info output ```yaml host: arch: arm64 buildahVersion: 1.33.2 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: Unknown path: /usr/bin/conmon version: 'conmon version 2.1.8, commit: 9aa8067313b1282097df84ab1a218149999397e4' cpuUtilization: idlePercent: 98.65 systemPercent: 1.05 userPercent: 0.29 cpus: 8 databaseBackend: sqlite distribution: distribution: pld version: &quot;3.0&quot; eventLogger: journald freeLocks: 2045 hostname: rock idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.10.110-8-rockchip linkmode: dynamic logDriver: journald memFree: 9260699648 memTotal: 16487534592 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: Unknown path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.9.0 package: Unknown path: /usr/libexec/podman/netavark version: netavark 1.9.0 ociRuntime: name: crun package: Unknown path: /usr/bin/crun version: |- crun version 1.12 commit: ce429cb2e277d001c2179df1ac66a470f00802ae rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux pasta: executable: &quot;&quot; package: &quot;&quot; version: &quot;&quot; remoteSocket: exists: false path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: &quot;&quot; selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: Unknown version: |- slirp4netns version 1.2.1 commit: 09e31e92fa3d2a1d3ca261adaeb012c8d75a8194 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 8096051200 swapTotal: 8242851840 uptime: 1284h 47m 38.00s (Approximately 53.50 days) variant: v8 plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /home/users/jan/.config/containers/storage.conf containerStore: number: 3 paused: 0 running: 3 stopped: 0 graphDriverName: btrfs graphOptions: {} graphRoot: /mnt/build-storage/jan/containers graphRootAllocated: 274877906944 graphRootUsed: 12658704384 graphStatus: Build Version: Btrfs v6.6.2 Library Version: &quot;102&quot; imageCopyTmpDir: /home/users/jan/tmp imageStore: number: 3 runRoot: /run/user/1000/containers transientStore: false volumePath: /mnt/build-storage/jan/containers/volumes version: APIVersion: 4.8.0 Built: 1701166825 BuiltTime: Tue Nov 28 11:20:25 2023 GitCommit: bf038720ca8b9301a3aa80966759567c41199dd0 GoVersion: go1.21.4 Os: linux OsArch: linux/arm64 Version: 4.8.0 ``` ### Podman in a container No ### Privileged Or Rootless Rootless ### Upstream Latest Release Yes ### Additional environment details _No response_ ### Additional information _No response_</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>1</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #20595 from containers/renovate/github.com-gorilla-schema-1.x fix(deps): update module github.com/gorilla/schema to v1.2.1</MESSAGE>
    <SHA>06c41b614db11382579ff2931b9dd145f241b485</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>sqlite: set busy timeout to 100s Only one process can write to the sqlite db at the same time, if another process tries to use it at that time it fails and a database is locked error is returned. If this happens sqlite should keep retrying until it can write. To do that we can just set the _busy_timeout option. A 100s timeout should be enough even on slower systems but not to much in case there is a deadlock so it still returns in a reasonable time. [NO NEW TESTS NEEDED] I think we strongly need to consider some form of parallel stress testing to catch bugs like this. Fixes #20809 Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>5b3d82f9bcb0170aa1fcb8695f101dbb4db898b5</SHA>
      <PATCHEDFILES>
        <FILE>libpod/sqlite_state.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
