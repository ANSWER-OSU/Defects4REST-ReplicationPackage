<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>20342</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/20342</ISSUEURL>
  <TITLE>podman-next regression: no healthcheck events for root (system) containers (works for rootless/user)</TITLE>
  <DESCRIPTION>### Issue Description A few days ago, a lot of cockpit &quot;revdeps&quot; tests started to fail `testHealthcheckSystem` in podman PRs. Unfortunately that is somewhat of a race condition, so we didn't spot it in the PR that introduced it (or it was ignored). But there's a real regression in podman-next: There are no health check events any more for system containers. ### Steps to reproduce the issue Steps to reproduce the issue: As root, do: 1. Update to podman-next COPR: `dnf -y copr enable rhcontainerbot/podman-next; dnf update --repo='copr*' -y` 2. `podman events` 3. `podman run --name sick -dt --health-cmd false --health-interval 5s quay.io/prometheus/busybox` ### Describe the results you received ``` 2023-10-12 04:51:15.437935694 +0000 UTC image pull 0d0e5ba3b6094b36fc6e2bcee8e9105103e35320db1b25af4def231d86e8362a quay.io/prometheus/busybox 2023-10-12 04:51:15.455042263 +0000 UTC container create 3531fefe3fc75b87f3723782b2700f497dfa8fc65369e9c298c98d92f08e1017 (image=quay.io/prometheus/busybox:latest, name=sick) 2023-10-12 04:51:15.698294296 +0000 UTC container init 3531fefe3fc75b87f3723782b2700f497dfa8fc65369e9c298c98d92f08e1017 (image=quay.io/prometheus/busybox:latest, name=sick) 2023-10-12 04:51:15.718114831 +0000 UTC container start 3531fefe3fc75b87f3723782b2700f497dfa8fc65369e9c298c98d92f08e1017 (image=quay.io/prometheus/busybox:latest, name=sick) ``` And that's it -- no `health_status` events whatsoever. `podman ps` does show it as unhealthy correctly, though: ``` 3531fefe3fc7 quay.io/prometheus/busybox:latest sh 27 seconds ago Up 28 seconds (unhealthy) sick ``` ### Describe the results you expected As non-root user, or with podman-4.7.0-1.fc38.x86_64 I get the `health_status` events: ``` 2023-10-12 04:42:43.670309886 +0000 UTC container create 66ccd69c75b7b526b71e961b18cb4b3e251ace6a2e9a856d5ec1d7b81e08fab8 (image=quay.io/prometheus/busybox:latest, name=sick) 2023-10-12 04:42:43.736600571 +0000 UTC container init 66ccd69c75b7b526b71e961b18cb4b3e251ace6a2e9a856d5ec1d7b81e08fab8 (image=quay.io/prometheus/busybox:latest, name=sick) 2023-10-12 04:42:43.657011254 +0000 UTC image pull 0d0e5ba3b6094b36fc6e2bcee8e9105103e35320db1b25af4def231d86e8362a quay.io/prometheus/busybox 2023-10-12 04:42:43.7530451 +0000 UTC container start 66ccd69c75b7b526b71e961b18cb4b3e251ace6a2e9a856d5ec1d7b81e08fab8 (image=quay.io/prometheus/busybox:latest, name=sick) 2023-10-12 04:42:43.85413455 +0000 UTC container health_status 66ccd69c75b7b526b71e961b18cb4b3e251ace6a2e9a856d5ec1d7b81e08fab8 (image=quay.io/prometheus/busybox:latest, name=sick, health_status=starting) 2023-10-12 04:42:49.928718548 +0000 UTC container health_status 66ccd69c75b7b526b71e961b18cb4b3e251ace6a2e9a856d5ec1d7b81e08fab8 (image=quay.io/prometheus/busybox:latest, name=sick, health_status=starting) 2023-10-12 04:42:55.916728557 +0000 UTC container health_status 66ccd69c75b7b526b71e961b18cb4b3e251ace6a2e9a856d5ec1d7b81e08fab8 (image=quay.io/prometheus/busybox:latest, name=sick, health_status=unhealthy) 2023-10-12 04:43:01.923989624 +0000 UTC container health_status 66ccd69c75b7b526b71e961b18cb4b3e251ace6a2e9a856d5ec1d7b81e08fab8 (image=quay.io/prometheus/busybox:latest, name=sick, health_status=unhealthy) 2024-10-12 04:43:07.910683716 +0000 UTC container health_status 66ccd69c75b7b526b71e961b18cb4b3e251ace6a2e9a856d5ec1d7b81e08fab8 (image=quay.io/prometheus/busybox:latest, name=sick, health_status=unhealthy) [...] ``` ### podman info output ```yaml host: arch: amd64 buildahVersion: 1.32.0 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.7-2.fc38.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 97.59 systemPercent: 1.18 userPercent: 1.23 cpus: 1 databaseBackend: boltdb distribution: distribution: fedora variant: cloud version: &quot;38&quot; eventLogger: journald freeLocks: 2047 hostname: fedora-38-127-0-0-2-2201 idMappings: gidmap: null uidmap: null kernel: 6.5.5-200.fc38.x86_64 linkmode: dynamic logDriver: journald memFree: 220635136 memTotal: 1123704832 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.8.0-1.20231011141439033331.main.4.g174a230.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.9.0-dev package: netavark-1.8.0-1.20231011140905546466.main.10.gbd17508.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.9.0-dev ociRuntime: name: crun package: crun-1.9.2-1.20231010065724094862.main.5.g75009ce.x86_64 path: /usr/bin/crun version: |- crun version UNKNOWN commit: 13559a1aa9c765eb1289db74ed8728b27ffbfd3d rundir: /run/user/0/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20230908.g05627dc-1.fc38.x86_64 version: | pasta 0^20230908.g05627dc-1.fc38.x86_64 Copyright Red Hat GNU General Public License, version 2 or later &lt;https://www.gnu.org/licenses/old-licenses/gpl-2.0.html&gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.1-1.fc38.x86_64 version: |- slirp4netns version 1.2.1 commit: 09e31e92fa3d2a1d3ca261adaeb012c8d75a8194 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 1109913600 swapTotal: 1123020800 uptime: 13h 22m 30.00s (Approximately 0.54 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 1 paused: 0 running: 1 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 12798898176 graphRootUsed: 2277593088 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: &quot;false&quot; Supports d_type: &quot;true&quot; Supports shifting: &quot;true&quot; Supports volatile: &quot;true&quot; Using metacopy: &quot;true&quot; imageCopyTmpDir: /var/tmp imageStore: number: 3 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.8.0-dev-d437ca8fd Built: 1697032510 BuiltTime: Wed Oct 11 13:55:10 2023 GitCommit: &quot;&quot; GoVersion: go1.20.8 Os: linux OsArch: linux/amd64 Version: 4.8.0-dev-d437ca8fd ``` ### Podman in a container No ### Privileged Or Rootless Privileged ### Upstream Latest Release Yes ### Additional environment details Fedora 38 x86_64 cloud image ### Additional information This is sometimes hidden by some random other event happening which will trigger an UI update. Plus, there is a bug in cockpit-podman which sometimes makes the test fail even earlier, I am fixing that in https://github.com/cockpit-project/cockpit-podman/pull/1447</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>0</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #20340 from baude/rtd [CI:DOCS]rtd: implement v2 build file</MESSAGE>
    <SHA>8de7c48df599c92726831f3b115a58c524622be9</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>healthcheck: make sure to always show health_status events This fixes a regression caused by commit 7e6e267329, unfortunately this was not caught during review as for some reason this works fine rootless and only fails as root. Because we set the systemd log level to notice in order to hide the unit started/stopped messages to prevent spamming the journal the issue is that this now also causes systemd to ignore the events we write to journald as we also send them as info level. To fix this we simply send health_status events now on notice level. I decided against sending all events on notice as I think info is fine for them. Whenever the notice level is right is of course debatable but given it may contain the unhealthy message I think having this a notice should be ok. The main reason this made it through testing is because we do not rely on the systemd unit to fire healthchecks in the tests as this is flaky. There is one test were we rely on it though and I added a check there to make sure events are displayed correctly when trigger via systemd. Fixes #20342 Signed-off-by: Paul Holzinger &lt;pholzing@redhat.com&gt;</MESSAGE>
      <SHA>99a14332ef2f29e9ac8243fe7e7a465bc7893207</SHA>
      <PATCHEDFILES>
        <FILE>libpod/events/journal_linux.go</FILE>
        <FILE>test/system/220-healthcheck.bats</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
