<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>20000</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/20000</ISSUEURL>
  <TITLE>feedback on new podman module capability</TITLE>
  <DESCRIPTION>### Feature request description Thanks podman devs, especially @vrothberg and @rhatdan, for your work on implementing the new `module` capability. We did a little initial testing and just wanted to share some feedback and ask a few questions. cc @scanon @danfulton Our goal would be to use this instead of the custom module functionality we developed for podman-hpc. I'm working with our `gpu` and `mpich` modules as examples. ## gpu testing First, working from our [gpu module](https://github.com/NERSC/podman-hpc/blob/main/etc/modules.d/gpu.yaml), I did the following test. ``` podman pull docker.io/nvidia/cuda:12.2.0-devel-ubuntu20.04 export CONTAINERS_CONF=/global/homes/s/stephey/.config/containers/containers.conf.modules stephey@perlmutter:login32:~/.config/containers&gt; cat containers.conf.modules [containers] mounts=[ &quot;type=glob,source=/usr/lib64/libnv*,destination=/usr/lib64,ro&quot;, &quot;type=glob,source=/usr/lib64/libcuda*,destination=/usr/lib64,ro&quot;, &quot;type=glob,source=/opt/cray/pe/mpich/default/gtl/lib/libmpi_gtl_cuda*,destination=/usr/lib64,ro&quot;, &quot;type=bind,source=/usr/bin/nvidia-smi,destination=/usr/bin/nvidia-smi,ro&quot;, &quot;type=bind,source=/dev/nvidiactl,destination=/dev/nvidiactl,ro&quot;, &quot;type=glob,source=/dev/nvidia*,destination=/dev,ro&quot;, &quot;type=bind,source=/dev/nvidia-uvm,destination=/dev/nvidia-uvm,ro&quot;, &quot;type=bind,source=/dev/nvidia-uvm-tools,destination=/dev/nvidia-uvm-tools,ro&quot; ] env=[&quot;LD_LIBRARY_PATH=/usr/lib64&quot;] env=[&quot;MODULE_TEST=hello&quot;] env=[&quot;NVIDIA_VISIBLE_DEVICES&quot;] stephey@perlmutter:login32:~/.config/containers&gt; ``` I've had trouble setting more than one environment variable. What is the right way to do this? I've tried ``` env=[&quot;LD_LIBRARY_PATH=/usr/lib64&quot;,&quot;MODULE_TEST=hello&quot;] ``` and ``` env=[&quot;LD_LIBRARY_PATH=/usr/lib64&quot;] env=[&quot;MODULE_TEST=hello&quot;] ``` but it seems like it only keeps the most recently set variable. **Feature request: make it possible to set more than one environment variable** Next, it's not clear to me how to copy in an environment variable that is set on the host into the container. For example, we'll want to copy in `NVIDIA_VISIBLE_DEVICES` from the host. **Feature request: make it possible to copy specific environment variables set on the host** Next, it's really great that `mount` has a glob option and I was pleased with how well it works, but to make sure the cuda driver works correctly, we also need to preserve the symlinks between the drivers. I don't know if that's possible with mount. **Feature request: add ability for mount to preserve symlinks.** Very basic gpu functionality seems to work while using the module: ``` stephey@perlmutter:login28:/global/common/shared/das/podman-4.7.0&gt; podman run --rm -it docker.io/nvidia/cuda:12.2.0-devel-ubuntu20.04 bash WARN[0000] Network file system detected as backing store. Enforcing overlay option `force_mask=&quot;700&quot;`. Add it to storage.conf to silence this warning WARN[0000] Path &quot;/etc/SUSEConnect&quot; from &quot;/etc/containers/mounts.conf&quot; doesn't exist, skipping WARN[0000] Path &quot;/etc/zypp/credentials.d/SCCcredentials&quot; from &quot;/etc/containers/mounts.conf&quot; doesn't exist, skipping ========== == CUDA == ========== CUDA Version 12.2.0 Container image Copyright (c) 2016-2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available. Use the NVIDIA Container Toolkit to start this container with GPU support; see https://docs.nvidia.com/datacenter/cloud-native/ . root@12047d54b6b8:/# nvidia-smi Fri Sep 15 23:24:36 2023 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 525.105.17 Driver Version: 525.105.17 CUDA Version: N/A | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA A100-PCI... Off | 00000000:C3:00.0 Off | 0 | | N/A 35C P0 38W / 250W | 38472MiB / 40960MiB | 0% Default | | | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ root@12047d54b6b8:/# ``` I think the reason the CUDA Version is N/A is because we don't have the drivers properly symlinked. But it's not bad! ## MPI thoughts Next, working from our [mpich module](https://github.com/NERSC/podman-hpc/blob/main/etc/modules.d/mpich.yaml), I didn't test anything, but @scanon and I looked through [containers.conf](https://github.com/containers/common/blob/main/docs/containers.conf.5.md) to see if it supports the capabilities we need. Does `containers.conf` support setting the privileged flag? That is one of the capabilities we currently set in our mpich module. Apologies if it does and we missed it. **Feature request: support setting privileged flag** Is [env-merge](https://docs.podman.io/en/latest/markdown/podman-run.1.html#env-merge-env) supported in `containers.conf` We'd need that to be able to append to `LD_LIBRARY_PATH`. **Feature request: support env-merge** Finally, another difficulty is that we initialize our `shared-run` mode for MPI in our current mpich module. I'm not sure if this would be possible in the podman module configuration. ## General feedback It would be really nice to have a way to easily toggle the individual modules on and off. For example, users might want both the `gpu` and `mpich` modules, but they may not want the `cvmfs` module that we also offer. Right now it seems like they could stack `containers.module.conf` files, but it would be really nice if they could do it via the CLI or in some other more user-friendly way. For example, `podman run --module=foo --module=bar`. Hopefully this initial feedback is helpful. Thanks for your work on this. ### Suggest potential solution A clear and concise description of what you want to happen. ### Have you considered any alternatives? A clear and concise description of any alternative solutions or features you've considered. ### Additional context Add any other context or screenshots about the feature request here.</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>600</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #19987 from edsantiago/xref_filters man page crossrefs: add --filter autocompletes</MESSAGE>
    <SHA>d912e735a30ab89e3b9522338afa5f89a73a85f1</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>error when --module is specified on the command level The --module can only be parsed on the root level. It cannot work on the command level, because it must be &quot;manually&quot; parsed on init() to make sure the specified configuration files/modules are loaded prior to parsing the flags via Cobra. Hence move --module from the &quot;persistent&quot; to the &quot;local&quot; flags which will yield an error instead of doing nothing when being specified on the command level: ``` $ ./bin/podman run --module=foo.conf --rm alpine Error: unknown flag: --module See 'podman run --help' ``` Reported in #20000. Signed-off-by: Valentin Rothberg &lt;vrothberg@redhat.com&gt;</MESSAGE>
      <SHA>0b7142f4a446463e626aff983feca16f816df698</SHA>
      <PATCHEDFILES>
        <FILE>cmd/podman/registry/config.go</FILE>
        <FILE>cmd/podman/root.go</FILE>
        <FILE>docs/source/markdown/podman.1.md</FILE>
        <FILE>test/system/800-config.bats</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>--env-host: use default from containers.conf As found while working on #20000, the `--env-host` flag should use the default from containers.conf. Add a new &quot;supported fields&quot; test to the system tests to make sure we have a goto test for catching such regressions. I suspect more flags to not use the defaults from containers.conf. Signed-off-by: Valentin Rothberg &lt;vrothberg@redhat.com&gt;</MESSAGE>
      <SHA>41beb53744b308b6130575b4c2c46981ea5e1bbb</SHA>
      <PATCHEDFILES>
        <FILE>cmd/podman/common/create.go</FILE>
        <FILE>test/system/800-config.bats</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
