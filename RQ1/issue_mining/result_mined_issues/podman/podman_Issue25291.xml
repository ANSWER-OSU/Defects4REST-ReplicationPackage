<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>25291</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/25291</ISSUEURL>
  <TITLE>Rootless container libpod/tmp/persist directories not cleaned up, fill up tmpfs</TITLE>
  <DESCRIPTION>### Issue Description We're running Quadlet-based rootless Python/Django containers on separate test and production servers on Ubuntu 22.04, AMD64, with Podman and its dependencies built from source or downloaded as release binaries from GitHub, as applicable. Today, upon doing a CI deploy to the test server, the job failed with this error: ``` Error: writing to file &quot;/run/user/1001/containers/auth.json&quot;: open /run/user/1001/containers/.tmp-auth.json3577526995: no space left on device ``` The part of the CI job that failed was a container registry login. I went to check on the server and saw that the tmpfs at `/run/user/1001` (the UID under which the rootless containers run) was halfway full, occupying about 340 MB. I don't know why the error said that space had run out, but then I don't know the inner workings of tmpfs. The normal space usage should be in the kilobytes, not the hundreds of MBs, so there was clearly a problem. Looking closer, the directory `/run/user/1001/libpod/tmp/persist` had tens of thousands of directories with 64-character hexadecimal names, corresponding to current or past container IDs of our application user. Nearly all of them contained a single 1-byte file called `exit` with the 0 character, and nothing else. Stopping containers, deleting the dirs and starting containers back up again worked, and the CI job was retried succesfully. We normally run ten containers on the server 24/7. Upon starting up, all these containers gained a directory there corresponding to their ID. None of them had `exit` files, which made sense as none had yet exited. When I stopped a container, the `exit` file would appear, and the directory would stick around. Nothing seemed to be cleaning it up. No error was printed in the `journalctl` output of the container service regarding the inability to clean up the directory. Worryingly, more directories and `exit` files kept being created at a constant rate without me restarting any containers. Then I remembered that this server is running several containers that are started up via systemd timers to act as cron jobs. They all run successfully to completion based on their `journalctl --user -u` output, corresponding to 0 in the `exit` files. And it's these that are really filling up the tmpfs, as they get run hundreds of times every day. This is only happening on the test server, not production, despite both running the same containers, including systemd-timed containers, the same OS version and an application user configured the same way. The salient difference is that the test server is running nearer up-to-date versions of Podman + dependencies whereas production has older versions. So it seems that some regression has been introduced in one of the components since when Podman 4.8.3 was current. **Test server, exhibiting the issue**: - Podman 5.2.1 - conmon 2.1.12 - netavark 1.12.2 - aardvark-dns 1.12.1 - crun 1.16.1 **Production server, NOT exhibiting the issue**: - Podman 4.8.3 - conmon 2.1.10 - netavark 1.9.0 - aardvark-dns 1.9.0 - crun 1.12 ### Steps to reproduce the issue I don't know if this is universally reproducible outside our environment, but: 1. Run a rootless Quadlet-based container as an unprivileged user with user mode systemd on a Ubuntu 22.04 AMD64 server with Podman 5.2.1 2. Watch as dirs and files accumulate under /run/user/[uid]/libpod/tmp/persist, corresponding to the IDs of the containers even after their exit, eventually filling the tmpfs. ### Describe the results you received Containers have their `/run/user/[uid]/libpod/tmp/persist/[container ID]` tmpfs dirs left over after exiting successfully (exit code 0). ### Describe the results you expected Any directories and files created under `/run/user/[uid]/libpod/tmp/persist` would get cleaned up as containers exit. ### podman info output ```yaml Note: this is from the test server. The production server didn't seem to have relevant differences outside component versions. host: arch: amd64 buildahVersion: 1.37.1 cgroupControllers: - cpuset - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: Unknown path: /usr/libexec/podman/conmon version: 'conmon version 2.1.12, commit: unknown' cpuUtilization: idlePercent: 92.85 systemPercent: 1.15 userPercent: 6 cpus: 4 databaseBackend: boltdb distribution: codename: jammy distribution: ubuntu version: &quot;22.04&quot; eventLogger: journald freeLocks: 2037 hostname: &lt;redacted&gt;-staging idMappings: gidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 165536 size: 65536 uidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 165536 size: 65536 kernel: 5.15.0-130-generic linkmode: dynamic logDriver: journald memFree: 1377988608 memTotal: 8322985984 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: Unknown path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.12.1 package: Unknown path: /usr/libexec/podman/netavark version: netavark 1.12.2 ociRuntime: name: crun package: Unknown path: /usr/bin/crun version: |- crun version 1.16.1 commit: afa829ca0122bd5e1d67f1f38e6cc348027e3c32 rundir: /run/user/1001/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux pasta: executable: /usr/local/bin/pasta package: Unknown version: | pasta unknown version Copyright Red Hat GNU General Public License, version 2 or later &lt;https://www.gnu.org/licenses/old-licenses/gpl-2.0.html&gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: false path: /run/user/1001/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: &quot;&quot; selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: &quot;&quot; package: &quot;&quot; version: &quot;&quot; swapFree: 312365056 swapTotal: 536866816 uptime: 1187h 1m 45.00s (Approximately 49.46 days) variant: &quot;&quot; plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: docker.io: Blocked: false Insecure: false Location: docker.io MirrorByDigestOnly: false Mirrors: null Prefix: docker.io PullFromMirror: &quot;&quot; search: - docker.io store: configFile: /home/appuser/.config/containers/storage.conf containerStore: number: 10 paused: 0 running: 10 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/appuser/.local/share/containers/storage graphRootAllocated: 168488570880 graphRootUsed: 37740863488 graphStatus: Backing Filesystem: extfs Native Overlay Diff: &quot;true&quot; Supports d_type: &quot;true&quot; Supports shifting: &quot;false&quot; Supports volatile: &quot;true&quot; Using metacopy: &quot;false&quot; imageCopyTmpDir: /var/tmp imageStore: number: 19 runRoot: /tmp/containers-user-1001/containers transientStore: false volumePath: /home/appuser/.local/share/containers/storage/volumes version: APIVersion: 5.2.1 Built: 1724236924 BuiltTime: Wed Aug 21 13:42:04 2024 GitCommit: &quot;&quot; GoVersion: go1.22.5 Os: linux OsArch: linux/amd64 Version: 5.2.1 ``` ### Podman in a container No ### Privileged Or Rootless Rootless ### Upstream Latest Release No ### Additional environment details Linode VPS. ### Additional information _No response_</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>0</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #25280 from containers/renovate/golang.org-x-net-0.x fix(deps): update module golang.org/x/net to v0.35.0</MESSAGE>
    <SHA>7e1ac1db4d3482bdf5e477d9a78b24676cb6f1e1</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>Remove persist directory when cleaning up Conmon files This seems to have been added as part of the cleanup of our handling of OOM files, but code was never added to remove it, so we leaked a single directory with an exit file and OOM file per container run. Apparently have been doing this for a while - I'd guess since March of '23 - so I'm surprised more people didn't notice. Fixes #25291 Signed-off-by: Matt Heon &lt;mheon@redhat.com&gt;</MESSAGE>
      <SHA>ce8813dc8d93f982634a88bb3cc54d334b5ebe69</SHA>
      <PATCHEDFILES>
        <FILE>libpod/container_internal.go</FILE>
        <FILE>libpod/oci.go</FILE>
        <FILE>libpod/oci_conmon_common.go</FILE>
        <FILE>libpod/oci_missing.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>Remove persist directory when cleaning up Conmon files This seems to have been added as part of the cleanup of our handling of OOM files, but code was never added to remove it, so we leaked a single directory with an exit file and OOM file per container run. Apparently have been doing this for a while - I'd guess since March of '23 - so I'm surprised more people didn't notice. Fixes #25291 Signed-off-by: Matt Heon &lt;mheon@redhat.com&gt;</MESSAGE>
      <SHA>fd5eb1fdf31e2bbea0544f46c40f3ba7a3ffd74e</SHA>
      <PATCHEDFILES>
        <FILE>libpod/container_internal.go</FILE>
        <FILE>libpod/oci.go</FILE>
        <FILE>libpod/oci_conmon_common.go</FILE>
        <FILE>libpod/oci_missing.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>[v5.4-rhel] Remove persist directory when cleaning up Conmon files This seems to have been added as part of the cleanup of our handling of OOM files, but code was never added to remove it, so we leaked a single directory with an exit file and OOM file per container run. Apparently have been doing this for a while - I'd guess since March of '23 - so I'm surprised more people didn't notice. Fixes #25291 Fixes: https://issues.redhat.com/browse/RHEL-86544, https://issues.redhat.com/browse/RHEL-86550 Signed-off-by: Matt Heon &lt;mheon@redhat.com&gt; Signed-off-by: tomsweeneyredhat &lt;tsweeney@redhat.com&gt;</MESSAGE>
      <SHA>7b53a64c3c6eecfb9542e81d0250cb9684fd51e0</SHA>
      <PATCHEDFILES>
        <FILE>libpod/container_internal.go</FILE>
        <FILE>libpod/oci.go</FILE>
        <FILE>libpod/oci_conmon_common.go</FILE>
        <FILE>libpod/oci_missing.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
    <COMMIT>
      <MESSAGE>[v5.2-rhel] Remove persist directory when cleaning up Conmon files This seems to have been added as part of the cleanup of our handling of OOM files, but code was never added to remove it, so we leaked a single directory with an exit file and OOM file per container run. Apparently have been doing this for a while - I'd guess since March of '23 - so I'm surprised more people didn't notice. Fixes #25291 Fixes: https://issues.redhat.com/browse/RHEL-86866 Signed-off-by: Matt Heon &lt;mheon@redhat.com&gt; Signed-off-by: tomsweeneyredhat &lt;tsweeney@redhat.com&gt;</MESSAGE>
      <SHA>bf1758ae487887a1e6ac0670a4b9418e85a6d688</SHA>
      <PATCHEDFILES>
        <FILE>libpod/container_internal.go</FILE>
        <FILE>libpod/oci.go</FILE>
        <FILE>libpod/oci_conmon_common.go</FILE>
        <FILE>libpod/oci_missing.go</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
