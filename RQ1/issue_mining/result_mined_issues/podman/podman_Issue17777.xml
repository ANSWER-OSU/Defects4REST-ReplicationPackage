<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>17777</ISSUENO>
  <ISSUEURL>https://github.com/containers/podman/issues/17777</ISSUEURL>
  <TITLE>--health-on-failure=restart doesn't restart container?</TITLE>
  <DESCRIPTION>### Issue Description This is an RFI and potentially a bug report. I've been working on setting up health checks for our podman containers and have followed the instructions on this page: [https://www.redhat.com/sysadmin/podman-edge-healthcheck](url) It's mentioned that one of the --health-on-failure= options is restart so I tried it rather than kill which is given in the example. However, it never appears to restart the container when the current one is set to unhealthy, is this a bug OR am I not using the option correctly? $ podman run --replace -d --name test-container --health-cmd /healthcheck --health-on-failure=restart --health-retries=1 health-check-action When I use the kill option this works, as does none from recollection. Some example commands below: ``` [gary@myServer tmp.cvo0HuLSA9]# podman run --replace -d --name test-container --health-cmd /healthcheck --health-on-failure=kill --health-retries=1 health-check-actions e0676d33a91dac37da670c3b45f2478e902186055ee4e7d0c02bc3b0843f3a95 d9a2e0114df37b32e5dab9fb8a258ecfbcf18263913acad2ef1a6e4592e6728c [gary@myServer tmp.cvo0HuLSA9]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d9a2e0114df3 localhost/health-check-actions:latest /entrypoint 2 seconds ago Up 2 seconds ago (healthy) test-container [gary@myServer tmp.cvo0HuLSA9]# podman exec test-container touch /uh-oh [gary@myServer tmp.cvo0HuLSA9]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d9a2e0114df3 localhost/health-check-actions:latest /entrypoint 18 seconds ago Up 19 seconds ago (healthy) test-container [gary@myServer tmp.cvo0HuLSA9]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d9a2e0114df3 localhost/health-check-actions:latest /entrypoint 21 seconds ago Up 21 seconds ago (healthy) test-container [gary@myServer tmp.cvo0HuLSA9]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d9a2e0114df3 localhost/health-check-actions:latest /entrypoint 34 seconds ago Exited (137) 3 seconds ago (unhealthy) test-container [gary@myServer tmp.cvo0HuLSA9]# podman run --replace -d --name test-container --health-cmd /healthcheck --health-on-failure=none --health-retries=1 health-check-actions d9a2e0114df37b32e5dab9fb8a258ecfbcf18263913acad2ef1a6e4592e6728c c0bc0b2668b5dc6c2c269e965f013626854d18320949f549d78eb2a968f84339 [gary@myServer tmp.cvo0HuLSA9]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c0bc0b2668b5 localhost/health-check-actions:latest /entrypoint 2 seconds ago Up 3 seconds ago (healthy) test-container [gary@myServer tmp.cvo0HuLSA9]# podman exec test-container touch /uh-oh [gary@myServer tmp.cvo0HuLSA9]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c0bc0b2668b5 localhost/health-check-actions:latest /entrypoint 15 seconds ago Up 16 seconds ago (healthy) test-container [gary@myServer tmp.cvo0HuLSA9]# podman healthcheck run test-container unhealthy [gary@myServer tmp.cvo0HuLSA9]# podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c0bc0b2668b5 localhost/health-check-actions:latest /entrypoint 36 seconds ago Up 36 seconds ago (unhealthy) test-container [gary@myServer tmp.cvo0HuLSA9]# ``` ### Steps to reproduce the issue Steps to reproduce the issue 1. I followed the steps as per the RedHat web page given above. 2. 3. ### Describe the results you received The container wasn't restarted as expected. ### Describe the results you expected I expected the container to be restarted and in a healthy state. ### podman info output ```yaml If you are unable to run podman info for any reason, please provide the podman version, operating system and its version and the architecture you are running. O/S: Oracle Linux V8.7 podman version: 4.2.0 podman info: host: arch: amd64 buildahVersion: 1.27.3 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma cgroupManager: systemd cgroupVersion: v1 conmon: package: conmon-2.1.4-1.module+el8.7.0+20930+90b24198.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.4, commit: 3922bff22a9c3ddaae27e66d280941f60a8b2554' cpuUtilization: idlePercent: 99.78 systemPercent: 0.08 userPercent: 0.14 cpus: 16 distribution: distribution: '&quot;ol&quot;' variant: server version: &quot;8.7&quot; eventLogger: file hostname: myServer idMappings: gidmap: null uidmap: null kernel: 5.4.17-2136.316.7.el8uek.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 63231135744 memTotal: 66397937664 networkBackend: cni ociRuntime: name: runc package: runc-1.1.4-1.module+el8.7.0+20930+90b24198.x86_64 path: /usr/bin/runc version: |- runc version 1.1.4 spec: 1.0.2-dev go: go1.18.9 libseccomp: 2.5.2 os: linux remoteSocket: path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /bin/slirp4netns package: slirp4netns-1.2.0-2.module+el8.7.0+20930+90b24198.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 4294963200 swapTotal: 4294963200 uptime: 82h 1m 50.00s (Approximately 3.42 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - container-registry.oracle.com - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 1 paused: 0 running: 1 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 212055355392 graphRootUsed: 9143263232 graphStatus: Backing Filesystem: xfs Native Overlay Diff: &quot;false&quot; Supports d_type: &quot;true&quot; Using metacopy: &quot;true&quot; imageCopyTmpDir: /var/tmp imageStore: number: 5 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.2.0 Built: 1677014962 BuiltTime: Tue Feb 21 13:29:22 2023 GitCommit: &quot;&quot; GoVersion: go1.18.9 Os: linux OsArch: linux/amd64 Version: 4.2.0 ``` ### Podman in a container No ### Privileged Or Rootless Privileged ### Upstream Latest Release Yes ### Additional environment details Additional environment details ### Additional information Happy to provide any extra information and screenshots needed. I'm running these tests as root as I was getting a podman build error when using my own account.</DESCRIPTION>
  <REPONAME>podman</REPONAME>
  <TIMEDIFFERENCEDAYS>6</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge pull request #17818 from edsantiago/logformatter_reliable_name logformatter: futureproof output filename</MESSAGE>
    <SHA>1ddf6fafcf84f17994e6b36aefa9ff9bd747a832</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>fix --health-on-failure=restart in transient unit As described in #17777, the `restart` on-failure action did not behave correctly when the health check is being run by a transient systemd unit. It ran just fine when being executed outside such a unit, for instance, manually or, as done in the system tests, in a scripted fashion. There were two issue causing the `restart` on-failure action to misbehave: 1) The transient systemd units used the default `KillMode=cgroup` which will nuke all processes in the specific cgroup including the recently restarted container/conmon once the main `podman healthcheck run` process exits. 2) Podman attempted to remove the transient systemd unit and timer during restart. That is perfectly fine when manually restarting the container but not when the restart itself is being executed inside such a transient unit. Ultimately, Podman tried to shoot itself in the foot. Fix both issues by moving the restart logic in the cleanup process. Instead of restarting the container, the `healthcheck run` will just stop the container and the cleanup process will restart the container once it has turned unhealthy. Fixes: #17777 Signed-off-by: Valentin Rothberg &lt;vrothberg@redhat.com&gt;</MESSAGE>
      <SHA>95634154303f5b8c3d5c92820e2a3545c54f0bc8</SHA>
      <PATCHEDFILES>
        <FILE>libpod/container_internal.go</FILE>
        <FILE>libpod/healthcheck.go</FILE>
        <FILE>test/system/220-healthcheck.bats</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
