<?xml version="1.0" ?>
<ISSUE>
  <ISSUENO>5546</ISSUENO>
  <ISSUEURL>https://github.com/nopSolutions/nopCommerce/issues/5546</ISSUEURL>
  <TITLE>Disallow links with returnUrl parameter in robots.txt</TITLE>
  <DESCRIPTION>nopCommerce version: 4.40 &gt; I had similar errors where google bot tries to index pages which have URL content as &quot;returnUrl&quot;. It just mess with google search console coverage, since returnUrl pages usually have duplicated results. &gt; Better add the robots.additions file with the below content. &gt; &gt; User-agent: * &gt; Disallow: */login?returnUrl=* &gt; Disallow: */register?returnUrl=* &gt; Disallow: */login?returnurl=* &gt; Disallow: */register?returnurl=* &gt; &gt; My search console coverage results jumped from 26000 to 42000 in a month after I updated the robot.txt with the additional file. &gt; Source: https://www.nopcommerce.com/boards/topic/82070/se-ranking-reported-errors-and-fixes#281771</DESCRIPTION>
  <REPONAME>nopCommerce</REPONAME>
  <TIMEDIFFERENCEDAYS>175</TIMEDIFFERENCEDAYS>
  <BUGGYCOMMIT>
    <MESSAGE>Merge branch 'issue-4325-Validation-delete-selected' into develop</MESSAGE>
    <SHA>7ffe2f6dde8e21bcbc7432fc726a53dc3b840e01</SHA>
  </BUGGYCOMMIT>
  <PATCHCOMMITS>
    <COMMIT>
      <MESSAGE>#5546 Disallow links with returnUrl parameter in robots.txt</MESSAGE>
      <SHA>ed136c69777b024ab85d024c1a99e6d5dfcc9fcd</SHA>
      <PATCHEDFILES>
        <FILE>src/Presentation/Nop.Web/Factories/CommonModelFactory.cs</FILE>
        <FILE>src/Tests/Nop.Tests/Nop.Web.Tests/Public/Factories/CommonModelFactoryTests.cs</FILE>
      </PATCHEDFILES>
    </COMMIT>
  </PATCHCOMMITS>
</ISSUE>
