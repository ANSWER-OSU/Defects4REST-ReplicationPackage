issue_no,repo,issue_url,title,description,patched_file_types,text_for_topic_modeling,prediction,confidence,combined_text
1776,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1776,S3 should return 204 on DELETE to nonexistent file (not 404),"**Describe the bug** The Amazon S3 api specification states that a DELETE request to a non-existing file should return HTTP status `HTTP/1.1 204 No Content`, as described [here](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteObject.html). Currently, seaweedfs S3 implementation returns `404 Not Found`. **System Setup** - List the command line to start: `./weed server -s3 -s3.config=config.json -dir=""/minio/weed"" -volume.max=100` - OS version: `CentOS 8` - output of `weed version`: `version 30GB 2.23 318a3d2 linux amd64` **Expected behavior** Seaweedfs should follow the Amazon S3 API specification and return HTTP 204 status-code when issuing a DELETE request to a non-existing file. This is important for compatibility with applications expecting S3 behavior. **Additional context** I stumbled upon this problem when attempting to use [pgbackrest](https://pgbackrest.org/) postgres backup software configured with seaweedfs S3 API as storage. When performing a regular backup, pgbackrest would almost finish but at the end run into an exception:  2021-02-02 16:04:39.364 P00 DEBUG: common/io/http/request::httpRequestResponse: => {code: 404, reason: Not Found, header: {accept-ranges: 'bytes', content-length: '234', content-type: 'application/xml', date: 'Tue, 02 Feb 2021 15:04:39 GMT', server: 'SeaweedFS S3 30GB 2.23', x-amz-request-id: '1612278279364142795'}, contentChunked: false, contentSize: 234, contentRemaining: 0, closeOnContentEof: false, contentExists: true, contentEof: true, contentCached: true} 2021-02-02 16:04:39.365 P00 DEBUG: common/exit::exitSafe: (result: 0, error: true, signalType: 0) ERROR: [039]: HTTP request failed with 404 (Not Found):  URI/Query : /dev-pgbackrest/backup/dev-pg-f3c001/latest  Request Headers : authorization: <redacted> content-length: 0 host: dev-seaweed-backup001-vip.<redacted> x-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 x-amz-date: <redacted>  Response Headers : accept-ranges: bytes content-length: 234 content-type: application/xml date: Tue, 02 Feb 2021 15:04:39 GMT server: SeaweedFS S3 30GB 2.23 x-amz-request-id: 1612278279364142795  Response Content : <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Resource>/dev-pgbackrest/backup/dev-pg-f3c001/latest</Resource><RequestId>1612278279364119515</RequestId></Error>  Searching for this error came up with this pgbackrest bug; https://github.com/pgbackrest/pgbackrest/issues/985 which details the same problem of an S3 implementation returning 404 for DELETE. I think this problem stems from issue #1160 where it was incorrectly stated that the API should return 404 in this case. Please let me know if I can provide additional information!",source-file,return delete nonexistent describe bug amazon api specification states delete non existing return http status http content described https docs aws amazon amazons latest api api deleteobject html currently implementation returns found system setup list command start weed server config config json dir minio weed volume max centos output weed linux amd expected behavior follow amazon api specification return http status issuing delete non existing important compatibility applications expecting behavior additional context stumbled upon attempting use pgbackrest https pgbackrest postgres backup software configured api storage performing regular backup pgbackrest would almost finish end exception common http httprequestresponse reason found header accept ranges bytes content length content type application xml date feb gmt server amz contentchunked false contentsize contentremaining closeoncontenteof false contentexists true contenteof true contentcached true common exit exitsafe result true signaltype http failed found uri query pgbackrest backup latest headers authorization redacted content length host seaweed backup vip redacted amz content sha afbf amz date redacted response headers accept ranges bytes content length content type application xml date feb gmt server amz response content xml encoding utf nosuchkey message specified key exist message resource pgbackrest backup latest resource requestid requestid searching came pgbackrest bug https github pgbackrest pgbackrest issues details implementation returning delete think stems incorrectly stated api return case please let know provide additional information,bug,0.95,"S3 should return 204 on DELETE to nonexistent file (not 404) **Describe the bug** The Amazon S3 api specification states that a DELETE request to a non-existing file should return HTTP status `HTTP/1.1 204 No Content`, as described [here](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteObject.html). Currently, seaweedfs S3 implementation returns `404 Not Found`. **System Setup** - List the command line to start: `./weed server -s3 -s3.config=config.json -dir=""/minio/weed"" -volume.max=100` - OS version: `CentOS 8` - output of `weed version`: `version 30GB 2.23 318a3d2 linux amd64` **Expected behavior** Seaweedfs should follow the Amazon S3 API specification and return HTTP 204 status-code when issuing a DELETE request to a non-existing file. This is important for compatibility with applications expecting S3 behavior. **Additional context** I stumbled upon this problem when attempting to use [pgbackrest](https://pgbackrest.org/) postgres backup software configured with seaweedfs S3 API as storage. When performing a regular backup, pgbackrest would almost finish but at the end run into an exception:  2021-02-02 16:04:39.364 P00 DEBUG: common/io/http/request::httpRequestResponse: => {code: 404, reason: Not Found, header: {accept-ranges: 'bytes', content-length: '234', content-type: 'application/xml', date: 'Tue, 02 Feb 2021 15:04:39 GMT', server: 'SeaweedFS S3 30GB 2.23', x-amz-request-id: '1612278279364142795'}, contentChunked: false, contentSize: 234, contentRemaining: 0, closeOnContentEof: false, contentExists: true, contentEof: true, contentCached: true} 2021-02-02 16:04:39.365 P00 DEBUG: common/exit::exitSafe: (result: 0, error: true, signalType: 0) ERROR: [039]: HTTP request failed with 404 (Not Found):  URI/Query : /dev-pgbackrest/backup/dev-pg-f3c001/latest  Request Headers : authorization: <redacted> content-length: 0 host: dev-seaweed-backup001-vip.<redacted> x-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 x-amz-date: <redacted>  Response Headers : accept-ranges: bytes content-length: 234 content-type: application/xml date: Tue, 02 Feb 2021 15:04:39 GMT server: SeaweedFS S3 30GB 2.23 x-amz-request-id: 1612278279364142795  Response Content : <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Resource>/dev-pgbackrest/backup/dev-pg-f3c001/latest</Resource><RequestId>1612278279364119515</RequestId></Error>  Searching for this error came up with this pgbackrest bug; https://github.com/pgbackrest/pgbackrest/issues/985 which details the same problem of an S3 implementation returning 404 for DELETE. I think this problem stems from issue #1160 where it was incorrectly stated that the API should return 404 in this case. Please let me know if I can provide additional information!"
1988,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1988,Upload file fail via filer WebUI,"**Describe the bug** When ""window.location.href"" is a directory and without ending of ""/"",upload file will get a reponse {""error"":""update entry /xxx/yyy: existing /xxx/yyy is a directory""} directory url like /xxx/yyy/ can be uploaded successfully **Expected behavior** File uploaded. **Additional context** https://github.com/chrislusf/seaweedfs/blob/c42b95c596f762dcca2bc9c7e7a918ab8ca8b206/weed/server/filer_ui/templates.go#L185",source-file,upload fail via filer webui describe bug window location href directory without ending upload get reponse entry xxx yyy existing xxx yyy directory directory url like xxx yyy uploaded successfully expected behavior uploaded additional context https github chrislusf blob dcca weed server filer templates,bug,0.9,"Upload file fail via filer WebUI **Describe the bug** When ""window.location.href"" is a directory and without ending of ""/"",upload file will get a reponse {""error"":""update entry /xxx/yyy: existing /xxx/yyy is a directory""} directory url like /xxx/yyy/ can be uploaded successfully **Expected behavior** File uploaded. **Additional context** https://github.com/chrislusf/seaweedfs/blob/c42b95c596f762dcca2bc9c7e7a918ab8ca8b206/weed/server/filer_ui/templates.go#L185"
1064,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1064,seaweed s3 list_buckets,"Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** I'm using s3 gateway and python boto3 and while trying to list all buckets I'm getting the error:   kwrgs = {'endpoint_url': 'http://seaweedfs-s3.default.svc.cluster.local:8333', 'aws_access_key_id': 'accessKey1', 'aws_secret_access_key': 'secretKey1'}  import boto3  cli = boto3.client('s3', **kwrgs)  cli.list_buckets() Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/local/lib/python3.7/site-packages/botocore/client.py"", line 357, in _api_call return self._make_api_call(operation_name, kwargs) File ""/usr/local/lib/python3.7/site-packages/botocore/client.py"", line 661, in _make_api_call raise error_class(parsed_response, operation_name) botocore.exceptions.ClientError: An error occurred (InternalError) when calling the ListBuckets operation (reached max retries: 4): We encountered an internal error, please try again.  I Found only Errors in the s3 gateway:  I0916 09:49:36 1 config.go:25] Reading security.toml from I0916 09:49:36 1 config.go:28] Reading : Config File ""security"" Not Found in ""[/ /root/.seaweedfs /etc/seaweedfs]"" I0916 09:49:36 1 s3.go:92] Start Seaweed S3 API Server 30GB 1.43 at http port 8333 I0916 09:50:58 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:51:18 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627478252701334</RequestId></Error> I0916 09:51:19 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:51:39 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627499203098406</RequestId></Error> I0916 09:51:40 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:52:00 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627520693419381</RequestId></Error> I0916 09:52:02 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:52:22 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627542703114894</RequestId></Error> I0916 09:52:30 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:52:50 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627570429772055</RequestId></Error  FYI : The weed shell can return the colletion.list and fs.tree without any issue The s3cmd can't list buckets too put file is working: cli.put_object(Bucket='test', Key='test', Body='test'). output:  {'ResponseMetadata': {'RequestId': '1568620869872887033', 'HostId': '', 'HTTPStatusCode': 200, 'HTTPHeaders': {'accept-ranges': 'bytes', 'etag': '""098f6bcd4621d373cade4e832627b4f6""', 'x-amz-request-id': '1568620869872887033', 'date': 'Mon, 16 Sep 2019 08:01:09 GMT', 'content-length': '0'}, 'RetryAttempts': 0}, 'ETag': '""098f6bcd4621d373cade4e832627b4f6""'}  **System Setup** - List the command line to start ""weed master"", ""weed volume"", ""weed filer"", ""weed s3"", ""weed mount"":  /usr/bin/weed -v=4 s3 -port=8333 -filer.dir.buckets=/buckets -filer=seaweedfs-filer.default:8888 /usr/bin/weed -v=4 master -port=9333 -mdir=/data -ip.bind=0.0.0.0 -volumeSizeLimitMB=30000 -defaultReplication=000 -ip=seaweedfs-master-0.seaweedfs-master -peers=seaweedfs-master-0.seaweedfs-master:9333 /usr/bin/weed -v=4 volume -port=8080 -dir=/data -max=1000 -ip.bind=0.0.0.0 -read.redirect=true -ip=10.244.83.158 -mserver=seaweedfs-master-0.seaweedfs-master:9333 /usr/bin/weed -v=4 filer -port=8888 -dirListLimit=100000 -ip=10.244.83.155 -master=seaweedfs-master-0.seaweedfs-master:9333  - OS version: Ubuntu 18.04.2 LTS - output of `weed version`: version 30GB 1.43 linux amd64 - if using filer, show the content of `filer.toml`  # A sample TOML config file for SeaweedFS filer store # Used with ""weed filer"" or ""weed server -filer"" [memory] # local in memory, mostly for testing purpose enabled = false [leveldb] # local on disk, mostly for simple single-machine setup, fairly scalable enabled = false dir = ""/data"" # directory to store level db files [leveldb2] # local on disk, mostly for simple single-machine setup, fairly scalable enabled = true dir = ""/data/filerldb2"" # directory to store level db 2 files  # multiple filers on shared storage, fairly scalable  [mysql] # CREATE TABLE IF NOT EXISTS filemeta ( # dirhash BIGINT COMMENT 'first 64 bits of MD5 hash value of directory field', # name VARCHAR(1000) COMMENT 'directory or file name', # directory TEXT COMMENT 'full path to parent directory', # meta BLOB, # PRIMARY KEY (dirhash, name) # ) DEFAULT CHARSET=utf8; enabled = false hostname = ""localhost"" port = 3306 username = ""root"" password = """" database = """" # create or use an existing database connection_max_idle = 2 connection_max_open = 100 [postgres] # CREATE TABLE IF NOT EXISTS filemeta ( # dirhash BIGINT, # name VARCHAR(65535), # directory VARCHAR(65535), # meta bytea, # PRIMARY KEY (dirhash, name) # ); enabled = false hostname = ""localhost"" port = 5432 username = ""postgres"" password = """" database = """" # create or use an existing database sslmode = ""disable"" connection_max_idle = 100 connection_max_open = 100 [cassandra] # CREATE TABLE filemeta ( # directory varchar, # name varchar, # meta blob, # PRIMARY KEY (directory, name) # ) WITH CLUSTERING ORDER BY (name ASC); enabled = false keyspace=""seaweedfs"" hosts=[ ""localhost:9042"", ] [redis] enabled = false address = ""localhost:6379"" password = """" db = 0 [redis_cluster] enabled = false addresses = [ ""localhost:30001"", ""localhost:30002"", ""localhost:30003"", ""localhost:30004"", ""localhost:30005"", ""localhost:30006"", ]  **Expected behavior** List all buckets when has only 1 bucket using python boto3",source-file,seaweed list buckets sponsors via patreon https www patreon describe bug gateway python boto trying list buckets getting kwrgs endpoint url http default svc cluster local aws access key accesskey aws secret access key secretkey import boto cli boto client kwrgs cli list buckets traceback recent call last stdin module local python site packages botocore client api call return self make api call operation name kwargs local python site packages botocore client make api call raise class parsed response operation name botocore exceptions clienterror occurred internalerror calling listbuckets operation reached max retries encountered internal please found errors gateway config reading security toml config reading config security found root etc start seaweed api server http port filer util read directory directory buckets limit api handlers status application xml xml encoding utf internalerror message encountered internal please message resource resource requestid requestid filer util read directory directory buckets limit api handlers status application xml xml encoding utf internalerror message encountered internal please message resource resource requestid requestid filer util read directory directory buckets limit api handlers status application xml xml encoding utf internalerror message encountered internal please message resource resource requestid requestid filer util read directory directory buckets limit api handlers status application xml xml encoding utf internalerror message encountered internal please message resource resource requestid requestid filer util read directory directory buckets limit api handlers status application xml xml encoding utf internalerror message encountered internal please message resource resource requestid requestid fyi weed shell return colletion list tree without cmd list buckets put working cli put object bucket key body output responsemetadata requestid hostid httpstatuscode httpheaders accept ranges bytes etag bcd cade amz date sep gmt content length retryattempts etag bcd cade system setup list command start weed weed volume weed filer weed weed mount weed port filer dir buckets buckets filer filer default weed port mdir data bind volumesizelimitmb defaultreplication peers weed volume port dir data max bind read redirect true mserver weed filer port dirlistlimit ubuntu lts output weed linux amd filer show content filer toml sample toml config filer store used weed filer weed server filer memory local memory mostly testing purpose enabled false leveldb local disk mostly simple single machine setup fairly scalable enabled false dir data directory store level files leveldb local disk mostly simple single machine setup fairly scalable enabled true dir data filerldb directory store level files multiple filers shared storage fairly scalable mysql create table exists filemeta dirhash bigint comment first bits hash value directory field name varchar comment directory name directory text comment full path parent directory meta blob primary key dirhash name default charset utf enabled false hostname localhost port username root password database create use existing database connection max idle connection max open postgres create table exists filemeta dirhash bigint name varchar directory varchar meta bytea primary key dirhash name enabled false hostname localhost port username postgres password database create use existing database sslmode disable connection max idle connection max open cassandra create table filemeta directory varchar name varchar meta blob primary key directory name clustering order name asc enabled false keyspace hosts localhost redis enabled false address localhost password redis cluster enabled false addresses localhost localhost localhost localhost localhost localhost expected behavior list buckets bucket python boto,bug,0.95,"seaweed s3 list_buckets Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** I'm using s3 gateway and python boto3 and while trying to list all buckets I'm getting the error:   kwrgs = {'endpoint_url': 'http://seaweedfs-s3.default.svc.cluster.local:8333', 'aws_access_key_id': 'accessKey1', 'aws_secret_access_key': 'secretKey1'}  import boto3  cli = boto3.client('s3', **kwrgs)  cli.list_buckets() Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/local/lib/python3.7/site-packages/botocore/client.py"", line 357, in _api_call return self._make_api_call(operation_name, kwargs) File ""/usr/local/lib/python3.7/site-packages/botocore/client.py"", line 661, in _make_api_call raise error_class(parsed_response, operation_name) botocore.exceptions.ClientError: An error occurred (InternalError) when calling the ListBuckets operation (reached max retries: 4): We encountered an internal error, please try again.  I Found only Errors in the s3 gateway:  I0916 09:49:36 1 config.go:25] Reading security.toml from I0916 09:49:36 1 config.go:28] Reading : Config File ""security"" Not Found in ""[/ /root/.seaweedfs /etc/seaweedfs]"" I0916 09:49:36 1 s3.go:92] Start Seaweed S3 API Server 30GB 1.43 at http port 8333 I0916 09:50:58 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:51:18 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627478252701334</RequestId></Error> I0916 09:51:19 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:51:39 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627499203098406</RequestId></Error> I0916 09:51:40 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:52:00 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627520693419381</RequestId></Error> I0916 09:52:02 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:52:22 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627542703114894</RequestId></Error> I0916 09:52:30 1 filer_util.go:89] read directory: directory:""/buckets"" limit:2147483647 I0916 09:52:50 1 s3api_handlers.go:78] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/</Resource><RequestId>1568627570429772055</RequestId></Error  FYI : The weed shell can return the colletion.list and fs.tree without any issue The s3cmd can't list buckets too put file is working: cli.put_object(Bucket='test', Key='test', Body='test'). output:  {'ResponseMetadata': {'RequestId': '1568620869872887033', 'HostId': '', 'HTTPStatusCode': 200, 'HTTPHeaders': {'accept-ranges': 'bytes', 'etag': '""098f6bcd4621d373cade4e832627b4f6""', 'x-amz-request-id': '1568620869872887033', 'date': 'Mon, 16 Sep 2019 08:01:09 GMT', 'content-length': '0'}, 'RetryAttempts': 0}, 'ETag': '""098f6bcd4621d373cade4e832627b4f6""'}  **System Setup** - List the command line to start ""weed master"", ""weed volume"", ""weed filer"", ""weed s3"", ""weed mount"":  /usr/bin/weed -v=4 s3 -port=8333 -filer.dir.buckets=/buckets -filer=seaweedfs-filer.default:8888 /usr/bin/weed -v=4 master -port=9333 -mdir=/data -ip.bind=0.0.0.0 -volumeSizeLimitMB=30000 -defaultReplication=000 -ip=seaweedfs-master-0.seaweedfs-master -peers=seaweedfs-master-0.seaweedfs-master:9333 /usr/bin/weed -v=4 volume -port=8080 -dir=/data -max=1000 -ip.bind=0.0.0.0 -read.redirect=true -ip=10.244.83.158 -mserver=seaweedfs-master-0.seaweedfs-master:9333 /usr/bin/weed -v=4 filer -port=8888 -dirListLimit=100000 -ip=10.244.83.155 -master=seaweedfs-master-0.seaweedfs-master:9333  - OS version: Ubuntu 18.04.2 LTS - output of `weed version`: version 30GB 1.43 linux amd64 - if using filer, show the content of `filer.toml`  # A sample TOML config file for SeaweedFS filer store # Used with ""weed filer"" or ""weed server -filer"" [memory] # local in memory, mostly for testing purpose enabled = false [leveldb] # local on disk, mostly for simple single-machine setup, fairly scalable enabled = false dir = ""/data"" # directory to store level db files [leveldb2] # local on disk, mostly for simple single-machine setup, fairly scalable enabled = true dir = ""/data/filerldb2"" # directory to store level db 2 files  # multiple filers on shared storage, fairly scalable  [mysql] # CREATE TABLE IF NOT EXISTS filemeta ( # dirhash BIGINT COMMENT 'first 64 bits of MD5 hash value of directory field', # name VARCHAR(1000) COMMENT 'directory or file name', # directory TEXT COMMENT 'full path to parent directory', # meta BLOB, # PRIMARY KEY (dirhash, name) # ) DEFAULT CHARSET=utf8; enabled = false hostname = ""localhost"" port = 3306 username = ""root"" password = """" database = """" # create or use an existing database connection_max_idle = 2 connection_max_open = 100 [postgres] # CREATE TABLE IF NOT EXISTS filemeta ( # dirhash BIGINT, # name VARCHAR(65535), # directory VARCHAR(65535), # meta bytea, # PRIMARY KEY (dirhash, name) # ); enabled = false hostname = ""localhost"" port = 5432 username = ""postgres"" password = """" database = """" # create or use an existing database sslmode = ""disable"" connection_max_idle = 100 connection_max_open = 100 [cassandra] # CREATE TABLE filemeta ( # directory varchar, # name varchar, # meta blob, # PRIMARY KEY (directory, name) # ) WITH CLUSTERING ORDER BY (name ASC); enabled = false keyspace=""seaweedfs"" hosts=[ ""localhost:9042"", ] [redis] enabled = false address = ""localhost:6379"" password = """" db = 0 [redis_cluster] enabled = false addresses = [ ""localhost:30001"", ""localhost:30002"", ""localhost:30003"", ""localhost:30004"", ""localhost:30005"", ""localhost:30006"", ]  **Expected behavior** List all buckets when has only 1 bucket using python boto3"
1838,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1838,[s3] http: superfluous response.WriteHeader,"**Describe the bug** [s3] http: superfluous response.WriteHeader after `raft.Server: Not current leader` per request  I0225 09:43:38 1 masterclient.go:120] filer masterClient failed to receive from fast-master-0.s3-fast-master-direct.service.dcix.consul:9333: rpc error: code = Unavailable desc = transport is closing 2021/02/25 09:43:38 http: superfluous response.WriteHeader call from github.com/chrislusf/seaweedfs/weed/server.processRangeRequest (common.go:283) E0225 09:43:38 1 filer_server_handlers_write.go:42] failing to assign a file id: failed to parse master : server should have hostname:port format: I0225 09:43:38 1 common.go:53] response method:PUT URL:/buckets/registry/docker/registry/v2/repositories/b2c/credits-admin/_uploads/71e12cb4-5c6e-405e-a6ca-1d491ec79441/hashstates/sha256/15728640 with httpStatus:500 and JSON:{""error"":""failed to parse master : server should have hostname:port format: ""} E0225 09:43:38 1 s3api_object_handlers.go:330] upload to filer error: failed to parse master : server should have hostname:port format: 2021/02/25 09:43:38 http: superfluous response.WriteHeader call from github.com/chrislusf/seaweedfs/weed/server.processRangeRequest (common.go:283) 2021/02/25 09:43:38 http: superfluous response.WriteHeader call from github.com/chrislusf/seaweedfs/weed/server.processRangeRequest (common.go:283)  https://github.com/chrislusf/seaweedfs/blob/master/weed/server/common.go#L284 **Expected behavior** automatic recovery after loss of master If there is no content, the S3 API returns 200 and 206 codes, need return 500",other-file | other-file | source-file | source-file | source-file | source-file,http superfluous response writeheader describe bug http superfluous response writeheader raft server current leader per masterclient filer masterclient failed receive fast fast direct service dcix consul rpc unavailable desc transport closing http superfluous response writeheader call github chrislusf weed server processrangerequest common filer server handlers write failing assign failed parse server hostname port format common response method put url buckets registry docker registry repositories credits admin uploads hashstates sha httpstatus json failed parse server hostname port format api object handlers upload filer failed parse server hostname port format http superfluous response writeheader call github chrislusf weed server processrangerequest common http superfluous response writeheader call github chrislusf weed server processrangerequest common https github chrislusf blob weed server common expected behavior automatic recovery loss content api returns codes need return,bug,0.9,"[s3] http: superfluous response.WriteHeader **Describe the bug** [s3] http: superfluous response.WriteHeader after `raft.Server: Not current leader` per request  I0225 09:43:38 1 masterclient.go:120] filer masterClient failed to receive from fast-master-0.s3-fast-master-direct.service.dcix.consul:9333: rpc error: code = Unavailable desc = transport is closing 2021/02/25 09:43:38 http: superfluous response.WriteHeader call from github.com/chrislusf/seaweedfs/weed/server.processRangeRequest (common.go:283) E0225 09:43:38 1 filer_server_handlers_write.go:42] failing to assign a file id: failed to parse master : server should have hostname:port format: I0225 09:43:38 1 common.go:53] response method:PUT URL:/buckets/registry/docker/registry/v2/repositories/b2c/credits-admin/_uploads/71e12cb4-5c6e-405e-a6ca-1d491ec79441/hashstates/sha256/15728640 with httpStatus:500 and JSON:{""error"":""failed to parse master : server should have hostname:port format: ""} E0225 09:43:38 1 s3api_object_handlers.go:330] upload to filer error: failed to parse master : server should have hostname:port format: 2021/02/25 09:43:38 http: superfluous response.WriteHeader call from github.com/chrislusf/seaweedfs/weed/server.processRangeRequest (common.go:283) 2021/02/25 09:43:38 http: superfluous response.WriteHeader call from github.com/chrislusf/seaweedfs/weed/server.processRangeRequest (common.go:283)  https://github.com/chrislusf/seaweedfs/blob/master/weed/server/common.go#L284 **Expected behavior** automatic recovery after loss of master If there is no content, the S3 API returns 200 and 206 codes, need return 500"
1724,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1724,One of the two volume servers that contain data returns error: invalid character '\x1f' and MIME type error,"Hi, I have 3 volume servers and 1 master node, I've created a collection name ""Animated_2tacopy_count"" with count=1 and replication=001 I upload my file with following command:  sweed@dev10:~$./weedfs/weed upload -collection=Animated_2tacopy_count=1 -dir=""/home/sweed/animate.mp4 [{""fileName"":""animate.mp4"",""fileUrl"":""192.168.200.23:8081/22,1d9ff15aa7"",""fid"":""22,1d9ff15aa7"",""size"":178481324}]  when I'm asking Master for my file, it return volume servers randomly:  sweed@dev10:~$ curl -I 192.168.200.20:9333/22,24dbfea5ed HTTP/1.1 308 Permanent Redirect Content-Type: text/html; charset=utf-8 Location: http://192.168.200.21:8081/22,24dbfea5ed Date: Thu, 31 Dec 2020 15:06:08 GMT sweed@dev10:~$ curl -I 192.168.200.20:9333/22,24dbfea5ed HTTP/1.1 308 Permanent Redirect Content-Type: text/html; charset=utf-8 Location: http://192.168.200.22:8081/22,24dbfea5ed Date: Thu, 31 Dec 2020 15:06:09 GMT  But in browser I can't see my file from volume serevr 192.168.200.21 but In another volume server ""192.168.200.22"" everything is ok, actully when browser returns the ip address of 200.22 I can see my file but when it return 200.21 I have mime type error: ![image](https://user-images.githubusercontent.com/43205944/103415573-0111fb80-4b98-11eb-9c53-568dab440166.png) And in journalctl of server 200.21 I have this error:  Dec 31 18:23:50 dev11 seaweedfs-volume[20791]: I1231 18:23:50 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:25:30 dev11 seaweedfs-volume[20791]: I1231 18:25:30 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:25:37 dev11 seaweedfs-volume[20791]: I1231 18:25:37 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:25:48 dev11 seaweedfs-volume[20791]: I1231 18:25:48 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:26:29 dev11 seaweedfs-volume[20791]: I1231 18:26:29 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:26:40 dev11 seaweedfs-volume[20791]: I1231 18:26:40 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:26:42 dev11 seaweedfs-volume[20791]: I1231 18:26:42 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:26:43 dev11 seaweedfs-volume[20791]: I1231 18:26:43 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:27:03 dev11 seaweedfs-volume[20791]: I1231 18:27:03 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value ",source-file | source-file,one two volume servers contain data returns invalid character mime type volume servers node created collection name animated tacopy count count replication upload following command sweed weedfs weed upload collection animated tacopy count dir home sweed animate filename animate fileurl fid size asking return volume servers randomly sweed curl dbfea http permanent redirect content type text html charset utf location http dbfea date dec gmt sweed curl dbfea http permanent redirect content type text html charset utf location http dbfea date dec gmt browser see volume serevr another volume server everything actully browser returns address see return mime type image https user images githubusercontent dab png journalctl server dec volume volume server handlers read load chunked manifest dbfea invalid character looking beginning value dec volume volume server handlers read load chunked manifest dbfea invalid character looking beginning value dec volume volume server handlers read load chunked manifest dbfea invalid character looking beginning value dec volume volume server handlers read load chunked manifest dbfea invalid character looking beginning value dec volume volume server handlers read load chunked manifest dbfea invalid character looking beginning value dec volume volume server handlers read load chunked manifest dbfea invalid character looking beginning value dec volume volume server handlers read load chunked manifest dbfea invalid character looking beginning value dec volume volume server handlers read load chunked manifest dbfea invalid character looking beginning value dec volume volume server handlers read load chunked manifest dbfea invalid character looking beginning value,bug,0.9,"One of the two volume servers that contain data returns error: invalid character '\x1f' and MIME type error Hi, I have 3 volume servers and 1 master node, I've created a collection name ""Animated_2tacopy_count"" with count=1 and replication=001 I upload my file with following command:  sweed@dev10:~$./weedfs/weed upload -collection=Animated_2tacopy_count=1 -dir=""/home/sweed/animate.mp4 [{""fileName"":""animate.mp4"",""fileUrl"":""192.168.200.23:8081/22,1d9ff15aa7"",""fid"":""22,1d9ff15aa7"",""size"":178481324}]  when I'm asking Master for my file, it return volume servers randomly:  sweed@dev10:~$ curl -I 192.168.200.20:9333/22,24dbfea5ed HTTP/1.1 308 Permanent Redirect Content-Type: text/html; charset=utf-8 Location: http://192.168.200.21:8081/22,24dbfea5ed Date: Thu, 31 Dec 2020 15:06:08 GMT sweed@dev10:~$ curl -I 192.168.200.20:9333/22,24dbfea5ed HTTP/1.1 308 Permanent Redirect Content-Type: text/html; charset=utf-8 Location: http://192.168.200.22:8081/22,24dbfea5ed Date: Thu, 31 Dec 2020 15:06:09 GMT  But in browser I can't see my file from volume serevr 192.168.200.21 but In another volume server ""192.168.200.22"" everything is ok, actully when browser returns the ip address of 200.22 I can see my file but when it return 200.21 I have mime type error: ![image](https://user-images.githubusercontent.com/43205944/103415573-0111fb80-4b98-11eb-9c53-568dab440166.png) And in journalctl of server 200.21 I have this error:  Dec 31 18:23:50 dev11 seaweedfs-volume[20791]: I1231 18:23:50 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:25:30 dev11 seaweedfs-volume[20791]: I1231 18:25:30 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:25:37 dev11 seaweedfs-volume[20791]: I1231 18:25:37 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:25:48 dev11 seaweedfs-volume[20791]: I1231 18:25:48 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:26:29 dev11 seaweedfs-volume[20791]: I1231 18:26:29 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:26:40 dev11 seaweedfs-volume[20791]: I1231 18:26:40 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:26:42 dev11 seaweedfs-volume[20791]: I1231 18:26:42 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:26:43 dev11 seaweedfs-volume[20791]: I1231 18:26:43 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value Dec 31 18:27:03 dev11 seaweedfs-volume[20791]: I1231 18:27:03 20791 volume_server_handlers_read.go:187] load chunked manifest (/22,24dbfea5ed) error: invalid character '\x1f' looking for beginning of value "
5155,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/5155,s3api v3.60: Missing VersionConfiguration node in get-bucket-versioning response,"Feature introduced in: https://github.com/seaweedfs/seaweedfs/pull/4998 **Describe the bug** When using aws-cli get-bucket-versioning will return no results; this is due to the malformed response. When running with debug, we can see that the response body is missing the VersionConfiguration node.  aws --endpoint-url http://localhost:8333 s3api get-bucket-versioning --bucket mybucket --debug  2024-01-03 01:11:43,611 - MainThread - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:8333 2024-01-03 01:11:43,618 - MainThread - urllib3.connectionpool - DEBUG - http://localhost:8333 ""GET /mybucket?versioning HTTP/1.1"" 200 26 2024-01-03 01:11:43,619 - MainThread - botocore.parsers - DEBUG - Response headers: {'Accept-Ranges': 'bytes', 'Content-Length': '26', 'Content-Type': 'application/xml', 'Server': 'SeaweedFS S3', 'X-Amz-Request-Id': '1704244303617654225' , 'Date': 'Wed, 03 Jan 2024 01:11:43 GMT'} 2024-01-03 01:11:43,619 - MainThread - botocore.parsers - DEBUG - Response body: b'<Status>Suspended</Status>'   **System Setup** - running seaweedfs:3.60_large_disk container image - command: weed server -s3 -iam - `# weed version: version 8000GB 3.60 d4e91b6ad linux amd64` **Expected behavior** Default response **""Status"":""Suspended""**:  aws --endpoint-url http://localhost:8333 s3api get-bucket-versioning --bucket mybucket { ""Status"": ""Suspended"" } ",source-file | source-file | source-file,api missing versionconfiguration node get bucket versioning response feature introduced https github describe bug aws cli get bucket versioning return results due malformed response running see response body missing versionconfiguration node aws endpoint url http localhost api get bucket versioning bucket mybucket mainthread urllib connectionpool starting new http connection localhost mainthread urllib connectionpool http localhost get mybucket versioning http mainthread botocore parsers response headers accept ranges bytes content length content type application xml server amz date jan gmt mainthread botocore parsers response body status suspended status system setup running large disk container image command weed server iam weed linux amd expected behavior default response status suspended aws endpoint url http localhost api get bucket versioning bucket mybucket status suspended,bug,0.95,"s3api v3.60: Missing VersionConfiguration node in get-bucket-versioning response Feature introduced in: https://github.com/seaweedfs/seaweedfs/pull/4998 **Describe the bug** When using aws-cli get-bucket-versioning will return no results; this is due to the malformed response. When running with debug, we can see that the response body is missing the VersionConfiguration node.  aws --endpoint-url http://localhost:8333 s3api get-bucket-versioning --bucket mybucket --debug  2024-01-03 01:11:43,611 - MainThread - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:8333 2024-01-03 01:11:43,618 - MainThread - urllib3.connectionpool - DEBUG - http://localhost:8333 ""GET /mybucket?versioning HTTP/1.1"" 200 26 2024-01-03 01:11:43,619 - MainThread - botocore.parsers - DEBUG - Response headers: {'Accept-Ranges': 'bytes', 'Content-Length': '26', 'Content-Type': 'application/xml', 'Server': 'SeaweedFS S3', 'X-Amz-Request-Id': '1704244303617654225' , 'Date': 'Wed, 03 Jan 2024 01:11:43 GMT'} 2024-01-03 01:11:43,619 - MainThread - botocore.parsers - DEBUG - Response body: b'<Status>Suspended</Status>'   **System Setup** - running seaweedfs:3.60_large_disk container image - command: weed server -s3 -iam - `# weed version: version 8000GB 3.60 d4e91b6ad linux amd64` **Expected behavior** Default response **""Status"":""Suspended""**:  aws --endpoint-url http://localhost:8333 s3api get-bucket-versioning --bucket mybucket { ""Status"": ""Suspended"" } "
2593,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2593,"When ""X-Amz-Copy-Source"" a folder, weed create a new file with ""Filer listing Html"" result.","The response of ""CopyObject"" should be 40x.",source-file,amz copy folder weed create new filer listing html result response copyobject,bug,0.9,"When ""X-Amz-Copy-Source"" a folder, weed create a new file with ""Filer listing Html"" result. The response of ""CopyObject"" should be 40x."
814,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/814,Seaweedfs client core dump when calling DeleteFiles,"Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** When seaweedfs client calls DeleteFiles (e.g. fileId=123,xxx, volume 123 not exist), it ALWAYS crashes. **Screenshots** goroutine 89 [running]: github.com/chrislusf/seaweedfs/weed/operation.DeleteFilesWithLookupVolumeId(0xc42061d578, 0x1, 0x1, 0xc4206a4160, 0x20, 0x20, 0xc4206a4140, 0x7fbfd979d1e0, 0x0) github.com/chrislusf/seaweedfs/weed/operation/delete_content.go:74 +0xce9 github.com/chrislusf/seaweedfs/weed/operation.DeleteFiles(0xc420286090, 0xe, 0xc42061d578, 0x1, 0x1, 0xc42061d538, 0x4126c8, 0x20, 0x9b9ca0, 0x725201) github.com/chrislusf/seaweedfs/weed/operation/delete_content.go:36 +0x8a",source-file,client core dump calling deletefiles sponsors via patreon https www patreon describe bug client calls deletefiles fileid xxx volume exist always crashes screenshots goroutine running github chrislusf weed operation deletefileswithlookupvolumeid fbfd github chrislusf weed operation delete content xce github chrislusf weed operation deletefiles github chrislusf weed operation delete content,bug,0.9,"Seaweedfs client core dump when calling DeleteFiles Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** When seaweedfs client calls DeleteFiles (e.g. fileId=123,xxx, volume 123 not exist), it ALWAYS crashes. **Screenshots** goroutine 89 [running]: github.com/chrislusf/seaweedfs/weed/operation.DeleteFilesWithLookupVolumeId(0xc42061d578, 0x1, 0x1, 0xc4206a4160, 0x20, 0x20, 0xc4206a4140, 0x7fbfd979d1e0, 0x0) github.com/chrislusf/seaweedfs/weed/operation/delete_content.go:74 +0xce9 github.com/chrislusf/seaweedfs/weed/operation.DeleteFiles(0xc420286090, 0xe, 0xc42061d578, 0x1, 0x1, 0xc42061d538, 0x4126c8, 0x20, 0x9b9ca0, 0x725201) github.com/chrislusf/seaweedfs/weed/operation/delete_content.go:36 +0x8a"
2177,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2177,Volume on ARMv7 throws,"**Describe the bug** Volume on ARM32 throws an ""unaligned 64-bit atomic operation"" error on upload of a file **System Setup** - /usr/bin/weed volume -ip=odroid3.node.dc1.consul -port=9444 -mserver=seaweedfsmaster0.service.dc1.consul:9333,seaweedfsmaster1.service.dc1.consul:9333,seaweedfsmaster2.service.dc1.consul:9333 -dir=/data/vol - OS version = Ubuntu 5.4.118-221 armv7l armv7l armv7l GNU/Linux - `weed version' = version 30GB 2.56 a2979aa linux arm - hardware = Odroid HC1 (Samsung Exynos5422 Cortex-A15 2Ghz and Cortex-A7 Octa cores) **Expected behavior** Not to crash **Additional context** 2021/07/02 15:46:52 http: panic serving 192.168.1.238:40226: unaligned 64-bit atomic operation goroutine 101 [running]: net/http.(*conn).serve.func1(0x32244e0) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:1824 +0x104 panic(0x171c1b8, 0x1e1dca8) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/runtime/panic.go:971 +0x4b4 runtime/internal/atomic.panicUnaligned() /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/runtime/internal/atomic/unaligned.go:8 +0x24 runtime/internal/atomic.Load64(0x31473e4, 0xe5b38, 0x0) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/runtime/internal/atomic/asm_arm.s:263 +0x14 github.com/chrislusf/seaweedfs/weed/server.(*VolumeServer).privateStoreHandler(0x3147380, 0x1e56c4c, 0x34581e0, 0x30de680) /home/travis/gopath/src/github.com/chrislusf/seaweedfs/weed/server/volume_server_handlers.go:49 +0x4bc net/http.HandlerFunc.ServeHTTP(0x300fa88, 0x1e56c4c, 0x34581e0, 0x30de680) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:2069 +0x34 net/http.(*ServeMux).ServeHTTP(0x325b440, 0x1e56c4c, 0x34581e0, 0x30de680) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:2448 +0x16c net/http.serverHandler.ServeHTTP(0x30ec360, 0x1e56c4c, 0x34581e0, 0x30de680) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:2887 +0x88 net/http.(*conn).serve(0x32244e0, 0x1e59564, 0x31c09c0) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:1952 +0x7f0 created by net/http.(*Server).Serve /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:3013 +0x318",source-file,volume armv throws describe bug volume arm throws unaligned bit atomic operation upload system setup weed volume odroid node consul port mserver seaweedfsmaster service consul seaweedfsmaster service consul seaweedfsmaster service consul dir data vol ubuntu armv armv armv gnu linux weed linux arm hardware odroid samsung exynos cortex ghz cortex octa cores expected behavior crash additional context http panic serving unaligned bit atomic operation goroutine running http conn serve func home travis gimme versions linux amd http server panic dca home travis gimme versions linux amd runtime panic runtime internal atomic panicunaligned home travis gimme versions linux amd runtime internal atomic unaligned runtime internal atomic load home travis gimme versions linux amd runtime internal atomic asm arm github chrislusf weed server volumeserver privatestorehandler home travis gopath github chrislusf weed server volume server handlers http handlerfunc servehttp home travis gimme versions linux amd http server http servemux servehttp home travis gimme versions linux amd http server http serverhandler servehttp home travis gimme versions linux amd http server http conn serve home travis gimme versions linux amd http server created http server serve home travis gimme versions linux amd http server,bug,0.9,"Volume on ARMv7 throws **Describe the bug** Volume on ARM32 throws an ""unaligned 64-bit atomic operation"" error on upload of a file **System Setup** - /usr/bin/weed volume -ip=odroid3.node.dc1.consul -port=9444 -mserver=seaweedfsmaster0.service.dc1.consul:9333,seaweedfsmaster1.service.dc1.consul:9333,seaweedfsmaster2.service.dc1.consul:9333 -dir=/data/vol - OS version = Ubuntu 5.4.118-221 armv7l armv7l armv7l GNU/Linux - `weed version' = version 30GB 2.56 a2979aa linux arm - hardware = Odroid HC1 (Samsung Exynos5422 Cortex-A15 2Ghz and Cortex-A7 Octa cores) **Expected behavior** Not to crash **Additional context** 2021/07/02 15:46:52 http: panic serving 192.168.1.238:40226: unaligned 64-bit atomic operation goroutine 101 [running]: net/http.(*conn).serve.func1(0x32244e0) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:1824 +0x104 panic(0x171c1b8, 0x1e1dca8) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/runtime/panic.go:971 +0x4b4 runtime/internal/atomic.panicUnaligned() /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/runtime/internal/atomic/unaligned.go:8 +0x24 runtime/internal/atomic.Load64(0x31473e4, 0xe5b38, 0x0) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/runtime/internal/atomic/asm_arm.s:263 +0x14 github.com/chrislusf/seaweedfs/weed/server.(*VolumeServer).privateStoreHandler(0x3147380, 0x1e56c4c, 0x34581e0, 0x30de680) /home/travis/gopath/src/github.com/chrislusf/seaweedfs/weed/server/volume_server_handlers.go:49 +0x4bc net/http.HandlerFunc.ServeHTTP(0x300fa88, 0x1e56c4c, 0x34581e0, 0x30de680) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:2069 +0x34 net/http.(*ServeMux).ServeHTTP(0x325b440, 0x1e56c4c, 0x34581e0, 0x30de680) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:2448 +0x16c net/http.serverHandler.ServeHTTP(0x30ec360, 0x1e56c4c, 0x34581e0, 0x30de680) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:2887 +0x88 net/http.(*conn).serve(0x32244e0, 0x1e59564, 0x31c09c0) /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:1952 +0x7f0 created by net/http.(*Server).Serve /home/travis/.gimme/versions/go1.16.5.linux.amd64/src/net/http/server.go:3013 +0x318"
1808,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1808,Jwt issue - wrong jwt,"**Describe the bug** Getting wrong jwt code from master.  C:\Users\lennie>curl -i ""http://172.16.100.1:9351/dir/lookup?volumeId=1,18ee7f3466&read=yes"" HTTP/1.1 200 OK Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTI0NDN9.AxGUN17e_RZ5g4tSZLFWfzqnIuYQqxDCMBPTxvnFpdk Content-Type: application/json Date: Tue, 16 Feb 2021 21:52:03 GMT Content-Length: 90 {""volumeId"":""1"",""locations"":[{""url"":""172.16.100.1:9831"",""publicUrl"":""172.16.100.1:9831""}]}  And then volume query.  C:\Users\lennie>curl -i ""http://172.16.100.1:9831/1,18ee7f3466"" -H ""Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTI0NDN9.AxGUN17e_RZ5g4tSZLFWfzqnIuYQqxDCMBPTxvnFpdk"" HTTP/1.1 401 Unauthorized Content-Type: application/json Server: SeaweedFS Volume 30GB 2.26 Date: Tue, 16 Feb 2021 21:52:27 GMT Content-Length: 21 {""error"":""wrong jwt""}  I Have read issue https://github.com/chrislusf/seaweedfs/issues/1399 But i only have one server and the time is correct on the system. When i do some request to the master to get the jwt token i can se this.  curl -i ""http://172.16.100.1:9351/dir/lookup?volumeId=1,18ee7f3466&read=yes""  Jwt response.  Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTM2MTN9.VfbCVHEhBMHMxWZrTpzOTYSU8DqWtKGs5_vaopdLiwY Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTM2MTd9.9yPL3dNuW3J4p68KARMWIAp_XlqDYkAN-NHHnYUrY4Q Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTM2MjB9.crUaZYFBCEDu57dOKAE-rJPw5nS-fpJP11Oz6Bv0Pok Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTM2MjF9.mRJQ0WtXu5JNfPAKIGiJP0CGrodiM_i8tOws9H7PRdA  There should not be underscore or dash in the base64 encoded values? So the question is. Why do I get underscores? I am running: OS: Ubuntu 18.04.4 LTS And weed version : version 30GB 2.26 b9b5b932 linux amd64 The Master is starting whit this command.  weed master -mdir=""/home/raid6/seaweed/mastershare"" -ip=""172.16.100.1"" -ip.bind=""172.16.100.1"" -port=9351  Log  I0216 22:42:56 19422 file_util.go:23] Folder /home/raid6/seaweed/mastershare Permission: -rwxr-xr-x I0216 22:42:56 19422 master.go:168] current: 172.16.100.1:9351 peers: I0216 22:42:56 19422 master_server.go:107] Volume Size Limit is 30000 MB I0216 22:42:56 19422 master_server.go:192] adminScripts: I0216 22:42:56 19422 master.go:122] Start Seaweed Master 30GB 2.26 b9b5b932 at 172.16.100.1:9351 I0216 22:42:56 19422 raft_server.go:70] Starting RaftServer with 172.16.100.1:9351 I0216 22:42:56 19422 raft_server.go:129] current cluster leader: I0216 22:42:56 19422 master.go:146] Start Seaweed Master 30GB 2.26 b9b5b932 grpc server at 172.16.100.1:19351 I0216 22:42:57 19422 masterclient.go:78] No existing leader found! I0216 22:42:57 19422 raft_server.go:154] Initializing new cluster I0216 22:42:57 19422 master_server.go:141] leader change event: => 172.16.100.1:9351 I0216 22:42:57 19422 master_server.go:143] [ 172.16.100.1:9351 ] 172.16.100.1:9351 becomes leader. I0216 22:43:00 19422 master_grpc_server.go:252] + client filer@172.16.100.1:8888 I0216 22:43:01 19422 masterclient.go:126] redirected to leader 172.16.100.1:9351 I0216 22:43:01 19422 master_grpc_server.go:252] + client master@172.16.100.1:37496 I0216 22:43:04 19422 node.go:322] topo adds child soder I0216 22:43:04 19422 node.go:322] topo:soder adds child r1u1 I0216 22:43:04 19422 node.go:322] topo:soder:r1u1 adds child 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:71] added volume server 172.16.100.1:9831 I0216 22:43:04 19422 volume_layout.go:354] Volume 13 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 10 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 8 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 7 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 6 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 3 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 1 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 11 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 2 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 12 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 5 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 4 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 9 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 14 becomes writable I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 13 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 10 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 8 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 7 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 6 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 3 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 1 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 11 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 2 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 12 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 5 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 4 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 9 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 14 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:157] master send to filer@172.16.100.1:8888: url:""172.16.100.1:9831"" public_url:""172.16.100.1:9831"" new_vids:13 new_vids:10 new_vids:8 new_vids:7 new_vids:6 new_vids:3 new_vids:1 new_vids:11 new_vids:2 new_vids:12 new_vids:5 new_vids:4 new_vids:9 new_vids:14 data_center:""soder"" I0216 22:43:04 19422 master_grpc_server.go:157] master send to master@172.16.100.1:37496: url:""172.16.100.1:9831"" public_url:""172.16.100.1:9831"" new_vids:13 new_vids:10 new_vids:8 new_vids:7 new_vids:6 new_vids:3 new_vids:1 new_vids:11 new_vids:2 new_vids:12 new_vids:5 new_vids:4 new_vids:9 new_vids:14 data_center:""soder""  The volume is starting whit this command.  root@spinky:/home/raid6/seaweed# weed volume -max=100 -ip=""172.16.100.1"" -ip.bind=""172.16.100.1"" -port=9831 -mserver=""172.16.100.1:9351"" -dir=""/home/raid6/seaweed/voltesting"" -dataCenter=""soder"" -rack=""r1u1""  Log  I0216 22:25:09 9983 file_util.go:23] Folder /home/raid6/seaweed/voltesting Permission: -rwxr-xr-x I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/2.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_12.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/5.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/3.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/1.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/7.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/6.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/4.idx to memory I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/2.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/3.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_10.idx to memory I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/7.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_12.dat, replicaPlacement=000 v=3 size=1864 ttl= I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_13.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_11.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_8.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_14.idx to memory I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/4.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_9.idx to memory I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/1.dat, replicaPlacement=000 v=3 size=112 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/5.dat, replicaPlacement=000 v=3 size=104 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/6.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_10.dat, replicaPlacement=000 v=3 size=7900832 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_13.dat, replicaPlacement=000 v=3 size=720 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_9.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_11.dat, replicaPlacement=000 v=3 size=664 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_8.dat, replicaPlacement=000 v=3 size=1600 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_14.dat, replicaPlacement=000 v=3 size=30789912 ttl= I0216 22:25:09 9983 disk_location.go:175] Store started on dir: /home/raid6/seaweed/voltesting with 14 volumes max 100 I0216 22:25:09 9983 disk_location.go:178] Store started on dir: /home/raid6/seaweed/voltesting with 0 ec shards I0216 22:25:09 9983 volume_grpc_client_to_master.go:52] Volume server start with seed master nodes: [172.16.100.1:9351] I0216 22:25:09 9983 volume.go:351] Start Seaweed volume server 30GB 2.26 b9b5b932 at 172.16.100.1:9831 I0216 22:25:09 9983 volume_grpc_client_to_master.go:114] Heartbeat to: 172.16.100.1:9351  This is the Log in volume when wrong jwt  I0216 22:27:26 9983 common.go:53] response method:GET URL:/1,18ee7f3466 with httpStatus:401 and JSON:{""error"":""wrong jwt""} I0216 22:27:27 9983 common.go:53] response method:GET URL:/1,18ee7f3466 with httpStatus:401 and JSON:{""error"":""wrong jwt""} I0216 22:28:32 9983 common.go:53] response method:GET URL:/1,18ee7f3466 with httpStatus:401 and JSON:{""error"":""wrong jwt""} I0216 22:28:34 9983 common.go:53] response method:GET URL:/1,18ee7f3466 with httpStatus:401 and JSON:{""error"":""wrong jwt""}  Security.toml file syntax:  # Put this file to one of the location, with descending priority # ./security.toml # $HOME/.seaweedfs/security.toml # /etc/seaweedfs/security.toml # this file is read by master, volume server, and filer # the jwt signing key is read by master and volume server. # a jwt defaults to expire after 10 seconds. [jwt.signing] key = ""NotShownToThePublic"" expires_after_seconds = 120 # seconds 10 default # jwt for read is only supported with master+volume setup. Filer does not support this mode. [jwt.signing.read] key = ""NotShownToThePublic"" expires_after_seconds = 120 # seconds # volume server also uses grpc that should be secured. # all grpc tls authentications are mutual # the values for the following ca, cert, and key are paths to the PERM files. # the host name is not checked, so the PERM files can be shared. [grpc] ca = ""/home/certstrap/certstrap/out/smtCertAuth.crt"" [grpc.volume] cert = ""/home/certstrap/certstrap/out/volume01.crt"" key = ""/home/certstrap/certstrap/out/volume01.key"" [grpc.master] cert = ""/home/certstrap/certstrap/out/master01.crt"" key = ""/home/certstrap/certstrap/out/master01.key"" [grpc.filer] cert = ""/home/certstrap/certstrap/out/filer01.crt"" key = ""/home/certstrap/certstrap/out/filer01.key"" # use this for any place needs a grpc client # i.e., ""weed backup|benchmark|filer.copy|filer.replicate|mount|s3|upload"" [grpc.client] cert = ""/home/certstrap/certstrap/out/client01.crt"" key = ""/home/certstrap/certstrap/out/client01.key""  If I turn of the security (jwt). The server is working perfect. I can save, show, delete object",source-file,jwt wrong jwt describe bug getting wrong jwt users lennie curl http dir lookup volumeid read yes http authorization bearer eyjhbgcioijiuzi niisinr cci ikpxvcj eyjmawqioiiilcjlehaioje mtm mti ndn axgun tszlfwfzqniuyqqxdcmbptxvnfpdk content type application json date feb gmt content length volumeid locations url publicurl volume query users lennie curl http authorization bearer eyjhbgcioijiuzi niisinr cci ikpxvcj eyjmawqioiiilcjlehaioje mtm mti ndn axgun tszlfwfzqniuyqqxdcmbptxvnfpdk http unauthorized content type application json server volume date feb gmt content length wrong jwt read https github chrislusf issues one server time correct system get jwt token curl http dir lookup volumeid read yes jwt response authorization bearer eyjhbgcioijiuzi niisinr cci ikpxvcj eyjmawqioiiilcjlehaioje mtm mtm mtn vfbcvhehbmhmxwzrtpzotysu dqwtkgs vaopdliwy authorization bearer eyjhbgcioijiuzi niisinr cci ikpxvcj eyjmawqioiiilcjlehaioje mtm mtm mtd ypl dnuw karmwiap xlqdykan nhhnyury authorization bearer eyjhbgcioijiuzi niisinr cci ikpxvcj eyjmawqioiiilcjlehaioje mtm mtm mjb cruazyfbcedu dokae rjpw fpjp pok authorization bearer eyjhbgcioijiuzi niisinr cci ikpxvcj eyjmawqioiiilcjlehaioje mtm mtm mjf mrjq wtxu jnfpakigijp cgrodim tows prda underscore dash base encoded values question get underscores running ubuntu lts weed linux amd starting whit command weed mdir home raid seaweed mastershare bind port log util folder home raid seaweed mastershare permission rwxr current peers server volume size limit server adminscripts start seaweed raft server starting raftserver raft server current cluster leader start seaweed grpc server masterclient existing leader found raft server initializing new cluster server leader event server becomes leader grpc server client filer masterclient redirected leader grpc server client node topo adds child soder node topo soder adds child node topo soder adds child grpc server added volume server volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable volume layout volume becomes writable grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server see new volume grpc server send filer url public url new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids data center soder grpc server send url public url new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids new vids data center soder volume starting whit command root spinky home raid seaweed weed volume max bind port mserver dir home raid seaweed voltesting datacenter soder rack log util folder home raid seaweed voltesting permission rwxr volume loading loading index home raid seaweed voltesting idx memory volume loading loading index home raid seaweed voltesting filestorage idx memory volume loading loading index home raid seaweed voltesting idx memory volume loading loading index home raid seaweed voltesting idx memory volume loading loading index home raid seaweed voltesting idx memory volume loading loading index home raid seaweed voltesting idx memory volume loading loading index home raid seaweed voltesting idx memory volume loading loading index home raid seaweed voltesting idx memory disk location data home raid seaweed voltesting dat replicaplacement size ttl disk location data home raid seaweed voltesting dat replicaplacement size ttl volume loading loading index home raid seaweed voltesting filestorage idx memory disk location data home raid seaweed voltesting dat replicaplacement size ttl disk location data home raid seaweed voltesting filestorage dat replicaplacement size ttl volume loading loading index home raid seaweed voltesting filestorage idx memory volume loading loading index home raid seaweed voltesting filestorage idx memory volume loading loading index home raid seaweed voltesting filestorage idx memory volume loading loading index home raid seaweed voltesting filestorage idx memory disk location data home raid seaweed voltesting dat replicaplacement size ttl volume loading loading index home raid seaweed voltesting filestorage idx memory disk location data home raid seaweed voltesting dat replicaplacement size ttl disk location data home raid seaweed voltesting dat replicaplacement size ttl disk location data home raid seaweed voltesting dat replicaplacement size ttl disk location data home raid seaweed voltesting filestorage dat replicaplacement size ttl disk location data home raid seaweed voltesting filestorage dat replicaplacement size ttl disk location data home raid seaweed voltesting filestorage dat replicaplacement size ttl disk location data home raid seaweed voltesting filestorage dat replicaplacement size ttl disk location data home raid seaweed voltesting filestorage dat replicaplacement size ttl disk location data home raid seaweed voltesting filestorage dat replicaplacement size ttl disk location store started dir home raid seaweed voltesting volumes max disk location store started dir home raid seaweed voltesting shards volume grpc client volume server start seed nodes volume start seaweed volume server volume grpc client heartbeat log volume wrong jwt common response method get url httpstatus json wrong jwt common response method get url httpstatus json wrong jwt common response method get url httpstatus json wrong jwt common response method get url httpstatus json wrong jwt security toml syntax put one location descending priority security toml home security toml etc security toml read volume server filer jwt signing key read volume server jwt defaults expire seconds jwt signing key notshowntothepublic expires seconds seconds default jwt read supported volume setup filer mode jwt signing read key notshowntothepublic expires seconds seconds volume server also uses grpc secured grpc tls authentications mutual values following cert key paths perm files host name checked perm files shared grpc home certstrap certstrap smtcertauth crt grpc volume cert home certstrap certstrap volume crt key home certstrap certstrap volume key grpc cert home certstrap certstrap crt key home certstrap certstrap key grpc filer cert home certstrap certstrap filer crt key home certstrap certstrap filer key use place needs grpc client weed backup benchmark filer copy filer replicate mount upload grpc client cert home certstrap certstrap client crt key home certstrap certstrap client key turn security jwt server working perfect save show delete object,bug,0.9,"Jwt issue - wrong jwt **Describe the bug** Getting wrong jwt code from master.  C:\Users\lennie>curl -i ""http://172.16.100.1:9351/dir/lookup?volumeId=1,18ee7f3466&read=yes"" HTTP/1.1 200 OK Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTI0NDN9.AxGUN17e_RZ5g4tSZLFWfzqnIuYQqxDCMBPTxvnFpdk Content-Type: application/json Date: Tue, 16 Feb 2021 21:52:03 GMT Content-Length: 90 {""volumeId"":""1"",""locations"":[{""url"":""172.16.100.1:9831"",""publicUrl"":""172.16.100.1:9831""}]}  And then volume query.  C:\Users\lennie>curl -i ""http://172.16.100.1:9831/1,18ee7f3466"" -H ""Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTI0NDN9.AxGUN17e_RZ5g4tSZLFWfzqnIuYQqxDCMBPTxvnFpdk"" HTTP/1.1 401 Unauthorized Content-Type: application/json Server: SeaweedFS Volume 30GB 2.26 Date: Tue, 16 Feb 2021 21:52:27 GMT Content-Length: 21 {""error"":""wrong jwt""}  I Have read issue https://github.com/chrislusf/seaweedfs/issues/1399 But i only have one server and the time is correct on the system. When i do some request to the master to get the jwt token i can se this.  curl -i ""http://172.16.100.1:9351/dir/lookup?volumeId=1,18ee7f3466&read=yes""  Jwt response.  Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTM2MTN9.VfbCVHEhBMHMxWZrTpzOTYSU8DqWtKGs5_vaopdLiwY Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTM2MTd9.9yPL3dNuW3J4p68KARMWIAp_XlqDYkAN-NHHnYUrY4Q Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTM2MjB9.crUaZYFBCEDu57dOKAE-rJPw5nS-fpJP11Oz6Bv0Pok Authorization: BEARER eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmaWQiOiIiLCJleHAiOjE2MTM1MTM2MjF9.mRJQ0WtXu5JNfPAKIGiJP0CGrodiM_i8tOws9H7PRdA  There should not be underscore or dash in the base64 encoded values? So the question is. Why do I get underscores? I am running: OS: Ubuntu 18.04.4 LTS And weed version : version 30GB 2.26 b9b5b932 linux amd64 The Master is starting whit this command.  weed master -mdir=""/home/raid6/seaweed/mastershare"" -ip=""172.16.100.1"" -ip.bind=""172.16.100.1"" -port=9351  Log  I0216 22:42:56 19422 file_util.go:23] Folder /home/raid6/seaweed/mastershare Permission: -rwxr-xr-x I0216 22:42:56 19422 master.go:168] current: 172.16.100.1:9351 peers: I0216 22:42:56 19422 master_server.go:107] Volume Size Limit is 30000 MB I0216 22:42:56 19422 master_server.go:192] adminScripts: I0216 22:42:56 19422 master.go:122] Start Seaweed Master 30GB 2.26 b9b5b932 at 172.16.100.1:9351 I0216 22:42:56 19422 raft_server.go:70] Starting RaftServer with 172.16.100.1:9351 I0216 22:42:56 19422 raft_server.go:129] current cluster leader: I0216 22:42:56 19422 master.go:146] Start Seaweed Master 30GB 2.26 b9b5b932 grpc server at 172.16.100.1:19351 I0216 22:42:57 19422 masterclient.go:78] No existing leader found! I0216 22:42:57 19422 raft_server.go:154] Initializing new cluster I0216 22:42:57 19422 master_server.go:141] leader change event: => 172.16.100.1:9351 I0216 22:42:57 19422 master_server.go:143] [ 172.16.100.1:9351 ] 172.16.100.1:9351 becomes leader. I0216 22:43:00 19422 master_grpc_server.go:252] + client filer@172.16.100.1:8888 I0216 22:43:01 19422 masterclient.go:126] redirected to leader 172.16.100.1:9351 I0216 22:43:01 19422 master_grpc_server.go:252] + client master@172.16.100.1:37496 I0216 22:43:04 19422 node.go:322] topo adds child soder I0216 22:43:04 19422 node.go:322] topo:soder adds child r1u1 I0216 22:43:04 19422 node.go:322] topo:soder:r1u1 adds child 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:71] added volume server 172.16.100.1:9831 I0216 22:43:04 19422 volume_layout.go:354] Volume 13 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 10 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 8 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 7 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 6 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 3 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 1 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 11 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 2 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 12 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 5 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 4 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 9 becomes writable I0216 22:43:04 19422 volume_layout.go:354] Volume 14 becomes writable I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 13 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 10 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 8 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 7 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 6 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 3 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 1 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 11 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 2 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 12 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 5 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 4 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 9 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:112] master see new volume 14 from 172.16.100.1:9831 I0216 22:43:04 19422 master_grpc_server.go:157] master send to filer@172.16.100.1:8888: url:""172.16.100.1:9831"" public_url:""172.16.100.1:9831"" new_vids:13 new_vids:10 new_vids:8 new_vids:7 new_vids:6 new_vids:3 new_vids:1 new_vids:11 new_vids:2 new_vids:12 new_vids:5 new_vids:4 new_vids:9 new_vids:14 data_center:""soder"" I0216 22:43:04 19422 master_grpc_server.go:157] master send to master@172.16.100.1:37496: url:""172.16.100.1:9831"" public_url:""172.16.100.1:9831"" new_vids:13 new_vids:10 new_vids:8 new_vids:7 new_vids:6 new_vids:3 new_vids:1 new_vids:11 new_vids:2 new_vids:12 new_vids:5 new_vids:4 new_vids:9 new_vids:14 data_center:""soder""  The volume is starting whit this command.  root@spinky:/home/raid6/seaweed# weed volume -max=100 -ip=""172.16.100.1"" -ip.bind=""172.16.100.1"" -port=9831 -mserver=""172.16.100.1:9351"" -dir=""/home/raid6/seaweed/voltesting"" -dataCenter=""soder"" -rack=""r1u1""  Log  I0216 22:25:09 9983 file_util.go:23] Folder /home/raid6/seaweed/voltesting Permission: -rwxr-xr-x I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/2.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_12.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/5.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/3.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/1.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/7.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/6.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/4.idx to memory I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/2.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/3.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_10.idx to memory I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/7.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_12.dat, replicaPlacement=000 v=3 size=1864 ttl= I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_13.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_11.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_8.idx to memory I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_14.idx to memory I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/4.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 volume_loading.go:127] loading index /home/raid6/seaweed/voltesting/filestorage_9.idx to memory I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/1.dat, replicaPlacement=000 v=3 size=112 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/5.dat, replicaPlacement=000 v=3 size=104 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/6.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_10.dat, replicaPlacement=000 v=3 size=7900832 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_13.dat, replicaPlacement=000 v=3 size=720 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_9.dat, replicaPlacement=000 v=3 size=8 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_11.dat, replicaPlacement=000 v=3 size=664 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_8.dat, replicaPlacement=000 v=3 size=1600 ttl= I0216 22:25:09 9983 disk_location.go:133] data file /home/raid6/seaweed/voltesting/filestorage_14.dat, replicaPlacement=000 v=3 size=30789912 ttl= I0216 22:25:09 9983 disk_location.go:175] Store started on dir: /home/raid6/seaweed/voltesting with 14 volumes max 100 I0216 22:25:09 9983 disk_location.go:178] Store started on dir: /home/raid6/seaweed/voltesting with 0 ec shards I0216 22:25:09 9983 volume_grpc_client_to_master.go:52] Volume server start with seed master nodes: [172.16.100.1:9351] I0216 22:25:09 9983 volume.go:351] Start Seaweed volume server 30GB 2.26 b9b5b932 at 172.16.100.1:9831 I0216 22:25:09 9983 volume_grpc_client_to_master.go:114] Heartbeat to: 172.16.100.1:9351  This is the Log in volume when wrong jwt  I0216 22:27:26 9983 common.go:53] response method:GET URL:/1,18ee7f3466 with httpStatus:401 and JSON:{""error"":""wrong jwt""} I0216 22:27:27 9983 common.go:53] response method:GET URL:/1,18ee7f3466 with httpStatus:401 and JSON:{""error"":""wrong jwt""} I0216 22:28:32 9983 common.go:53] response method:GET URL:/1,18ee7f3466 with httpStatus:401 and JSON:{""error"":""wrong jwt""} I0216 22:28:34 9983 common.go:53] response method:GET URL:/1,18ee7f3466 with httpStatus:401 and JSON:{""error"":""wrong jwt""}  Security.toml file syntax:  # Put this file to one of the location, with descending priority # ./security.toml # $HOME/.seaweedfs/security.toml # /etc/seaweedfs/security.toml # this file is read by master, volume server, and filer # the jwt signing key is read by master and volume server. # a jwt defaults to expire after 10 seconds. [jwt.signing] key = ""NotShownToThePublic"" expires_after_seconds = 120 # seconds 10 default # jwt for read is only supported with master+volume setup. Filer does not support this mode. [jwt.signing.read] key = ""NotShownToThePublic"" expires_after_seconds = 120 # seconds # volume server also uses grpc that should be secured. # all grpc tls authentications are mutual # the values for the following ca, cert, and key are paths to the PERM files. # the host name is not checked, so the PERM files can be shared. [grpc] ca = ""/home/certstrap/certstrap/out/smtCertAuth.crt"" [grpc.volume] cert = ""/home/certstrap/certstrap/out/volume01.crt"" key = ""/home/certstrap/certstrap/out/volume01.key"" [grpc.master] cert = ""/home/certstrap/certstrap/out/master01.crt"" key = ""/home/certstrap/certstrap/out/master01.key"" [grpc.filer] cert = ""/home/certstrap/certstrap/out/filer01.crt"" key = ""/home/certstrap/certstrap/out/filer01.key"" # use this for any place needs a grpc client # i.e., ""weed backup|benchmark|filer.copy|filer.replicate|mount|s3|upload"" [grpc.client] cert = ""/home/certstrap/certstrap/out/client01.crt"" key = ""/home/certstrap/certstrap/out/client01.key""  If I turn of the security (jwt). The server is working perfect. I can save, show, delete object"
2039,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2039,Unexpected response to s3 GET call on non-existent bucket,"**Describe the bug** Running a ""GET /nonexistent-bucket?acl"" - which is what a number of libraries use for ""does this bucket exist"" call, returns a ListBucketResult. According to S3 api docs, it should return a 'AccessControlPolicy' or at least a 404 for the nonexistent resource. **System Setup** docker run -p 8333:8333 chrislusf/seaweedfs server -s3  nneul@optic:~ $ curl http://localhost:8333/nonexistent-bucket?acl <?xml version=""1.0"" encoding=""UTF-8""?> <ListBucketResult xmlns=""http://s3.amazonaws.com/doc/2006-03-01/""><Name>nonexistent-bucket</Name><Prefix></Prefix><Marker></Marker><MaxKeys>10000</MaxKeys><IsTruncated>false</IsTruncated></ListBucketResult>  **Expected behavior** AccessControlPolicy in response - even if it's a dummy/empty policy document, or a 404 or other error indicating it couldn't handle the request due to the bucket not being there.",source-file,unexpected response get call non existent bucket describe bug running get nonexistent bucket acl number libraries use bucket exist call returns listbucketresult according api docs return accesscontrolpolicy least nonexistent resource system setup docker chrislusf server nneul optic curl http localhost nonexistent bucket acl xml encoding utf listbucketresult xmlns http amazonaws name nonexistent bucket name prefix prefix marker marker maxkeys maxkeys istruncated false istruncated listbucketresult expected behavior accesscontrolpolicy response even dummy empty policy document indicating handle due bucket,bug,0.95,"Unexpected response to s3 GET call on non-existent bucket **Describe the bug** Running a ""GET /nonexistent-bucket?acl"" - which is what a number of libraries use for ""does this bucket exist"" call, returns a ListBucketResult. According to S3 api docs, it should return a 'AccessControlPolicy' or at least a 404 for the nonexistent resource. **System Setup** docker run -p 8333:8333 chrislusf/seaweedfs server -s3  nneul@optic:~ $ curl http://localhost:8333/nonexistent-bucket?acl <?xml version=""1.0"" encoding=""UTF-8""?> <ListBucketResult xmlns=""http://s3.amazonaws.com/doc/2006-03-01/""><Name>nonexistent-bucket</Name><Prefix></Prefix><Marker></Marker><MaxKeys>10000</MaxKeys><IsTruncated>false</IsTruncated></ListBucketResult>  **Expected behavior** AccessControlPolicy in response - even if it's a dummy/empty policy document, or a 404 or other error indicating it couldn't handle the request due to the bucket not being there."
82,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/82,get content type error,"hi, I use weed-fs 1 years ago. It is an old version [0.45] of weed. I post an image to weedfs, then I get it from browser , It will display correctly. curl -F file=@/home/chris/myphoto.jpg http://localhost:9333/submit when I use 0.67 of weed. I do the same post, But the same images cannot display in the browser correctly. It cannot display then do download in the chrome. what is wrong about the weed version up? How can I config to this get header output ? please help me.",config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | documentation-file | container-file | other-file | other-file | other-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | other-file | config-file | other-file | config-file | source-file | source-file | source-file | source-file | source-file | other-file | config-file | config-file | config-file | source-file | source-file | config-file | config-file | source-file | source-file | test-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | other-file | source-file | test-file | source-file | source-file | source-file,get content type use weed years ago old weed post image weedfs get browser display correctly curl home chris myphoto jpg http localhost submit use weed post images cannot display browser correctly cannot display download chrome wrong weed config get header output please help,bug,0.85,"get content type error hi, I use weed-fs 1 years ago. It is an old version [0.45] of weed. I post an image to weedfs, then I get it from browser , It will display correctly. curl -F file=@/home/chris/myphoto.jpg http://localhost:9333/submit when I use 0.67 of weed. I do the same post, But the same images cannot display in the browser correctly. It cannot display then do download in the chrome. what is wrong about the weed version up? How can I config to this get header output ? please help me."
214,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/214,seaweedfs should check WhiteList before proxyToLeader,"` r.HandleFunc(""/"", ms.uiStatusHandler) r.HandleFunc(""/ui/index.html"", ms.uiStatusHandler) r.HandleFunc(""/dir/assign"", ms.proxyToLeader(ms.guard.WhiteList(ms.dirAssignHandler r.HandleFunc(""/dir/lookup"", ms.proxyToLeader(ms.guard.WhiteList(ms.dirLookupHandler r.HandleFunc(""/dir/join"", ms.proxyToLeader(ms.guard.WhiteList(ms.dirJoinHandler r.HandleFunc(""/dir/status"", ms.proxyToLeader(ms.guard.WhiteList(ms.dirStatusHandler r.HandleFunc(""/col/delete"", ms.proxyToLeader(ms.guard.WhiteList(ms.collectionDeleteHandler r.HandleFunc(""/vol/lookup"", ms.proxyToLeader(ms.guard.WhiteList(ms.volumeLookupHandler ` If a request not in whitelist is sent to a non-leader master, this request will be handled by leader, which is unexpected.",source-file | source-file,check whitelist proxytoleader handlefunc uistatushandler handlefunc index html uistatushandler handlefunc dir assign proxytoleader guard whitelist dirassignhandler handlefunc dir lookup proxytoleader guard whitelist dirlookuphandler handlefunc dir join proxytoleader guard whitelist dirjoinhandler handlefunc dir status proxytoleader guard whitelist dirstatushandler handlefunc col delete proxytoleader guard whitelist collectiondeletehandler handlefunc vol lookup proxytoleader guard whitelist volumelookuphandler whitelist sent non leader handled leader unexpected,bug,0.85,"seaweedfs should check WhiteList before proxyToLeader ` r.HandleFunc(""/"", ms.uiStatusHandler) r.HandleFunc(""/ui/index.html"", ms.uiStatusHandler) r.HandleFunc(""/dir/assign"", ms.proxyToLeader(ms.guard.WhiteList(ms.dirAssignHandler r.HandleFunc(""/dir/lookup"", ms.proxyToLeader(ms.guard.WhiteList(ms.dirLookupHandler r.HandleFunc(""/dir/join"", ms.proxyToLeader(ms.guard.WhiteList(ms.dirJoinHandler r.HandleFunc(""/dir/status"", ms.proxyToLeader(ms.guard.WhiteList(ms.dirStatusHandler r.HandleFunc(""/col/delete"", ms.proxyToLeader(ms.guard.WhiteList(ms.collectionDeleteHandler r.HandleFunc(""/vol/lookup"", ms.proxyToLeader(ms.guard.WhiteList(ms.volumeLookupHandler ` If a request not in whitelist is sent to a non-leader master, this request will be handled by leader, which is unexpected."
4143,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/4143,The bucket naming rules conflict when docking with minio,"When uploading buckets through java minio, minio prompts me that the buckets to be transferred do not conform to the naming rules of S3 buckets My bucket name is 10000_ test However, it cannot be used in the S3 naming specification_ To name it. aws-cli passed the name, but the seaweedfs server did not verify the bucket name So I suggest checking the incoming bucket name on the seeweedfs server to ensure that the system has stronger compatibility",source-file,bucket naming rules conflict docking minio uploading buckets minio minio prompts buckets transferred conform naming rules buckets bucket name however cannot used naming specification name aws cli passed name server verify bucket name suggest checking incoming bucket name seeweedfs server ensure system stronger compatibility,bug,0.85,"The bucket naming rules conflict when docking with minio When uploading buckets through java minio, minio prompts me that the buckets to be transferred do not conform to the naming rules of S3 buckets My bucket name is 10000_ test However, it cannot be used in the S3 naming specification_ To name it. aws-cli passed the name, but the seaweedfs server did not verify the bucket name So I suggest checking the incoming bucket name on the seeweedfs server to ensure that the system has stronger compatibility"
26,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/26,File id assign with collection does not work,"File id assign with collection does not work on master server curl http://master:9333/dir/assign?collection=pictures returns {""fid"":""22,0139804a34"",""url"":""127.0.0.1:8080"",""publicUrl"":""172.17.42.1:8080"",""count"":1} But volume id 22 is not collection named volume id such as 22.dat 22.idx No new collection is created with above command. weed version is 0.65 linux amd64 Thanks in advance",documentation-file | documentation-file | config-file | other-file | config-file | config-file | config-file | config-file | config-file | config-file | test-file | source-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file,assign collection work assign collection work server curl http dir assign collection pictures returns fid url publicurl count volume collection named volume dat idx new collection created command weed linux amd thanks advance,bug,0.85,"File id assign with collection does not work File id assign with collection does not work on master server curl http://master:9333/dir/assign?collection=pictures returns {""fid"":""22,0139804a34"",""url"":""127.0.0.1:8080"",""publicUrl"":""172.17.42.1:8080"",""count"":1} But volume id 22 is not collection named volume id such as 22.dat 22.idx No new collection is created with above command. weed version is 0.65 linux amd64 Thanks in advance"
861,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/861,TTL marshal result is empty influencing /vol/status api result,"Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** because TTL's members are invisible out of package, so when json.Marshal is called, all the TTL info are empty, which influences the /vol/status api golang type TTL struct { count byte unit byte } ",source-file | source-file,ttl marshal result empty influencing vol status api result sponsors via patreon https www patreon describe bug ttl members invisible package json marshal called ttl empty influences vol status api golang type ttl struct count byte unit byte,bug,0.9,"TTL marshal result is empty influencing /vol/status api result Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** because TTL's members are invisible out of package, so when json.Marshal is called, all the TTL info are empty, which influences the /vol/status api golang type TTL struct { count byte unit byte } "
913,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/913,allocating all available volumes fails,"Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** I tried to allocate all available volumes (which was 6,400 volumes) and it allocated all but 42 of the volumes and then threw an error (then I was able to allocate the last 42 volumes successfully)  $ curl ""http://seaweed-master:9333/vol/grow?collection=test&count=6400"" {""error"":""Failed to assign 6625: rpc error: code = Unknown desc = No more free space left""} $ curl ""http://seaweed-master:9333/vol/grow?collection=test&count=6400"" {""error"":""Only 42 volumes left! Not enough for 6400""} $ curl ""http://seaweed-master:9333/vol/grow?collection=test&count=42"" {""count"":42}  (sorry, didn't get screen shots) after deleting all volumes and running again, I got got a different error and strangely I see 6,400 volumes allocated but it says 397 volumes free (which should be 0 volumes free)  $ curl ""http://seaweed-master:9333/vol/grow?collection=test&count=6400"" {""error"":""Failed to assign 6669: rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \""transport: Error while dialing dial tcp 192.168.X.XXX:18080: connect: connection refused\""""}  and strangely it say 397 volumes are free ![image](https://user-images.githubusercontent.com/4112046/55459751-1b730b80-55a5-11e9-9bd8-7121623f067b.png) even though the volume servers show a total of 6400 volumes ![image](https://user-images.githubusercontent.com/4112046/55459802-3c3b6100-55a5-11e9-9249-359442fc9d19.png) **System Setup** `master` `filer` and `s3` on one machine and `volume` on 4 separate machines **Expected behavior** allocating all available volumes should succeed and counts should make sense **Screenshots** n/a **Additional context** n/a",source-file | source-file | source-file | source-file | source-file,allocating available volumes fails sponsors via patreon https www patreon describe bug tried allocate available volumes volumes allocated volumes threw able allocate last volumes successfully curl http seaweed vol grow collection count failed assign rpc unknown desc free space left curl http seaweed vol grow collection count volumes left enough curl http seaweed vol grow collection count count sorry get screen shots deleting volumes running got got different strangely see volumes allocated says volumes free volumes free curl http seaweed vol grow collection count failed assign rpc unavailable desc subconns transientfailure latest connection connection desc transport dialing dial tcp xxx connect connection refused strangely say volumes free image https user images githubusercontent png even though volume servers show total volumes image https user images githubusercontent png system setup filer one machine volume separate machines expected behavior allocating available volumes succeed counts make sense screenshots additional context,bug,0.9,"allocating all available volumes fails Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** I tried to allocate all available volumes (which was 6,400 volumes) and it allocated all but 42 of the volumes and then threw an error (then I was able to allocate the last 42 volumes successfully)  $ curl ""http://seaweed-master:9333/vol/grow?collection=test&count=6400"" {""error"":""Failed to assign 6625: rpc error: code = Unknown desc = No more free space left""} $ curl ""http://seaweed-master:9333/vol/grow?collection=test&count=6400"" {""error"":""Only 42 volumes left! Not enough for 6400""} $ curl ""http://seaweed-master:9333/vol/grow?collection=test&count=42"" {""count"":42}  (sorry, didn't get screen shots) after deleting all volumes and running again, I got got a different error and strangely I see 6,400 volumes allocated but it says 397 volumes free (which should be 0 volumes free)  $ curl ""http://seaweed-master:9333/vol/grow?collection=test&count=6400"" {""error"":""Failed to assign 6669: rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \""transport: Error while dialing dial tcp 192.168.X.XXX:18080: connect: connection refused\""""}  and strangely it say 397 volumes are free ![image](https://user-images.githubusercontent.com/4112046/55459751-1b730b80-55a5-11e9-9bd8-7121623f067b.png) even though the volume servers show a total of 6400 volumes ![image](https://user-images.githubusercontent.com/4112046/55459802-3c3b6100-55a5-11e9-9249-359442fc9d19.png) **System Setup** `master` `filer` and `s3` on one machine and `volume` on 4 separate machines **Expected behavior** allocating all available volumes should succeed and counts should make sense **Screenshots** n/a **Additional context** n/a"
1161,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1161,How do I create an empty directory?,"This is what I tried: curl -X PUT ""http://localhost:8888/test/"" => {""error"":""can not to write to folder /test/ without a file name""} curl -X POST ""http://localhost:8888/test/"" => {""error"":""request Content-Type isn't multipart/form-data""} curl -X PUT ""http://localhost:8888/test"" => creates a 0 byte file curl -X POST ""http://localhost:8888/test"" => {""error"":""request Content-Type isn't multipart/form-data""}",source-file,create empty directory tried curl put http localhost write folder without name curl post http localhost content type multipart form data curl put http localhost creates byte curl post http localhost content type multipart form data,bug,0.9,"How do I create an empty directory? This is what I tried: curl -X PUT ""http://localhost:8888/test/"" => {""error"":""can not to write to folder /test/ without a file name""} curl -X POST ""http://localhost:8888/test/"" => {""error"":""request Content-Type isn't multipart/form-data""} curl -X PUT ""http://localhost:8888/test"" => creates a 0 byte file curl -X POST ""http://localhost:8888/test"" => {""error"":""request Content-Type isn't multipart/form-data""}"
543,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/543,benchmark report 404 on /dir/assign,"Hi, I need some help here. I set up a seaweadfs cluster, which works fine. Passed all my reads/writes tests. But when I run ./weed benchmark command, it reports 100% 404 Not Found on /dir/assign request: ./weed benchmark -server=127.0.0.1:9333 -n=100  Writing Benchmark  writing file error: http://127.0.0.1:9333/dir/assign: 404 Not Found writing file error: http://127.0.0.1:9333/dir/assign: 404 Not Found 98 more of this I tested http://127.0.0.1:9333/dir/assign with both curl and browser, it works well: curl http://127.0.0.1:9333/dir/assign {""fid"":""9,07a4983afa"",""url"":""someurl:8080"",""publicUrl"":""someurl:18080"",""count"":1} I use version 0.76, linux_arm64.tar.gz my machine runs on Centos7, kernel 4.4.36-1.el7.elrepo.x86_64 thanks in advance",source-file | source-file | source-file,benchmark report dir assign need help set seaweadfs cluster works fine passed reads writes tests weed benchmark command reports found dir assign weed benchmark server writing benchmark writing http dir assign found writing http dir assign found tested http dir assign curl browser works well curl http dir assign fid afa url someurl publicurl someurl count use linux arm tar machine runs centos kernel elrepo thanks advance,bug,0.9,"benchmark report 404 on /dir/assign Hi, I need some help here. I set up a seaweadfs cluster, which works fine. Passed all my reads/writes tests. But when I run ./weed benchmark command, it reports 100% 404 Not Found on /dir/assign request: ./weed benchmark -server=127.0.0.1:9333 -n=100  Writing Benchmark  writing file error: http://127.0.0.1:9333/dir/assign: 404 Not Found writing file error: http://127.0.0.1:9333/dir/assign: 404 Not Found 98 more of this I tested http://127.0.0.1:9333/dir/assign with both curl and browser, it works well: curl http://127.0.0.1:9333/dir/assign {""fid"":""9,07a4983afa"",""url"":""someurl:8080"",""publicUrl"":""someurl:18080"",""count"":1} I use version 0.76, linux_arm64.tar.gz my machine runs on Centos7, kernel 4.4.36-1.el7.elrepo.x86_64 thanks in advance"
5213,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/5213,Master timeouts during dirAssign volume growth,"**Describe the bug** Timeouts when requesting a /dir/assign at the master(s). **System Setup** - master(s) 3 of them, but I get the same issues when I only start one: - `/usr/local/bin/weed -v=3 -logdir=/var/log/seaweedfs master -mdir=/etc/seaweedfs -ip=10.0.9.15 -port=9333 -metrics.address=10.0.9.17:9091 -defaultReplication=010 -volumePreallocate -garbageThreshold=0.3 -volumeSizeLimitMB=20000 -peers=10.0.9.17:9333,10.0.9.14:9333,10.0.9.15:9333` - volume(s) 7 of them, I use different IP's, racks, and volumes. - `/usr/local/bin/weed -v=3 -logdir=/var/log/seaweedfs volume -index=leveldb -mserver=10.0.9.17:9333,10.0.9.14:9333,10.0.9.15:9333 -dir=/volumes/98fb3388c280,/volumes/LHHGS,/volumes/e000c055cbe4,/volumes/c5a9aff45527,/volumes/619c9a0827f4,/volumes/f8c44345756f,/volumes/eeedca023938,/volumes/cae089cd2dd9,/volumes/20F30GRVRD,/volumes/KWEGS,/volumes/3d5638f4fd34,/volumes/18f39b04390d,/volumes/6a9e8c97ba2a,/volumes/LDTGS,/volumes/a9a6e2d048de,/volumes/20F308T27D,/volumes/19641ea6d6c5,/volumes/20F30JB3JE,/volumes/6e19dd8da77b,/volumes/3d1614841bcc,/volumes/372cb7e5ac18,/volumes/152d865d39ce,/volumes/20F305249D,/volumes/1289675b7f03,/volumes/222079443d03,/volumes/cc66d284719d,/volumes/ca6f98cd3c16,/volumes/6611045c7cf2,/volumes/381ee044d930,/volumes/ff81968af32c,/volumes/9d611128cfed,/volumes/21F306AD4F,/volumes/595892cb8709,/volumes/0553ccb52b90 -max=0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 -concurrentDownloadLimitMB=20000 -concurrentUploadLimitMB=20000 -hasSlowRead=true -readBufferSizeMB=8 -compactionMBps=10 -rack=store02 -ip=10.0.9.2` - - OS version - `Debian GNU/Linux 12 (bookworm) / Linux store02 6.1.0-17-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.69-1 (2023-12-30) x86_64 GNU/Linux` - output of `weed version` - version `30GB 3.62 59b8af99b0aca1b9e88fec7b5f27c7d15e5e8604 linux amd64` - no filer only masters & volume servers, we have an own metadata store. **Expected behavior** When we assign/request multiple keys or a single key at the Leader master, we don't expect timeouts. Normally we get an instant response. But sometimes (every x minutes) we get a timeout on the a request like: http://10.0.9.17:9333/dir/assign?collection=nntp&count=10000&replication=001. Also when we lower the count. We sadly don't get an error at this request, but I noticed when this happens I see the following log entry: ` seaweedfs-master[459769]: I0116 14:50:10.468052 master_server_handlers.go:125 dirAssign volume growth {""collection"":""nntp"",""replication"":{""node"":1},""ttl"":{""Count"":0,""Unit"":0}} from 10.0.9.12:40308` It looks that this always happens when there is a dirAssign volume growth. In parallel there are constantly POST requests directly to the volume servers to store data. I thought a work-around was to use replication 010 or 002, but working only for a while. We just started testing SeaweedFS and started with 3.60, but also after the upgrades to 3.62 we still see this issue. ( I didn't tested older versions ) **Additional context** How I test/reproduce it: `while true; do curl --max-time 3 'http://localhost:9333/dir/assign?collection=nntp&replication=001'; echo """" ; done` I get once in a couple of seconds/minutes a timeout (also when I increase the max-time): `curl: (28) Operation timed out after 3000 milliseconds with 0 bytes received` at that moment I see always a volume growth message in the logging: `seaweedfs-master[459769]: I0116 14:50:10.468052 master_server_handlers.go:125 dirAssign volume growth {""collection"":""nntp"",""replication"":{""node"":1},""ttl"":{""Count"":0,""Unit"":0}} from 10.0.9.12:40308` **Screen shot** ![b13912075ec83a54736ed1da4a98b5fcbe](https://github.com/seaweedfs/seaweedfs/assets/11386125/f6dec3c3-c6ae-40cf-a018-3e27fb7f4760) <img width=""1770"" alt=""Screenshot 2024-01-17 at 23 09 24"" src=""https://github.com/seaweedfs/seaweedfs/assets/11386125/e142a531-b314-442e-938e-c49925f41007"">",source-file | source-file | source-file,timeouts dirassign volume growth describe bug timeouts requesting dir assign system setup get issues start one local weed logdir var log mdir etc port metrics address defaultreplication volumepreallocate garbagethreshold volumesizelimitmb peers volume use different racks volumes local weed logdir var log volume index leveldb mserver dir volumes volumes lhhgs volumes cbe volumes aff volumes volumes volumes eeedca volumes cae volumes grvrd volumes kwegs volumes volumes volumes volumes ldtgs volumes volumes volumes volumes volumes volumes bcc volumes volumes volumes volumes volumes volumes volumes volumes volumes volumes volumes cfed volumes volumes volumes ccb max concurrentdownloadlimitmb concurrentuploadlimitmb hasslowread true readbuffersizemb compactionmbps rack store debian gnu linux bookworm linux store amd smp preempt dynamic debian gnu linux output weed aca fec linux amd filer masters volume servers metadata store expected behavior assign multiple keys single key leader expect timeouts normally get instant response sometimes every minutes get timeout like http dir assign collection nntp count replication also lower count sadly get noticed happens see following log entry server handlers dirassign volume growth collection nntp replication node ttl count unit looks always happens dirassign volume growth parallel constantly post requests directly volume servers store data thought work around use replication working started testing started also upgrades still see tested older versions additional context reproduce true curl max time http localhost dir assign collection nntp replication echo done get couple seconds minutes timeout also increase max time curl operation timed milliseconds bytes received moment see always volume growth message logging server handlers dirassign volume growth collection nntp replication node ttl count unit screen shot fcbe https github assets dec img width alt screenshot https github assets,bug,0.9,"Master timeouts during dirAssign volume growth **Describe the bug** Timeouts when requesting a /dir/assign at the master(s). **System Setup** - master(s) 3 of them, but I get the same issues when I only start one: - `/usr/local/bin/weed -v=3 -logdir=/var/log/seaweedfs master -mdir=/etc/seaweedfs -ip=10.0.9.15 -port=9333 -metrics.address=10.0.9.17:9091 -defaultReplication=010 -volumePreallocate -garbageThreshold=0.3 -volumeSizeLimitMB=20000 -peers=10.0.9.17:9333,10.0.9.14:9333,10.0.9.15:9333` - volume(s) 7 of them, I use different IP's, racks, and volumes. - `/usr/local/bin/weed -v=3 -logdir=/var/log/seaweedfs volume -index=leveldb -mserver=10.0.9.17:9333,10.0.9.14:9333,10.0.9.15:9333 -dir=/volumes/98fb3388c280,/volumes/LHHGS,/volumes/e000c055cbe4,/volumes/c5a9aff45527,/volumes/619c9a0827f4,/volumes/f8c44345756f,/volumes/eeedca023938,/volumes/cae089cd2dd9,/volumes/20F30GRVRD,/volumes/KWEGS,/volumes/3d5638f4fd34,/volumes/18f39b04390d,/volumes/6a9e8c97ba2a,/volumes/LDTGS,/volumes/a9a6e2d048de,/volumes/20F308T27D,/volumes/19641ea6d6c5,/volumes/20F30JB3JE,/volumes/6e19dd8da77b,/volumes/3d1614841bcc,/volumes/372cb7e5ac18,/volumes/152d865d39ce,/volumes/20F305249D,/volumes/1289675b7f03,/volumes/222079443d03,/volumes/cc66d284719d,/volumes/ca6f98cd3c16,/volumes/6611045c7cf2,/volumes/381ee044d930,/volumes/ff81968af32c,/volumes/9d611128cfed,/volumes/21F306AD4F,/volumes/595892cb8709,/volumes/0553ccb52b90 -max=0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 -concurrentDownloadLimitMB=20000 -concurrentUploadLimitMB=20000 -hasSlowRead=true -readBufferSizeMB=8 -compactionMBps=10 -rack=store02 -ip=10.0.9.2` - - OS version - `Debian GNU/Linux 12 (bookworm) / Linux store02 6.1.0-17-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.69-1 (2023-12-30) x86_64 GNU/Linux` - output of `weed version` - version `30GB 3.62 59b8af99b0aca1b9e88fec7b5f27c7d15e5e8604 linux amd64` - no filer only masters & volume servers, we have an own metadata store. **Expected behavior** When we assign/request multiple keys or a single key at the Leader master, we don't expect timeouts. Normally we get an instant response. But sometimes (every x minutes) we get a timeout on the a request like: http://10.0.9.17:9333/dir/assign?collection=nntp&count=10000&replication=001. Also when we lower the count. We sadly don't get an error at this request, but I noticed when this happens I see the following log entry: ` seaweedfs-master[459769]: I0116 14:50:10.468052 master_server_handlers.go:125 dirAssign volume growth {""collection"":""nntp"",""replication"":{""node"":1},""ttl"":{""Count"":0,""Unit"":0}} from 10.0.9.12:40308` It looks that this always happens when there is a dirAssign volume growth. In parallel there are constantly POST requests directly to the volume servers to store data. I thought a work-around was to use replication 010 or 002, but working only for a while. We just started testing SeaweedFS and started with 3.60, but also after the upgrades to 3.62 we still see this issue. ( I didn't tested older versions ) **Additional context** How I test/reproduce it: `while true; do curl --max-time 3 'http://localhost:9333/dir/assign?collection=nntp&replication=001'; echo """" ; done` I get once in a couple of seconds/minutes a timeout (also when I increase the max-time): `curl: (28) Operation timed out after 3000 milliseconds with 0 bytes received` at that moment I see always a volume growth message in the logging: `seaweedfs-master[459769]: I0116 14:50:10.468052 master_server_handlers.go:125 dirAssign volume growth {""collection"":""nntp"",""replication"":{""node"":1},""ttl"":{""Count"":0,""Unit"":0}} from 10.0.9.12:40308` **Screen shot** ![b13912075ec83a54736ed1da4a98b5fcbe](https://github.com/seaweedfs/seaweedfs/assets/11386125/f6dec3c3-c6ae-40cf-a018-3e27fb7f4760) <img width=""1770"" alt=""Screenshot 2024-01-17 at 23 09 24"" src=""https://github.com/seaweedfs/seaweedfs/assets/11386125/e142a531-b314-442e-938e-c49925f41007"">"
2065,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2065,docker-registry with seaweedfs weed mount has error,"**Describe the bug** docker-registry with seaweedfs weed mount, push image error **System Setup** - List the command line to start ""weed master"", ""weed volume"", ""weed filer"", ""weed s3"", ""weed mount"".  # setup master weed master -port=9333 -mdir=/tmp/master0 -defaultReplication=000 -ip=localhost # setup volume weed volume -port=8080 -dir=/tmp/volume0 -ip=localhost -mserver=localhost:9333 # setup filer weed filer -port=8888 -ip=localhost -master=localhost:9333 #weed mount weed mount -filer=localhost:8888 -cacheCapacityMB=0 -cacheDir=/tmp/cache001 -dir=/root/mnt #setup docker-registry docker run -p 5001:5000 --rm \ --name registry2 \ -v /root/config.yml:/etc/docker/registry/config.yml -v /root/mnt/data:/data registry:2 #push image root@qa-4:~# docker push 10.10.10.224:5001/ubuntu:16.04 The push refers to repository [10.10.10.224:5001/ubuntu] 1a1a19626b20: Pushing [>] 3.072kB 5b7dc8292d9b: Pushing 11.78kB bbc674332e2e: Pushing [>] 15.87kB da2785b7bb16: Pushing [> ] 525.3kB/130.7MB unknown blob  - OS version `Ubuntu 18.04.4 LTS` - output of `weed version` `version 30GB 2.41 8618526 linux amd64` - if using filer, show the content of `filer.toml` default /root/config.yaml(without cache)  version: 0.1 log: fields: service: registry storage: # cache: # blobdescriptor: inmemory filesystem: rootdirectory: /data http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3  **Expected behavior** push & pull image successfully **Screenshots** ![image](https://user-images.githubusercontent.com/28077875/117948127-6abec500-b343-11eb-8a4e-c6a2e81281c0.png) **Additional context** mastervolumefiler and weed mount had no related warning and error log. if use local filesystem, everything is fine. if `/root/configy.yaml` enable cache:  version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /data http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3  then, push image can be successful, but pull image will error(some small images pull many times will eventually succeedbut large images will not): **pull big images**: btw, the test image can be pulled down from dockerhub(`docker pull jupyter/tensorflow-notebook:ubuntu-20.04`) ![image](https://user-images.githubusercontent.com/28077875/117952978-444f5880-b348-11eb-92f2-09cca527e42e.png) ![image](https://user-images.githubusercontent.com/28077875/117953890-0f8fd100-b349-11eb-846c-be53c01e471a.png) At this time, if stop registry and restart it, then pull image is ok. restart registry use same command:  docker run -p 5001:5000 --rm \ --name registry2 \ -v /root/config.yml:/etc/docker/registry/config.yml -v /root/mnt/data:/data registry:2  **pull small image**(pull many times, eventually succeed): ![image](https://user-images.githubusercontent.com/28077875/117951873-2c2b0980-b347-11eb-98f7-26cb4a7d8508.png) BTW, `version 30GB 2.28 37f104f linux amd64` version does not seem to have the above problem.",source-file,docker registry weed mount describe bug docker registry weed mount push image system setup list command start weed weed volume weed filer weed weed mount setup weed port mdir tmp defaultreplication localhost setup volume weed volume port dir tmp volume localhost mserver localhost setup filer weed filer port localhost localhost weed mount weed mount filer localhost cachecapacitymb cachedir tmp cache dir root mnt setup docker registry docker name registry root config yml etc docker registry config yml root mnt data data registry push image root docker push ubuntu push refers repository ubuntu pushing pushing bbc pushing pushing unknown blob ubuntu lts output weed linux amd filer show content filer toml default root config yaml without cache log fields service registry storage cache blobdescriptor inmemory filesystem rootdirectory data http addr headers content type options nosniff health storagedriver enabled true interval threshold expected behavior push image successfully screenshots image https user images githubusercontent abec png additional context mastervolumefiler weed mount related warning log use local filesystem everything fine root configy yaml enable cache log fields service registry storage cache blobdescriptor inmemory filesystem rootdirectory data http addr headers content type options nosniff health storagedriver enabled true interval threshold push image successful image small images many times eventually succeedbut large images big images btw image pulled dockerhub docker jupyter tensorflow notebook ubuntu image https user images githubusercontent cca png image https user images githubusercontent png time stop registry restart image restart registry use command docker name registry root config yml etc docker registry config yml root mnt data data registry small image many times eventually succeed image https user images githubusercontent png btw linux amd seem,bug,0.9,"docker-registry with seaweedfs weed mount has error **Describe the bug** docker-registry with seaweedfs weed mount, push image error **System Setup** - List the command line to start ""weed master"", ""weed volume"", ""weed filer"", ""weed s3"", ""weed mount"".  # setup master weed master -port=9333 -mdir=/tmp/master0 -defaultReplication=000 -ip=localhost # setup volume weed volume -port=8080 -dir=/tmp/volume0 -ip=localhost -mserver=localhost:9333 # setup filer weed filer -port=8888 -ip=localhost -master=localhost:9333 #weed mount weed mount -filer=localhost:8888 -cacheCapacityMB=0 -cacheDir=/tmp/cache001 -dir=/root/mnt #setup docker-registry docker run -p 5001:5000 --rm \ --name registry2 \ -v /root/config.yml:/etc/docker/registry/config.yml -v /root/mnt/data:/data registry:2 #push image root@qa-4:~# docker push 10.10.10.224:5001/ubuntu:16.04 The push refers to repository [10.10.10.224:5001/ubuntu] 1a1a19626b20: Pushing [>] 3.072kB 5b7dc8292d9b: Pushing 11.78kB bbc674332e2e: Pushing [>] 15.87kB da2785b7bb16: Pushing [> ] 525.3kB/130.7MB unknown blob  - OS version `Ubuntu 18.04.4 LTS` - output of `weed version` `version 30GB 2.41 8618526 linux amd64` - if using filer, show the content of `filer.toml` default /root/config.yaml(without cache)  version: 0.1 log: fields: service: registry storage: # cache: # blobdescriptor: inmemory filesystem: rootdirectory: /data http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3  **Expected behavior** push & pull image successfully **Screenshots** ![image](https://user-images.githubusercontent.com/28077875/117948127-6abec500-b343-11eb-8a4e-c6a2e81281c0.png) **Additional context** mastervolumefiler and weed mount had no related warning and error log. if use local filesystem, everything is fine. if `/root/configy.yaml` enable cache:  version: 0.1 log: fields: service: registry storage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /data http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3  then, push image can be successful, but pull image will error(some small images pull many times will eventually succeedbut large images will not): **pull big images**: btw, the test image can be pulled down from dockerhub(`docker pull jupyter/tensorflow-notebook:ubuntu-20.04`) ![image](https://user-images.githubusercontent.com/28077875/117952978-444f5880-b348-11eb-92f2-09cca527e42e.png) ![image](https://user-images.githubusercontent.com/28077875/117953890-0f8fd100-b349-11eb-846c-be53c01e471a.png) At this time, if stop registry and restart it, then pull image is ok. restart registry use same command:  docker run -p 5001:5000 --rm \ --name registry2 \ -v /root/config.yml:/etc/docker/registry/config.yml -v /root/mnt/data:/data registry:2  **pull small image**(pull many times, eventually succeed): ![image](https://user-images.githubusercontent.com/28077875/117951873-2c2b0980-b347-11eb-98f7-26cb4a7d8508.png) BTW, `version 30GB 2.28 37f104f linux amd64` version does not seem to have the above problem."
2583,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2583,[s3] Bug with backward compatibility of accesses 2.85,"version  2.85   Jan 12, 2022 @ 15:05:13.935 | I0112 10:05:13 1 auth_credentials.go:219] v4 auth type -- | -- | Jan 12, 2022 @ 15:05:13.935 | I0112 10:05:13 1 error_handler.go:85] status 403 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> | Jan 12, 2022 @ 15:05:13.935 | I0112 10:05:13 1 auth_credentials.go:248] user name: cdn actions: [Admin:cdn-*], action: Write | Jan 12, 2022 @ 15:05:13.935 | I0112 10:05:13 1 error_handler.go:85] status 403 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> | Jan 12, 2022 @ 15:05:13.935 | <Error><Code>AccessDenied</Code><Message>Access Denied.</Message><Resource>/d28547ea7e450b9d42b86d2dd1dd89b02598dce2.original</Resource><RequestId>1641981913107869339</RequestId><Key>d28547ea7e450b9d42b86d2dd1dd89b02598dce2.original</Key><BucketName>cdn</BucketName></Error> ",source-file | test-file,bug backward compatibility accesses jan auth credentials auth type jan handler status application xml xml encoding utf jan auth credentials user name cdn actions admin cdn action write jan handler status application xml xml encoding utf jan accessdenied message access denied message resource dce original resource requestid requestid key dce original key bucketname cdn bucketname,bug,0.85,"[s3] Bug with backward compatibility of accesses 2.85 version  2.85   Jan 12, 2022 @ 15:05:13.935 | I0112 10:05:13 1 auth_credentials.go:219] v4 auth type -- | -- | Jan 12, 2022 @ 15:05:13.935 | I0112 10:05:13 1 error_handler.go:85] status 403 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> | Jan 12, 2022 @ 15:05:13.935 | I0112 10:05:13 1 auth_credentials.go:248] user name: cdn actions: [Admin:cdn-*], action: Write | Jan 12, 2022 @ 15:05:13.935 | I0112 10:05:13 1 error_handler.go:85] status 403 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> | Jan 12, 2022 @ 15:05:13.935 | <Error><Code>AccessDenied</Code><Message>Access Denied.</Message><Resource>/d28547ea7e450b9d42b86d2dd1dd89b02598dce2.original</Resource><RequestId>1641981913107869339</RequestId><Key>d28547ea7e450b9d42b86d2dd1dd89b02598dce2.original</Key><BucketName>cdn</BucketName></Error> "
2389,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2389,"[s3test] PutObjectTagging, GetObjectTagging and DeleteObjectTagging","Nexus Blob Storage requires PutObjectTagging, GetObjectTagging and DeleteObjectTagging https://help.sonatype.com/repomanager3/repository-management/storage-guide/configuring-blob-stores#ConfiguringBlobStores-AWSSimpleStorageService(S3)  s3tests_1 |  s3tests_1 | FAIL: s3tests_boto3.functional.test_s3.test_put_obj_with_tags s3tests_1 |  s3tests_1 | Traceback (most recent call last): s3tests_1 | File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest s3tests_1 | self.test(*self.arg) s3tests_1 | File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 11449, in test_put_obj_with_tags s3tests_1 | eq(response_tagset, tagset) s3tests_1 | AssertionError: [{'Key': 'foo', 'Value': 'bar'}] != [{'Key': 'bar', 'Value': ''}, {'Key': 'foo', 'Value': 'bar'}] s3tests_1 | >> raise AssertionError(None or ""%r != %r"" % ([{'Key': 'foo', 'Value': 'bar'}], [{'Key': 'bar', 'Value': ''}, {'Key': 'foo', 'Value': 'bar'}]))   s3tests_1 |  s3tests_1 | FAIL: s3tests_boto3.functional.test_s3.test_put_delete_tags s3tests_1 |  s3tests_1 | Traceback (most recent call last): s3tests_1 | File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest s3tests_1 | self.test(*self.arg) s3tests_1 | File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 11323, in test_put_delete_tags s3tests_1 | eq(response['ResponseMetadata']['HTTPStatusCode'], 200) s3tests_1 | AssertionError: 204 != 200 s3tests_1 | >> raise AssertionError(None or ""%r != %r"" % (204, 200))   s3tests_1 |  s3tests_1 | FAIL: s3tests_boto3.functional.test_s3.test_get_obj_tagging s3tests_1 |  s3tests_1 | Traceback (most recent call last): s3tests_1 | File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest s3tests_1 | self.test(*self.arg) s3tests_1 | File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 11140, in test_get_obj_tagging s3tests_1 | eq(response['ResponseMetadata']['HTTPStatusCode'], 200) s3tests_1 | AssertionError: 204 != 200 s3tests_1 | >> raise AssertionError(None or ""%r != %r"" % (204, 200))   E1020 11:37:12 1 s3api_object_tagging_handlers.go:59] PutObjectTaggingHandler Unmarshal /package-cache-nexus-proxy-cache/nexus/cache/content/vol-31/chap-12/aecabacb-1999-47b8-80f7-1dad5b639ed0.bytes?tagging: expected element <Tagging> in name space http://s3.amazonaws.com/doc/2006-03-01/ but have no name space",source-file | source-file | source-file | source-file,putobjecttagging getobjecttagging deleteobjecttagging nexus blob storage requires putobjecttagging getobjecttagging deleteobjecttagging https help sonatype repomanager repository management storage guide configuring blob stores configuringblobstores awssimplestorageservice tests tests fail tests boto functional put obj tags tests tests traceback recent call last tests opt tests virtualenv python site packages nose case runtest tests self self arg tests opt tests tests boto functional put obj tags tests response tagset tagset tests assertionerror key foo value bar key bar value key foo value bar tests raise assertionerror none key foo value bar key bar value key foo value bar tests tests fail tests boto functional put delete tags tests tests traceback recent call last tests opt tests virtualenv python site packages nose case runtest tests self self arg tests opt tests tests boto functional put delete tags tests response responsemetadata httpstatuscode tests assertionerror tests raise assertionerror none tests tests fail tests boto functional get obj tagging tests tests traceback recent call last tests opt tests virtualenv python site packages nose case runtest tests self self arg tests opt tests tests boto functional get obj tagging tests response responsemetadata httpstatuscode tests assertionerror tests raise assertionerror none api object tagging handlers putobjecttagginghandler unmarshal package cache nexus proxy cache nexus cache content vol chap aecabacb dad bytes tagging expected element tagging name space http amazonaws name space,bug,0.95,"[s3test] PutObjectTagging, GetObjectTagging and DeleteObjectTagging Nexus Blob Storage requires PutObjectTagging, GetObjectTagging and DeleteObjectTagging https://help.sonatype.com/repomanager3/repository-management/storage-guide/configuring-blob-stores#ConfiguringBlobStores-AWSSimpleStorageService(S3)  s3tests_1 |  s3tests_1 | FAIL: s3tests_boto3.functional.test_s3.test_put_obj_with_tags s3tests_1 |  s3tests_1 | Traceback (most recent call last): s3tests_1 | File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest s3tests_1 | self.test(*self.arg) s3tests_1 | File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 11449, in test_put_obj_with_tags s3tests_1 | eq(response_tagset, tagset) s3tests_1 | AssertionError: [{'Key': 'foo', 'Value': 'bar'}] != [{'Key': 'bar', 'Value': ''}, {'Key': 'foo', 'Value': 'bar'}] s3tests_1 | >> raise AssertionError(None or ""%r != %r"" % ([{'Key': 'foo', 'Value': 'bar'}], [{'Key': 'bar', 'Value': ''}, {'Key': 'foo', 'Value': 'bar'}]))   s3tests_1 |  s3tests_1 | FAIL: s3tests_boto3.functional.test_s3.test_put_delete_tags s3tests_1 |  s3tests_1 | Traceback (most recent call last): s3tests_1 | File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest s3tests_1 | self.test(*self.arg) s3tests_1 | File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 11323, in test_put_delete_tags s3tests_1 | eq(response['ResponseMetadata']['HTTPStatusCode'], 200) s3tests_1 | AssertionError: 204 != 200 s3tests_1 | >> raise AssertionError(None or ""%r != %r"" % (204, 200))   s3tests_1 |  s3tests_1 | FAIL: s3tests_boto3.functional.test_s3.test_get_obj_tagging s3tests_1 |  s3tests_1 | Traceback (most recent call last): s3tests_1 | File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest s3tests_1 | self.test(*self.arg) s3tests_1 | File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 11140, in test_get_obj_tagging s3tests_1 | eq(response['ResponseMetadata']['HTTPStatusCode'], 200) s3tests_1 | AssertionError: 204 != 200 s3tests_1 | >> raise AssertionError(None or ""%r != %r"" % (204, 200))   E1020 11:37:12 1 s3api_object_tagging_handlers.go:59] PutObjectTaggingHandler Unmarshal /package-cache-nexus-proxy-cache/nexus/cache/content/vol-31/chap-12/aecabacb-1999-47b8-80f7-1dad5b639ed0.bytes?tagging: expected element <Tagging> in name space http://s3.amazonaws.com/doc/2006-03-01/ but have no name space"
4967,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/4967,webdav bug creates directories in wrong folder,"**Describe the bug** When using `weed webdav -filer.path` for a non `/` path, folders get created on the `/` of the filer instance the `weed webdav` is connected to **System Setup** weed 3.58 on seaweedfs docker container, arguments used for webdav:  -logtostderr webdav -collection=arman -replication=010 -port=${NOMAD_PORT_webdav} -filer=seaweedfs-filer-http.nomad:8888.18888 -filer.path=/buckets/arman -cacheDir=/alloc/data/ -cacheCapacityMB=1024  on seaweedfs docker container, arguments used for filer:  -logtostderr filer -ip=${NOMAD_IP_http} -ip.bind=0.0.0.0 -master=seaweedfs-master-http.nomad:${var.master_port_http}.${var.master_port_grpc} -port=${NOMAD_PORT_http} -port.grpc=${NOMAD_PORT_grpc} -metricsPort=${NOMAD_PORT_metrics} -webdav -webdav.collection= -webdav.replication=020 -webdav.port=${NOMAD_PORT_webdav}  (so as you can see I am running a normal webdav instance with filer, and I am trying to set up a new one on a different container that points to a different path) **Expected behavior** When creating a folder in `weed webdav -filer.path=/buckets/arman` directory gets created under `/buckets/arman` in filer (for the example above). **Actual Behaviour** It gets created under the top-level `/` instead",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file,webdav bug creates directories wrong folder describe bug weed webdav filer path non path folders get created filer instance weed webdav connected system setup weed docker container arguments used webdav logtostderr webdav collection arman replication port nomad port webdav filer filer http nomad filer path buckets arman cachedir alloc data cachecapacitymb docker container arguments used filer logtostderr filer nomad http bind http nomad var port http var port grpc port nomad port http port grpc nomad port grpc metricsport nomad port metrics webdav webdav collection webdav replication webdav port nomad port webdav see running normal webdav instance filer trying set new one different container points different path expected behavior creating folder weed webdav filer path buckets arman directory gets created buckets arman filer example actual behaviour gets created top level instead,bug,0.9,"webdav bug creates directories in wrong folder **Describe the bug** When using `weed webdav -filer.path` for a non `/` path, folders get created on the `/` of the filer instance the `weed webdav` is connected to **System Setup** weed 3.58 on seaweedfs docker container, arguments used for webdav:  -logtostderr webdav -collection=arman -replication=010 -port=${NOMAD_PORT_webdav} -filer=seaweedfs-filer-http.nomad:8888.18888 -filer.path=/buckets/arman -cacheDir=/alloc/data/ -cacheCapacityMB=1024  on seaweedfs docker container, arguments used for filer:  -logtostderr filer -ip=${NOMAD_IP_http} -ip.bind=0.0.0.0 -master=seaweedfs-master-http.nomad:${var.master_port_http}.${var.master_port_grpc} -port=${NOMAD_PORT_http} -port.grpc=${NOMAD_PORT_grpc} -metricsPort=${NOMAD_PORT_metrics} -webdav -webdav.collection= -webdav.replication=020 -webdav.port=${NOMAD_PORT_webdav}  (so as you can see I am running a normal webdav instance with filer, and I am trying to set up a new one on a different container that points to a different path) **Expected behavior** When creating a folder in `weed webdav -filer.path=/buckets/arman` directory gets created under `/buckets/arman` in filer (for the example above). **Actual Behaviour** It gets created under the top-level `/` instead"
4467,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/4467,Master assigns write requests to unavailable volumes,"**Describe the bug** We performed reliability test for SeaweedFS in such a manner: - We wrote files through Filer in a loop - Sometimes we restarted one of Volume Servers (by random) to check how it will work, how many time will take recovery process depending on data size stored in SeaweedFS, how write process will work during partial outage (outage one of Volume Servers), etc. And during this simple test sometimes we observed such a problem: Filer reports that it cannot save the file with one of the following errors:  {""error"":""unmarshalled error http://seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444/16498,4f3cc6ea382612: failed to write to replicas for volume 16498: [seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444]: upload 489.part 4194304 bytes to http://seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444/16498,4f3cc6ea382612?ts=1683812487\u0026ttl=\u0026type=replicate: Post \""http://seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444/16498,4f3cc6ea382612?ts=1683812487\u0026ttl=\u0026type=replicate\"": dial tcp 10.183.0.39:8444: connect: connection refused""}  or  {""error"":""upload 486.part 4194304 bytes to http://seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444/5122,1740bcaa7be3be: Post \""http://seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444/5122,1740bcaa7be3be\"": dial tcp 10.161.3.122:8444: connect: connection refused""}  So, on a first sight it looks quite obvious, we shutdown one of Volume Servers, and Filer cannot write to it (we see `connection refused`). The question is: **_why_** Master assigns volumes for write, if they should be unwritable? Deeper investigation into the issue showed that it could be some issue of synchronization between threads on Master's side. This is a piece of SeaweedFS Volume Server log which was shutdown:  I0511 13:40:18.303550 signal_handling.go:48 exec interrupt hook func name:github.com/seaweedfs/seaweedfs/weed/command.VolumeServerOptions.startVolumeServer.func1 I0511 13:40:18.303585 volume_server.go:139 Stopping volume server volume server has been killed I0511 13:40:18.303671 volume_grpc_client_to_master.go:258 volume server seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 stops and deletes all volumes I0511 13:40:19.587873 volume.go:280 stop send heartbeat and wait 10 seconds until shutdown  I0511 13:40:19.587922 store.go:166 In dir /data0 adds volume:16498 collection:test replicaPlacement:001 ttl: I0511 13:40:19.587969 volume_info.go:20 maybeLoadVolumeInfo checks /data0/test_16498.vif I0511 13:40:19.588117 volume_loading.go:121 open to write file /data0/test_16498.idx I0511 13:40:19.588154 volume_loading.go:142 loading memory index /data0/test_16498.idx to memory I0511 13:40:19.588208 needle_map_memory.go:54 max file key: 0 for file: /data0/test_16498.idx I0511 13:40:19.588721 store_replicate.go:35 replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:19.588734 common.go:113 error JSON response status 500: replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:19.588750 common.go:70 response method:POST URL:/16496,4f3ade2d2f7065 with httpStatus:500 and JSON:{""error"":""replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:19.606846 common.go:106 error writing JSON status /status 200: write tcp 10.183.0.38:8444->10.183.0.1:46920: write: connection reset by peer I0511 13:40:19.606865 common.go:107 JSON content: map[DiskStatuses:[dir:""/data0"" all:98953909501952 used:9914417262592 free:89039492239360 percent_free:89.980774 percent_used:10.019227] <omitted> I0511 13:40:19.731598 disk_location.go:440 dir /data0 disk free 89.98% >= required 1.00% I0511 13:40:19.916394 store.go:170 add volume 16498 I0511 13:40:19.916412 volume_grpc_admin.go:60 assign volume volume_id:16498 collection:""test"" replication:""001"" I0511 13:40:20.074794 store_replicate.go:35 replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.074812 common.go:113 error JSON response status 500: replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.074826 common.go:70 response method:POST URL:/16496,4f3ade2d2f7065 with httpStatus:500 and JSON:{""error"":""replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.798244 store_replicate.go:35 replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.798261 common.go:113 error JSON response status 500: replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.798275 common.go:70 response method:POST URL:/16496,4f3ade2d2f7065 with httpStatus:500 and JSON:{""error"":""replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:29.588357 volume.go:304 graceful stop cluster http server  I0511 13:40:29.588947 volume.go:309 graceful stop gRPC  I0511 13:40:29.589452 volume_server.go:149 Shutting down volume server I0511 13:40:29.695784 volume_server.go:151 Shut down successfully!  Point to take a look is assignment of new volume (id=16498) on this Volume Server after its termination. And piece of log from Master:  I0511 13:40:18.304201 master_grpc_server.go:162 master received heartbeat ip:""seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test"" port:8444 public_url:""seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444"" has_no_volumes:true I0511 13:40:18.446855 cluster_commands.go:32 max volume id 16497 ==> 16498 I0511 13:40:18.464245 volume_growth.go:244 Created Volume 16498 on topo:DefaultDataCenter:DefaultRack:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:18.464308 master_grpc_server.go:162 master received heartbeat ip:""seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test"" port:8444 new_volumes:{id:16498 collection:""test"" replica_placement:1 version:3} I0511 13:40:18.464338 volume_layout.go:223 volume 16498 does not have enough copies I0511 13:40:18.464344 volume_layout.go:228 volume 16498 remove from writable I0511 13:40:18.464453 masterclient.go:277 .master: seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 masterClient adds volume 16498 I0511 13:40:18.464460 vid_map.go:160 + volume id 16498: {Url:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 PublicUrl:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 DataCenter:DefaultDataCenter GrpcPort:0} W0511 13:40:18.469625 master_grpc_server.go:100 SendHeartbeat.Recv server seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 : rpc error: code = Canceled desc = context canceled I0511 13:40:18.469648 node.go:237 topo:DefaultDataCenter:DefaultRack removes seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:18.469655 master_grpc_server.go:87 unregister disconnected volume server seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:18.469659 master_grpc_server.go:58 remove volume server seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444, online volume server: map[seaweedfs-test-volume-0.seaweedfs-test-volume-peer.seaweedfs-test:8444:[b81f6a9c-e4c1-4702-88d5-51974f4ca3d6] seaweedfs-test-volume-1.seaweedfs-test-volume-peer.seaweedfs-test:8444:[58a7e74e-84e2-48df-b52b-dc9555afb548] seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444:[ff43c257-c736-4b6b-a930-db16c409c12b]] I0511 13:40:18.527309 masterclient.go:292 updateVidMap(DefaultDataCenter) .master: seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 volume add: 0, del: 8263, add ec: 0 del ec: 0 I0511 13:40:19.771416 master_grpc_server.go:162 master received heartbeat ip:""seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test"" port:8444 public_url:""seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444"" max_file_key:5192448 volumes:{id:10324 size:1128282400 <omitted> I0511 13:40:19.916729 volume_growth.go:244 Created Volume 16498 on seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:19.916752 volume_layout.go:223 volume 16498 does not have enough copies I0511 13:40:19.916756 volume_layout.go:228 volume 16498 remove from writable I0511 13:40:19.916761 volume_growth.go:257 Registered Volume 16498 on topo:DefaultDataCenter:DefaultRack:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:19.916771 volume_layout.go:393 Volume 16498 becomes writable I0511 13:40:19.916776 volume_growth.go:257 Registered Volume 16498 on seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:19.919012 cluster_commands.go:32 max volume id 16498 ==> 16499 I0511 13:40:19.957609 masterclient.go:277 .master: seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 masterClient adds volume 16498 I0511 13:40:19.957611 vid_map.go:160 + volume id 16498: {Url:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 PublicUrl:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 DataCenter:DefaultDataCenter GrpcPort:0} I0511 13:40:19.957620 masterclient.go:277 .master: seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 masterClient adds volume 16498 I0511 13:40:19.957622 vid_map.go:160 + volume id 16498: {Url:seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 PublicUrl:seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 DataCenter: GrpcPort:0} I0511 13:40:19.957626 masterclient.go:292 updateVidMap() .master: seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 volume add: 1, del: 0, add ec: 0 del ec: 0  There also were a lot of messages like these:  I0511 13:40:18.329755 topology.go:252 removing volume info: Id:2458, Size:1329612128, ReplicaPlacement:001, Collection:test, Version:3, FileCount:317, DeleteCount:0, DeletedByteCount:0, ReadOnly:false from seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:18.431844 master_grpc_server.go:203 master see deleted volume 2458 from seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444  But there is no such message for Volume 16498, so it is writeable from Master's perspective (but in fact it doesn't), and Filer gets assignments for write requests on this volume. **System Setup** We have SeaweedFS running on Kubernetes via SeaweedFS Operator. There are 3 Masters, 4 Volume Servers, 4 Filers (with Scylla as its backend) - List the command line to start ""weed master"", ""weed volume"", ""weed filer"", ""weed s3"", ""weed mount"": - Master:  weed -v 4 -logtostderr=true master -volumeSizeLimitMB=1024 -defaultReplication=001 -ip=$(POD_NAME).seaweedfs-test-master-peer.seaweedfs-test -peers=seaweedfs-test-master-0.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-1.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-2.seaweedfs-test-master-peer.seaweedfs-test:9333 -metricsPort=9999  - Volume Server:  weed -v 4 -logtostderr=true volume -port=8444 -max=0 -ip=$(POD_NAME).seaweedfs-test-volume-peer.seaweedfs-test -metricsPort=9999 -mserver=seaweedfs-test-master-0.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-1.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-2.seaweedfs-test-master-peer.seaweedfs-test:9333 -dir=/data0  - Filer:  weed -v 4 -logtostderr=true filer -port=8888 -ip=$(POD_NAME).seaweedfs-test-filer-peer.seaweedfs-test -master=seaweedfs-test-master-0.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-1.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-2.seaweedfs-test-master-peer.seaweedfs-test:9333 -metricsPort=9999 -s3  - OS version: Fedora CoreOS 33.20210426.3.0 - output of `weed version`: `version 8000GB 3.43 673214574 linux amd64` - if using filer, show the content of `filer.toml`:  [leveldb2] enabled = false [etcd] enabled = false servers = ""seaweed-etcd.seaweedfs-test:2379"" timeout = ""3s"" [cassandra] enabled = true keyspace=""seaweedfs"" hosts=[ ""seaweed-scylla-client.seaweedfs-test:9042"", ] superLargeDirectories = [ ]  **Expected behavior** We expect unwritable (de facto) volumes will not be assigned for write requests.",source-file | source-file | source-file,assigns write requests unavailable volumes describe bug performed reliability manner wrote files filer loop sometimes restarted one volume servers random check work many time take recovery process depending data size stored write process work partial outage outage one volume servers etc simple sometimes observed filer reports cannot save one following errors unmarshalled http volume volume peer failed write replicas volume volume volume peer upload part bytes http volume volume peer ttl type replicate post http volume volume peer ttl type replicate dial tcp connect connection refused upload part bytes http volume volume peer bcaa post http volume volume peer bcaa dial tcp connect connection refused first sight looks quite obvious shutdown one volume servers filer cannot write see connection refused question assigns volumes write unwritable deeper investigation showed could synchronization threads side piece volume server log shutdown signal handling exec interrupt hook func name github weed command volumeserveroptions startvolumeserver func volume server stopping volume server volume server killed volume grpc client volume server volume volume peer stops deletes volumes volume stop send heartbeat wait seconds shutdown store dir data adds volume collection replicaplacement ttl volume maybeloadvolumeinfo checks data vif volume loading open write data idx volume loading loading memory index data idx memory needle map memory max key data idx store replicate replicating operations less volume replication copy count common json response status replicating operations less volume replication copy count common response method post url ade httpstatus json replicating operations less volume replication copy count common writing json status status write tcp write connection reset peer common json content map diskstatuses dir data used free percent free percent used omitted disk location dir data disk free required store volume volume grpc admin assign volume volume collection replication store replicate replicating operations less volume replication copy count common json response status replicating operations less volume replication copy count common response method post url ade httpstatus json replicating operations less volume replication copy count store replicate replicating operations less volume replication copy count common json response status replicating operations less volume replication copy count common response method post url ade httpstatus json replicating operations less volume replication copy count volume graceful stop cluster http server volume graceful stop grpc volume server shutting volume server volume server shut successfully point take look assignment new volume volume server termination piece log grpc server received heartbeat volume volume peer port public url volume volume peer volumes true cluster commands max volume volume growth created volume topo defaultdatacenter defaultrack volume volume peer grpc server received heartbeat volume volume peer port new volumes collection replica placement volume layout volume enough copies volume layout volume writable masterclient volume volume peer masterclient adds volume vid map volume url volume volume peer publicurl volume volume peer datacenter defaultdatacenter grpcport grpc server sendheartbeat recv server volume volume peer rpc canceled desc context canceled node topo defaultdatacenter defaultrack removes volume volume peer grpc server unregister disconnected volume server volume volume peer grpc server volume server volume volume peer online volume server map volume volume peer volume volume peer afb volume volume peer masterclient updatevidmap defaultdatacenter volume volume peer volume del del grpc server received heartbeat volume volume peer port public url volume volume peer max key volumes size omitted volume growth created volume volume volume peer volume layout volume enough copies volume layout volume writable volume growth registered volume topo defaultdatacenter defaultrack volume volume peer volume layout volume becomes writable volume growth registered volume volume volume peer cluster commands max volume masterclient volume volume peer masterclient adds volume vid map volume url volume volume peer publicurl volume volume peer datacenter defaultdatacenter grpcport masterclient volume volume peer masterclient adds volume vid map volume url volume volume peer publicurl volume volume peer datacenter grpcport masterclient updatevidmap volume volume peer volume del del also lot messages like topology removing volume size replicaplacement collection filecount deletecount deletedbytecount readonly false volume volume peer grpc server see deleted volume volume volume peer message volume writeable perspective fact filer gets assignments write requests volume system setup running kubernetes via operator masters volume servers filers scylla backend list command start weed weed volume weed filer weed weed mount weed logtostderr true volumesizelimitmb defaultreplication pod name peer peers peer peer peer metricsport volume server weed logtostderr true volume port max pod name volume peer metricsport mserver peer peer peer dir data filer weed logtostderr true filer port pod name filer peer peer peer peer metricsport fedora coreos output weed linux amd filer show content filer toml leveldb enabled false etcd enabled false servers seaweed etcd timeout cassandra enabled true keyspace hosts seaweed scylla client superlargedirectories expected behavior expect unwritable facto volumes assigned write requests,bug,0.95,"Master assigns write requests to unavailable volumes **Describe the bug** We performed reliability test for SeaweedFS in such a manner: - We wrote files through Filer in a loop - Sometimes we restarted one of Volume Servers (by random) to check how it will work, how many time will take recovery process depending on data size stored in SeaweedFS, how write process will work during partial outage (outage one of Volume Servers), etc. And during this simple test sometimes we observed such a problem: Filer reports that it cannot save the file with one of the following errors:  {""error"":""unmarshalled error http://seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444/16498,4f3cc6ea382612: failed to write to replicas for volume 16498: [seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444]: upload 489.part 4194304 bytes to http://seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444/16498,4f3cc6ea382612?ts=1683812487\u0026ttl=\u0026type=replicate: Post \""http://seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444/16498,4f3cc6ea382612?ts=1683812487\u0026ttl=\u0026type=replicate\"": dial tcp 10.183.0.39:8444: connect: connection refused""}  or  {""error"":""upload 486.part 4194304 bytes to http://seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444/5122,1740bcaa7be3be: Post \""http://seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444/5122,1740bcaa7be3be\"": dial tcp 10.161.3.122:8444: connect: connection refused""}  So, on a first sight it looks quite obvious, we shutdown one of Volume Servers, and Filer cannot write to it (we see `connection refused`). The question is: **_why_** Master assigns volumes for write, if they should be unwritable? Deeper investigation into the issue showed that it could be some issue of synchronization between threads on Master's side. This is a piece of SeaweedFS Volume Server log which was shutdown:  I0511 13:40:18.303550 signal_handling.go:48 exec interrupt hook func name:github.com/seaweedfs/seaweedfs/weed/command.VolumeServerOptions.startVolumeServer.func1 I0511 13:40:18.303585 volume_server.go:139 Stopping volume server volume server has been killed I0511 13:40:18.303671 volume_grpc_client_to_master.go:258 volume server seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 stops and deletes all volumes I0511 13:40:19.587873 volume.go:280 stop send heartbeat and wait 10 seconds until shutdown  I0511 13:40:19.587922 store.go:166 In dir /data0 adds volume:16498 collection:test replicaPlacement:001 ttl: I0511 13:40:19.587969 volume_info.go:20 maybeLoadVolumeInfo checks /data0/test_16498.vif I0511 13:40:19.588117 volume_loading.go:121 open to write file /data0/test_16498.idx I0511 13:40:19.588154 volume_loading.go:142 loading memory index /data0/test_16498.idx to memory I0511 13:40:19.588208 needle_map_memory.go:54 max file key: 0 for file: /data0/test_16498.idx I0511 13:40:19.588721 store_replicate.go:35 replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:19.588734 common.go:113 error JSON response status 500: replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:19.588750 common.go:70 response method:POST URL:/16496,4f3ade2d2f7065 with httpStatus:500 and JSON:{""error"":""replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:19.606846 common.go:106 error writing JSON status /status 200: write tcp 10.183.0.38:8444->10.183.0.1:46920: write: connection reset by peer I0511 13:40:19.606865 common.go:107 JSON content: map[DiskStatuses:[dir:""/data0"" all:98953909501952 used:9914417262592 free:89039492239360 percent_free:89.980774 percent_used:10.019227] <omitted> I0511 13:40:19.731598 disk_location.go:440 dir /data0 disk free 89.98% >= required 1.00% I0511 13:40:19.916394 store.go:170 add volume 16498 I0511 13:40:19.916412 volume_grpc_admin.go:60 assign volume volume_id:16498 collection:""test"" replication:""001"" I0511 13:40:20.074794 store_replicate.go:35 replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.074812 common.go:113 error JSON response status 500: replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.074826 common.go:70 response method:POST URL:/16496,4f3ade2d2f7065 with httpStatus:500 and JSON:{""error"":""replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.798244 store_replicate.go:35 replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.798261 common.go:113 error JSON response status 500: replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:20.798275 common.go:70 response method:POST URL:/16496,4f3ade2d2f7065 with httpStatus:500 and JSON:{""error"":""replicating operations [1] is less than volume 16496 replication copy count [2] I0511 13:40:29.588357 volume.go:304 graceful stop cluster http server  I0511 13:40:29.588947 volume.go:309 graceful stop gRPC  I0511 13:40:29.589452 volume_server.go:149 Shutting down volume server I0511 13:40:29.695784 volume_server.go:151 Shut down successfully!  Point to take a look is assignment of new volume (id=16498) on this Volume Server after its termination. And piece of log from Master:  I0511 13:40:18.304201 master_grpc_server.go:162 master received heartbeat ip:""seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test"" port:8444 public_url:""seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444"" has_no_volumes:true I0511 13:40:18.446855 cluster_commands.go:32 max volume id 16497 ==> 16498 I0511 13:40:18.464245 volume_growth.go:244 Created Volume 16498 on topo:DefaultDataCenter:DefaultRack:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:18.464308 master_grpc_server.go:162 master received heartbeat ip:""seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test"" port:8444 new_volumes:{id:16498 collection:""test"" replica_placement:1 version:3} I0511 13:40:18.464338 volume_layout.go:223 volume 16498 does not have enough copies I0511 13:40:18.464344 volume_layout.go:228 volume 16498 remove from writable I0511 13:40:18.464453 masterclient.go:277 .master: seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 masterClient adds volume 16498 I0511 13:40:18.464460 vid_map.go:160 + volume id 16498: {Url:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 PublicUrl:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 DataCenter:DefaultDataCenter GrpcPort:0} W0511 13:40:18.469625 master_grpc_server.go:100 SendHeartbeat.Recv server seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 : rpc error: code = Canceled desc = context canceled I0511 13:40:18.469648 node.go:237 topo:DefaultDataCenter:DefaultRack removes seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:18.469655 master_grpc_server.go:87 unregister disconnected volume server seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:18.469659 master_grpc_server.go:58 remove volume server seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444, online volume server: map[seaweedfs-test-volume-0.seaweedfs-test-volume-peer.seaweedfs-test:8444:[b81f6a9c-e4c1-4702-88d5-51974f4ca3d6] seaweedfs-test-volume-1.seaweedfs-test-volume-peer.seaweedfs-test:8444:[58a7e74e-84e2-48df-b52b-dc9555afb548] seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444:[ff43c257-c736-4b6b-a930-db16c409c12b]] I0511 13:40:18.527309 masterclient.go:292 updateVidMap(DefaultDataCenter) .master: seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 volume add: 0, del: 8263, add ec: 0 del ec: 0 I0511 13:40:19.771416 master_grpc_server.go:162 master received heartbeat ip:""seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test"" port:8444 public_url:""seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444"" max_file_key:5192448 volumes:{id:10324 size:1128282400 <omitted> I0511 13:40:19.916729 volume_growth.go:244 Created Volume 16498 on seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:19.916752 volume_layout.go:223 volume 16498 does not have enough copies I0511 13:40:19.916756 volume_layout.go:228 volume 16498 remove from writable I0511 13:40:19.916761 volume_growth.go:257 Registered Volume 16498 on topo:DefaultDataCenter:DefaultRack:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:19.916771 volume_layout.go:393 Volume 16498 becomes writable I0511 13:40:19.916776 volume_growth.go:257 Registered Volume 16498 on seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:19.919012 cluster_commands.go:32 max volume id 16498 ==> 16499 I0511 13:40:19.957609 masterclient.go:277 .master: seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 masterClient adds volume 16498 I0511 13:40:19.957611 vid_map.go:160 + volume id 16498: {Url:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 PublicUrl:seaweedfs-test-volume-2.seaweedfs-test-volume-peer.seaweedfs-test:8444 DataCenter:DefaultDataCenter GrpcPort:0} I0511 13:40:19.957620 masterclient.go:277 .master: seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 masterClient adds volume 16498 I0511 13:40:19.957622 vid_map.go:160 + volume id 16498: {Url:seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 PublicUrl:seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 DataCenter: GrpcPort:0} I0511 13:40:19.957626 masterclient.go:292 updateVidMap() .master: seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 volume add: 1, del: 0, add ec: 0 del ec: 0  There also were a lot of messages like these:  I0511 13:40:18.329755 topology.go:252 removing volume info: Id:2458, Size:1329612128, ReplicaPlacement:001, Collection:test, Version:3, FileCount:317, DeleteCount:0, DeletedByteCount:0, ReadOnly:false from seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444 I0511 13:40:18.431844 master_grpc_server.go:203 master see deleted volume 2458 from seaweedfs-test-volume-3.seaweedfs-test-volume-peer.seaweedfs-test:8444  But there is no such message for Volume 16498, so it is writeable from Master's perspective (but in fact it doesn't), and Filer gets assignments for write requests on this volume. **System Setup** We have SeaweedFS running on Kubernetes via SeaweedFS Operator. There are 3 Masters, 4 Volume Servers, 4 Filers (with Scylla as its backend) - List the command line to start ""weed master"", ""weed volume"", ""weed filer"", ""weed s3"", ""weed mount"": - Master:  weed -v 4 -logtostderr=true master -volumeSizeLimitMB=1024 -defaultReplication=001 -ip=$(POD_NAME).seaweedfs-test-master-peer.seaweedfs-test -peers=seaweedfs-test-master-0.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-1.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-2.seaweedfs-test-master-peer.seaweedfs-test:9333 -metricsPort=9999  - Volume Server:  weed -v 4 -logtostderr=true volume -port=8444 -max=0 -ip=$(POD_NAME).seaweedfs-test-volume-peer.seaweedfs-test -metricsPort=9999 -mserver=seaweedfs-test-master-0.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-1.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-2.seaweedfs-test-master-peer.seaweedfs-test:9333 -dir=/data0  - Filer:  weed -v 4 -logtostderr=true filer -port=8888 -ip=$(POD_NAME).seaweedfs-test-filer-peer.seaweedfs-test -master=seaweedfs-test-master-0.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-1.seaweedfs-test-master-peer.seaweedfs-test:9333,seaweedfs-test-master-2.seaweedfs-test-master-peer.seaweedfs-test:9333 -metricsPort=9999 -s3  - OS version: Fedora CoreOS 33.20210426.3.0 - output of `weed version`: `version 8000GB 3.43 673214574 linux amd64` - if using filer, show the content of `filer.toml`:  [leveldb2] enabled = false [etcd] enabled = false servers = ""seaweed-etcd.seaweedfs-test:2379"" timeout = ""3s"" [cassandra] enabled = true keyspace=""seaweedfs"" hosts=[ ""seaweed-scylla-client.seaweedfs-test:9042"", ] superLargeDirectories = [ ]  **Expected behavior** We expect unwritable (de facto) volumes will not be assigned for write requests."
1722,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1722,s3 gateway ListObjects does not support arbitrary prefix,"**Describe the bug** The S3 gateway of seaweedfs does not support listing objects with arbitrary prefix. **System Setup** - filer leveldb2 backend, seaweedfs master **Expected behavior** Calling `GET` on `http://seaweedfs_s3_gw/bucket/?prefix=arbitrary_prefix` should always return files whose path starts with `arbitrary_prefix`, no matter whether `arbitrary_prefix` is an actual directory or not. **Actual behavior** Calling `GET` on `http://seaweedfs_s3_gw/bucket/?prefix=arbitrary_prefix` returns every single object in the bucket regardless of their prefix if `arbitrary_prefix` is not a folder inside `bucket`. **Additional context** After a bit of digging around myself, it seems that this issue is actually two issues (both of which are about : 1. The `namePattern` parameter passed to `ListDirectoryEntries` in `weed/filer/filer_search.go` should contain a wildcard character `*` at the end of the prefix (i.e. `prefix*`, not `prefix`); this is what caused the list request to return every single file 2. ~~(After making the change above) the `ListDirectoryPrefixedEntries` (at least in the leveldb2 backend) does not seem to work properly (I cannot figure out why yet because I don't really understand the filer store design). In my case, it simply does not return any entry if the prefix should match more than one files. I think it is related to the latest commit that implements hash-prefix-based searching in leveldb / leveldb2.~~ I think https://github.com/chrislusf/seaweedfs/blob/master/weed/filer/leveldb2/leveldb2_store.go#L188 needs to be `continue` instead of `break` Some programs expect ListObjects to be able to list files with arbitrary prefix (such as `s3ql`) and will error if this does not work. Most other S3 implementations, including the official S3 service, support this correctly.",source-file | source-file | source-file,gateway listobjects arbitrary prefix describe bug gateway listing objects arbitrary prefix system setup filer leveldb backend expected behavior calling get http bucket prefix arbitrary prefix always return files whose path starts arbitrary prefix matter whether arbitrary prefix actual directory actual behavior calling get http bucket prefix arbitrary prefix returns every single object bucket regardless prefix arbitrary prefix folder inside bucket additional context bit digging around seems actually two issues namepattern parameter passed listdirectoryentries weed filer filer search contain wildcard character end prefix prefix prefix caused list return every single making listdirectoryprefixedentries least leveldb backend seem work properly cannot figure yet really understand filer store design case simply return entry prefix match one files think related latest implements hash prefix based searching leveldb leveldb think https github chrislusf blob weed filer leveldb leveldb store needs continue instead break programs expect listobjects able list files arbitrary prefix work implementations including official service correctly,bug,0.95,"s3 gateway ListObjects does not support arbitrary prefix **Describe the bug** The S3 gateway of seaweedfs does not support listing objects with arbitrary prefix. **System Setup** - filer leveldb2 backend, seaweedfs master **Expected behavior** Calling `GET` on `http://seaweedfs_s3_gw/bucket/?prefix=arbitrary_prefix` should always return files whose path starts with `arbitrary_prefix`, no matter whether `arbitrary_prefix` is an actual directory or not. **Actual behavior** Calling `GET` on `http://seaweedfs_s3_gw/bucket/?prefix=arbitrary_prefix` returns every single object in the bucket regardless of their prefix if `arbitrary_prefix` is not a folder inside `bucket`. **Additional context** After a bit of digging around myself, it seems that this issue is actually two issues (both of which are about : 1. The `namePattern` parameter passed to `ListDirectoryEntries` in `weed/filer/filer_search.go` should contain a wildcard character `*` at the end of the prefix (i.e. `prefix*`, not `prefix`); this is what caused the list request to return every single file 2. ~~(After making the change above) the `ListDirectoryPrefixedEntries` (at least in the leveldb2 backend) does not seem to work properly (I cannot figure out why yet because I don't really understand the filer store design). In my case, it simply does not return any entry if the prefix should match more than one files. I think it is related to the latest commit that implements hash-prefix-based searching in leveldb / leveldb2.~~ I think https://github.com/chrislusf/seaweedfs/blob/master/weed/filer/leveldb2/leveldb2_store.go#L188 needs to be `continue` instead of `break` Some programs expect ListObjects to be able to list files with arbitrary prefix (such as `s3ql`) and will error if this does not work. Most other S3 implementations, including the official S3 service, support this correctly."
1160,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1160,Deleting a non-existing file returns a 500 internal server error,"I'm testing a seaweedfs setup with a cassandra filer. When I try to delete a non-existing resource, the filer API returns a 500 error code. I would expect a 404.",source-file,deleting non existing returns internal server testing setup cassandra filer delete non existing resource filer api returns would expect,bug,0.95,"Deleting a non-existing file returns a 500 internal server error I'm testing a seaweedfs setup with a cassandra filer. When I try to delete a non-existing resource, the filer API returns a 500 error code. I would expect a 404."
2417,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2417,S3 using IAM action Write does not work properly,"**Describe the bug** I have followed the documentation to enable S3 IAM identities but it seems that the Write action for an identity does not allow to actually create files using S3. **System Setup** - Running seaweedfs from Docker, config below  docker_container: name: seaweedfs image: chrislusf/seaweedfs state: started restart_policy: unless-stopped pull: true user: nobody:20000 command: server -dir=""/data"" -volume.max=0 -master.volumePreallocate=false -master.volumeSizeLimitMB=1024 -s3 -s3.config=/etc/seaweedfs-s3-config.json comparisons: image: strict ports: - ""127.0.0.1:28080:8080"" - ""127.0.0.1:18080:18080"" - ""127.0.0.1:8333:8333"" volumes: - ""/etc/seaweedfs-s3-config.json:/etc/seaweedfs-s3-config.json:ro"" - ""/var/lib/seaweedfs-data:/data""  - `version 30GB 2.76 1b90d607 linux amd64` - Using `rclone` for file operations **Expected behavior** I would expect that the `Write` permission allow to create a file in a bucket, otherwise I have to grant `Admin` which is not very granular. **IAM config**:  { ""identities"": [ { ""name"": ""admin"", ""credentials"": [ { ""accessKey"": ""admin_access"", ""secretKey"": ""admin_secret"" } ], ""actions"": [ ""Admin"", ""Read"", ""List"", ""Tagging"", ""Write"" ] }, { ""name"": ""rclone"", ""credentials"": [ { ""accessKey"": ""rclone_access"", ""secretKey"": ""rclone_secret"" } ], ""actions"": [ ""Read"", ""Write"", ""List"" ] } ] }  **rclone config**:  [weed] type = s3 provider = Other access_key_id = rclone_access secret_access_key = rclone_secret endpoint = http://127.0.0.1:8333 acl = private [weedAdmin] type = s3 provider = Other access_key_id = admin_access secret_access_key = admin_secret endpoint = http://127.0.0.1:8333 acl = private  **Using rclone with low-priv user**:  #Bucket listing works # rclone lsd weed: -1 2021-11-02 15:11:23 -1 photos_staging_dir #File listing works # rclone ls weed:photos_staging_dir #Creating a file fails # rclone touch weed:photos_staging_dir/fstab 2021/11/02 16:06:26 ERROR : Attempt 1/3 failed with 1 errors and: failed to touch (create): AccessDenied: Access Denied. status code: 403, request id: 1635869186380983481, host id: 2021/11/02 16:06:26 ERROR : Attempt 2/3 failed with 1 errors and: failed to touch (create): AccessDenied: Access Denied. status code: 403, request id: 1635869186384910685, host id: 2021/11/02 16:06:26 ERROR : Attempt 3/3 failed with 1 errors and: failed to touch (create): AccessDenied: Access Denied. status code: 403, request id: 1635869186388820806, host id: 2021/11/02 16:06:26 Failed to touch: failed to touch (create): AccessDenied: Access Denied. status code: 403, request id: 1635869186388820806, host id:  Creating empty file with admin identity:  rclone touch weedAdmin:photos_staging_dir/fstab  Created file can now be written/deleted by low-priv identity:  # rclone copy /etc/fstab weed:photos_staging_dir/ # rclone ls weed:photos_staging_dir/ 881 fstab #rclone delete weed:photos_staging_dir/fstab ",source-file,iam action write work properly describe bug followed documentation enable iam identities seems write action identity allow actually create files system setup running docker config docker container name image chrislusf state started restart policy unless stopped true user nobody command server dir data volume max volumepreallocate false volumesizelimitmb config etc config json comparisons image strict ports volumes etc config json etc config json var data data linux amd rclone operations expected behavior would expect write permission allow create bucket otherwise grant admin granular iam config identities name admin credentials accesskey admin access secretkey admin secret actions admin read list tagging write name rclone credentials accesskey rclone access secretkey rclone secret actions read write list rclone config weed type provider access key rclone access secret access key rclone secret endpoint http acl private weedadmin type provider access key admin access secret access key admin secret endpoint http acl private rclone low priv user bucket listing works rclone lsd weed photos staging dir listing works rclone weed photos staging dir creating fails rclone touch weed photos staging dir fstab attempt failed errors failed touch create accessdenied access denied status host attempt failed errors failed touch create accessdenied access denied status host attempt failed errors failed touch create accessdenied access denied status host failed touch failed touch create accessdenied access denied status host creating empty admin identity rclone touch weedadmin photos staging dir fstab created written deleted low priv identity rclone copy etc fstab weed photos staging dir rclone weed photos staging dir fstab rclone delete weed photos staging dir fstab,bug,0.9,"S3 using IAM action Write does not work properly **Describe the bug** I have followed the documentation to enable S3 IAM identities but it seems that the Write action for an identity does not allow to actually create files using S3. **System Setup** - Running seaweedfs from Docker, config below  docker_container: name: seaweedfs image: chrislusf/seaweedfs state: started restart_policy: unless-stopped pull: true user: nobody:20000 command: server -dir=""/data"" -volume.max=0 -master.volumePreallocate=false -master.volumeSizeLimitMB=1024 -s3 -s3.config=/etc/seaweedfs-s3-config.json comparisons: image: strict ports: - ""127.0.0.1:28080:8080"" - ""127.0.0.1:18080:18080"" - ""127.0.0.1:8333:8333"" volumes: - ""/etc/seaweedfs-s3-config.json:/etc/seaweedfs-s3-config.json:ro"" - ""/var/lib/seaweedfs-data:/data""  - `version 30GB 2.76 1b90d607 linux amd64` - Using `rclone` for file operations **Expected behavior** I would expect that the `Write` permission allow to create a file in a bucket, otherwise I have to grant `Admin` which is not very granular. **IAM config**:  { ""identities"": [ { ""name"": ""admin"", ""credentials"": [ { ""accessKey"": ""admin_access"", ""secretKey"": ""admin_secret"" } ], ""actions"": [ ""Admin"", ""Read"", ""List"", ""Tagging"", ""Write"" ] }, { ""name"": ""rclone"", ""credentials"": [ { ""accessKey"": ""rclone_access"", ""secretKey"": ""rclone_secret"" } ], ""actions"": [ ""Read"", ""Write"", ""List"" ] } ] }  **rclone config**:  [weed] type = s3 provider = Other access_key_id = rclone_access secret_access_key = rclone_secret endpoint = http://127.0.0.1:8333 acl = private [weedAdmin] type = s3 provider = Other access_key_id = admin_access secret_access_key = admin_secret endpoint = http://127.0.0.1:8333 acl = private  **Using rclone with low-priv user**:  #Bucket listing works # rclone lsd weed: -1 2021-11-02 15:11:23 -1 photos_staging_dir #File listing works # rclone ls weed:photos_staging_dir #Creating a file fails # rclone touch weed:photos_staging_dir/fstab 2021/11/02 16:06:26 ERROR : Attempt 1/3 failed with 1 errors and: failed to touch (create): AccessDenied: Access Denied. status code: 403, request id: 1635869186380983481, host id: 2021/11/02 16:06:26 ERROR : Attempt 2/3 failed with 1 errors and: failed to touch (create): AccessDenied: Access Denied. status code: 403, request id: 1635869186384910685, host id: 2021/11/02 16:06:26 ERROR : Attempt 3/3 failed with 1 errors and: failed to touch (create): AccessDenied: Access Denied. status code: 403, request id: 1635869186388820806, host id: 2021/11/02 16:06:26 Failed to touch: failed to touch (create): AccessDenied: Access Denied. status code: 403, request id: 1635869186388820806, host id:  Creating empty file with admin identity:  rclone touch weedAdmin:photos_staging_dir/fstab  Created file can now be written/deleted by low-priv identity:  # rclone copy /etc/fstab weed:photos_staging_dir/ # rclone ls weed:photos_staging_dir/ 881 fstab #rclone delete weed:photos_staging_dir/fstab "
93,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/93,"Lots of ""volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01"" errors in our production server log files","Here is the  Deploy structure  10.252.130.159:9333(master) 10.252.130.159:5088(haproxy) / \ 10.252.133.22:5083 (volume1) 10.252.135.207:5084(volume2) Replication Stratage: 001  So, it always gets the content via haproxy from the volume server. But after running few days, we get a 404 error sometimes from the 10.252.130.159:5088 for a special fid such as :5,1001e1b02c1b01. And we did a check and found that one of volume server(different fid on the random different server) always output the following logs:  I0303 15:44:23 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:45:14 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:52:52 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:52:52 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:05 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:06 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:07 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:07 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:18 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01  It means that the file for [fid:5,1001e1b02c1b01] has be damaged? And how to fix this? How could be happened? any suggestions?",source-file | documentation-file | source-file,lots volume server handlers read nil errors production server log files deploy structure haproxy volume volume replication stratage always gets content via haproxy volume server running days get sometimes special fid check found one volume server different fid random different server always output following logs volume server handlers read nil volume server handlers read nil volume server handlers read nil volume server handlers read nil volume server handlers read nil volume server handlers read nil volume server handlers read nil volume server handlers read nil volume server handlers read nil means fid damaged could happened suggestions,bug,0.85,"Lots of ""volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01"" errors in our production server log files Here is the  Deploy structure  10.252.130.159:9333(master) 10.252.130.159:5088(haproxy) / \ 10.252.133.22:5083 (volume1) 10.252.135.207:5084(volume2) Replication Stratage: 001  So, it always gets the content via haproxy from the volume server. But after running few days, we get a 404 error sometimes from the 10.252.130.159:5088 for a special fid such as :5,1001e1b02c1b01. And we did a check and found that one of volume server(different fid on the random different server) always output the following logs:  I0303 15:44:23 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:45:14 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:52:52 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:52:52 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:05 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:06 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:07 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:07 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01 I0303 15:53:18 21828 volume_server_handlers.go:75] read error: <nil> /5,1001e1b02c1b01  It means that the file for [fid:5,1001e1b02c1b01] has be damaged? And how to fix this? How could be happened? any suggestions?"
2648,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2648,[store] failed to write to local disk: invalid argument,"**Describe the bug** logs  upload_content.go:109] uploading to http://fast-volume-6.s3-fast-volume.service.dcdp.consul:8080/825,01366342ecebf5fc: unmarshalled error http://fast-volume-6.s3-fast-volume.service.dcdp.consul:8080/825,01366342ecebf5fc: failed to write to local disk: invalid argument | fast-api-658d59f67b-vd9ct store_replicate.go:48] failed to write to local disk: invalid argument | fast-volume-6 common.go:69] response method:POST URL:/825,01366342ecebf5fc with httpStatus:500 and JSON:{""name"":""0006.part"",""size"":5242880,""error"":""failed to write to local disk: invalid argument"",""eTag"":""971972b3""} | fast-volume-6  **System Setup**  2.88 ",source-file,store failed write local disk invalid argument describe bug logs upload content uploading http fast volume fast volume service dcdp consul ecebf unmarshalled http fast volume fast volume service dcdp consul ecebf failed write local disk invalid argument fast api store replicate failed write local disk invalid argument fast volume common response method post url ecebf httpstatus json name part size failed write local disk invalid argument etag fast volume system setup,bug,0.9,"[store] failed to write to local disk: invalid argument **Describe the bug** logs  upload_content.go:109] uploading to http://fast-volume-6.s3-fast-volume.service.dcdp.consul:8080/825,01366342ecebf5fc: unmarshalled error http://fast-volume-6.s3-fast-volume.service.dcdp.consul:8080/825,01366342ecebf5fc: failed to write to local disk: invalid argument | fast-api-658d59f67b-vd9ct store_replicate.go:48] failed to write to local disk: invalid argument | fast-volume-6 common.go:69] response method:POST URL:/825,01366342ecebf5fc with httpStatus:500 and JSON:{""name"":""0006.part"",""size"":5242880,""error"":""failed to write to local disk: invalid argument"",""eTag"":""971972b3""} | fast-volume-6  **System Setup**  2.88 "
5864,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/5864,"[webdav] When the webdav service stops, a 404 is returned for an existing file","**Describe the bug** When the webdav service stops, a 404 is returned for an existing file **System Setup** `3.71` **Expected behavior** Returning always 200 or return nothing **Additional context**  1. make server with -webdav option 2. while true; do curl -sI ""http://127.0.0.1:7333/buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd"" | grep HTTP; done 3. Crtl+C stop server 4. Got HTTP/1.1 200 OK HTTP/1.1 404 Not Found  logs:  I0806 14:38:20.436016 filer_grpc_server.go:31 LookupDirectoryEntry /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd: get /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd : leveldb: closed, I0806 14:38:20.436090 filer_pb_helper.go:139 read /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd: rpc error: code = Unknown desc = get /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd : leveldb: closed I0806 14:38:20.436096 filer_client.go:42 read /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd <nil>: LookupEntry1: rpc error: code = Unknown desc = get /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd : leveldb: closed ",source-file,webdav webdav service stops returned existing describe bug webdav service stops returned existing system setup expected behavior returning always return nothing additional context make server webdav option true curl http buckets warp benchmark bucket xyla sas diwkjopavnlg rnd grep http done crtl stop server got http http found logs filer grpc server lookupdirectoryentry buckets warp benchmark bucket xyla sas diwkjopavnlg rnd get buckets warp benchmark bucket xyla sas diwkjopavnlg rnd leveldb closed filer helper read buckets warp benchmark bucket xyla sas diwkjopavnlg rnd rpc unknown desc get buckets warp benchmark bucket xyla sas diwkjopavnlg rnd leveldb closed filer client read buckets warp benchmark bucket xyla sas diwkjopavnlg rnd nil lookupentry rpc unknown desc get buckets warp benchmark bucket xyla sas diwkjopavnlg rnd leveldb closed,bug,0.9,"[webdav] When the webdav service stops, a 404 is returned for an existing file **Describe the bug** When the webdav service stops, a 404 is returned for an existing file **System Setup** `3.71` **Expected behavior** Returning always 200 or return nothing **Additional context**  1. make server with -webdav option 2. while true; do curl -sI ""http://127.0.0.1:7333/buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd"" | grep HTTP; done 3. Crtl+C stop server 4. Got HTTP/1.1 200 OK HTTP/1.1 404 Not Found  logs:  I0806 14:38:20.436016 filer_grpc_server.go:31 LookupDirectoryEntry /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd: get /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd : leveldb: closed, I0806 14:38:20.436090 filer_pb_helper.go:139 read /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd: rpc error: code = Unknown desc = get /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd : leveldb: closed I0806 14:38:20.436096 filer_client.go:42 read /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd <nil>: LookupEntry1: rpc error: code = Unknown desc = get /buckets/warp-benchmark-bucket/xyla1SAS/99.dIWkjopAvnLG(3VR.rnd : leveldb: closed "
1036,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1036,Update existing file fails - Even possible?,"**Describe the bug** I want to perform updates to existing files by uploading the data to the already existing fid. The first upload works, but then all following updates will fail with an HTTP error: **System Setup** Running version 1.4.2 on Linux64 ./weed master -mdir=""master01"" -defaultReplication=002 ./weed volume -port=8081 -mserver=""localhost:9333"" -dir=""volume01"" -max=100 ./weed volume -port=8082 -mserver=""localhost:9333"" -dir=""volume02"" -max=100 ./weed volume -port=8083 -mserver=""localhost:9333"" -dir=""volume03"" -max=100 **Expected behavior** I would expect that the file upload works like the first with this file. But maybe I'm wrong and the update of an existing file is not possible. An other way would be to assign a new file id for every update and to delete the old. **Sequence** _Client:_ curl -F file=myFile http://127.0.0.1:8083/3,02b3f5780c Response: `{""size"":8,""error"":""failed to write to replicas for volume 3: [127.0.0.1:8081]: unexpected end of JSON input\n[127.0.0.1:8082]: unexpected end of JSON input"",""eTag"":""0ca153a6""}` _Master:_ No related console output _volume01:_ `I0809 23:03:11 11674 common.go:73] error writing JSON {Name: Size:8 Error: ETag:0ca153a6} status 304: http: req` _volume02:_ `I0809 23:03:11 11704 common.go:73] error writing JSON {Name: Size:8 Error: ETag:0ca153a6} status 304: http: request method or response status code does not allow body` _volume03:_  I0809 23:03:11 11688 upload_content.go:136] failing to read upload response http://127.0.0.1:8081/3,02b3f5780c?ts=1565384591&ttl=&type=replicate I0809 23:03:11 11688 upload_content.go:136] failing to read upload response http://127.0.0.1:8082/3,02b3f5780c?ts=1565384591&ttl=&type=replicate  Thank you!",source-file,existing fails even possible describe bug want perform updates existing files uploading data already existing fid first upload works following updates fail http system setup running linux weed mdir defaultreplication weed volume port mserver localhost dir volume max weed volume port mserver localhost dir volume max weed volume port mserver localhost dir volume max expected behavior would expect upload works like first maybe wrong existing possible way would assign new every delete old sequence client curl myfile http response size failed write replicas volume unexpected end json input unexpected end json input etag related console output volume common writing json name size etag status http req volume common writing json name size etag status http method response status allow body volume upload content failing read upload response http ttl type replicate upload content failing read upload response http ttl type replicate thank,bug,0.95,"Update existing file fails - Even possible? **Describe the bug** I want to perform updates to existing files by uploading the data to the already existing fid. The first upload works, but then all following updates will fail with an HTTP error: **System Setup** Running version 1.4.2 on Linux64 ./weed master -mdir=""master01"" -defaultReplication=002 ./weed volume -port=8081 -mserver=""localhost:9333"" -dir=""volume01"" -max=100 ./weed volume -port=8082 -mserver=""localhost:9333"" -dir=""volume02"" -max=100 ./weed volume -port=8083 -mserver=""localhost:9333"" -dir=""volume03"" -max=100 **Expected behavior** I would expect that the file upload works like the first with this file. But maybe I'm wrong and the update of an existing file is not possible. An other way would be to assign a new file id for every update and to delete the old. **Sequence** _Client:_ curl -F file=myFile http://127.0.0.1:8083/3,02b3f5780c Response: `{""size"":8,""error"":""failed to write to replicas for volume 3: [127.0.0.1:8081]: unexpected end of JSON input\n[127.0.0.1:8082]: unexpected end of JSON input"",""eTag"":""0ca153a6""}` _Master:_ No related console output _volume01:_ `I0809 23:03:11 11674 common.go:73] error writing JSON {Name: Size:8 Error: ETag:0ca153a6} status 304: http: req` _volume02:_ `I0809 23:03:11 11704 common.go:73] error writing JSON {Name: Size:8 Error: ETag:0ca153a6} status 304: http: request method or response status code does not allow body` _volume03:_  I0809 23:03:11 11688 upload_content.go:136] failing to read upload response http://127.0.0.1:8081/3,02b3f5780c?ts=1565384591&ttl=&type=replicate I0809 23:03:11 11688 upload_content.go:136] failing to read upload response http://127.0.0.1:8082/3,02b3f5780c?ts=1565384591&ttl=&type=replicate  Thank you!"
6497,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/6497,Using PUT object with bucket of invalid name leads to unremovable bucket," root@Hepatitis:~# s3cmd put ab s3://aa/a upload: 'ab' -> 's3://aa/a' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done WARNING: Upload failed: /a (500 (InternalError): We encountered an internal error, please try again.) WARNING: Waiting 3 sec upload: 'ab' -> 's3://aa/a' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done WARNING: Upload failed: /a (500 (InternalError): We encountered an internal error, please try again.) WARNING: Waiting 6 sec ^CSee ya! root@Hepatitis:~# s3cmd put ab s3://aa  The created aa bucket can then not be removed even via ``weed shell`` due to the invalid bucket name of ``aa``",source-file | test-file | source-file,put object bucket invalid name leads unremovable bucket root hepatitis cmd put upload done warning upload failed internalerror encountered internal please warning waiting sec upload done warning upload failed internalerror encountered internal please warning waiting sec csee root hepatitis cmd put created bucket removed even via weed shell due invalid bucket name,bug,0.85,"Using PUT object with bucket of invalid name leads to unremovable bucket  root@Hepatitis:~# s3cmd put ab s3://aa/a upload: 'ab' -> 's3://aa/a' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done WARNING: Upload failed: /a (500 (InternalError): We encountered an internal error, please try again.) WARNING: Waiting 3 sec upload: 'ab' -> 's3://aa/a' [1 of 1] 0 of 0 0% in 0s 0.00 B/s done WARNING: Upload failed: /a (500 (InternalError): We encountered an internal error, please try again.) WARNING: Waiting 6 sec ^CSee ya! root@Hepatitis:~# s3cmd put ab s3://aa  The created aa bucket can then not be removed even via ``weed shell`` due to the invalid bucket name of ``aa``"
2434,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2434,[s3] HeadBucketHandler forbidden if is not super Admin or Owner,"**Describe the bug** HeadBucketHandler forbidden if is not super Admin  I1110 17:30:03 1 auth_credentials.go:225] user name: bennu actions: [Admin:bennu-*], action: Admin I1110 17:30:03 1 s3api_bucket_handlers.go:171] HeadBucketHandler bennu-client-files I1110 17:30:03 1 error_handler.go:79] status 403 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>AccessDenied</Code><Message>Access Denied.</Message><Resource>/</Resource><RequestId>1636565403160014337</RequestId><BucketName>bennu-client-files</BucketName></Error>   aws --profile bennu --endpoint-url http://localhost:8333 s3api head-bucket --bucket bennu-client-files --debug  I don't quite understand what is being cheked here https://github.com/chrislusf/seaweedfs/blob/35c37562bc34393853de1c54ed06740bdffdf919/weed/s3api/s3api_bucket_handlers.go#L195  > fs.meta.cat /buckets/bennu-client-files { ""name"": ""bennu-client-files"", ""isDirectory"": true, ""chunks"": [ ], ""attributes"": { ""fileSize"": ""0"", ""mtime"": ""1633508622"", ""fileMode"": 2147484159, ""uid"": 0, ""gid"": 0, ""crtime"": ""1633508622"", ""mime"": """", ""replication"": """", ""collection"": """", ""ttlSec"": 0, ""userName"": """", ""groupName"": [ ], ""symlinkTarget"": """", ""md5"": null, ""diskType"": """" }, ""extended"": { ""s3-identity-id"": ""c2VydmljZS1saWJyYXJ5"" }, ""hardLinkId"": null, ""hardLinkCounter"": 0, ""content"": null, ""remoteEntry"": null }  **System Setup**  2.75 ",source-file,headbuckethandler forbidden super admin owner describe bug headbuckethandler forbidden super admin auth credentials user name bennu actions admin bennu action admin api bucket handlers headbuckethandler bennu client files handler status application xml xml encoding utf accessdenied message access denied message resource resource requestid requestid bucketname bennu client files bucketname aws profile bennu endpoint url http localhost api head bucket bucket bennu client files quite understand cheked https github chrislusf blob bdffdf weed api api bucket handlers meta cat buckets bennu client files name bennu client files isdirectory true chunks attributes filesize mtime filemode uid gid crtime mime replication collection ttlsec username groupname symlinktarget null disktype extended identity vydmljzs sawjyyxj hardlinkid null hardlinkcounter content null remoteentry null system setup,bug,0.9,"[s3] HeadBucketHandler forbidden if is not super Admin or Owner **Describe the bug** HeadBucketHandler forbidden if is not super Admin  I1110 17:30:03 1 auth_credentials.go:225] user name: bennu actions: [Admin:bennu-*], action: Admin I1110 17:30:03 1 s3api_bucket_handlers.go:171] HeadBucketHandler bennu-client-files I1110 17:30:03 1 error_handler.go:79] status 403 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> <Error><Code>AccessDenied</Code><Message>Access Denied.</Message><Resource>/</Resource><RequestId>1636565403160014337</RequestId><BucketName>bennu-client-files</BucketName></Error>   aws --profile bennu --endpoint-url http://localhost:8333 s3api head-bucket --bucket bennu-client-files --debug  I don't quite understand what is being cheked here https://github.com/chrislusf/seaweedfs/blob/35c37562bc34393853de1c54ed06740bdffdf919/weed/s3api/s3api_bucket_handlers.go#L195  > fs.meta.cat /buckets/bennu-client-files { ""name"": ""bennu-client-files"", ""isDirectory"": true, ""chunks"": [ ], ""attributes"": { ""fileSize"": ""0"", ""mtime"": ""1633508622"", ""fileMode"": 2147484159, ""uid"": 0, ""gid"": 0, ""crtime"": ""1633508622"", ""mime"": """", ""replication"": """", ""collection"": """", ""ttlSec"": 0, ""userName"": """", ""groupName"": [ ], ""symlinkTarget"": """", ""md5"": null, ""diskType"": """" }, ""extended"": { ""s3-identity-id"": ""c2VydmljZS1saWJyYXJ5"" }, ""hardLinkId"": null, ""hardLinkCounter"": 0, ""content"": null, ""remoteEntry"": null }  **System Setup**  2.75 "
568,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/568,Multi master server setup failure,"Hi, Chirslusf. I have encountered a problem when setting up a multi master server cluster according the wiki. I have 3 vm:  vm1: 10.100.35.21 vm2: 10.100.35.20 vm3: 10.100.35.22  I start weed master server one by one in below order:  vm1: ./weed master -ip 10.100.35.21 -port=9333 -mdir=./master vm2: ./weed master -ip 10.100.35.20 -port=9333 -mdir=./master -peers=10.100.35.21:9333 vm3: ./weed master -ip 10.100.35.22 -port=9333 -mdir=./master -peers=10.100.35.20:9333  And then we can see the log output of each start:  vm1: # ./weed master -ip 10.100.35.21 -port=9333 -mdir=./master I0927 15:13:02 14610 file_util.go:20] Folder ./master Permission: -rwxr-xr-x I0927 15:13:02 14610 master_server.go:62] Volume Size Limit is 30000 MB I0927 15:13:02 14610 master.go:87] Start Seaweed Master 0.76 at 0.0.0.0:9333 I0927 15:13:02 14610 raft_server.go:56] Peers Change: [10.100.35.21:9333] => [] I0927 15:13:02 14610 raft_server.go:98] Initializing new cluster I0927 15:13:02 14610 master_server.go:95] [ 10.100.35.21:9333 ] I am the leader! I0927 15:13:12 14610 raft_server_handlers.go:16] Processing incoming join. Current Leader 10.100.35.21:9333 Self 10.100.35.21:9333 Peers map[] I0927 15:13:12 14610 raft_server_handlers.go:20] Command:{""name"":""10.100.35.20:9333"",""connectionString"":""http://10.100.35.20:9333""} I0927 15:13:12 14610 raft_server_handlers.go:27] join command from Name 10.100.35.20:9333 Connection http://10.100.35.20:9333 vm2: # ./weed master -ip 10.100.35.20 -port=9333 -mdir=./master -peers=10.100.35.21:9333 I0927 15:13:11 3117 file_util.go:20] Folder ./master Permission: -rwxr-xr-x I0927 15:13:11 3117 master_server.go:62] Volume Size Limit is 30000 MB I0927 15:13:11 3117 master.go:87] Start Seaweed Master 0.76 at 0.0.0.0:9333 I0927 15:13:11 3117 raft_server.go:56] Peers Change: [10.100.35.20:9333] => [10.100.35.21:9333] I0927 15:13:11 3117 raft_server.go:78] Joining cluster: 10.100.35.21:9333 I0927 15:13:12 3117 raft_server.go:166] Attempting to connect to: http://10.100.35.21:9333/cluster/join I0927 15:13:12 3117 raft_server.go:211] Post returned status: 200 I0927 15:13:52 3117 raft_server_handlers.go:16] Processing incoming join. Current Leader 10.100.35.21:9333 Self 10.100.35.20:9333 Peers map[10.100.35.21:9333:0xc42021fa40] I0927 15:13:52 3117 raft_server_handlers.go:20] Command:{""name"":""10.100.35.22:9333"",""connectionString"":""http://10.100.35.22:9333""} I0927 15:13:52 3117 raft_server_handlers.go:27] join command from Name 10.100.35.22:9333 Connection http://10.100.35.22:9333 I0927 15:13:52 3117 raft_server_handlers.go:47] Redirecting to 301 http://10.100.35.21:9333/cluster/join vm3: # ./weed master -ip 10.100.35.22 -port=9333 -mdir=./master -peers=10.100.35.20:9333 I0927 15:13:51 2964 file_util.go:20] Folder ./master Permission: -rwxr-xr-x I0927 15:13:51 2964 master_server.go:62] Volume Size Limit is 30000 MB I0927 15:13:51 2964 master.go:87] Start Seaweed Master 0.76 at 0.0.0.0:9333 I0927 15:13:51 2964 raft_server.go:56] Peers Change: [] => [10.100.35.20:9333] I0927 15:13:51 2964 raft_server.go:78] Joining cluster: 10.100.35.20:9333 I0927 15:13:52 2964 raft_server.go:166] Attempting to connect to: http://10.100.35.20:9333/cluster/join I0927 15:13:52 2964 raft_server.go:211] Post returned status: 404 404 page not found I0927 15:13:52 2964 raft_server.go:171] Post returned error: 404 page not found I0927 15:13:52 2964 raft_server.go:82] No existing server found. Starting as leader in the new cluster. I0927 15:13:52 2964 master_server.go:95] [ 10.100.35.22:9333 ] I am the leader!  It seems that the weed master on vm3 did not handle the 301 redirect to connect to the master(current leader) on vm1. Is this a bug? looking forward to your reply. 3ks.",source-file | source-file,multi server setup failure chirslusf encountered setting multi server cluster according wiki start weed server one one order weed port mdir weed port mdir peers weed port mdir peers see log output start weed port mdir util folder permission rwxr server volume size limit start seaweed raft server peers raft server initializing new cluster server leader raft server handlers processing incoming join current leader self peers map raft server handlers command name connectionstring http raft server handlers join command name connection http weed port mdir peers util folder permission rwxr server volume size limit start seaweed raft server peers raft server joining cluster raft server attempting connect http cluster join raft server post returned status raft server handlers processing incoming join current leader self peers map raft server handlers command name connectionstring http raft server handlers join command name connection http raft server handlers redirecting http cluster join weed port mdir peers util folder permission rwxr server volume size limit start seaweed raft server peers raft server joining cluster raft server attempting connect http cluster join raft server post returned status page found raft server post returned page found raft server existing server found starting leader new cluster server leader seems weed handle redirect connect current leader bug looking forward reply,bug,0.9,"Multi master server setup failure Hi, Chirslusf. I have encountered a problem when setting up a multi master server cluster according the wiki. I have 3 vm:  vm1: 10.100.35.21 vm2: 10.100.35.20 vm3: 10.100.35.22  I start weed master server one by one in below order:  vm1: ./weed master -ip 10.100.35.21 -port=9333 -mdir=./master vm2: ./weed master -ip 10.100.35.20 -port=9333 -mdir=./master -peers=10.100.35.21:9333 vm3: ./weed master -ip 10.100.35.22 -port=9333 -mdir=./master -peers=10.100.35.20:9333  And then we can see the log output of each start:  vm1: # ./weed master -ip 10.100.35.21 -port=9333 -mdir=./master I0927 15:13:02 14610 file_util.go:20] Folder ./master Permission: -rwxr-xr-x I0927 15:13:02 14610 master_server.go:62] Volume Size Limit is 30000 MB I0927 15:13:02 14610 master.go:87] Start Seaweed Master 0.76 at 0.0.0.0:9333 I0927 15:13:02 14610 raft_server.go:56] Peers Change: [10.100.35.21:9333] => [] I0927 15:13:02 14610 raft_server.go:98] Initializing new cluster I0927 15:13:02 14610 master_server.go:95] [ 10.100.35.21:9333 ] I am the leader! I0927 15:13:12 14610 raft_server_handlers.go:16] Processing incoming join. Current Leader 10.100.35.21:9333 Self 10.100.35.21:9333 Peers map[] I0927 15:13:12 14610 raft_server_handlers.go:20] Command:{""name"":""10.100.35.20:9333"",""connectionString"":""http://10.100.35.20:9333""} I0927 15:13:12 14610 raft_server_handlers.go:27] join command from Name 10.100.35.20:9333 Connection http://10.100.35.20:9333 vm2: # ./weed master -ip 10.100.35.20 -port=9333 -mdir=./master -peers=10.100.35.21:9333 I0927 15:13:11 3117 file_util.go:20] Folder ./master Permission: -rwxr-xr-x I0927 15:13:11 3117 master_server.go:62] Volume Size Limit is 30000 MB I0927 15:13:11 3117 master.go:87] Start Seaweed Master 0.76 at 0.0.0.0:9333 I0927 15:13:11 3117 raft_server.go:56] Peers Change: [10.100.35.20:9333] => [10.100.35.21:9333] I0927 15:13:11 3117 raft_server.go:78] Joining cluster: 10.100.35.21:9333 I0927 15:13:12 3117 raft_server.go:166] Attempting to connect to: http://10.100.35.21:9333/cluster/join I0927 15:13:12 3117 raft_server.go:211] Post returned status: 200 I0927 15:13:52 3117 raft_server_handlers.go:16] Processing incoming join. Current Leader 10.100.35.21:9333 Self 10.100.35.20:9333 Peers map[10.100.35.21:9333:0xc42021fa40] I0927 15:13:52 3117 raft_server_handlers.go:20] Command:{""name"":""10.100.35.22:9333"",""connectionString"":""http://10.100.35.22:9333""} I0927 15:13:52 3117 raft_server_handlers.go:27] join command from Name 10.100.35.22:9333 Connection http://10.100.35.22:9333 I0927 15:13:52 3117 raft_server_handlers.go:47] Redirecting to 301 http://10.100.35.21:9333/cluster/join vm3: # ./weed master -ip 10.100.35.22 -port=9333 -mdir=./master -peers=10.100.35.20:9333 I0927 15:13:51 2964 file_util.go:20] Folder ./master Permission: -rwxr-xr-x I0927 15:13:51 2964 master_server.go:62] Volume Size Limit is 30000 MB I0927 15:13:51 2964 master.go:87] Start Seaweed Master 0.76 at 0.0.0.0:9333 I0927 15:13:51 2964 raft_server.go:56] Peers Change: [] => [10.100.35.20:9333] I0927 15:13:51 2964 raft_server.go:78] Joining cluster: 10.100.35.20:9333 I0927 15:13:52 2964 raft_server.go:166] Attempting to connect to: http://10.100.35.20:9333/cluster/join I0927 15:13:52 2964 raft_server.go:211] Post returned status: 404 404 page not found I0927 15:13:52 2964 raft_server.go:171] Post returned error: 404 page not found I0927 15:13:52 2964 raft_server.go:82] No existing server found. Starting as leader in the new cluster. I0927 15:13:52 2964 master_server.go:95] [ 10.100.35.22:9333 ] I am the leader!  It seems that the weed master on vm3 did not handle the 301 redirect to connect to the master(current leader) on vm1. Is this a bug? looking forward to your reply. 3ks."
1239,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1239,filler does not pass query params to volume server,Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** requests to filer like http://172.18.0.18:8888/test/1.jpg?width=200 didnt resize image it looks like something was broken in version 1.59 (1.58 works as expected) **System Setup** version 30GB 1.64 linux amd64,source-file | source-file | source-file,filler pass query params volume server sponsors via patreon https www patreon describe bug requests filer like http jpg width didnt resize image looks like something broken works expected system setup linux amd,bug,0.9,filler does not pass query params to volume server Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** requests to filer like http://172.18.0.18:8888/test/1.jpg?width=200 didnt resize image it looks like something was broken in version 1.59 (1.58 works as expected) **System Setup** version 30GB 1.64 linux amd64
4270,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/4270,filer api should return client error (4XX) code instead 500 if dir already exists,"Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** Right now if `dir` (filer.path) exists and `/POST` request is sent to that dir, filer returns `500 internal server error` with message `{""error"":""dir /foo already exists""}`. Ideally here filer should return Client Error (4XX) code like `409` instead of `500`. **System Setup** Setup contains empty storage with default settings started using below commands  weed master -mdir=""."" -ip=localhost weed volume -max=100 -mserver=""localhost:9333"" -dir=""./data"" weed filer -ip=localhost -port=8888 -master=localhost:9333  Weed version is `version 30GB 3.33 ee7bf69c0396098bce2d030f24dba5085c392b7c linux amd64`. Below are steps to reproduce  curl -vvv -X POST ""http://localhost:8888/foo/"" * Trying 127.0.0.1:8888 * Connected to localhost (127.0.0.1) port 8888 (#0) > POST /foo/ HTTP/1.1 > Host: localhost:8888 > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 201 Created < Content-Type: application/json < Server: SeaweedFS Filer 30GB 3.33 < Date: Sun, 05 Mar 2023 07:25:46 GMT < Content-Length: 14 < * Connection #0 to host localhost left intact {""name"":""foo""} % curl -vvv -X POST ""http://localhost:8888/foo/"" * Trying 127.0.0.1:8888 * Connected to localhost (127.0.0.1) port 8888 (#0) > POST /foo/ HTTP/1.1 > Host: localhost:8888 > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 500 Internal Server Error < Content-Type: application/json < Server: SeaweedFS Filer 30GB 3.33 < Date: Sun, 05 Mar 2023 07:27:18 GMT < Content-Length: 35 < * Connection #0 to host localhost left intact {""error"":""dir /foo already exists""}%  **Expected behavior** filer should return Client Error (4XX) code like `409` instead of `500`",source-file | source-file,filer api return client instead dir already exists sponsors via patreon https www patreon describe bug right dir filer path exists post sent dir filer returns internal server message dir foo already exists ideally filer return client like instead system setup setup contains empty storage default settings started commands weed mdir localhost weed volume max mserver localhost dir data weed filer localhost port localhost weed bce dba linux amd steps reproduce curl vvv post http localhost foo trying connected localhost port post foo http host localhost user agent curl accept mark bundle supporting multiuse http created content type application json server filer date mar gmt content length connection host localhost left intact name foo curl vvv post http localhost foo trying connected localhost port post foo http host localhost user agent curl accept mark bundle supporting multiuse http internal server content type application json server filer date mar gmt content length connection host localhost left intact dir foo already exists expected behavior filer return client like instead,bug,0.95,"filer api should return client error (4XX) code instead 500 if dir already exists Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** Right now if `dir` (filer.path) exists and `/POST` request is sent to that dir, filer returns `500 internal server error` with message `{""error"":""dir /foo already exists""}`. Ideally here filer should return Client Error (4XX) code like `409` instead of `500`. **System Setup** Setup contains empty storage with default settings started using below commands  weed master -mdir=""."" -ip=localhost weed volume -max=100 -mserver=""localhost:9333"" -dir=""./data"" weed filer -ip=localhost -port=8888 -master=localhost:9333  Weed version is `version 30GB 3.33 ee7bf69c0396098bce2d030f24dba5085c392b7c linux amd64`. Below are steps to reproduce  curl -vvv -X POST ""http://localhost:8888/foo/"" * Trying 127.0.0.1:8888 * Connected to localhost (127.0.0.1) port 8888 (#0) > POST /foo/ HTTP/1.1 > Host: localhost:8888 > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 201 Created < Content-Type: application/json < Server: SeaweedFS Filer 30GB 3.33 < Date: Sun, 05 Mar 2023 07:25:46 GMT < Content-Length: 14 < * Connection #0 to host localhost left intact {""name"":""foo""} % curl -vvv -X POST ""http://localhost:8888/foo/"" * Trying 127.0.0.1:8888 * Connected to localhost (127.0.0.1) port 8888 (#0) > POST /foo/ HTTP/1.1 > Host: localhost:8888 > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 500 Internal Server Error < Content-Type: application/json < Server: SeaweedFS Filer 30GB 3.33 < Date: Sun, 05 Mar 2023 07:27:18 GMT < Content-Length: 35 < * Connection #0 to host localhost left intact {""error"":""dir /foo already exists""}%  **Expected behavior** filer should return Client Error (4XX) code like `409` instead of `500`"
1999,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1999,[export] huge error output for non-existent volumeId,"**Screenshots** <img width=""1442"" alt=""Screenshot 2021-04-14 at 18 22 40"" src=""https://user-images.githubusercontent.com/9688322/114702666-8f2c7f00-9d4e-11eb-94e0-dccba0df3605.png"">",source-file,export huge output non existent volumeid screenshots img width alt screenshot https user images githubusercontent dccba png,bug,0.8,"[export] huge error output for non-existent volumeId **Screenshots** <img width=""1442"" alt=""Screenshot 2021-04-14 at 18 22 40"" src=""https://user-images.githubusercontent.com/9688322/114702666-8f2c7f00-9d4e-11eb-94e0-dccba0df3605.png"">"
37,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/37,redirectOnRead goes to localhost,"I've got it running as a server: ./weed server -filer.redirectOnRead -filer Here's the curl. Note the hostname in the Location header: bash-3.2$ curl -vI http://weed1:8888/path/to/sources/README.md > /dev/null > HEAD /path/to/sources/README.md HTTP/1.1 > User-Agent: curl/7.30.0 > Host: weed1:8888 > Accept: _/_ > > < HTTP/1.1 302 Found > < Location: http://localhost:8080/5,01afbd8c36 > < Date: Sat, 20 Dec 2014 17:15:34 GMT > < Content-Type: text/plain; charset=utf-8 > < > 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 > - Connection #0 to host weed1 left intact > bash-3.2$",config-file | other-file | other-file | documentation-file | documentation-file | config-file | other-file | config-file | source-file | source-file | source-file | other-file | config-file | source-file | source-file | config-file | config-file | config-file | config-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file,redirectonread goes localhost got running server weed server filer redirectonread filer curl note hostname location header bash curl http weed path sources readme null head path sources readme http user agent curl host weed accept http found location http localhost afbd date dec gmt content type text plain charset utf connection host weed left intact bash,bug,0.9,"redirectOnRead goes to localhost I've got it running as a server: ./weed server -filer.redirectOnRead -filer Here's the curl. Note the hostname in the Location header: bash-3.2$ curl -vI http://weed1:8888/path/to/sources/README.md > /dev/null > HEAD /path/to/sources/README.md HTTP/1.1 > User-Agent: curl/7.30.0 > Host: weed1:8888 > Accept: _/_ > > < HTTP/1.1 302 Found > < Location: http://localhost:8080/5,01afbd8c36 > < Date: Sat, 20 Dec 2014 17:15:34 GMT > < Content-Type: text/plain; charset=utf-8 > < > 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 > - Connection #0 to host weed1 left intact > bash-3.2$"
6379,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/6379,S3 DeleteMultipleObjectsHandler does not honour AllowEmptyFolder setting,"Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** When running the S3 DeleteMultipleObjectsHandler function, the setting ""AllowEmptyFolder"" is not honoured. In circumstances where the filer folders are mistakenly identified as being empty, this causes the entire bucket contents to be deleted. **System Setup** - We setup an integrated Filer + S3 handler, using ""leveldb2"" driver - Photon Linux 5 - Weed version 3.79 **Expected behavior** When AllowEmptyFolder setting is set to true, only the items specified in the DeleteMultipleObjectsHandler call should have been deleted. **Additional context** The environment on which this runs is suspected to have very slow disk access, which we believe to be the cause for the folders being identified as empty.",source-file | source-file,deletemultipleobjectshandler honour allowemptyfolder setting sponsors via patreon https www patreon describe bug running deletemultipleobjectshandler function setting allowemptyfolder honoured circumstances filer folders mistakenly identified empty causes entire bucket contents deleted system setup setup integrated filer handler leveldb driver photon linux weed expected behavior allowemptyfolder setting set true items specified deletemultipleobjectshandler call deleted additional context environment runs suspected slow disk access believe cause folders identified empty,bug,0.85,"S3 DeleteMultipleObjectsHandler does not honour AllowEmptyFolder setting Sponsors SeaweedFS via Patreon https://www.patreon.com/seaweedfs **Describe the bug** When running the S3 DeleteMultipleObjectsHandler function, the setting ""AllowEmptyFolder"" is not honoured. In circumstances where the filer folders are mistakenly identified as being empty, this causes the entire bucket contents to be deleted. **System Setup** - We setup an integrated Filer + S3 handler, using ""leveldb2"" driver - Photon Linux 5 - Weed version 3.79 **Expected behavior** When AllowEmptyFolder setting is set to true, only the items specified in the DeleteMultipleObjectsHandler call should have been deleted. **Additional context** The environment on which this runs is suspected to have very slow disk access, which we believe to be the cause for the folders being identified as empty."
4934,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/4934,volume.configure.replication seems to not work,"**Describe the bug** When using volume.configure.replication, the actual volume files still retain their old configuration. **Test case** Start a weed master (defaultReplication=020) and 3 volume servers:  weed master -defaultReplication=020 weed volume -port=8081 -dir=/tmp/volume1 -max=100 -mserver=""127.0.0.1:9333"" -dataCenter=dc1 -rack=rack1 weed volume -port=8082 -dir=/tmp/volume2 -max=100 -mserver=""127.0.0.1:9333"" -dataCenter=dc1 -rack=rack2 weed volume -port=8083 -dir=/tmp/volume3 -max=100 -mserver=""127.0.0.1:9333"" -dataCenter=dc1 -rack=rack3  Create some volumes:  curl http://localhost:9333/dir/assign  It should look like this: ![Screenshot from 2023-10-23 16-49-42](https://github.com/seaweedfs/seaweedfs/assets/7746192/e9eea85c-6f77-4d12-9b48-740d0cd94c59) Now, let's change the replication settings using weed shell:  weed shell volume.configure.replication -collectionPattern * -replication 010  The volume servers correctly show the change in the logs:  I1023 14:52:01.272205 disk_location.go:182 data file /tmp/volume2/1.dat, replication=010 v=3 size=8 ttl=  The API also thinks, it is changed:  $ curl http://localhost:9333/vol/status?pretty=y|grep ""ReplicaPlacement"" -A 2 ""ReplicaPlacement"": { ""rack"": 1 },  But when we look at the .dat files using `change_superblock.go`, we see that it is still 020:  $ go run change_superblock.go -volumeId=1 -dir=/tmp/volume1 Current Volume Replication: 020 Current Volume TTL:  Just to make sure, we run some tests. First, we fix the replication:  $ weed shell lock volume.fix.replication > volume 2 replication 010, but over replicated +3 > volume 3 replication 010, but over replicated +3 > volume 1 replication 010, but over replicated +3 > deleting volume 2 from 10.23.1.43:8083  > deleting volume 3 from 10.23.1.43:8083  > deleting volume 1 from 10.23.1.43:8083   Looks good. We now have 2 copies of every volume file: ![Screenshot from 2023-10-23 16-58-31](https://github.com/seaweedfs/seaweedfs/assets/7746192/58ec1a28-2cde-4d3d-a590-127338fcd9ca) If we delete one and fix replication again, it should add it back.  weed shell lock volume.delete -node 10.23.1.43:8081 -volumeId 1 volume.fix.replication > the number of locations for volume 1 has not increased yet, let's wait > the number of locations for volume 1 has not increased yet, let's wait > the number of locations for volume 1 has not increased yet, let's wait > the number of locations for volume 1 has not increased yet, let's wait > the number of locations for volume 1 has not increased yet, let's wait > error: replicas volume 1 mismatch in topology  It did restore the missing copy but showed the error `error: replicas volume 1 mismatch in topology`. The volume server log shows that it added the volume with replication 020:  I1023 15:03:29.264620 disk_location.go:182 data file /tmp/volume3/1.dat, replication=020 v=3 size=8 ttl=  If we query the API, it now also shows replication 020 for the restored copy:  curl http://localhost:9333/vol/status?pretty=y|grep ""ReplicaPlacement"" -A 2 ""ReplicaPlacement"": { ""rack"": 1 }, ""ReplicaPlacement"": { ""rack"": 2 },  **System Setup** - Ubuntu 22.04 - output of `weed version`: version 30GB 3.57 0f8168c0c928bba3d2f48b0680d3bdce9c617559 linux amd64 **Expected behavior** `volume.configure.replication` changes the replication setting in the volume files.",source-file | source-file | source-file,volume configure replication seems work describe bug volume configure replication actual volume files still retain old configuration case start weed defaultreplication volume servers weed defaultreplication weed volume port dir tmp volume max mserver datacenter rack rack weed volume port dir tmp volume max mserver datacenter rack rack weed volume port dir tmp volume max mserver datacenter rack rack create volumes curl http localhost dir assign look like screenshot https github assets eea let replication settings weed shell weed shell volume configure replication collectionpattern replication volume servers correctly show logs disk location data tmp volume dat replication size ttl api also thinks changed curl http localhost vol status pretty grep replicaplacement replicaplacement rack look dat files superblock see still superblock volumeid dir tmp volume current volume replication current volume ttl make sure tests first replication weed shell lock volume replication volume replication replicated volume replication replicated volume replication replicated deleting volume deleting volume deleting volume looks good copies every volume screenshot https github assets cde fcd delete one replication back weed shell lock volume delete node volumeid volume replication number locations volume increased yet let wait number locations volume increased yet let wait number locations volume increased yet let wait number locations volume increased yet let wait number locations volume increased yet let wait replicas volume mismatch topology restore missing copy showed replicas volume mismatch topology volume server log shows added volume replication disk location data tmp volume dat replication size ttl query api also shows replication restored copy curl http localhost vol status pretty grep replicaplacement replicaplacement rack replicaplacement rack system setup ubuntu output weed bba bdce linux amd expected behavior volume configure replication changes replication setting volume files,bug,0.9,"volume.configure.replication seems to not work **Describe the bug** When using volume.configure.replication, the actual volume files still retain their old configuration. **Test case** Start a weed master (defaultReplication=020) and 3 volume servers:  weed master -defaultReplication=020 weed volume -port=8081 -dir=/tmp/volume1 -max=100 -mserver=""127.0.0.1:9333"" -dataCenter=dc1 -rack=rack1 weed volume -port=8082 -dir=/tmp/volume2 -max=100 -mserver=""127.0.0.1:9333"" -dataCenter=dc1 -rack=rack2 weed volume -port=8083 -dir=/tmp/volume3 -max=100 -mserver=""127.0.0.1:9333"" -dataCenter=dc1 -rack=rack3  Create some volumes:  curl http://localhost:9333/dir/assign  It should look like this: ![Screenshot from 2023-10-23 16-49-42](https://github.com/seaweedfs/seaweedfs/assets/7746192/e9eea85c-6f77-4d12-9b48-740d0cd94c59) Now, let's change the replication settings using weed shell:  weed shell volume.configure.replication -collectionPattern * -replication 010  The volume servers correctly show the change in the logs:  I1023 14:52:01.272205 disk_location.go:182 data file /tmp/volume2/1.dat, replication=010 v=3 size=8 ttl=  The API also thinks, it is changed:  $ curl http://localhost:9333/vol/status?pretty=y|grep ""ReplicaPlacement"" -A 2 ""ReplicaPlacement"": { ""rack"": 1 },  But when we look at the .dat files using `change_superblock.go`, we see that it is still 020:  $ go run change_superblock.go -volumeId=1 -dir=/tmp/volume1 Current Volume Replication: 020 Current Volume TTL:  Just to make sure, we run some tests. First, we fix the replication:  $ weed shell lock volume.fix.replication > volume 2 replication 010, but over replicated +3 > volume 3 replication 010, but over replicated +3 > volume 1 replication 010, but over replicated +3 > deleting volume 2 from 10.23.1.43:8083  > deleting volume 3 from 10.23.1.43:8083  > deleting volume 1 from 10.23.1.43:8083   Looks good. We now have 2 copies of every volume file: ![Screenshot from 2023-10-23 16-58-31](https://github.com/seaweedfs/seaweedfs/assets/7746192/58ec1a28-2cde-4d3d-a590-127338fcd9ca) If we delete one and fix replication again, it should add it back.  weed shell lock volume.delete -node 10.23.1.43:8081 -volumeId 1 volume.fix.replication > the number of locations for volume 1 has not increased yet, let's wait > the number of locations for volume 1 has not increased yet, let's wait > the number of locations for volume 1 has not increased yet, let's wait > the number of locations for volume 1 has not increased yet, let's wait > the number of locations for volume 1 has not increased yet, let's wait > error: replicas volume 1 mismatch in topology  It did restore the missing copy but showed the error `error: replicas volume 1 mismatch in topology`. The volume server log shows that it added the volume with replication 020:  I1023 15:03:29.264620 disk_location.go:182 data file /tmp/volume3/1.dat, replication=020 v=3 size=8 ttl=  If we query the API, it now also shows replication 020 for the restored copy:  curl http://localhost:9333/vol/status?pretty=y|grep ""ReplicaPlacement"" -A 2 ""ReplicaPlacement"": { ""rack"": 1 }, ""ReplicaPlacement"": { ""rack"": 2 },  **System Setup** - Ubuntu 22.04 - output of `weed version`: version 30GB 3.57 0f8168c0c928bba3d2f48b0680d3bdce9c617559 linux amd64 **Expected behavior** `volume.configure.replication` changes the replication setting in the volume files."
409,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/409,seaweedfs should support 0-size file uploading and downloading,"Now, 0-size text file can be uploaded and download successfully. But, 0-size jpg file(or other not isGzippable()) can not be uploaded, report `{""name"":""empty.doc"",""error"":""Failed to write to local disk""}`. I suggest seaweedfs should support 0-size all format file unploading and downloading, or return `400 bad request` when being uploaded. 0-size text file uploaded successfully:  # curl -F file=@empty.txt 192.168.4.18:28083/45,6c8916e496119e -v * Hostname was NOT found in DNS cache * Trying 192.168.4.18 * Connected to 192.168.4.18 (192.168.4.18) port 28083 (#0) > POST /45,6c8916e496119e HTTP/1.1 > User-Agent: curl/7.35.0 > Host: 192.168.4.18:28083 > Accept: */* > Content-Length: 187 > Expect: 100-continue > Content-Type: multipart/form-data; boundary=bcba47b57757d366 > < HTTP/1.1 100 Continue < HTTP/1.1 201 Created < Content-Type: application/json < Date: Tue, 06 Dec 2016 07:34:49 GMT < Content-Length: 30 < * Connection #0 to host 192.168.4.18 left intact {""name"":""empty.txt"",""size"":23}  0-size doc file uploaded unsuccessfully:  # curl -F file=@empty.doc 192.168.4.18:28083/45,6c8916e496119e -v * Hostname was NOT found in DNS cache * Trying 192.168.4.18 * Connected to 192.168.4.18 (192.168.4.18) port 28083 (#0) > POST /45,6c8916e496119e HTTP/1.1 > User-Agent: curl/7.35.0 > Host: 192.168.4.18:28083 > Accept: */* > Content-Length: 201 > Expect: 100-continue > Content-Type: multipart/form-data; boundary=8228c550c48374e5 > < HTTP/1.1 100 Continue < HTTP/1.1 500 Internal Server Error < Content-Type: application/json < Date: Tue, 06 Dec 2016 07:30:34 GMT < Content-Length: 60 * HTTP error before end of send, stop sending < * Closing connection 0 {""name"":""empty.doc"",""error"":""Failed to write to local disk""} ",source-file,size uploading downloading size text uploaded download successfully size jpg isgzippable uploaded report name empty failed write local disk suggest size format unploading downloading return bad uploaded size text uploaded successfully curl empty txt hostname found dns cache trying connected port post http user agent curl host accept content length expect continue content type multipart form data boundary bcba http continue http created content type application json date dec gmt content length connection host left intact name empty txt size size uploaded unsuccessfully curl empty hostname found dns cache trying connected port post http user agent curl host accept content length expect continue content type multipart form data boundary http continue http internal server content type application json date dec gmt content length http end send stop sending closing connection name empty failed write local disk,bug,0.95,"seaweedfs should support 0-size file uploading and downloading Now, 0-size text file can be uploaded and download successfully. But, 0-size jpg file(or other not isGzippable()) can not be uploaded, report `{""name"":""empty.doc"",""error"":""Failed to write to local disk""}`. I suggest seaweedfs should support 0-size all format file unploading and downloading, or return `400 bad request` when being uploaded. 0-size text file uploaded successfully:  # curl -F file=@empty.txt 192.168.4.18:28083/45,6c8916e496119e -v * Hostname was NOT found in DNS cache * Trying 192.168.4.18 * Connected to 192.168.4.18 (192.168.4.18) port 28083 (#0) > POST /45,6c8916e496119e HTTP/1.1 > User-Agent: curl/7.35.0 > Host: 192.168.4.18:28083 > Accept: */* > Content-Length: 187 > Expect: 100-continue > Content-Type: multipart/form-data; boundary=bcba47b57757d366 > < HTTP/1.1 100 Continue < HTTP/1.1 201 Created < Content-Type: application/json < Date: Tue, 06 Dec 2016 07:34:49 GMT < Content-Length: 30 < * Connection #0 to host 192.168.4.18 left intact {""name"":""empty.txt"",""size"":23}  0-size doc file uploaded unsuccessfully:  # curl -F file=@empty.doc 192.168.4.18:28083/45,6c8916e496119e -v * Hostname was NOT found in DNS cache * Trying 192.168.4.18 * Connected to 192.168.4.18 (192.168.4.18) port 28083 (#0) > POST /45,6c8916e496119e HTTP/1.1 > User-Agent: curl/7.35.0 > Host: 192.168.4.18:28083 > Accept: */* > Content-Length: 201 > Expect: 100-continue > Content-Type: multipart/form-data; boundary=8228c550c48374e5 > < HTTP/1.1 100 Continue < HTTP/1.1 500 Internal Server Error < Content-Type: application/json < Date: Tue, 06 Dec 2016 07:30:34 GMT < Content-Length: 60 * HTTP error before end of send, stop sending < * Closing connection 0 {""name"":""empty.doc"",""error"":""Failed to write to local disk""} "
524,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/524,hanged when run weed master,"  tmp git:(master)  weed master -peers 10.64.7.106:9444,10.4.23.114:9444,10.4.23.115:9444 -ip 10.64.7.106 -ip.bind 10.64.7.106 -port 9444 I0703 19:46:40 39010 file_util.go:20] Folder /tmp Permission: -rwxrwxrwx I0703 19:46:40 39010 master_server.go:62] Volume Size Limit is 30000 MB I0703 19:46:40 39010 master.go:87] Start Seaweed Master 0.76 at 10.64.7.106:9444 I0703 19:46:40 39010 raft_server.go:56] Peers Change: [] => [10.64.7.106:9444 10.4.23.114:9444 10.4.23.115:9444] I0703 19:46:40 39010 raft_server.go:78] Joining cluster: 10.64.7.106:9444,10.4.23.114:9444,10.4.23.115:9444 I0703 19:46:41 39010 raft_server.go:166] Attempting to connect to: http://10.4.23.114:9444/cluster/join  using browser open this url , crashed.  2017/07/03 19:47:13 http: panic serving 10.232.4.175:59240: runtime error: invalid memory address or nil pointer dereference goroutine 55 [running]: net/http.(*conn).serve.func1(0xc4202d6140) /home/jinlei1/os/go/src/net/http/server.go:1721 +0xd0 panic(0xbab320, 0x10782f0) /home/jinlei1/os/go/src/runtime/panic.go:489 +0x2cf github.com/chrislusf/seaweedfs/weed/server.(*MasterServer).uiStatusHandler(0xc4201e2380, 0x1047400, 0xc42030a000, 0xc4202de500) /home/jinlei1/ksyun/src/github.com/chrislusf/seaweedfs/weed/server/master_server_handlers_ui.go:24 +0x13b github.com/chrislusf/seaweedfs/weed/server.(*MasterServer).(github.com/chrislusf/seaweedfs/weed/server.uiStatusHandler)-fm(0x1047400, 0xc42030a000, 0xc4202de500) /home/jinlei1/ksyun/src/github.com/chrislusf/seaweedfs/weed/server/master_server.go:66 +0x48 net/http.HandlerFunc.ServeHTTP(0xc4201c7010, 0x1047400, 0xc42030a000, 0xc4202de500) /home/jinlei1/os/go/src/net/http/server.go:1942 +0x44 github.com/gorilla/mux.(*Router).ServeHTTP(0xc4201ce960, 0x1047400, 0xc42030a000, 0xc4202de500) /home/jinlei1/ksyun/src/github.com/gorilla/mux/mux.go:114 +0x10c net/http.serverHandler.ServeHTTP(0xc4202049a0, 0x1047400, 0xc42030a000, 0xc4202de300) /home/jinlei1/os/go/src/net/http/server.go:2568 +0x92 net/http.(*conn).serve(0xc4202d6140, 0x1047f00, 0xc4202b0340) /home/jinlei1/os/go/src/net/http/server.go:1825 +0x612 created by net/http.(*Server).Serve /home/jinlei1/os/go/src/net/http/server.go:2668 +0x2ce ",source-file | source-file,hanged weed tmp git weed peers bind port util folder tmp permission rwxrwxrwx server volume size limit start seaweed raft server peers raft server joining cluster raft server attempting connect http cluster join browser open url crashed http panic serving runtime invalid memory address nil pointer dereference goroutine running http conn serve func home jinlei http server panic xbab home jinlei runtime panic github chrislusf weed server masterserver uistatushandler home jinlei ksyun github chrislusf weed server server handlers github chrislusf weed server masterserver github chrislusf weed server uistatushandler home jinlei ksyun github chrislusf weed server server http handlerfunc servehttp home jinlei http server github gorilla mux router servehttp home jinlei ksyun github gorilla mux mux http serverhandler servehttp home jinlei http server http conn serve home jinlei http server created http server serve home jinlei http server,bug,0.9,"hanged when run weed master   tmp git:(master)  weed master -peers 10.64.7.106:9444,10.4.23.114:9444,10.4.23.115:9444 -ip 10.64.7.106 -ip.bind 10.64.7.106 -port 9444 I0703 19:46:40 39010 file_util.go:20] Folder /tmp Permission: -rwxrwxrwx I0703 19:46:40 39010 master_server.go:62] Volume Size Limit is 30000 MB I0703 19:46:40 39010 master.go:87] Start Seaweed Master 0.76 at 10.64.7.106:9444 I0703 19:46:40 39010 raft_server.go:56] Peers Change: [] => [10.64.7.106:9444 10.4.23.114:9444 10.4.23.115:9444] I0703 19:46:40 39010 raft_server.go:78] Joining cluster: 10.64.7.106:9444,10.4.23.114:9444,10.4.23.115:9444 I0703 19:46:41 39010 raft_server.go:166] Attempting to connect to: http://10.4.23.114:9444/cluster/join  using browser open this url , crashed.  2017/07/03 19:47:13 http: panic serving 10.232.4.175:59240: runtime error: invalid memory address or nil pointer dereference goroutine 55 [running]: net/http.(*conn).serve.func1(0xc4202d6140) /home/jinlei1/os/go/src/net/http/server.go:1721 +0xd0 panic(0xbab320, 0x10782f0) /home/jinlei1/os/go/src/runtime/panic.go:489 +0x2cf github.com/chrislusf/seaweedfs/weed/server.(*MasterServer).uiStatusHandler(0xc4201e2380, 0x1047400, 0xc42030a000, 0xc4202de500) /home/jinlei1/ksyun/src/github.com/chrislusf/seaweedfs/weed/server/master_server_handlers_ui.go:24 +0x13b github.com/chrislusf/seaweedfs/weed/server.(*MasterServer).(github.com/chrislusf/seaweedfs/weed/server.uiStatusHandler)-fm(0x1047400, 0xc42030a000, 0xc4202de500) /home/jinlei1/ksyun/src/github.com/chrislusf/seaweedfs/weed/server/master_server.go:66 +0x48 net/http.HandlerFunc.ServeHTTP(0xc4201c7010, 0x1047400, 0xc42030a000, 0xc4202de500) /home/jinlei1/os/go/src/net/http/server.go:1942 +0x44 github.com/gorilla/mux.(*Router).ServeHTTP(0xc4201ce960, 0x1047400, 0xc42030a000, 0xc4202de500) /home/jinlei1/ksyun/src/github.com/gorilla/mux/mux.go:114 +0x10c net/http.serverHandler.ServeHTTP(0xc4202049a0, 0x1047400, 0xc42030a000, 0xc4202de300) /home/jinlei1/os/go/src/net/http/server.go:2568 +0x92 net/http.(*conn).serve(0xc4202d6140, 0x1047f00, 0xc4202b0340) /home/jinlei1/os/go/src/net/http/server.go:1825 +0x612 created by net/http.(*Server).Serve /home/jinlei1/os/go/src/net/http/server.go:2668 +0x2ce "
16,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/16,content type from volume post upload always text/plain?,"I'm assuming the return content-type should be application/json?  $ curl -v -F ""file=@/tmp/test.json;type=application/json"" localhost:8080/7,07a27ee211 > POST /7,07a27ee211 HTTP/1.1 > User-Agent: curl/7.30.0 > Host: localhost:8080 > Accept: */* > Content-Length: 223 > Expect: 100-continue > Content-Type: multipart/form-data; boundary=2f1cf300fd2b > < HTTP/1.1 100 Continue < HTTP/1.1 201 Created < Date: Mon, 29 Sep 2014 20:08:45 GMT < Content-Length: 30 < Content-Type: text/plain; charset=utf-8 < * Connection #0 to host localhost left intact {""name"":""test.json"",""size"":74} ",config-file | documentation-file | documentation-file | other-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | test-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file,content type volume post upload always text plain assuming return content type application json curl tmp json type application json localhost post http user agent curl host localhost accept content length expect continue content type multipart form data boundary http continue http created date sep gmt content length content type text plain charset utf connection host localhost left intact name json size,bug,0.9,"content type from volume post upload always text/plain? I'm assuming the return content-type should be application/json?  $ curl -v -F ""file=@/tmp/test.json;type=application/json"" localhost:8080/7,07a27ee211 > POST /7,07a27ee211 HTTP/1.1 > User-Agent: curl/7.30.0 > Host: localhost:8080 > Accept: */* > Content-Length: 223 > Expect: 100-continue > Content-Type: multipart/form-data; boundary=2f1cf300fd2b > < HTTP/1.1 100 Continue < HTTP/1.1 201 Created < Date: Mon, 29 Sep 2014 20:08:45 GMT < Content-Length: 30 < Content-Type: text/plain; charset=utf-8 < * Connection #0 to host localhost left intact {""name"":""test.json"",""size"":74} "
1884,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1884,S3 upload failed for filenames including '%' character,"**Describe the bug** S3 filer API can't handle files with '%' sign despite it being the valid name (and valid object for s3 and minio, I found plenty of those inside Photo library on Mac):  touch %%test.file.txt touch %%test.file.txt aws --endpoint-url http://rpi4node3:8333/ s3 cp %test.file.txt s3://testbucket/ upload failed: ./%test.file.txt to s3://testbucket/%test.file.txt An error occurred (InternalError) when calling the PutObject operation (reached max retries: 2): We encountered an internal error, please try again.  Upload via weed upload works:  > weed upload -master=""rpi4node3:9333"" %%test.file.txt [{""fileName"":""%%test.file.txt"",""url"":""rpi4node3:8088/13,022beaf4f28a09"",""fid"":""13,022beaf4f28a09""}] > weed upload -master=""rpi4node3:9333"" %test.file.txt [{""fileName"":""%test.file.txt"",""url"":""rpi4node3:8088/6,022beb5bdfd3a2"",""fid"":""6,022beb5bdfd3a2""}]  **System Setup** Master started with:  ~/go/bin/weed server -filer -s3 -ip=rpi4node3 -volume.max=0,0 -master.volumeSizeLimitMB=1024 -volume.port=8088 -dir=/mnt/usbslow/weed,/mnt/usbfast/weed/ -master.dir=/mnt/usbfast/weed/master  Filer config:  [redis_cluster2] enabled = true addresses = [ ""rpi4node1:6379"", ""rpi4node2:6379"", ""rpi4node3:6379"" ] password = """" # allows reads from slave servers or the master, but all writes still go to the master readOnly = false # automatically use the closest Redis server for reads routeByLatency = false # This changes the data layout. Only add new directories. Removing/Updating will cause data loss. superLargeDirectories = []  Volumes:  /usr/local/bin/weed volume -mserver=""rpi4node3:9333"" -max=1000,1000 -port=8088 -disk=hdd,ssd -dir=/mnt/usbslow/weed,/mnt/usbfast/weed/ &  Master weed version:  ~/go/bin/weed version version 30GB 2.29 400de380 linux arm64  Complied from:  commit 400de380f48c44c7700fdf7e8f247bf856662c10 (HEAD -> master, origin/master, origin/HEAD) Author: Chris Lu <chris.lu@gmail.com> Date: Fri Mar 5 02:29:38 2021 -0800 volume server: support tcp direct put/get/delete  **Expected behavior** Successful upload of files via S3. **Additional context** You guessed seaweed fs runs on RPI4 cluster. I found it's a really amazing project and very easy to setup. ",source-file | source-file,upload failed filenames including character describe bug filer api handle files sign despite valid name valid object minio found plenty inside photo library mac touch txt touch txt aws endpoint url http rpi node txt testbucket upload failed txt testbucket txt occurred internalerror calling putobject operation reached max retries encountered internal please upload via weed upload works weed upload rpi node txt filename txt url rpi node beaf fid beaf weed upload rpi node txt filename txt url rpi node beb bdfd fid beb bdfd system setup started weed server filer rpi node volume max volumesizelimitmb volume port dir mnt usbslow weed mnt usbfast weed dir mnt usbfast weed filer config redis cluster enabled true addresses rpi node rpi node rpi node password allows reads slave servers writes still readonly false automatically use closest redis server reads routebylatency false changes data layout new directories removing updating cause data loss superlargedirectories volumes local weed volume mserver rpi node max port disk hdd ssd dir mnt usbslow weed mnt usbfast weed weed weed linux arm complied fdf head origin origin head chris chris gmail date mar volume server tcp direct put get delete expected behavior successful upload files via additional context guessed seaweed runs rpi cluster found really amazing project easy setup,bug,0.9,"S3 upload failed for filenames including '%' character **Describe the bug** S3 filer API can't handle files with '%' sign despite it being the valid name (and valid object for s3 and minio, I found plenty of those inside Photo library on Mac):  touch %%test.file.txt touch %%test.file.txt aws --endpoint-url http://rpi4node3:8333/ s3 cp %test.file.txt s3://testbucket/ upload failed: ./%test.file.txt to s3://testbucket/%test.file.txt An error occurred (InternalError) when calling the PutObject operation (reached max retries: 2): We encountered an internal error, please try again.  Upload via weed upload works:  > weed upload -master=""rpi4node3:9333"" %%test.file.txt [{""fileName"":""%%test.file.txt"",""url"":""rpi4node3:8088/13,022beaf4f28a09"",""fid"":""13,022beaf4f28a09""}] > weed upload -master=""rpi4node3:9333"" %test.file.txt [{""fileName"":""%test.file.txt"",""url"":""rpi4node3:8088/6,022beb5bdfd3a2"",""fid"":""6,022beb5bdfd3a2""}]  **System Setup** Master started with:  ~/go/bin/weed server -filer -s3 -ip=rpi4node3 -volume.max=0,0 -master.volumeSizeLimitMB=1024 -volume.port=8088 -dir=/mnt/usbslow/weed,/mnt/usbfast/weed/ -master.dir=/mnt/usbfast/weed/master  Filer config:  [redis_cluster2] enabled = true addresses = [ ""rpi4node1:6379"", ""rpi4node2:6379"", ""rpi4node3:6379"" ] password = """" # allows reads from slave servers or the master, but all writes still go to the master readOnly = false # automatically use the closest Redis server for reads routeByLatency = false # This changes the data layout. Only add new directories. Removing/Updating will cause data loss. superLargeDirectories = []  Volumes:  /usr/local/bin/weed volume -mserver=""rpi4node3:9333"" -max=1000,1000 -port=8088 -disk=hdd,ssd -dir=/mnt/usbslow/weed,/mnt/usbfast/weed/ &  Master weed version:  ~/go/bin/weed version version 30GB 2.29 400de380 linux arm64  Complied from:  commit 400de380f48c44c7700fdf7e8f247bf856662c10 (HEAD -> master, origin/master, origin/HEAD) Author: Chris Lu <chris.lu@gmail.com> Date: Fri Mar 5 02:29:38 2021 -0800 volume server: support tcp direct put/get/delete  **Expected behavior** Successful upload of files via S3. **Additional context** You guessed seaweed fs runs on RPI4 cluster. I found it's a really amazing project and very easy to setup. "
5774,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/5774,remote sync unexpected behavior,"Not mounted directory has same prefix as mounted directory pushed to remote storage when running filer.remote.sync. for example, there is two dir, /foo and /foo-bar, only /foo mounted to remote storage remote:/foo, when running: `weed filer.remote.sync -dir=/foo` files in /foo-bar, for example /foo-bar/baz/qux was pushed to remote:/foo/-bar/baz/qux",test-file | source-file | source-file | source-file | source-file | source-file | source-file,remote sync unexpected behavior mounted directory prefix mounted directory pushed remote storage running filer remote sync example two dir foo foo bar foo mounted remote storage remote foo running weed filer remote sync dir foo files foo bar example foo bar baz qux pushed remote foo bar baz qux,bug,0.9,"remote sync unexpected behavior Not mounted directory has same prefix as mounted directory pushed to remote storage when running filer.remote.sync. for example, there is two dir, /foo and /foo-bar, only /foo mounted to remote storage remote:/foo, when running: `weed filer.remote.sync -dir=/foo` files in /foo-bar, for example /foo-bar/baz/qux was pushed to remote:/foo/-bar/baz/qux"
2125,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2125,[filer] Error delete many files with SQL,"**Describe the bug** Error filer store delete Folder Children See https://fromdual.com/keep-your-galera-cluster-up-and-running-by-all-means  filer_1 | I0611 10:13:05 1 abstract_sql_store.go:282] delete /buckets/registry/docker/registry/v2/blobs/sha256 SQL DELETE FROM `filemeta` WHERE dirhash=? AND directory=? -4910119483806141165 filer_1 | I0611 10:13:05 1 filer_delete_entry.go:39] delete directory /buckets/registry: filer store delete: deleteFolderChildren /buckets/registry/docker/registry/v2/blobs/sha256: Error 1180: wsrep_max_ws_rows exceeded filer_1 | I0611 10:13:05 1 error_handler.go:79] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> filer_1 | <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/registry</Resource><RequestId>1623406385367150308</RequestId><BucketName>registry</BucketName></Error>  **System Setup** master branch **Expected behavior** force small transactions when deleting Folder Children",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file,filer delete many files sql describe bug filer store delete folder children see https fromdual keep galera cluster running means filer abstract sql store delete buckets registry docker registry blobs sha sql delete filemeta dirhash directory filer filer delete entry delete directory buckets registry filer store delete deletefolderchildren buckets registry docker registry blobs sha wsrep max rows exceeded filer handler status application xml xml encoding utf filer internalerror message encountered internal please message resource registry resource requestid requestid bucketname registry bucketname system setup branch expected behavior force small transactions deleting folder children,bug,0.9,"[filer] Error delete many files with SQL **Describe the bug** Error filer store delete Folder Children See https://fromdual.com/keep-your-galera-cluster-up-and-running-by-all-means  filer_1 | I0611 10:13:05 1 abstract_sql_store.go:282] delete /buckets/registry/docker/registry/v2/blobs/sha256 SQL DELETE FROM `filemeta` WHERE dirhash=? AND directory=? -4910119483806141165 filer_1 | I0611 10:13:05 1 filer_delete_entry.go:39] delete directory /buckets/registry: filer store delete: deleteFolderChildren /buckets/registry/docker/registry/v2/blobs/sha256: Error 1180: wsrep_max_ws_rows exceeded filer_1 | I0611 10:13:05 1 error_handler.go:79] status 500 application/xml: <?xml version=""1.0"" encoding=""UTF-8""?> filer_1 | <Error><Code>InternalError</Code><Message>We encountered an internal error, please try again.</Message><Resource>/registry</Resource><RequestId>1623406385367150308</RequestId><BucketName>registry</BucketName></Error>  **System Setup** master branch **Expected behavior** force small transactions when deleting Folder Children"
1917,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1917,[s3] aws s3 copy dir/ fatal error: 'ContentLength',"**Describe the bug**  aws --profile local --endpoint http://127.0.0.1:8333/ s3 cp s3://test/dir /tmp/ fatal error: An error occurred (404) when calling the HeadObject operation: Key ""dir"" does not exist aws --profile local --endpoint http://127.0.0.1:8333/ s3 cp s3://test/dir/ /tmp/ fatal error: 'ContentLength'  **System Setup** `2.34` **Expected behavior** return 404",source-file,aws copy dir fatal contentlength describe bug aws profile local endpoint http dir tmp fatal occurred calling headobject operation key dir exist aws profile local endpoint http dir tmp fatal contentlength system setup expected behavior return,bug,0.9,"[s3] aws s3 copy dir/ fatal error: 'ContentLength' **Describe the bug**  aws --profile local --endpoint http://127.0.0.1:8333/ s3 cp s3://test/dir /tmp/ fatal error: An error occurred (404) when calling the HeadObject operation: Key ""dir"" does not exist aws --profile local --endpoint http://127.0.0.1:8333/ s3 cp s3://test/dir/ /tmp/ fatal error: 'ContentLength'  **System Setup** `2.34` **Expected behavior** return 404"
166,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/166,"when the former ttl(upload ttl, not assign ttl) expired, same file uploaded without ttl option, but it can't be downloaded"," # curl -F file=@start_master.sh http://192.168.23.70:18080/xxxx?ttl=1m {""name"":""start_master.sh"",""size"":645} // after 1m # curl -i http://192.168.23.70:18080/xxxx HTTP/1.1 404 Not Found Date: Fri, 10 Jul 2015 15:17:47 GMT Content-Length: 0 # curl -F file=@start_master.sh http://192.168.23.70:18080/xxxx {""name"":""start_master.sh"",""size"":645} # curl -i http://192.168.23.70:18080/xxxx HTTP/1.1 404 Not Found Date: Fri, 10 Jul 2015 15:18:12 GMT Content-Length: 0  If I uploaded another file, it can be downloaded:  # curl -F file=@start_volume.sh http://192.168.23.70:18080/xxxx {""name"":""start_volume.sh"",""size"":411} # curl -i http://192.168.23.70:18080/xxxx HTTP/1.1 200 OK Accept-Ranges: bytes Content-Disposition: filename=""start_volume.sh"" Content-Length: 411 Content-Type: text/x-sh; charset=utf-8 Etag: ""63bdc9b5"" Last-Modified: Fri, 10 Jul 2015 15:19:28 GMT Date: Fri, 10 Jul 2015 15:19:31 GMT <>  I think if file unchanged but ttl changed, needle should be written to data file:  golang func (v *Volume) write(n *Needle) (size uint32, err error) { if v.isFileUnchanged(n) { size = n.DataSize glog.V(4).Infof(""needle is unchanged!"") return } ",source-file | source-file,former ttl upload ttl assign ttl expired uploaded without ttl option downloaded curl start http xxxx ttl name start size curl http xxxx http found date jul gmt content length curl start http xxxx name start size curl http xxxx http found date jul gmt content length uploaded another downloaded curl start volume http xxxx name start volume size curl http xxxx http accept ranges bytes content disposition filename start volume content length content type text charset utf etag bdc last modified jul gmt date jul gmt think unchanged ttl changed needle written data golang func volume write needle size uint err isfileunchanged size datasize glog infof needle unchanged return,bug,0.9,"when the former ttl(upload ttl, not assign ttl) expired, same file uploaded without ttl option, but it can't be downloaded  # curl -F file=@start_master.sh http://192.168.23.70:18080/xxxx?ttl=1m {""name"":""start_master.sh"",""size"":645} // after 1m # curl -i http://192.168.23.70:18080/xxxx HTTP/1.1 404 Not Found Date: Fri, 10 Jul 2015 15:17:47 GMT Content-Length: 0 # curl -F file=@start_master.sh http://192.168.23.70:18080/xxxx {""name"":""start_master.sh"",""size"":645} # curl -i http://192.168.23.70:18080/xxxx HTTP/1.1 404 Not Found Date: Fri, 10 Jul 2015 15:18:12 GMT Content-Length: 0  If I uploaded another file, it can be downloaded:  # curl -F file=@start_volume.sh http://192.168.23.70:18080/xxxx {""name"":""start_volume.sh"",""size"":411} # curl -i http://192.168.23.70:18080/xxxx HTTP/1.1 200 OK Accept-Ranges: bytes Content-Disposition: filename=""start_volume.sh"" Content-Length: 411 Content-Type: text/x-sh; charset=utf-8 Etag: ""63bdc9b5"" Last-Modified: Fri, 10 Jul 2015 15:19:28 GMT Date: Fri, 10 Jul 2015 15:19:31 GMT <>  I think if file unchanged but ttl changed, needle should be written to data file:  golang func (v *Volume) write(n *Needle) (size uint32, err error) { if v.isFileUnchanged(n) { size = n.DataSize glog.V(4).Infof(""needle is unchanged!"") return } "
1436,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/1436,"Volume not found, but it actually exists.","Version: 1.91 There are times that filer return 500 when downloading file. And the error said ""volume 603 not found"". But it actually exists on volume server. Ant when I restart the filer. It will be correct. ![image](https://user-images.githubusercontent.com/11285030/91559504-ca6a7100-e96a-11ea-8ecf-384f7f7cd148.png) ![image](https://user-images.githubusercontent.com/11285030/91559548-dfdf9b00-e96a-11ea-9f41-b85dd81a1a71.png) I didn't open the verbose log on that case. I'm still checking stack trace. I will paste more information here next time it happened",source-file,volume found actually exists times filer return downloading said volume found actually exists volume server ant restart filer correct image https user images githubusercontent ecf png image https user images githubusercontent dfdf png open verbose log case still checking stack trace paste information next time happened,bug,0.85,"Volume not found, but it actually exists. Version: 1.91 There are times that filer return 500 when downloading file. And the error said ""volume 603 not found"". But it actually exists on volume server. Ant when I restart the filer. It will be correct. ![image](https://user-images.githubusercontent.com/11285030/91559504-ca6a7100-e96a-11ea-8ecf-384f7f7cd148.png) ![image](https://user-images.githubusercontent.com/11285030/91559548-dfdf9b00-e96a-11ea-9f41-b85dd81a1a71.png) I didn't open the verbose log on that case. I'm still checking stack trace. I will paste more information here next time it happened"
2371,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2371,[s3] Content Disposition support,"**Describe the bug** ContentDisposition and ResponseContentDisposition parameters are ignored When using s3 api, ContentDisposition is always set to 'inline; filename=<key>' Example with boto3: Setting ContentDisposition during put: python3 s3.put_object(Bucket='test', Key='test.jpeg', Body='test.jpeg', ContentDisposition='attachment; filename=""test.jpeg') {'ResponseMetadata': {'RequestId': '1633946607781063224', 'HostId': '', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 11 Oct 2021 10:03:27 GMT', 'content-length': '0', 'connection': 'keep-alive', 'accept-ranges': 'bytes', 'etag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'x-amz-request-id': '1633946607781063224', 'strict-transport-security': 'max-age=15724800; includeSubDomains', }, 'RetryAttempts': 0}, 'ETag': '""bb1469ad3d21a04760cf719a86f2e7be""'} s3.get_object(Bucket='test', Key='test.jpeg') {'ResponseMetadata': {'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 11 Oct 2021 10:03:45 GMT', 'content-type': 'image/jpeg', 'content-length': '9', 'connection': 'keep-alive', 'accept-ranges': 'bytes', 'content-disposition': 'inline; filename=""test.jpeg""', 'etag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'last-modified': 'Mon, 11 Oct 2021 10:03:27 GMT', 'strict-transport-security': 'max-age=15724800; includeSubDomains', }, 'RetryAttempts': 0}, 'AcceptRanges': 'bytes', 'LastModified': datetime.datetime(2021, 10, 11, 10, 3, 27, tzinfo=tzutc()), 'ContentLength': 9, 'ETag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'ContentDisposition': 'inline; filename=""test.jpeg""', 'ContentType': 'image/jpeg', 'Metadata': {}, 'Body': <botocore.response.StreamingBody at 0x1108820d0>}  Setting ResponseContentDisposition: python3 s3.get_object(Bucket='test', Key='test.jpeg', ResponseContentDisposition='attachment; filename=test.jpeg') {'ResponseMetadata': {'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 11 Oct 2021 10:07:56 GMT', 'content-type': 'image/jpeg', 'content-length': '9', 'connection': 'keep-alive', 'accept-ranges': 'bytes', 'content-disposition': 'inline; filename=""test.jpeg""', 'etag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'last-modified': 'Mon, 11 Oct 2021 10:03:27 GMT', 'strict-transport-security': 'max-age=15724800; includeSubDomains' }, 'RetryAttempts': 0}, 'AcceptRanges': 'bytes', 'LastModified': datetime.datetime(2021, 10, 11, 10, 3, 27, tzinfo=tzutc()), 'ContentLength': 9, 'ETag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'ContentDisposition': 'inline; filename=""test.jpeg""', 'ContentType': 'image/jpeg', 'Metadata': {}, 'Body': <botocore.response.StreamingBody at 0x110882340>}  **System Setup** seaweedfs 2.70 **Expected behavior** Setting ContentDisposition parameter during put should set the ContentDisposition in object metadata, setting ResponseContentDisposition during get should only set this in reposnse metadata",source-file | source-file | source-file,content disposition describe bug contentdisposition responsecontentdisposition parameters ignored api contentdisposition always set inline filename key example boto setting contentdisposition put python put object bucket key jpeg body jpeg contentdisposition attachment filename jpeg responsemetadata requestid hostid httpstatuscode httpheaders date oct gmt content length connection keep alive accept ranges bytes etag amz strict transport security max age includesubdomains retryattempts etag get object bucket key jpeg responsemetadata httpstatuscode httpheaders date oct gmt content type image jpeg content length connection keep alive accept ranges bytes content disposition inline filename jpeg etag last modified oct gmt strict transport security max age includesubdomains retryattempts acceptranges bytes lastmodified datetime datetime tzinfo tzutc contentlength etag contentdisposition inline filename jpeg contenttype image jpeg metadata body botocore response streamingbody setting responsecontentdisposition python get object bucket key jpeg responsecontentdisposition attachment filename jpeg responsemetadata httpstatuscode httpheaders date oct gmt content type image jpeg content length connection keep alive accept ranges bytes content disposition inline filename jpeg etag last modified oct gmt strict transport security max age includesubdomains retryattempts acceptranges bytes lastmodified datetime datetime tzinfo tzutc contentlength etag contentdisposition inline filename jpeg contenttype image jpeg metadata body botocore response streamingbody system setup expected behavior setting contentdisposition parameter put set contentdisposition object metadata setting responsecontentdisposition get set reposnse metadata,bug,0.95,"[s3] Content Disposition support **Describe the bug** ContentDisposition and ResponseContentDisposition parameters are ignored When using s3 api, ContentDisposition is always set to 'inline; filename=<key>' Example with boto3: Setting ContentDisposition during put: python3 s3.put_object(Bucket='test', Key='test.jpeg', Body='test.jpeg', ContentDisposition='attachment; filename=""test.jpeg') {'ResponseMetadata': {'RequestId': '1633946607781063224', 'HostId': '', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 11 Oct 2021 10:03:27 GMT', 'content-length': '0', 'connection': 'keep-alive', 'accept-ranges': 'bytes', 'etag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'x-amz-request-id': '1633946607781063224', 'strict-transport-security': 'max-age=15724800; includeSubDomains', }, 'RetryAttempts': 0}, 'ETag': '""bb1469ad3d21a04760cf719a86f2e7be""'} s3.get_object(Bucket='test', Key='test.jpeg') {'ResponseMetadata': {'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 11 Oct 2021 10:03:45 GMT', 'content-type': 'image/jpeg', 'content-length': '9', 'connection': 'keep-alive', 'accept-ranges': 'bytes', 'content-disposition': 'inline; filename=""test.jpeg""', 'etag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'last-modified': 'Mon, 11 Oct 2021 10:03:27 GMT', 'strict-transport-security': 'max-age=15724800; includeSubDomains', }, 'RetryAttempts': 0}, 'AcceptRanges': 'bytes', 'LastModified': datetime.datetime(2021, 10, 11, 10, 3, 27, tzinfo=tzutc()), 'ContentLength': 9, 'ETag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'ContentDisposition': 'inline; filename=""test.jpeg""', 'ContentType': 'image/jpeg', 'Metadata': {}, 'Body': <botocore.response.StreamingBody at 0x1108820d0>}  Setting ResponseContentDisposition: python3 s3.get_object(Bucket='test', Key='test.jpeg', ResponseContentDisposition='attachment; filename=test.jpeg') {'ResponseMetadata': {'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 11 Oct 2021 10:07:56 GMT', 'content-type': 'image/jpeg', 'content-length': '9', 'connection': 'keep-alive', 'accept-ranges': 'bytes', 'content-disposition': 'inline; filename=""test.jpeg""', 'etag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'last-modified': 'Mon, 11 Oct 2021 10:03:27 GMT', 'strict-transport-security': 'max-age=15724800; includeSubDomains' }, 'RetryAttempts': 0}, 'AcceptRanges': 'bytes', 'LastModified': datetime.datetime(2021, 10, 11, 10, 3, 27, tzinfo=tzutc()), 'ContentLength': 9, 'ETag': '""bb1469ad3d21a04760cf719a86f2e7be""', 'ContentDisposition': 'inline; filename=""test.jpeg""', 'ContentType': 'image/jpeg', 'Metadata': {}, 'Body': <botocore.response.StreamingBody at 0x110882340>}  **System Setup** seaweedfs 2.70 **Expected behavior** Setting ContentDisposition parameter during put should set the ContentDisposition in object metadata, setting ResponseContentDisposition during get should only set this in reposnse metadata"
3467,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/3467,"Filer if-modified-since check doesn't fire on ""exact""","**Describe the bug** Filer does not return HTTP 304 if the `if-modified-since` value is exact the same as the `Last-Modfied` header **System Setup** - `weed server -dataCenter=$HOSTNAME -ip=$HOSTNAME -master.dir=/meta -master.peers=""app1:9333,app2:9333,app3:9333"" -dir=/data -volume.dir.idx=/meta -volume.index=leveldbLarge -volume.max=0 -rack=1 -filer.encryptVolumeData -volume.fileSizeLimitMB=4096 -master.volumeSizeLimitMB=1024 -filer.defaultReplicaPlacement=100 -filer.saveToFilerLimit=4096 -filer""` - Ubuntu Server **Expected behavior** Filer should return 304: Not modified to save bandwidth **Screenshots** bash system@app2:~$ curl --head --http1.1 -H 'if-modified-since: Thu, 18 Aug 2022 15:40:16 GMT' 'http://localhost:8888/qr-code.svg' HTTP/1.1 304 Not Modified Last-Modified: Thu, 18 Aug 2022 15:40:15 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Fri, 19 Aug 2022 15:29:42 GMT system@app2:~$ curl --head --http1.1 -H 'if-modified-since: Thu, 18 Aug 2022 15:40:15 GMT' 'http://localhost:8888/qr-code.svg' HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Cache-Control: no-cache Content-Disposition: inline; filename=""qr-code.svg"" Content-Length: 209510 Content-Type: image/svg+xml Etag: ""8ab857fb2ea442f951af494a746b1abf"" Last-Modified: Thu, 18 Aug 2022 15:40:15 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Fri, 19 Aug 2022 15:29:50 GMT  **Additional context** Filer will say `HTTP/1.1 304 Not Modified` with a value later than it's own `Last-Modified` but this will most likely never be the same in the case of a browser. I was testing via filer web ui in chrome.",source-file | source-file,filer modified since check fire exact describe bug filer return http modified since value exact last modfied header system setup weed server datacenter hostname hostname dir meta peers app app app dir data volume dir idx meta volume index leveldblarge volume max rack filer encryptvolumedata volume filesizelimitmb volumesizelimitmb filer defaultreplicaplacement filer savetofilerlimit filer ubuntu server expected behavior filer return modified save bandwidth screenshots bash system app curl head http modified since aug gmt http localhost svg http modified last modified aug gmt server filer date aug gmt system app curl head http modified since aug gmt http localhost svg http accept ranges bytes access control expose headers content disposition cache control cache content disposition inline filename svg content length content type image svg xml etag abf last modified aug gmt server filer date aug gmt additional context filer say http modified value later last modified likely never case browser testing via filer web chrome,bug,0.95,"Filer if-modified-since check doesn't fire on ""exact"" **Describe the bug** Filer does not return HTTP 304 if the `if-modified-since` value is exact the same as the `Last-Modfied` header **System Setup** - `weed server -dataCenter=$HOSTNAME -ip=$HOSTNAME -master.dir=/meta -master.peers=""app1:9333,app2:9333,app3:9333"" -dir=/data -volume.dir.idx=/meta -volume.index=leveldbLarge -volume.max=0 -rack=1 -filer.encryptVolumeData -volume.fileSizeLimitMB=4096 -master.volumeSizeLimitMB=1024 -filer.defaultReplicaPlacement=100 -filer.saveToFilerLimit=4096 -filer""` - Ubuntu Server **Expected behavior** Filer should return 304: Not modified to save bandwidth **Screenshots** bash system@app2:~$ curl --head --http1.1 -H 'if-modified-since: Thu, 18 Aug 2022 15:40:16 GMT' 'http://localhost:8888/qr-code.svg' HTTP/1.1 304 Not Modified Last-Modified: Thu, 18 Aug 2022 15:40:15 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Fri, 19 Aug 2022 15:29:42 GMT system@app2:~$ curl --head --http1.1 -H 'if-modified-since: Thu, 18 Aug 2022 15:40:15 GMT' 'http://localhost:8888/qr-code.svg' HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Cache-Control: no-cache Content-Disposition: inline; filename=""qr-code.svg"" Content-Length: 209510 Content-Type: image/svg+xml Etag: ""8ab857fb2ea442f951af494a746b1abf"" Last-Modified: Thu, 18 Aug 2022 15:40:15 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Fri, 19 Aug 2022 15:29:50 GMT  **Additional context** Filer will say `HTTP/1.1 304 Not Modified` with a value later than it's own `Last-Modified` but this will most likely never be the same in the case of a browser. I was testing via filer web ui in chrome."
3476,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/3476,HTTP Filer truncates uploads which begin with /etc,"The built-in http server in `filer` cannot handle uploads where the root of the path begins with `/etc`. The file is always truncated to `4MB`. Tested using `Docker version 20.10.12-ce, build 459d0dfbbb51` on `openSUSE Leap 15.3`. There is a `Makefile` and attached server log at the end of this post. Uploading files with `curl`  dd if=/dev/urandom of=rands bs=1k count=100k curl -F file=@rands http://10.1.0.8:8889/petc33333/etc3 {""name"":""rands"",""size"":104857600} curl -F file=@rands http://10.1.0.8:8889/etc33333/etc3 {""name"":""rands"",""size"":4194304} curl -F file=@rands http://10.1.0.8:8889/petc932 {""name"":""rands"",""size"":104857600} curl -F file=@rands http://10.1.0.8:8889/etc932 {""name"":""rands"",""size"":4194304}  Running a `HEAD` request against each uploaded file.  curl -I http://10.1.0.8:8889/petc33333/etc3 HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Content-Disposition: inline; filename=""etc3"" Content-Length: 104857600 Etag: ""3b3a28a324f125fb2423f4aa627c1e74"" Last-Modified: Sun, 21 Aug 2022 19:32:01 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Sun, 21 Aug 2022 19:32:04 GMT curl -I http://10.1.0.8:8889/etc33333/etc3 HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Content-Disposition: inline; filename=""etc3"" Content-Length: 4194304 Etag: ""1d65cef5475887ca24f320e9d7cb5d9f"" Last-Modified: Sun, 21 Aug 2022 19:32:02 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Sun, 21 Aug 2022 19:32:04 GMT curl -I http://10.1.0.8:8889/petc932 HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Content-Disposition: inline; filename=""petc932"" Content-Length: 104857600 Etag: ""3b3a28a324f125fb2423f4aa627c1e74"" Last-Modified: Sun, 21 Aug 2022 19:32:03 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Sun, 21 Aug 2022 19:32:04 GMT curl -I http://10.1.0.8:8889/etc932 HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Content-Disposition: inline; filename=""etc932"" Content-Length: 4194304 Etag: ""1d65cef5475887ca24f320e9d7cb5d9f"" Last-Modified: Sun, 21 Aug 2022 19:32:03 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Sun, 21 Aug 2022 19:32:04 GMT  Makefile to replicate the behavior makefile .PHONY: example upload heads dev-filer HOST:=http://10.1.0.8:8889 dev-filer: docker run -it --rm --name $@ --entrypoint /usr/bin/weed \ -p 8889:8889 -p 6062:6062 \ chrislusf/seaweedfs:3.22 \ -v 2 server -volume.max=5000 -dir=""/data"" -master.volumeSizeLimitMB=2048 -master.electionTimeout 1s -master.port=9444 -volume.port=9445 \ -filer -ip.bind 0.0.0.0 -filer.port=8889 -debug -debug.port 6062 rands: dd if=/dev/urandom of=rands bs=1k count=100k example: upload heads upload: rands curl -F file=@rands ${HOST}/petc33333/etc3 @echo curl -F file=@rands ${HOST}/etc33333/etc3 @echo curl -F file=@rands ${HOST}/petc932 @echo curl -F file=@rands ${HOST}/etc932 @echo @echo heads: curl -I ${HOST}/petc33333/etc3 curl -I ${HOST}/etc33333/etc3 curl -I ${HOST}/petc932 curl -I ${HOST}/etc932  [log.txt](https://github.com/seaweedfs/seaweedfs/files/9389838/log.txt)",source-file | source-file,http filer truncates uploads begin etc built http server filer cannot handle uploads root path begins etc always truncated tested docker dfbbb opensuse leap makefile attached server log end post uploading files curl urandom rands count curl rands http petc etc name rands size curl rands http etc etc name rands size curl rands http petc name rands size curl rands http etc name rands size running head uploaded curl http petc etc http accept ranges bytes access control expose headers content disposition content disposition inline filename etc content length etag last modified aug gmt server filer date aug gmt curl http etc etc http accept ranges bytes access control expose headers content disposition content disposition inline filename etc content length etag cef last modified aug gmt server filer date aug gmt curl http petc http accept ranges bytes access control expose headers content disposition content disposition inline filename petc content length etag last modified aug gmt server filer date aug gmt curl http etc http accept ranges bytes access control expose headers content disposition content disposition inline filename etc content length etag cef last modified aug gmt server filer date aug gmt makefile replicate behavior makefile phony example upload heads filer host http filer docker name entrypoint weed chrislusf server volume max dir data volumesizelimitmb electiontimeout port volume port filer bind filer port port rands urandom rands count example upload heads upload rands curl rands host petc etc echo curl rands host etc etc echo curl rands host petc echo curl rands host etc echo echo heads curl host petc etc curl host etc etc curl host petc curl host etc log txt https github files log txt,bug,0.95,"HTTP Filer truncates uploads which begin with /etc The built-in http server in `filer` cannot handle uploads where the root of the path begins with `/etc`. The file is always truncated to `4MB`. Tested using `Docker version 20.10.12-ce, build 459d0dfbbb51` on `openSUSE Leap 15.3`. There is a `Makefile` and attached server log at the end of this post. Uploading files with `curl`  dd if=/dev/urandom of=rands bs=1k count=100k curl -F file=@rands http://10.1.0.8:8889/petc33333/etc3 {""name"":""rands"",""size"":104857600} curl -F file=@rands http://10.1.0.8:8889/etc33333/etc3 {""name"":""rands"",""size"":4194304} curl -F file=@rands http://10.1.0.8:8889/petc932 {""name"":""rands"",""size"":104857600} curl -F file=@rands http://10.1.0.8:8889/etc932 {""name"":""rands"",""size"":4194304}  Running a `HEAD` request against each uploaded file.  curl -I http://10.1.0.8:8889/petc33333/etc3 HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Content-Disposition: inline; filename=""etc3"" Content-Length: 104857600 Etag: ""3b3a28a324f125fb2423f4aa627c1e74"" Last-Modified: Sun, 21 Aug 2022 19:32:01 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Sun, 21 Aug 2022 19:32:04 GMT curl -I http://10.1.0.8:8889/etc33333/etc3 HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Content-Disposition: inline; filename=""etc3"" Content-Length: 4194304 Etag: ""1d65cef5475887ca24f320e9d7cb5d9f"" Last-Modified: Sun, 21 Aug 2022 19:32:02 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Sun, 21 Aug 2022 19:32:04 GMT curl -I http://10.1.0.8:8889/petc932 HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Content-Disposition: inline; filename=""petc932"" Content-Length: 104857600 Etag: ""3b3a28a324f125fb2423f4aa627c1e74"" Last-Modified: Sun, 21 Aug 2022 19:32:03 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Sun, 21 Aug 2022 19:32:04 GMT curl -I http://10.1.0.8:8889/etc932 HTTP/1.1 200 OK Accept-Ranges: bytes Access-Control-Expose-Headers: Content-Disposition Content-Disposition: inline; filename=""etc932"" Content-Length: 4194304 Etag: ""1d65cef5475887ca24f320e9d7cb5d9f"" Last-Modified: Sun, 21 Aug 2022 19:32:03 GMT Server: SeaweedFS Filer 30GB 3.22 Date: Sun, 21 Aug 2022 19:32:04 GMT  Makefile to replicate the behavior makefile .PHONY: example upload heads dev-filer HOST:=http://10.1.0.8:8889 dev-filer: docker run -it --rm --name $@ --entrypoint /usr/bin/weed \ -p 8889:8889 -p 6062:6062 \ chrislusf/seaweedfs:3.22 \ -v 2 server -volume.max=5000 -dir=""/data"" -master.volumeSizeLimitMB=2048 -master.electionTimeout 1s -master.port=9444 -volume.port=9445 \ -filer -ip.bind 0.0.0.0 -filer.port=8889 -debug -debug.port 6062 rands: dd if=/dev/urandom of=rands bs=1k count=100k example: upload heads upload: rands curl -F file=@rands ${HOST}/petc33333/etc3 @echo curl -F file=@rands ${HOST}/etc33333/etc3 @echo curl -F file=@rands ${HOST}/petc932 @echo curl -F file=@rands ${HOST}/etc932 @echo @echo heads: curl -I ${HOST}/petc33333/etc3 curl -I ${HOST}/etc33333/etc3 curl -I ${HOST}/petc932 curl -I ${HOST}/etc932  [log.txt](https://github.com/seaweedfs/seaweedfs/files/9389838/log.txt)"
4088,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/4088,It is not possible to request a file via master if the volume is in a read-only state,"I have one master server and one volume server, I have moved one of the volumes to the read-only state. The server has lost information about volumes that are in the read-only state. The file can be requested only from the volume server. Procedure of actions: 1) Uploading a file 2) I make the volume read-only  lock volume.marknode 172.16.202.6:8080 -volumeId 3 -readonly unlock  3) Trying to upload a file  wget 172.16.202.6:9333/3,013c6a774d --2022-12-27 11:52:18-- http://172.16.202.6:9333/3,013c6a774d Connecting to 172.16.202.6:9333 connected. HTTP request sent, awaiting response 404 Not Found 2022-12-27 11:52:18 ERROR 404: Not Found.  **System Setup** ./weed master -ip=172.16.202.6 ./weed -v=4 volume -index=leveldb -pprof=true -max=100 -mserver=""172.16.202.6:9333"" -port=8080 -dir=/storage - ubuntu 22/04 - The problem was found on version 3.37, 3.36, 3.28, the latest version on which it works is 3.26 - version 3.27 have problem with set read-only from shell  volume.mark -node 172.16.202.6:8080 -volumeId 3 -readonly error: rpc error: code = Unknown desc = grpc VolumeMarkReadonly with master: 172.16.202.6:9333%!(EXTRA *errors.errorString=set volume 3 to read only on master: rpc error: code = Unimplemented desc = method VolumeMarkReadonly not implemented)  **Expected behavior** I expect that the master will allow requesting files from volumes that have moved to the read-only state.",source-file | source-file,possible via volume read state one server one volume server moved one volumes read state server lost information volumes read state requested volume server procedure actions uploading make volume read lock volume marknode volumeid readonly unlock trying upload wget http connecting connected http sent awaiting response found found system setup weed weed volume index leveldb pprof true max mserver port dir storage ubuntu found latest works set read shell volume mark node volumeid readonly rpc unknown desc grpc volumemarkreadonly extra errors errorstring set volume read rpc unimplemented desc method volumemarkreadonly implemented expected behavior expect allow requesting files volumes moved read state,bug,0.9,"It is not possible to request a file via master if the volume is in a read-only state I have one master server and one volume server, I have moved one of the volumes to the read-only state. The server has lost information about volumes that are in the read-only state. The file can be requested only from the volume server. Procedure of actions: 1) Uploading a file 2) I make the volume read-only  lock volume.marknode 172.16.202.6:8080 -volumeId 3 -readonly unlock  3) Trying to upload a file  wget 172.16.202.6:9333/3,013c6a774d --2022-12-27 11:52:18-- http://172.16.202.6:9333/3,013c6a774d Connecting to 172.16.202.6:9333 connected. HTTP request sent, awaiting response 404 Not Found 2022-12-27 11:52:18 ERROR 404: Not Found.  **System Setup** ./weed master -ip=172.16.202.6 ./weed -v=4 volume -index=leveldb -pprof=true -max=100 -mserver=""172.16.202.6:9333"" -port=8080 -dir=/storage - ubuntu 22/04 - The problem was found on version 3.37, 3.36, 3.28, the latest version on which it works is 3.26 - version 3.27 have problem with set read-only from shell  volume.mark -node 172.16.202.6:8080 -volumeId 3 -readonly error: rpc error: code = Unknown desc = grpc VolumeMarkReadonly with master: 172.16.202.6:9333%!(EXTRA *errors.errorString=set volume 3 to read only on master: rpc error: code = Unimplemented desc = method VolumeMarkReadonly not implemented)  **Expected behavior** I expect that the master will allow requesting files from volumes that have moved to the read-only state."
2387,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/2387,[s3] FAILED tests," 2.72    FAIL: s3tests_boto3.functional.test_s3.test_copy_object_ifnonematch_failed  Traceback (most recent call last): File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest self.test(*self.arg) File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 13028, in test_copy_object_ifnonematch_failed eq(body, 'bar') AssertionError: 'bar\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00' != 'bar'    FAIL: s3tests_boto3.functional.test_s3.test_object_write_check_etag  Traceback (most recent call last): File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest self.test(*self.arg) File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 2094, in test_object_write_check_etag eq(response['ETag'], '""37b51d194a7513e45b56f6524f2d51f2""') AssertionError: '""9997a36a88a451df9f4c4552f8e884fa""' != '""37b51d194a7513e45b56f6524f2d51f2""' >> raise AssertionError(None or ""%r != %r"" % ('""9997a36a88a451df9f4c4552f8e884fa""', '""37b51d194a7513e45b56f6524f2d51f2""'))    FAIL: s3tests_boto3.functional.test_s3.test_object_head_zero_bytes  Traceback (most recent call last): File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest self.test(*self.arg) File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 2083, in test_object_head_zero_bytes eq(response['ContentLength'], 0) AssertionError: 512 != 0 >> raise AssertionError(None or ""%r != %r"" % (512, 0)) ",source-file,failed tests fail tests boto functional copy object ifnonematch failed traceback recent call last opt tests virtualenv python site packages nose case runtest self self arg opt tests tests boto functional copy object ifnonematch failed body bar assertionerror bar bar fail tests boto functional object write check etag traceback recent call last opt tests virtualenv python site packages nose case runtest self self arg opt tests tests boto functional object write check etag response etag assertionerror raise assertionerror none fail tests boto functional object head zero bytes traceback recent call last opt tests virtualenv python site packages nose case runtest self self arg opt tests tests boto functional object head zero bytes response contentlength assertionerror raise assertionerror none,bug,0.9,"[s3] FAILED tests  2.72    FAIL: s3tests_boto3.functional.test_s3.test_copy_object_ifnonematch_failed  Traceback (most recent call last): File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest self.test(*self.arg) File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 13028, in test_copy_object_ifnonematch_failed eq(body, 'bar') AssertionError: 'bar\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00' != 'bar'    FAIL: s3tests_boto3.functional.test_s3.test_object_write_check_etag  Traceback (most recent call last): File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest self.test(*self.arg) File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 2094, in test_object_write_check_etag eq(response['ETag'], '""37b51d194a7513e45b56f6524f2d51f2""') AssertionError: '""9997a36a88a451df9f4c4552f8e884fa""' != '""37b51d194a7513e45b56f6524f2d51f2""' >> raise AssertionError(None or ""%r != %r"" % ('""9997a36a88a451df9f4c4552f8e884fa""', '""37b51d194a7513e45b56f6524f2d51f2""'))    FAIL: s3tests_boto3.functional.test_s3.test_object_head_zero_bytes  Traceback (most recent call last): File ""/opt/s3-tests/virtualenv/lib/python3.8/site-packages/nose/case.py"", line 198, in runTest self.test(*self.arg) File ""/opt/s3-tests/s3tests_boto3/functional/test_s3.py"", line 2083, in test_object_head_zero_bytes eq(response['ContentLength'], 0) AssertionError: 512 != 0 >> raise AssertionError(None or ""%r != %r"" % (512, 0)) "
6262,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/6262,Multipart upload without S3 authentication fails with the wrong message,"**Describe the bug** If S3 authentication is not setup, attempting to upload multipart will fail with the error message: Expected hash not equal to calculated hash While trying to make a minimal reproduction of this behavior to report the bug, I noticed that when using the amazon SDK `TransferUtility`, I get a different error message: Signed request requires setting up SeaweedFS S3 authentication This led me to find the otherwise completely opaque issue at hand. **System Setup** `podman run --name seaweed-12312 --rm -p 12312:12312 docker.io/chrislusf/seaweedfs server -s3 -s3.port 12312` To run seaweed. Then in C#: cs var s3Client = new AmazonS3Client( new AmazonS3Config { ServiceURL = ""http://localhost:12312"", ForcePathStyle = true, } ); var bucketName = ""my-test-bucket""; var keyName = ""dir/file.txt""; var fakeFileData = new StringBuilder(); fakeFileData.AppendLine(""a,b,c,d,e""); for (var i = 0; i < 10000; i++) { fakeFileData.AppendLine($""a{i},b{i},c{i},d{i},e{i}""); } var cancellationToken = CancellationToken.None; var initiateRequest = new InitiateMultipartUploadRequest { BucketName = bucketName, Key = keyName, }; var initiateResponse = await s3Client.InitiateMultipartUploadAsync(initiateRequest, cancellationToken); var partETags = new List<PartETag>(); using (var fakeFileStream = new MemoryStream(Encoding.UTF8.GetBytes(fakeFileData.ToString( { var uploadRequest = new UploadPartRequest { BucketName = bucketName, Key = keyName, UploadId = initiateResponse.UploadId, PartNumber = 1, PartSize = fakeFileStream.Length, InputStream = fakeFileStream }; var uploadResponse = await s3Client.UploadPartAsync(uploadRequest, cancellationToken); partETags.Add(new PartETag(uploadResponse.PartNumber, uploadResponse.ETag)); } var completeRequest = new CompleteMultipartUploadRequest { BucketName = bucketName, Key = keyName, UploadId = initiateResponse.UploadId, PartETags = partETags }; await s3Client.CompleteMultipartUploadAsync(completeRequest, cancellationToken);  The above fails with `Expected hash not equal to calculated hash`. However, running the code below: cs var s3Client = new AmazonS3Client( new AmazonS3Config { ServiceURL = ""http://localhost:12312"", ForcePathStyle = true, } ); var bucketName = ""my-test-bucket""; var keyName = ""dir/file.txt""; var fakeFileData = new StringBuilder(); fakeFileData.AppendLine(""a,b,c,d,e""); for (var i = 0; i < 10000; i++) { fakeFileData.AppendLine($""a{i},b{i},c{i},d{i},e{i}""); } var fileTransferUtility = new TransferUtility(s3Client); using (var fakeFileStream = new MemoryStream(Encoding.UTF8.GetBytes(fakeFileData.ToString( { var fileTransferUtilityRequest = new TransferUtilityUploadRequest { BucketName = bucketName, InputStream = fakeFileStream, PartSize = 6*1024*1024, // 6 MB. Key = keyName, // DisablePayloadSigning = true, }; await fileTransferUtility.UploadAsync(fileTransferUtilityRequest); }  This fails with the much more useful `Signed request requires setting up SeaweedFS S3 authentication`. It's important to note, that if I setup seaweed using: `podman run --name seaweed-12312 -v /home/$USER/s3_config.json:/app/:Z --rm -p 12312:12312 docker.io/chrislusf/seaweedfs server -s3 -s3.port 12312 -s3.config ""/app/s3_config.json""` While filling `~/s3_config.json` with:  { ""identities"": [ { ""name"": ""me"", ""credentials"": [ { ""accessKey"": ""seaweed"", ""secretKey"": ""seaweed"" } ], ""actions"": [ ""Read"", ""Write"", ""List"", ""Tagging"", ""Admin"" ] } ] }  And then setting up the client accordingly: cs var s3Client = new AmazonS3Client( new BasicAWSCredentials(""seaweed"", ""seaweed""), new AmazonS3Config { ServiceURL = ""http://localhost:12312"", ForcePathStyle = true, } );  Both examples work. **Expected behavior** I'd like for the simple case to fail with an informative error message, instead of giving a hash equality message which implies some usage bug. I'm glad I could find the issue eventually, but I could've saved a lot of time with the right error message!",source-file,multipart upload without authentication fails wrong message describe bug authentication setup attempting upload multipart fail message expected hash equal calculated hash trying make minimal reproduction behavior report bug noticed amazon sdk transferutility get different message signed requires setting authentication led find otherwise completely opaque hand system setup name seaweed docker chrislusf server port seaweed var client new amazons client new amazons config serviceurl http localhost forcepathstyle true var bucketname bucket var keyname dir txt var fakefiledata new stringbuilder fakefiledata appendline var fakefiledata appendline var cancellationtoken cancellationtoken none var initiaterequest new initiatemultipartuploadrequest bucketname bucketname key keyname var initiateresponse await client initiatemultipartuploadasync initiaterequest cancellationtoken var partetags new list partetag var fakefilestream new memorystream encoding utf getbytes fakefiledata tostring var uploadrequest new uploadpartrequest bucketname bucketname key keyname uploadid initiateresponse uploadid partnumber partsize fakefilestream length inputstream fakefilestream var uploadresponse await client uploadpartasync uploadrequest cancellationtoken partetags new partetag uploadresponse partnumber uploadresponse etag var completerequest new completemultipartuploadrequest bucketname bucketname key keyname uploadid initiateresponse uploadid partetags partetags await client completemultipartuploadasync completerequest cancellationtoken fails expected hash equal calculated hash however running var client new amazons client new amazons config serviceurl http localhost forcepathstyle true var bucketname bucket var keyname dir txt var fakefiledata new stringbuilder fakefiledata appendline var fakefiledata appendline var filetransferutility new transferutility client var fakefilestream new memorystream encoding utf getbytes fakefiledata tostring var filetransferutilityrequest new transferutilityuploadrequest bucketname bucketname inputstream fakefilestream partsize key keyname disablepayloadsigning true await filetransferutility uploadasync filetransferutilityrequest fails much useful signed requires setting authentication important note setup seaweed name seaweed home user config json app docker chrislusf server port config app config json filling config json identities name credentials accesskey seaweed secretkey seaweed actions read write list tagging admin setting client accordingly var client new amazons client new basicawscredentials seaweed seaweed new amazons config serviceurl http localhost forcepathstyle true examples work expected behavior like simple case fail informative message instead giving hash equality message implies usage bug glad could find eventually could saved lot time right message,bug,0.9,"Multipart upload without S3 authentication fails with the wrong message **Describe the bug** If S3 authentication is not setup, attempting to upload multipart will fail with the error message: Expected hash not equal to calculated hash While trying to make a minimal reproduction of this behavior to report the bug, I noticed that when using the amazon SDK `TransferUtility`, I get a different error message: Signed request requires setting up SeaweedFS S3 authentication This led me to find the otherwise completely opaque issue at hand. **System Setup** `podman run --name seaweed-12312 --rm -p 12312:12312 docker.io/chrislusf/seaweedfs server -s3 -s3.port 12312` To run seaweed. Then in C#: cs var s3Client = new AmazonS3Client( new AmazonS3Config { ServiceURL = ""http://localhost:12312"", ForcePathStyle = true, } ); var bucketName = ""my-test-bucket""; var keyName = ""dir/file.txt""; var fakeFileData = new StringBuilder(); fakeFileData.AppendLine(""a,b,c,d,e""); for (var i = 0; i < 10000; i++) { fakeFileData.AppendLine($""a{i},b{i},c{i},d{i},e{i}""); } var cancellationToken = CancellationToken.None; var initiateRequest = new InitiateMultipartUploadRequest { BucketName = bucketName, Key = keyName, }; var initiateResponse = await s3Client.InitiateMultipartUploadAsync(initiateRequest, cancellationToken); var partETags = new List<PartETag>(); using (var fakeFileStream = new MemoryStream(Encoding.UTF8.GetBytes(fakeFileData.ToString( { var uploadRequest = new UploadPartRequest { BucketName = bucketName, Key = keyName, UploadId = initiateResponse.UploadId, PartNumber = 1, PartSize = fakeFileStream.Length, InputStream = fakeFileStream }; var uploadResponse = await s3Client.UploadPartAsync(uploadRequest, cancellationToken); partETags.Add(new PartETag(uploadResponse.PartNumber, uploadResponse.ETag)); } var completeRequest = new CompleteMultipartUploadRequest { BucketName = bucketName, Key = keyName, UploadId = initiateResponse.UploadId, PartETags = partETags }; await s3Client.CompleteMultipartUploadAsync(completeRequest, cancellationToken);  The above fails with `Expected hash not equal to calculated hash`. However, running the code below: cs var s3Client = new AmazonS3Client( new AmazonS3Config { ServiceURL = ""http://localhost:12312"", ForcePathStyle = true, } ); var bucketName = ""my-test-bucket""; var keyName = ""dir/file.txt""; var fakeFileData = new StringBuilder(); fakeFileData.AppendLine(""a,b,c,d,e""); for (var i = 0; i < 10000; i++) { fakeFileData.AppendLine($""a{i},b{i},c{i},d{i},e{i}""); } var fileTransferUtility = new TransferUtility(s3Client); using (var fakeFileStream = new MemoryStream(Encoding.UTF8.GetBytes(fakeFileData.ToString( { var fileTransferUtilityRequest = new TransferUtilityUploadRequest { BucketName = bucketName, InputStream = fakeFileStream, PartSize = 6*1024*1024, // 6 MB. Key = keyName, // DisablePayloadSigning = true, }; await fileTransferUtility.UploadAsync(fileTransferUtilityRequest); }  This fails with the much more useful `Signed request requires setting up SeaweedFS S3 authentication`. It's important to note, that if I setup seaweed using: `podman run --name seaweed-12312 -v /home/$USER/s3_config.json:/app/:Z --rm -p 12312:12312 docker.io/chrislusf/seaweedfs server -s3 -s3.port 12312 -s3.config ""/app/s3_config.json""` While filling `~/s3_config.json` with:  { ""identities"": [ { ""name"": ""me"", ""credentials"": [ { ""accessKey"": ""seaweed"", ""secretKey"": ""seaweed"" } ], ""actions"": [ ""Read"", ""Write"", ""List"", ""Tagging"", ""Admin"" ] } ] }  And then setting up the client accordingly: cs var s3Client = new AmazonS3Client( new BasicAWSCredentials(""seaweed"", ""seaweed""), new AmazonS3Config { ServiceURL = ""http://localhost:12312"", ForcePathStyle = true, } );  Both examples work. **Expected behavior** I'd like for the simple case to fail with an informative error message, instead of giving a hash equality message which implies some usage bug. I'm glad I could find the issue eventually, but I could've saved a lot of time with the right error message!"
6576,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/6576,Seaweed does not handle x-id,AWS SDK's emit an x-id query parameter which is not supported by seaweed leading to issues handling presigned requests,source-file,seaweed handle aws sdk emit query parameter supported seaweed leading issues handling presigned requests,bug,0.85,Seaweed does not handle x-id AWS SDK's emit an x-id query parameter which is not supported by seaweed leading to issues handling presigned requests
4305,seaweedfs,https://github.com/seaweedfs/seaweedfs/issues/4305,s3.bucket.list file count mismatch,"**Describe the bug** s3.bucket.list file count is not correct **System Setup** - weed server -s3 -ip.bind=0.0.0.0 -dir=/disk/data1/weed,/disk/data2/weed -master.defaultReplication=002 -master.peers=192.168.3.101:9333,192.168.3.102:9333,192.168.3.103:9333,192.168.3.104:9333,192.168.3.105:9333 - Ubuntu 22.04.1 LTS - version 30GB 3.43 3227e4175e2bf8df2ac8aeeff8cf73a819abc5a7 linux amd64 - content of `filer.toml`  [leveldb2] enabled = false dir = ""./filerldb2"" # directory to store level db files [tikv] enabled = true pdaddrs = ""pdhost1:2379, pdhost2:2379, pdhost3:2379""  **Expected behavior**  > fs.ls /buckets/kuro3 prometheus.yml redirect.html weed > s3.bucket.list kuro3 size:67565992 file:22  file count should be 3 **Screenshots** ![image](https://user-images.githubusercontent.com/12230174/224946805-9e45e296-35b9-4234-8015-61efb289116b.png) ![image](https://user-images.githubusercontent.com/12230174/224947463-8ca7b206-b065-4d5c-b62a-7adb847caa22.png) **Additional context** It seems that filer slices files into segments of 4MB, and `s3.bucket.list` counts each segment as a separate file.",source-file,bucket list count mismatch describe bug bucket list count correct system setup weed server bind dir disk data weed disk data weed defaultreplication peers ubuntu lts aeeff abc linux amd content filer toml leveldb enabled false dir filerldb directory store level files tikv enabled true pdaddrs pdhost pdhost pdhost expected behavior buckets kuro prometheus yml redirect html weed bucket list kuro size count screenshots image https user images githubusercontent efb png image https user images githubusercontent adb caa png additional context seems filer slices files segments bucket list counts segment separate,bug,0.9,"s3.bucket.list file count mismatch **Describe the bug** s3.bucket.list file count is not correct **System Setup** - weed server -s3 -ip.bind=0.0.0.0 -dir=/disk/data1/weed,/disk/data2/weed -master.defaultReplication=002 -master.peers=192.168.3.101:9333,192.168.3.102:9333,192.168.3.103:9333,192.168.3.104:9333,192.168.3.105:9333 - Ubuntu 22.04.1 LTS - version 30GB 3.43 3227e4175e2bf8df2ac8aeeff8cf73a819abc5a7 linux amd64 - content of `filer.toml`  [leveldb2] enabled = false dir = ""./filerldb2"" # directory to store level db files [tikv] enabled = true pdaddrs = ""pdhost1:2379, pdhost2:2379, pdhost3:2379""  **Expected behavior**  > fs.ls /buckets/kuro3 prometheus.yml redirect.html weed > s3.bucket.list kuro3 size:67565992 file:22  file count should be 3 **Screenshots** ![image](https://user-images.githubusercontent.com/12230174/224946805-9e45e296-35b9-4234-8015-61efb289116b.png) ![image](https://user-images.githubusercontent.com/12230174/224947463-8ca7b206-b065-4d5c-b62a-7adb847caa22.png) **Additional context** It seems that filer slices files into segments of 4MB, and `s3.bucket.list` counts each segment as a separate file."
