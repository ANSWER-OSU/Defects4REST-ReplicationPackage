issue_no,repo,issue_url,title,description,patched_file_types,text_for_topic_modeling,prediction,confidence,combined_text
65,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/65,Consumer read request hangs if you try to create two consumer instances with the same ID,"If you: 1. Create consumer with an ID 2. Read from a topic 3. Create consumer with same ID (don't delete previous one) 4. Read from topic the second create call succeeds and the read hangs. The stack trace shows it stuck trying to create a zookeeper node in a method that can also handle conflicts, which we'd expect since the IDs are the same. Turns out that we ended up conflating the meaning of ""ID"", which is used both to name the consumer nicely so you have readable consumer URLs and it got passed in as the ID for the underlying old Kafka consumer. The latter should really only be happening for debugging purposes since a random UUID (generated automatically for the consumer if the ID is omitted) is way more reliable. Things get a bit more confusing when we consider moving to the new consumer, which has a separate client ID that it uses to make logging on the brokers clearer and traceable to specific applications. A fix for this has to be careful about compatibility. I think the right thing to do is add a new parameter (""name"") which will be used in the URL. If the ""id"" is specified and ""name"" isn't, ""name"" will default to ""id"". The underlying Kafka ""consumer.id"" will only be set if ""id"" was explicitly set. Then, the name is only relevant to the single instance of the REST proxy since consumer URLs are specific to a single process. We can return a 409 if we see a naming conflict during creation, which will prevent the subsequent hang during read. Finally, we'll want to deprecate and deemphasize the ""id"" field in the docs since ""name"" will now be the field they really should be using.",documentation-file | documentation-file | documentation-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file,consumer read hangs create two consumer instances create consumer read topic create consumer delete previous one read topic second create call succeeds read hangs stack trace shows stuck trying create zookeeper node method also handle conflicts expect since ids turns ended conflating meaning used name consumer nicely readable consumer urls got passed underlying old kafka consumer latter really happening debugging purposes since random uuid generated automatically consumer omitted way reliable things get bit confusing consider moving new consumer separate client uses make logging brokers clearer traceable specific applications careful compatibility think right thing new parameter name used url specified name name default underlying kafka consumer set explicitly set name relevant single instance rest proxy since consumer urls specific single process return see naming conflict creation prevent subsequent hang read finally want deprecate deemphasize field docs since name field really,bug,0.9,"Consumer read request hangs if you try to create two consumer instances with the same ID If you: 1. Create consumer with an ID 2. Read from a topic 3. Create consumer with same ID (don't delete previous one) 4. Read from topic the second create call succeeds and the read hangs. The stack trace shows it stuck trying to create a zookeeper node in a method that can also handle conflicts, which we'd expect since the IDs are the same. Turns out that we ended up conflating the meaning of ""ID"", which is used both to name the consumer nicely so you have readable consumer URLs and it got passed in as the ID for the underlying old Kafka consumer. The latter should really only be happening for debugging purposes since a random UUID (generated automatically for the consumer if the ID is omitted) is way more reliable. Things get a bit more confusing when we consider moving to the new consumer, which has a separate client ID that it uses to make logging on the brokers clearer and traceable to specific applications. A fix for this has to be careful about compatibility. I think the right thing to do is add a new parameter (""name"") which will be used in the URL. If the ""id"" is specified and ""name"" isn't, ""name"" will default to ""id"". The underlying Kafka ""consumer.id"" will only be set if ""id"" was explicitly set. Then, the name is only relevant to the single instance of the REST proxy since consumer URLs are specific to a single process. We can return a 409 if we see a naming conflict during creation, which will prevent the subsequent hang during read. Finally, we'll want to deprecate and deemphasize the ""id"" field in the docs since ""name"" will now be the field they really should be using."
118,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/118,Not possible to send keyed Avro messages via Proxy,"We tried different ways, like curl or [kafka-node-rest](https://github.com/confluentinc/kafka-rest-node), to send a Avro message to a topic with a key. The topic is created and schema submitted to schema registry, however, sending the message fails with a HTTP error code 500. Setting like ports etc. are all defaults for the platform. Here an example what we tried with kafka-node-rest: - create the schemata for key and value  javascript var idSchema = new KafkaRest.AvroSchema('string'); var valueSchema = new KafkaRest.AvroSchema( { 'namespace': 'mynamespace', 'type': 'record', 'name': 'myschema', 'fields': [ { 'name': 'key', 'type': 'string' }, { 'name': 'date', 'type': 'string' }, { 'name': 'title', 'type': 'string' } ] });  - send it  javascript topic.produce(idSchema, valueSchema, { 'key': link, 'value': { 'key': link, 'date': date, 'title': title } }, function(err, res) {  });  what throws an error.  javascript { name: 'APIError', status: 500, message: 'Internal Server Error', data: { error_code: 500, message: 'Internal Server Error' } }  - sending without a key works  javascript topic.produce(valueSchema, { 'key': link, 'date': date, 'title': title }, function(err, res) {  });  result:  javascript { offsets: [ { partition: 0, offset: 750, error_code: null, error: null } ], key_schema_id: null, value_schema_id: 121 }  However, with the **kafka-avro-console-producer** works fine  shell ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic rss \ --property parse.key=true \ --property key.separator=, \ --property key.schema='{""type"":""string""}' \ --property value.schema=""{\""type\"": \""record\"", \""name\"": \""rss\"", \""fields\"": [{\""name\"": \""key\"", \""type\"": \""string\""}, {\""name\"": \""title\"", \""type\"": \""string\""}, {\""name\"": \""content\"", \""type\"": \""string\""}, {\""name\"": \""date\"", \""type\"": \""string\""}]}"" ""test"",{""key"": ""http://example.com"", ""title"": ""test title"", ""content"": ""test content"", ""date"": ""2015-10-18'T'20:20:20""}  Any idea what could be wrong? Maybe this issue relates to [#114](https://github.com/confluentinc/kafka-rest/issues/114). Thanks in advance.",source-file | test-file,possible send keyed avro messages via proxy tried different ways like curl kafka node rest https github confluentinc kafka rest node send avro message topic key topic created schema submitted schema registry however sending message fails http setting like ports etc defaults platform example tried kafka node rest create schemata key value javascript var idschema new kafkarest avroschema string var valueschema new kafkarest avroschema namespace mynamespace type record name myschema fields name key type string name date type string name title type string send javascript topic produce idschema valueschema key link value key link date date title title function err res throws javascript name apierror status message internal server data message internal server sending without key works javascript topic produce valueschema key link date date title title function err res result javascript offsets partition offset null null key schema null value schema however kafka avro console producer works fine shell kafka avro console producer broker list localhost topic rss property parse key true property key separator property key schema type string property value schema type record name rss fields name key type string name title type string name content type string name date type string key http example title title content content date idea could wrong maybe relates https github confluentinc kafka rest issues thanks advance,bug,0.9,"Not possible to send keyed Avro messages via Proxy We tried different ways, like curl or [kafka-node-rest](https://github.com/confluentinc/kafka-rest-node), to send a Avro message to a topic with a key. The topic is created and schema submitted to schema registry, however, sending the message fails with a HTTP error code 500. Setting like ports etc. are all defaults for the platform. Here an example what we tried with kafka-node-rest: - create the schemata for key and value  javascript var idSchema = new KafkaRest.AvroSchema('string'); var valueSchema = new KafkaRest.AvroSchema( { 'namespace': 'mynamespace', 'type': 'record', 'name': 'myschema', 'fields': [ { 'name': 'key', 'type': 'string' }, { 'name': 'date', 'type': 'string' }, { 'name': 'title', 'type': 'string' } ] });  - send it  javascript topic.produce(idSchema, valueSchema, { 'key': link, 'value': { 'key': link, 'date': date, 'title': title } }, function(err, res) {  });  what throws an error.  javascript { name: 'APIError', status: 500, message: 'Internal Server Error', data: { error_code: 500, message: 'Internal Server Error' } }  - sending without a key works  javascript topic.produce(valueSchema, { 'key': link, 'date': date, 'title': title }, function(err, res) {  });  result:  javascript { offsets: [ { partition: 0, offset: 750, error_code: null, error: null } ], key_schema_id: null, value_schema_id: 121 }  However, with the **kafka-avro-console-producer** works fine  shell ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic rss \ --property parse.key=true \ --property key.separator=, \ --property key.schema='{""type"":""string""}' \ --property value.schema=""{\""type\"": \""record\"", \""name\"": \""rss\"", \""fields\"": [{\""name\"": \""key\"", \""type\"": \""string\""}, {\""name\"": \""title\"", \""type\"": \""string\""}, {\""name\"": \""content\"", \""type\"": \""string\""}, {\""name\"": \""date\"", \""type\"": \""string\""}]}"" ""test"",{""key"": ""http://example.com"", ""title"": ""test title"", ""content"": ""test content"", ""date"": ""2015-10-18'T'20:20:20""}  Any idea what could be wrong? Maybe this issue relates to [#114](https://github.com/confluentinc/kafka-rest/issues/114). Thanks in advance."
37,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/37,Catch MessageStreamsExistException from ConsumerConnector.createMessageStreams,"When ConsumersResource.readTopic is invoked, it calls createMessageStreams if there isn't already a stream. However, if the consumer has already invoked the method for a different topic, it will throw a MessageStreamsExistException. Currently this isn't being caught, so it'll be turned into a 500. We should catch it and turn it into a more useful error code. This will also require updating the docs since it'll introduce a new error code.",documentation-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | documentation-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file,catch messagestreamsexistexception consumerconnector createmessagestreams consumersresource readtopic invoked calls createmessagestreams already stream however consumer already invoked method different topic throw messagestreamsexistexception currently caught turned catch turn useful also require updating docs since introduce new,bug,0.9,"Catch MessageStreamsExistException from ConsumerConnector.createMessageStreams When ConsumersResource.readTopic is invoked, it calls createMessageStreams if there isn't already a stream. However, if the consumer has already invoked the method for a different topic, it will throw a MessageStreamsExistException. Currently this isn't being caught, so it'll be turned into a 500. We should catch it and turn it into a more useful error code. This will also require updating the docs since it'll introduce a new error code."
475,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/475,Kafka Rest API incorrect message returned by GET request for specified offset when data is compressed,"Test on a compressed topic vs a non-compressed topic. Should fail on compressed topic. GET /topics/myTopic/partitions/0/messages?offset=10&count=1 Tested using this call on the rest api using docker image version 4.1.0 and 5.0.0 and re-created on both. When requesting an offset between 0-8 it returns offset 0, when requesting from 9-149 it returns 9, when requesting from 150-265 it returns 150 etc Performing a /topics/myTopic/partitions/0/messages?offset=0&count=1000 shows that we have no offset gaps from 0 to 1000 Compressed format: ByteBufferMessageSet( LegacyRecordBatch(offset=8, Record(magic=1, attributes=1, compression=GZIP, crc=3359863988, CreateTime=1537196426232, key=0 bytes, value=703 bytes)), LegacyRecordBatch(offset=149, Record(magic=1, attributes=1, compression=GZIP, crc=712877603, CreateTime=1537196426317, key=0 bytes, value=6199 bytes)), Regular format: ByteBufferMessageSet( LegacyRecordBatch(offset=1, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=2, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=3, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)),",source-file,kafka rest api incorrect message returned get specified offset data compressed compressed topic non compressed topic fail compressed topic get topics mytopic partitions messages offset count tested call rest api docker image created requesting offset returns offset requesting returns requesting returns etc performing topics mytopic partitions messages offset count shows offset gaps compressed format bytebuffermessageset legacyrecordbatch offset record magic attributes compression gzip crc createtime key bytes value bytes legacyrecordbatch offset record magic attributes compression gzip crc createtime key bytes value bytes regular format bytebuffermessageset legacyrecordbatch offset record magic attributes compression none crc createtime key bytes value bytes legacyrecordbatch offset record magic attributes compression none crc createtime key bytes value bytes legacyrecordbatch offset record magic attributes compression none crc createtime key bytes value bytes,bug,0.95,"Kafka Rest API incorrect message returned by GET request for specified offset when data is compressed Test on a compressed topic vs a non-compressed topic. Should fail on compressed topic. GET /topics/myTopic/partitions/0/messages?offset=10&count=1 Tested using this call on the rest api using docker image version 4.1.0 and 5.0.0 and re-created on both. When requesting an offset between 0-8 it returns offset 0, when requesting from 9-149 it returns 9, when requesting from 150-265 it returns 150 etc Performing a /topics/myTopic/partitions/0/messages?offset=0&count=1000 shows that we have no offset gaps from 0 to 1000 Compressed format: ByteBufferMessageSet( LegacyRecordBatch(offset=8, Record(magic=1, attributes=1, compression=GZIP, crc=3359863988, CreateTime=1537196426232, key=0 bytes, value=703 bytes)), LegacyRecordBatch(offset=149, Record(magic=1, attributes=1, compression=GZIP, crc=712877603, CreateTime=1537196426317, key=0 bytes, value=6199 bytes)), Regular format: ByteBufferMessageSet( LegacyRecordBatch(offset=1, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=2, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=3, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)),"
77,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/77,Get java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable when calling the /topics/:topic endpoint,"Whenever I call the topics/:topic endpoint I get the following error  java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable(Ljava/util/Collection;)Lscala/collection/Iterable; at io.confluent.kafkarest.MetadataObserver.getTopic(MetadataObserver.java:91) at io.confluent.kafkarest.resources.TopicsResource.getTopic(TopicsResource.java:70) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) (stack trace continues)  From some preliminary research it appear this could be due to differences between the compile time version of scala and the runtime. http://stackoverflow.com/questions/4393946/nosuchmethoderror-when-attempting-to-implicitly-convert-a-java-to-scala-collecti I'm running on Centos. Here's the output of java -version  java version ""1.8.0_05"" Java(TM) SE Runtime Environment (build 1.8.0_05-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)  I installed the package `confluent-platform-2.11.5`",other-file,get lang nosuchmethoderror scala collection javaconversions asscalaiterable calling topics topic endpoint whenever call topics topic endpoint get following lang nosuchmethoderror scala collection javaconversions asscalaiterable ljava util collection lscala collection iterable confluent kafkarest metadataobserver gettopic metadataobserver confluent kafkarest resources topicsresource gettopic topicsresource reflect nativemethodaccessorimpl invoke native method reflect nativemethodaccessorimpl invoke nativemethodaccessorimpl reflect delegatingmethodaccessorimpl invoke delegatingmethodaccessorimpl lang reflect method invoke method stack trace continues preliminary research appear could due differences compile time scala runtime http stackoverflow questions nosuchmethoderror attempting implicitly convert scala collecti running centos output runtime environment hotspot bit server mixed mode installed package confluent platform,bug,0.8,"Get java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable when calling the /topics/:topic endpoint Whenever I call the topics/:topic endpoint I get the following error  java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable(Ljava/util/Collection;)Lscala/collection/Iterable; at io.confluent.kafkarest.MetadataObserver.getTopic(MetadataObserver.java:91) at io.confluent.kafkarest.resources.TopicsResource.getTopic(TopicsResource.java:70) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) (stack trace continues)  From some preliminary research it appear this could be due to differences between the compile time version of scala and the runtime. http://stackoverflow.com/questions/4393946/nosuchmethoderror-when-attempting-to-implicitly-convert-a-java-to-scala-collecti I'm running on Centos. Here's the output of java -version  java version ""1.8.0_05"" Java(TM) SE Runtime Environment (build 1.8.0_05-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)  I installed the package `confluent-platform-2.11.5`"
341,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/341,Consumer read takes long time with multiple consumers in consumer group,"A fundamental issue is seen with REST Proxy. Suppose if we create two consumer instances under the same consumer group, read call is taking longer time with calls with subsequent instance. Steps to reproduce: 1. Create my_consumer_instance1 and my_consumer_instance2 under my_json_consumer1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance1"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance2"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 2. Subscribe both the instances to topic test1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/subscription curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/subscription 3. Read the message from my_consumer_instance1 several times and we will get responses back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records 4. Read the message from my_consumer_instance2 and the call will hang. It will take around 8 minutes to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/records 5. Again read the message from my_consumer_instance1 and the call will hang. It will take some time to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records",source-file | test-file | source-file | test-file | source-file | test-file,consumer read takes long time multiple consumers consumer group fundamental seen rest proxy suppose create two consumer instances consumer group read call taking longer time calls subsequent instance steps reproduce create consumer instance consumer instance json consumer curl content type application vnd kafka json name consumer instance format json auto offset reset earliest http localhost consumers json consumer curl content type application vnd kafka json name consumer instance format json auto offset reset earliest http localhost consumers json consumer subscribe instances topic curl content type application vnd kafka json topics http localhost consumers json consumer instances consumer instance subscription curl content type application vnd kafka json topics http localhost consumers json consumer instances consumer instance subscription read message consumer instance several times get responses back curl get accept application vnd kafka json json http localhost consumers json consumer instances consumer instance records read message consumer instance call hang take around minutes get response back curl get accept application vnd kafka json json http localhost consumers json consumer instances consumer instance records read message consumer instance call hang take time get response back curl get accept application vnd kafka json json http localhost consumers json consumer instances consumer instance records,bug,0.9,"Consumer read takes long time with multiple consumers in consumer group A fundamental issue is seen with REST Proxy. Suppose if we create two consumer instances under the same consumer group, read call is taking longer time with calls with subsequent instance. Steps to reproduce: 1. Create my_consumer_instance1 and my_consumer_instance2 under my_json_consumer1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance1"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance2"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 2. Subscribe both the instances to topic test1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/subscription curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/subscription 3. Read the message from my_consumer_instance1 several times and we will get responses back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records 4. Read the message from my_consumer_instance2 and the call will hang. It will take around 8 minutes to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/records 5. Again read the message from my_consumer_instance1 and the call will hang. It will take some time to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records"
