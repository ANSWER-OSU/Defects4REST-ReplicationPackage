issue_no,repo,issue_url,title,description,patched_file_types,text_for_topic_modeling,prediction,confidence
656,digdag,https://github.com/treasure-data/digdag/issues/656,Failed to update secret value when new-value is shorter than old-value.,"Prepared 2 script. **test.dig** yaml timezone: UTC +test_secret: _env: s: ${secret:secret_val} py>: tasks.secret_test.test  **tasks/secret_test.py** py import digdag import os def test(): print('secret_value:', os.environ.get('s'))  Set secret value and run workflow. sh $ digdag secrets --local --set secret_val=12345 $ digdag run test.dig --rerun  output:  secret_value: 12345  Reset secret value and rerun workflow. sh $ digdag secrets --local --set secret_val=67 $ digdag run test.dig --rerun  output:  secret_value: 67345  Failed to update value 12345 -> 67. FYI:Success to update secret value after delete old-value.",source-file | test-file,"Failed to update secret value when new-value is shorter than old-value. Prepared 2 script. **test.dig** yaml timezone: UTC +test_secret: _env: s: ${secret:secret_val} py>: tasks.secret_test.test  **tasks/secret_test.py** py import digdag import os def test(): print('secret_value:', os.environ.get('s'))  Set secret value and run workflow. sh $ digdag secrets --local --set secret_val=12345 $ digdag run test.dig --rerun  output:  secret_value: 12345  Reset secret value and rerun workflow. sh $ digdag secrets --local --set secret_val=67 $ digdag run test.dig --rerun  output:  secret_value: 67345  Failed to update value 12345 -> 67. FYI:Success to update secret value after delete old-value. source-file test-file",no-bug,0.9
93,digdag,https://github.com/treasure-data/digdag/issues/93,server side workflow log filtering,"It would be useful for users to be able to filter logs from workflows executed on the server. Currently all output from workflow and its tasks, both stdout/err as well as java log messages are concatenated into a single log file. Thus it's not easy to reliably filter out only e.g. stdout or on java logging level. I propose changing the digdag log file format to be a structured stream of messages (e.g. msgpack) that combines the raw log message with metadata like timestamp and whether the message is from stdout/err or java logging and then the logging level. I.e. something like:  json //  { ""ts"": 1463387968, ""stream"": ""OUT"", ""message"": ""hello world"", }, { ""ts"": 1463388012, ""stream"": ""LOG"", ""level"": ""WARN"", ""message"": ""the foo failed to bar the baz because quux"", }, { ""ts"": 1463388153, ""stream"": ""ERR"", ""message"": ""The quick brown fox jumps over the lazy dog"", }, { ""ts"": 1463388265, ""stream"": ""LOG"", ""level"": ""DEBUG"", ""message"": """", }, //   We could then have server the expose a rest api endpoint for querying this stream or have the client fetch all log entries and perform the filtering client-side.",config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file,"server side workflow log filtering It would be useful for users to be able to filter logs from workflows executed on the server. Currently all output from workflow and its tasks, both stdout/err as well as java log messages are concatenated into a single log file. Thus it's not easy to reliably filter out only e.g. stdout or on java logging level. I propose changing the digdag log file format to be a structured stream of messages (e.g. msgpack) that combines the raw log message with metadata like timestamp and whether the message is from stdout/err or java logging and then the logging level. I.e. something like:  json //  { ""ts"": 1463387968, ""stream"": ""OUT"", ""message"": ""hello world"", }, { ""ts"": 1463388012, ""stream"": ""LOG"", ""level"": ""WARN"", ""message"": ""the foo failed to bar the baz because quux"", }, { ""ts"": 1463388153, ""stream"": ""ERR"", ""message"": ""The quick brown fox jumps over the lazy dog"", }, { ""ts"": 1463388265, ""stream"": ""LOG"", ""level"": ""DEBUG"", ""message"": """", }, //   We could then have server the expose a rest api endpoint for querying this stream or have the client fetch all log entries and perform the filtering client-side. config-file source-file source-file source-file source-file source-file source-file source-file",no-bug,0.9
709,digdag,https://github.com/treasure-data/digdag/issues/709,[Q] How can I set retry wait interval of _retry parameter?,"I want to retry after 1 hours. How can I set this? If this feature is not included, would it be possible to add in next release?",test-file | test-file | documentation-file | source-file,"[Q] How can I set retry wait interval of _retry parameter? I want to retry after 1 hours. How can I set this? If this feature is not included, would it be possible to add in next release? test-file test-file documentation-file source-file",no-bug,0.9
26,digdag,https://github.com/treasure-data/digdag/issues/26,"org.h2.jdbc.JdbcSQLException: Table ""QUEUED_SHARED_TASK_LOCKS"" not found","I ran into this error when running a workflow as a local mode.  2016-04-01 14:25:30 +0900 [ERROR] (local-agent-0): Uncaught exception org.skife.jdbi.v2.exceptions.UnableToCreateStatementException: org.h2.jdbc.JdbcSQLException: Table ""QUEUED_SHARED_TASK_LOCKS"" not found; SQL statement: with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null [42102-191] [statement:""with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null"", located:""with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null"", rewritten:""with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null"", arguments:{ positional:{}, named:{}, finder:[]}] at org.skife.jdbi.v2.SQLStatement.internalExecute(SQLStatement.java:1306) at org.skife.jdbi.v2.Query.fold(Query.java:173) at org.skife.jdbi.v2.Query.list(Query.java:82) at org.skife.jdbi.v2.Query.list(Query.java:75) at io.digdag.core.database.DatabaseTaskQueueStore.lambda$lockSharedTasks$147(DatabaseTaskQueueStore.java:195) at io.digdag.core.database.BasicDatabaseStoreManager.lambda$transaction$54(BasicDatabaseStoreManager.java:192) at org.skife.jdbi.v2.tweak.transactions.LocalTransactionHandler.inTransaction(LocalTransactionHandler.java:183) at org.skife.jdbi.v2.BasicHandle.inTransaction(BasicHandle.java:330) at io.digdag.core.database.BasicDatabaseStoreManager.transaction(BasicDatabaseStoreManager.java:192) at io.digdag.core.database.DatabaseTaskQueueStore.lockSharedTasks(DatabaseTaskQueueStore.java:176) at io.digdag.core.database.DatabaseTaskQueueFactory$DatabaseTaskQueue.lockSharedTasks(DatabaseTaskQueueFactory.java:87) at io.digdag.core.agent.LocalAgent.run(LocalAgent.java:57) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: org.h2.jdbc.JdbcSQLException: Table ""QUEUED_SHARED_TASK_LOCKS"" not found; SQL statement: with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null [42102-191] at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) at org.h2.message.DbException.get(DbException.java:179) at org.h2.message.DbException.get(DbException.java:155) at org.h2.command.Parser.readTableOrView(Parser.java:5349) at org.h2.command.Parser.readTableFilter(Parser.java:1245) at org.h2.command.Parser.parseSelectSimpleFromPart(Parser.java:1884) at org.h2.command.Parser.parseSelectSimple(Parser.java:2032) at org.h2.command.Parser.parseSelectSub(Parser.java:1878) at org.h2.command.Parser.parseSelectUnion(Parser.java:1699) at org.h2.command.Parser.parseSelectSub(Parser.java:1874) at org.h2.command.Parser.parseSelectUnion(Parser.java:1699) at org.h2.command.Parser.parseSelect(Parser.java:1687) at org.h2.command.Parser.parseWith(Parser.java:4745) at org.h2.command.Parser.parsePrepared(Parser.java:479) at org.h2.command.Parser.parse(Parser.java:315) at org.h2.command.Parser.parse(Parser.java:287) at org.h2.command.Parser.prepareCommand(Parser.java:252) at org.h2.engine.Session.prepareLocal(Session.java:560) at org.h2.engine.Session.prepareCommand(Session.java:501) at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1188) at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:73) at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:276) at com.zaxxer.hikari.pool.ProxyConnection.prepareStatement(ProxyConnection.java:308) at com.zaxxer.hikari.pool.HikariProxyConnection.prepareStatement(HikariProxyConnection.java) at org.skife.jdbi.v2.DefaultStatementBuilder.create(DefaultStatementBuilder.java:54) at org.skife.jdbi.v2.SQLStatement.internalExecute(SQLStatement.java:1302)  16 common frames omitted  This is the workflow:  run: +main +main: +step1: sh>: ./tasks/bin/enc-tool -e development-ec2 -a 1 -c ENCRYPT -t 200 ",source-file | source-file | test-file,"org.h2.jdbc.JdbcSQLException: Table ""QUEUED_SHARED_TASK_LOCKS"" not found I ran into this error when running a workflow as a local mode.  2016-04-01 14:25:30 +0900 [ERROR] (local-agent-0): Uncaught exception org.skife.jdbi.v2.exceptions.UnableToCreateStatementException: org.h2.jdbc.JdbcSQLException: Table ""QUEUED_SHARED_TASK_LOCKS"" not found; SQL statement: with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null [42102-191] [statement:""with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null"", located:""with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null"", rewritten:""with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null"", arguments:{ positional:{}, named:{}, finder:[]}] at org.skife.jdbi.v2.SQLStatement.internalExecute(SQLStatement.java:1306) at org.skife.jdbi.v2.Query.fold(Query.java:173) at org.skife.jdbi.v2.Query.list(Query.java:82) at org.skife.jdbi.v2.Query.list(Query.java:75) at io.digdag.core.database.DatabaseTaskQueueStore.lambda$lockSharedTasks$147(DatabaseTaskQueueStore.java:195) at io.digdag.core.database.BasicDatabaseStoreManager.lambda$transaction$54(BasicDatabaseStoreManager.java:192) at org.skife.jdbi.v2.tweak.transactions.LocalTransactionHandler.inTransaction(LocalTransactionHandler.java:183) at org.skife.jdbi.v2.BasicHandle.inTransaction(BasicHandle.java:330) at io.digdag.core.database.BasicDatabaseStoreManager.transaction(BasicDatabaseStoreManager.java:192) at io.digdag.core.database.DatabaseTaskQueueStore.lockSharedTasks(DatabaseTaskQueueStore.java:176) at io.digdag.core.database.DatabaseTaskQueueFactory$DatabaseTaskQueue.lockSharedTasks(DatabaseTaskQueueFactory.java:87) at io.digdag.core.agent.LocalAgent.run(LocalAgent.java:57) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: org.h2.jdbc.JdbcSQLException: Table ""QUEUED_SHARED_TASK_LOCKS"" not found; SQL statement: with recursive t (queue_id) as ((select queue_id from queued_shared_task_locks where hold_expire_time is null order by queue_id limit 1) union all select (select queue_id from queued_shared_task_locks where hold_expire_time is null and queue_id > t.queue_id order by queue_id limit 1) from t where t.queue_id is not null) select queue_id as id from t where queue_id is not null [42102-191] at org.h2.message.DbException.getJdbcSQLException(DbException.java:345) at org.h2.message.DbException.get(DbException.java:179) at org.h2.message.DbException.get(DbException.java:155) at org.h2.command.Parser.readTableOrView(Parser.java:5349) at org.h2.command.Parser.readTableFilter(Parser.java:1245) at org.h2.command.Parser.parseSelectSimpleFromPart(Parser.java:1884) at org.h2.command.Parser.parseSelectSimple(Parser.java:2032) at org.h2.command.Parser.parseSelectSub(Parser.java:1878) at org.h2.command.Parser.parseSelectUnion(Parser.java:1699) at org.h2.command.Parser.parseSelectSub(Parser.java:1874) at org.h2.command.Parser.parseSelectUnion(Parser.java:1699) at org.h2.command.Parser.parseSelect(Parser.java:1687) at org.h2.command.Parser.parseWith(Parser.java:4745) at org.h2.command.Parser.parsePrepared(Parser.java:479) at org.h2.command.Parser.parse(Parser.java:315) at org.h2.command.Parser.parse(Parser.java:287) at org.h2.command.Parser.prepareCommand(Parser.java:252) at org.h2.engine.Session.prepareLocal(Session.java:560) at org.h2.engine.Session.prepareCommand(Session.java:501) at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1188) at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:73) at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:276) at com.zaxxer.hikari.pool.ProxyConnection.prepareStatement(ProxyConnection.java:308) at com.zaxxer.hikari.pool.HikariProxyConnection.prepareStatement(HikariProxyConnection.java) at org.skife.jdbi.v2.DefaultStatementBuilder.create(DefaultStatementBuilder.java:54) at org.skife.jdbi.v2.SQLStatement.internalExecute(SQLStatement.java:1302)  16 common frames omitted  This is the workflow:  run: +main +main: +step1: sh>: ./tasks/bin/enc-tool -e development-ec2 -a 1 -c ENCRYPT -t 200  source-file source-file test-file",no-bug,0.9
487,digdag,https://github.com/treasure-data/digdag/issues/487,Duplicate task execution when running long tasks in parallel,"## Summary I often get duplicate task execution problem when running many tasks in parallel with following conditions. * specify `--max-task-threads` to digdag server. * run tasks that larger than the value of `--max-task-threads` in parallel. * each tasks run for over 5 minutes. ## Steps to reproduce 1) start digdag server with `--max-task-threads` option.  digdag server --max-task-threads 10  2) push the following workflow and script. test-duplicate.dig  timezone: Asia/Tokyo +main: _parallel: true loop>: 20 _do: +run: sh>: scripts/sleep-6min.sh ${(i+1)}  scripts/sleep-6min.sh  #!/bin/bash sleep 360 exit 0  3) start workflow `test-duplicate`. The example of this execution result is:  2017-02-13 15:39:43 +0900 [INFO] (0062@+test-duplicate+main^sub+loop-2+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 3 2017-02-13 15:39:43 +0900 [INFO] (0064@+test-duplicate+main^sub+loop-4+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 5 2017-02-13 15:39:43 +0900 [INFO] (0068@+test-duplicate+main^sub+loop-8+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 9 2017-02-13 15:39:43 +0900 [INFO] (0060@+test-duplicate+main^sub+loop-0+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 1 2017-02-13 15:39:43 +0900 [INFO] (0061@+test-duplicate+main^sub+loop-1+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 2 2017-02-13 15:39:43 +0900 [INFO] (0067@+test-duplicate+main^sub+loop-7+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 8 2017-02-13 15:39:43 +0900 [INFO] (0063@+test-duplicate+main^sub+loop-3+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 4 2017-02-13 15:39:43 +0900 [INFO] (0066@+test-duplicate+main^sub+loop-6+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 7 2017-02-13 15:39:43 +0900 [INFO] (0065@+test-duplicate+main^sub+loop-5+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 6 2017-02-13 15:39:43 +0900 [INFO] (0056@+test-duplicate+main^sub+loop-9+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 10 2017-02-13 15:44:45 +0900 [WARN] (lock-expire-0) io.digdag.core.database.DatabaseTaskQueueServer: 1 task locks are expired. Tasks will be retried. 2017-02-13 15:45:44 +0900 [INFO] (0062@+test-duplicate+main^sub+loop-10+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 11 2017-02-13 15:45:44 +0900 [INFO] (0067@+test-duplicate+main^sub+loop-14+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 15 2017-02-13 15:45:44 +0900 [INFO] (0064@+test-duplicate+main^sub+loop-10+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 11 2017-02-13 15:45:44 +0900 [INFO] (0068@+test-duplicate+main^sub+loop-11+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 12 2017-02-13 15:45:44 +0900 [INFO] (0063@+test-duplicate+main^sub+loop-15+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 16 2017-02-13 15:45:44 +0900 [INFO] (0060@+test-duplicate+main^sub+loop-12+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 13 2017-02-13 15:45:44 +0900 [INFO] (0065@+test-duplicate+main^sub+loop-17+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 18 2017-02-13 15:45:44 +0900 [INFO] (0061@+test-duplicate+main^sub+loop-13+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 14 2017-02-13 15:45:44 +0900 [INFO] (0066@+test-duplicate+main^sub+loop-16+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 17 2017-02-13 15:45:44 +0900 [INFO] (0056@+test-duplicate+main^sub+loop-18+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 19 2017-02-13 15:50:45 +0900 [WARN] (lock-expire-0) io.digdag.core.database.DatabaseTaskQueueServer: 1 task locks are expired. Tasks will be retried. 2017-02-13 15:51:45 +0900 [INFO] (0062@+test-duplicate+main^sub+loop-19+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 20 2017-02-13 15:51:45 +0900 [INFO] (0065@+test-duplicate+main^sub+loop-19+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 20  In this case, task `+loop-10+run` and `loop-19+run` were executed twice. ## Details I think the cause of this problem seems to be how to start the task execution thread on MultiThreadAgent#run. https://github.com/treasure-data/digdag/blob/v0.9.4/digdag-core/src/main/java/io/digdag/core/agent/MultiThreadAgent.java#L89 ThreadPoolExecutor#getActiveCount returns **approximate** active number of threads. For example, if a certain thread is started and getActiveCount called immediately after that, the thread count that was just started thread may not be reflected. To testing this problem, I add some info logs to MultiThreadAgent#run as follows.  @Override public void run() { while (!stop) { try { synchronized (newTaskLock) { if (executor.isShutdown()) { break; } int max = Math.min(executor.getMaximumPoolSize() - executor.getActiveCount(), 10); logger.info(""max={}"", max); // Add for tesitng if (max > 0) { List<TaskRequest> reqs = taskServer.lockSharedAgentTasks(max, agentId, config.getLockRetentionTime(), 1000); logger.info(""reqs.size={}"", reqs.size()); // Add for tesitng for (TaskRequest req : reqs) { executor.submit(() -> { try { runner.run(req); }   The execution result is:  2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=10 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: reqs.size=1 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=9 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: reqs.size=3 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=6 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: reqs.size=6 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=1 // previous thread count is not reflected correctly 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: reqs.size=1 // will start task over max thread size 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=0 2017-02-13 15:39:43 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=0   And at this time, `queued_task_locks` had 11 records of updated `lock_expire_time`.  digdag=# select * from queued_task_locks order by id; id | site_id | queue_id | priority | retry_count | lock_expire_time | lock_agent_id ++++++ 150 | 0 | | 0 | 0 | 1486968325 | 38159@ 151 | 0 | | 0 | 0 | 1486968325 | 38159@ 152 | 0 | | 0 | 0 | 1486968325 | 38159@ 153 | 0 | | 0 | 0 | 1486968325 | 38159@ 154 | 0 | | 0 | 0 | 1486968325 | 38159@ 155 | 0 | | 0 | 0 | 1486968325 | 38159@ 156 | 0 | | 0 | 0 | 1486968325 | 38159@ 157 | 0 | | 0 | 0 | 1486968325 | 38159@ 158 | 0 | | 0 | 0 | 1486968325 | 38159@ 159 | 0 | | 0 | 0 | 1486968325 | 38159@ 160 | 0 | | 0 | 0 | 1486968283 | 38159@ 161 | 0 | | 0 | 0 | | 162 | 0 | | 0 | 0 | | 163 | 0 | | 0 | 0 | | 164 | 0 | | 0 | 0 | | 165 | 0 | | 0 | 0 | | 166 | 0 | | 0 | 0 | | 167 | 0 | | 0 | 0 | | 168 | 0 | | 0 | 0 | | 169 | 0 | | 0 | 0 | |  In ths case, task with id 160 (task `+loop-10+run`) is not executed yet, waiting in the queue. Since this task is inactive and cannot send a heartbeat, this task expires lock after 5 minutes (if another task does not finished during this time) and will be retried with following logs.  2017-02-13 15:44:45 +0900 [WARN] (lock-expire-0) io.digdag.core.database.DatabaseTaskQueueServer: 1 task locks are expired. Tasks will be retried.  And I have confirmed that updating `retry_count` and `lock_expire_time` on id 160 at this time.  digdag=# select * from queued_task_locks order by id; id | site_id | queue_id | priority | retry_count | lock_expire_time | lock_agent_id ++++++ 150 | 0 | | 0 | 0 | 1486968625 | 38159@ 151 | 0 | | 0 | 0 | 1486968625 | 38159@ 152 | 0 | | 0 | 0 | 1486968625 | 38159@ 153 | 0 | | 0 | 0 | 1486968625 | 38159@ 154 | 0 | | 0 | 0 | 1486968625 | 38159@ 155 | 0 | | 0 | 0 | 1486968625 | 38159@ 156 | 0 | | 0 | 0 | 1486968625 | 38159@ 157 | 0 | | 0 | 0 | 1486968625 | 38159@ 158 | 0 | | 0 | 0 | 1486968625 | 38159@ 159 | 0 | | 0 | 0 | 1486968625 | 38159@ 160 | 0 | | 0 | 1 | | 161 | 0 | | 0 | 0 | | 162 | 0 | | 0 | 0 | | 163 | 0 | | 0 | 0 | | 164 | 0 | | 0 | 0 | | 165 | 0 | | 0 | 0 | | 166 | 0 | | 0 | 0 | | 167 | 0 | | 0 | 0 | | 168 | 0 | | 0 | 0 | | 169 | 0 | | 0 | 0 | |  This may cause the duplicate task execution. I also confirmed that the task with id 169 (task `+loop-19+run`) also behaved similarly.  id | site_id | queue_id | priority | retry_count | lock_expire_time | lock_agent_id ++++++ 169 | 0 | | 0 | 1 | 1486969165 | 38159@  ## System configuration digdag version: 0.9.4",source-file | test-file | test-file,"Duplicate task execution when running long tasks in parallel ## Summary I often get duplicate task execution problem when running many tasks in parallel with following conditions. * specify `--max-task-threads` to digdag server. * run tasks that larger than the value of `--max-task-threads` in parallel. * each tasks run for over 5 minutes. ## Steps to reproduce 1) start digdag server with `--max-task-threads` option.  digdag server --max-task-threads 10  2) push the following workflow and script. test-duplicate.dig  timezone: Asia/Tokyo +main: _parallel: true loop>: 20 _do: +run: sh>: scripts/sleep-6min.sh ${(i+1)}  scripts/sleep-6min.sh  #!/bin/bash sleep 360 exit 0  3) start workflow `test-duplicate`. The example of this execution result is:  2017-02-13 15:39:43 +0900 [INFO] (0062@+test-duplicate+main^sub+loop-2+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 3 2017-02-13 15:39:43 +0900 [INFO] (0064@+test-duplicate+main^sub+loop-4+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 5 2017-02-13 15:39:43 +0900 [INFO] (0068@+test-duplicate+main^sub+loop-8+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 9 2017-02-13 15:39:43 +0900 [INFO] (0060@+test-duplicate+main^sub+loop-0+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 1 2017-02-13 15:39:43 +0900 [INFO] (0061@+test-duplicate+main^sub+loop-1+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 2 2017-02-13 15:39:43 +0900 [INFO] (0067@+test-duplicate+main^sub+loop-7+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 8 2017-02-13 15:39:43 +0900 [INFO] (0063@+test-duplicate+main^sub+loop-3+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 4 2017-02-13 15:39:43 +0900 [INFO] (0066@+test-duplicate+main^sub+loop-6+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 7 2017-02-13 15:39:43 +0900 [INFO] (0065@+test-duplicate+main^sub+loop-5+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 6 2017-02-13 15:39:43 +0900 [INFO] (0056@+test-duplicate+main^sub+loop-9+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 10 2017-02-13 15:44:45 +0900 [WARN] (lock-expire-0) io.digdag.core.database.DatabaseTaskQueueServer: 1 task locks are expired. Tasks will be retried. 2017-02-13 15:45:44 +0900 [INFO] (0062@+test-duplicate+main^sub+loop-10+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 11 2017-02-13 15:45:44 +0900 [INFO] (0067@+test-duplicate+main^sub+loop-14+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 15 2017-02-13 15:45:44 +0900 [INFO] (0064@+test-duplicate+main^sub+loop-10+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 11 2017-02-13 15:45:44 +0900 [INFO] (0068@+test-duplicate+main^sub+loop-11+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 12 2017-02-13 15:45:44 +0900 [INFO] (0063@+test-duplicate+main^sub+loop-15+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 16 2017-02-13 15:45:44 +0900 [INFO] (0060@+test-duplicate+main^sub+loop-12+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 13 2017-02-13 15:45:44 +0900 [INFO] (0065@+test-duplicate+main^sub+loop-17+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 18 2017-02-13 15:45:44 +0900 [INFO] (0061@+test-duplicate+main^sub+loop-13+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 14 2017-02-13 15:45:44 +0900 [INFO] (0066@+test-duplicate+main^sub+loop-16+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 17 2017-02-13 15:45:44 +0900 [INFO] (0056@+test-duplicate+main^sub+loop-18+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 19 2017-02-13 15:50:45 +0900 [WARN] (lock-expire-0) io.digdag.core.database.DatabaseTaskQueueServer: 1 task locks are expired. Tasks will be retried. 2017-02-13 15:51:45 +0900 [INFO] (0062@+test-duplicate+main^sub+loop-19+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 20 2017-02-13 15:51:45 +0900 [INFO] (0065@+test-duplicate+main^sub+loop-19+run) io.digdag.core.agent.OperatorManager: sh>: scripts/sleep-6min.sh 20  In this case, task `+loop-10+run` and `loop-19+run` were executed twice. ## Details I think the cause of this problem seems to be how to start the task execution thread on MultiThreadAgent#run. https://github.com/treasure-data/digdag/blob/v0.9.4/digdag-core/src/main/java/io/digdag/core/agent/MultiThreadAgent.java#L89 ThreadPoolExecutor#getActiveCount returns **approximate** active number of threads. For example, if a certain thread is started and getActiveCount called immediately after that, the thread count that was just started thread may not be reflected. To testing this problem, I add some info logs to MultiThreadAgent#run as follows.  @Override public void run() { while (!stop) { try { synchronized (newTaskLock) { if (executor.isShutdown()) { break; } int max = Math.min(executor.getMaximumPoolSize() - executor.getActiveCount(), 10); logger.info(""max={}"", max); // Add for tesitng if (max > 0) { List<TaskRequest> reqs = taskServer.lockSharedAgentTasks(max, agentId, config.getLockRetentionTime(), 1000); logger.info(""reqs.size={}"", reqs.size()); // Add for tesitng for (TaskRequest req : reqs) { executor.submit(() -> { try { runner.run(req); }   The execution result is:  2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=10 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: reqs.size=1 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=9 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: reqs.size=3 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=6 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: reqs.size=6 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=1 // previous thread count is not reflected correctly 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: reqs.size=1 // will start task over max thread size 2017-02-13 15:39:42 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=0 2017-02-13 15:39:43 +0900 [INFO] (local-agent-0) io.digdag.core.agent.MultiThreadAgent: max=0   And at this time, `queued_task_locks` had 11 records of updated `lock_expire_time`.  digdag=# select * from queued_task_locks order by id; id | site_id | queue_id | priority | retry_count | lock_expire_time | lock_agent_id  150 | 0 | | 0 | 0 | 1486968325 | 38159@ 151 | 0 | | 0 | 0 | 1486968325 | 38159@ 152 | 0 | | 0 | 0 | 1486968325 | 38159@ 153 | 0 | | 0 | 0 | 1486968325 | 38159@ 154 | 0 | | 0 | 0 | 1486968325 | 38159@ 155 | 0 | | 0 | 0 | 1486968325 | 38159@ 156 | 0 | | 0 | 0 | 1486968325 | 38159@ 157 | 0 | | 0 | 0 | 1486968325 | 38159@ 158 | 0 | | 0 | 0 | 1486968325 | 38159@ 159 | 0 | | 0 | 0 | 1486968325 | 38159@ 160 | 0 | | 0 | 0 | 1486968283 | 38159@ 161 | 0 | | 0 | 0 | | 162 | 0 | | 0 | 0 | | 163 | 0 | | 0 | 0 | | 164 | 0 | | 0 | 0 | | 165 | 0 | | 0 | 0 | | 166 | 0 | | 0 | 0 | | 167 | 0 | | 0 | 0 | | 168 | 0 | | 0 | 0 | | 169 | 0 | | 0 | 0 | |  In ths case, task with id 160 (task `+loop-10+run`) is not executed yet, waiting in the queue. Since this task is inactive and cannot send a heartbeat, this task expires lock after 5 minutes (if another task does not finished during this time) and will be retried with following logs.  2017-02-13 15:44:45 +0900 [WARN] (lock-expire-0) io.digdag.core.database.DatabaseTaskQueueServer: 1 task locks are expired. Tasks will be retried.  And I have confirmed that updating `retry_count` and `lock_expire_time` on id 160 at this time.  digdag=# select * from queued_task_locks order by id; id | site_id | queue_id | priority | retry_count | lock_expire_time | lock_agent_id  150 | 0 | | 0 | 0 | 1486968625 | 38159@ 151 | 0 | | 0 | 0 | 1486968625 | 38159@ 152 | 0 | | 0 | 0 | 1486968625 | 38159@ 153 | 0 | | 0 | 0 | 1486968625 | 38159@ 154 | 0 | | 0 | 0 | 1486968625 | 38159@ 155 | 0 | | 0 | 0 | 1486968625 | 38159@ 156 | 0 | | 0 | 0 | 1486968625 | 38159@ 157 | 0 | | 0 | 0 | 1486968625 | 38159@ 158 | 0 | | 0 | 0 | 1486968625 | 38159@ 159 | 0 | | 0 | 0 | 1486968625 | 38159@ 160 | 0 | | 0 | 1 | | 161 | 0 | | 0 | 0 | | 162 | 0 | | 0 | 0 | | 163 | 0 | | 0 | 0 | | 164 | 0 | | 0 | 0 | | 165 | 0 | | 0 | 0 | | 166 | 0 | | 0 | 0 | | 167 | 0 | | 0 | 0 | | 168 | 0 | | 0 | 0 | | 169 | 0 | | 0 | 0 | |  This may cause the duplicate task execution. I also confirmed that the task with id 169 (task `+loop-19+run`) also behaved similarly.  id | site_id | queue_id | priority | retry_count | lock_expire_time | lock_agent_id  169 | 0 | | 0 | 1 | 1486969165 | 38159@  ## System configuration digdag version: 0.9.4 source-file test-file test-file",no-bug,0.9
9,digdag,https://github.com/treasure-data/digdag/issues/9,Add ${user.name} property,To know who is running/submitting the workflow.,source-file | test-file,Add ${user.name} property To know who is running/submitting the workflow. source-file test-file,no-bug,0.7
513,digdag,https://github.com/treasure-data/digdag/issues/513,"""retry: false"" of ""http>:"" operator is not working","When a url is wrong or temporarily down, the `http>:` operator keeps retrying even if `retry: false` is specified. Digdag version is 0.9.6. A sample digdag workflow is as following.  +check: +call: _error: fail>: ""Error!"" +call: http>: http://wwwW.xxxx/v1/stats/qps retry: false store_content: true  And digdag server log is as follows.  2017-03-15 18:18:54 +0900 [INFO] (0785@+test+check+call+call) io.digdag.core.agent.OperatorManager: http>: http://wwww.xxxx/v1/stats/qps 2017-03-15 18:18:54 +0900 [INFO] (0785@+test+check+call+call) io.digdag.standards.operator.HttpOperatorFactory$HttpOperator: Sending HTTP request: GET http://wwww.xxxx/v1/stats/qps 2017-03-15 18:18:54 +0900 [WARN] (0785@+test+check+call+call) io.digdag.standards.operator.state.PollingRetryExecutor: HTTP request failed: retrying in 1 seconds java.lang.RuntimeException: java.net.UnknownHostException: wwww.xxxx: Name or service not known at com.google.api.client.repackaged.com.google.common.base.Throwables.propagate(Throwables.java:160) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.send(HttpOperatorFactory.java:324) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.execute(HttpOperatorFactory.java:250) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.lambda$run$1(HttpOperatorFactory.java:195) at io.digdag.standards.operator.state.PollingRetryExecutor.run(PollingRetryExecutor.java:176) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.run(HttpOperatorFactory.java:195) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.runTask(HttpOperatorFactory.java:136) at io.digdag.util.BaseOperator.run(BaseOperator.java:35) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:314) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:255) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$2(OperatorManager.java:138) at io.digdag.core.agent.ExtractArchiveWorkspaceManager.withExtractedArchive(ExtractArchiveWorkspaceManager.java:36) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:136) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:120) at io.digdag.core.agent.MultiThreadAgent.lambda$run$0(MultiThreadAgent.java:121) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: java.net.UnknownHostException: wwww.xxxx: Name or service not known at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323) at java.net.InetAddress.getAllByName0(InetAddress.java:1276) at java.net.InetAddress.getAllByName(InetAddress.java:1192) at java.net.InetAddress.getAllByName(InetAddress.java:1126) at org.eclipse.jetty.util.SocketAddressResolver$Async.lambda$resolve$1(SocketAddressResolver.java:167) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)  1 common frames omitted 2017-03-15 18:18:56 +0900 [INFO] (0785@+test+check+call+call) io.digdag.core.agent.OperatorManager: http>: http://wwww.xxxx/v1/stats/qps 2017-03-15 18:18:56 +0900 [INFO] (0785@+test+check+call+call) io.digdag.standards.operator.HttpOperatorFactory$HttpOperator: Sending HTTP request: GET http://wwww.xxxx/v1/stats/qps 2017-03-15 18:18:56 +0900 [WARN] (0785@+test+check+call+call) io.digdag.standards.operator.state.PollingRetryExecutor: HTTP request failed: retrying in 2 seconds java.lang.RuntimeException: java.net.UnknownHostException: wwww.xxxx at com.google.api.client.repackaged.com.google.common.base.Throwables.propagate(Throwables.java:160) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.send(HttpOperatorFactory.java:324) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.execute(HttpOperatorFactory.java:250) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.lambda$run$1(HttpOperatorFactory.java:195) at io.digdag.standards.operator.state.PollingRetryExecutor.run(PollingRetryExecutor.java:176) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.run(HttpOperatorFactory.java:195) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.runTask(HttpOperatorFactory.java:136) at io.digdag.util.BaseOperator.run(BaseOperator.java:35) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:314) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:255) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$2(OperatorManager.java:138) at io.digdag.core.agent.ExtractArchiveWorkspaceManager.withExtractedArchive(ExtractArchiveWorkspaceManager.java:36) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:136) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:120) at io.digdag.core.agent.MultiThreadAgent.lambda$run$0(MultiThreadAgent.java:121)  (repeating) ",source-file | source-file | source-file,"""retry: false"" of ""http>:"" operator is not working When a url is wrong or temporarily down, the `http>:` operator keeps retrying even if `retry: false` is specified. Digdag version is 0.9.6. A sample digdag workflow is as following.  +check: +call: _error: fail>: ""Error!"" +call: http>: http://wwwW.xxxx/v1/stats/qps retry: false store_content: true  And digdag server log is as follows.  2017-03-15 18:18:54 +0900 [INFO] (0785@+test+check+call+call) io.digdag.core.agent.OperatorManager: http>: http://wwww.xxxx/v1/stats/qps 2017-03-15 18:18:54 +0900 [INFO] (0785@+test+check+call+call) io.digdag.standards.operator.HttpOperatorFactory$HttpOperator: Sending HTTP request: GET http://wwww.xxxx/v1/stats/qps 2017-03-15 18:18:54 +0900 [WARN] (0785@+test+check+call+call) io.digdag.standards.operator.state.PollingRetryExecutor: HTTP request failed: retrying in 1 seconds java.lang.RuntimeException: java.net.UnknownHostException: wwww.xxxx: Name or service not known at com.google.api.client.repackaged.com.google.common.base.Throwables.propagate(Throwables.java:160) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.send(HttpOperatorFactory.java:324) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.execute(HttpOperatorFactory.java:250) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.lambda$run$1(HttpOperatorFactory.java:195) at io.digdag.standards.operator.state.PollingRetryExecutor.run(PollingRetryExecutor.java:176) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.run(HttpOperatorFactory.java:195) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.runTask(HttpOperatorFactory.java:136) at io.digdag.util.BaseOperator.run(BaseOperator.java:35) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:314) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:255) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$2(OperatorManager.java:138) at io.digdag.core.agent.ExtractArchiveWorkspaceManager.withExtractedArchive(ExtractArchiveWorkspaceManager.java:36) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:136) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:120) at io.digdag.core.agent.MultiThreadAgent.lambda$run$0(MultiThreadAgent.java:121) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: java.net.UnknownHostException: wwww.xxxx: Name or service not known at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method) at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928) at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323) at java.net.InetAddress.getAllByName0(InetAddress.java:1276) at java.net.InetAddress.getAllByName(InetAddress.java:1192) at java.net.InetAddress.getAllByName(InetAddress.java:1126) at org.eclipse.jetty.util.SocketAddressResolver$Async.lambda$resolve$1(SocketAddressResolver.java:167) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)  1 common frames omitted 2017-03-15 18:18:56 +0900 [INFO] (0785@+test+check+call+call) io.digdag.core.agent.OperatorManager: http>: http://wwww.xxxx/v1/stats/qps 2017-03-15 18:18:56 +0900 [INFO] (0785@+test+check+call+call) io.digdag.standards.operator.HttpOperatorFactory$HttpOperator: Sending HTTP request: GET http://wwww.xxxx/v1/stats/qps 2017-03-15 18:18:56 +0900 [WARN] (0785@+test+check+call+call) io.digdag.standards.operator.state.PollingRetryExecutor: HTTP request failed: retrying in 2 seconds java.lang.RuntimeException: java.net.UnknownHostException: wwww.xxxx at com.google.api.client.repackaged.com.google.common.base.Throwables.propagate(Throwables.java:160) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.send(HttpOperatorFactory.java:324) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.execute(HttpOperatorFactory.java:250) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.lambda$run$1(HttpOperatorFactory.java:195) at io.digdag.standards.operator.state.PollingRetryExecutor.run(PollingRetryExecutor.java:176) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.run(HttpOperatorFactory.java:195) at io.digdag.standards.operator.HttpOperatorFactory$HttpOperator.runTask(HttpOperatorFactory.java:136) at io.digdag.util.BaseOperator.run(BaseOperator.java:35) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:314) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:255) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$2(OperatorManager.java:138) at io.digdag.core.agent.ExtractArchiveWorkspaceManager.withExtractedArchive(ExtractArchiveWorkspaceManager.java:36) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:136) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:120) at io.digdag.core.agent.MultiThreadAgent.lambda$run$0(MultiThreadAgent.java:121)  (repeating)  source-file source-file source-file",no-bug,0.8
45,digdag,https://github.com/treasure-data/digdag/issues/45,workflow per file and digdag.yml,"Design: `$ digdag init <name>` creates `digdag.yml` and `<name>.yml` files in `<name>` directory as following:  # digdag.yml name: <name> workflows: - <name>.yml   # <name>.yml +step1: sh>: some example here +step2: sh>: some example here  All digdag commands (run, push, etc.) use `digdag.yml` as the entry point. `name` section in digdag.yml is the name of project. Name of a workflow matches with the file name (so workflow yml files don't need `name` section). `digdag push` doesn't require `-f` option. It reads ./digdag.yml.",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | config-file | config-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | documentation-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file,"workflow per file and digdag.yml Design: `$ digdag init <name>` creates `digdag.yml` and `<name>.yml` files in `<name>` directory as following:  # digdag.yml name: <name> workflows: - <name>.yml   # <name>.yml +step1: sh>: some example here +step2: sh>: some example here  All digdag commands (run, push, etc.) use `digdag.yml` as the entry point. `name` section in digdag.yml is the name of project. Name of a workflow matches with the file name (so workflow yml files don't need `name` section). `digdag push` doesn't require `-f` option. It reads ./digdag.yml. source-file source-file source-file source-file source-file source-file source-file source-file source-file config-file config-file source-file source-file source-file source-file documentation-file documentation-file documentation-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file",no-bug,0.9
31,digdag,https://github.com/treasure-data/digdag/issues/31,digdag init should set default timezone," $ digdag --version 0.5.9   $ digdag init foo 2016-04-11 14:42:38 -0700: Digdag v0.5.9 Creating foo/digdag Creating foo/.digdag-wrapper/digdag.jar Creating foo/.gitignore Creating foo/tasks/shell_sample.sh Creating foo/tasks/repeat_hello.sh Creating foo/tasks/__init__.py Creating foo/digdag.yml Done. Type `cd foo` and `./digdag r` to run the workflow. Enjoy!   $ cd foo $ digdag push foo -r test 2016-04-11 14:42:52 -0700: Digdag v0.5.9 Creating digdag.archive.tar.gz Exception in thread ""main"" io.digdag.client.config.ConfigException: timezone: parameter is required but not set at digdag.yml. Example is 'timezone: America/Los_Angeles'. at io.digdag.cli.client.Archive.runArchive(Archive.java:152) at io.digdag.cli.client.Archive.archive(Archive.java:105) at io.digdag.cli.client.Push.push(Push.java:66) at io.digdag.cli.client.Push.mainWithClientException(Push.java:41) at io.digdag.cli.client.ClientCommand.main(ClientCommand.java:55) at io.digdag.cli.Main.main(Main.java:176) ",test-file | test-file | test-file | test-file | test-file | test-file | test-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file,"digdag init should set default timezone  $ digdag --version 0.5.9   $ digdag init foo 2016-04-11 14:42:38 -0700: Digdag v0.5.9 Creating foo/digdag Creating foo/.digdag-wrapper/digdag.jar Creating foo/.gitignore Creating foo/tasks/shell_sample.sh Creating foo/tasks/repeat_hello.sh Creating foo/tasks/__init__.py Creating foo/digdag.yml Done. Type `cd foo` and `./digdag r` to run the workflow. Enjoy!   $ cd foo $ digdag push foo -r test 2016-04-11 14:42:52 -0700: Digdag v0.5.9 Creating digdag.archive.tar.gz Exception in thread ""main"" io.digdag.client.config.ConfigException: timezone: parameter is required but not set at digdag.yml. Example is 'timezone: America/Los_Angeles'. at io.digdag.cli.client.Archive.runArchive(Archive.java:152) at io.digdag.cli.client.Archive.archive(Archive.java:105) at io.digdag.cli.client.Push.push(Push.java:66) at io.digdag.cli.client.Push.mainWithClientException(Push.java:41) at io.digdag.cli.client.ClientCommand.main(ClientCommand.java:55) at io.digdag.cli.Main.main(Main.java:176)  test-file test-file test-file test-file test-file test-file test-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file",no-bug,0.9
52,digdag,https://github.com/treasure-data/digdag/issues/52,server mode should notify client when version is different same,"Right now, a user of digdag could be confused by creating & pushing a workflow to the server that is not longer in spec. The server mode should check the versions are not the same and when it doesn't do the following: 1. Not accept the workflow submission 2. Provide a note to the user to update the client. This note should include the following: - user should run ""digdag selfupdate"" - user should try re-running workflow to ensure compatibility with new version - then re-submit to the server mode",source-file | source-file | test-file | test-file,"server mode should notify client when version is different same Right now, a user of digdag could be confused by creating & pushing a workflow to the server that is not longer in spec. The server mode should check the versions are not the same and when it doesn't do the following: 1. Not accept the workflow submission 2. Provide a note to the user to update the client. This note should include the following: - user should run ""digdag selfupdate"" - user should try re-running workflow to ensure compatibility with new version - then re-submit to the server mode source-file source-file test-file test-file",bug,0.9
37,digdag,https://github.com/treasure-data/digdag/issues/37,cannot use _schedule cron>," $ cat digdag.yml run: +main timezone: ""America/Los_Angeles"" +main: _schedule: cron>: 42 4 1 * * sh>: echo hello world   $ digdag push dano-cron-test -r 1 2016-04-11 15:46:26 -0700: Digdag v0.5.9 Creating digdag.archive.tar.gz Archiving digdag.yml Workflows: +main error: Status code 400: {""message"":""Parameter 'cron' is required but not set"",""status"":400} ",source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | test-file,"cannot use _schedule cron>  $ cat digdag.yml run: +main timezone: ""America/Los_Angeles"" +main: _schedule: cron>: 42 4 1 * * sh>: echo hello world   $ digdag push dano-cron-test -r 1 2016-04-11 15:46:26 -0700: Digdag v0.5.9 Creating digdag.archive.tar.gz Archiving digdag.yml Workflows: +main error: Status code 400: {""message"":""Parameter 'cron' is required but not set"",""status"":400}  source-file source-file source-file source-file source-file test-file source-file source-file source-file source-file test-file",bug,0.9
18,digdag,https://github.com/treasure-data/digdag/issues/18,Using ${td.last_results.job_count} causes an error with --dry-run, $ digdag run -d  2016-03-11 21:27:04 +0900 [ERROR] (0021@+main+tfidf): Task failed java.lang.RuntimeException: Failed to process task config templates at io.digdag.core.agent.OperatorManager.runWithArchive(OperatorManager.java:156) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$1(OperatorManager.java:128) at io.digdag.core.agent.CurrentDirectoryArchiveManager.withExtractedArchive(CurrentDirectoryArchiveManager.java:20) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:127) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:106) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:567) at io.digdag.core.agent.LocalAgent.lambda$run$0(LocalAgent.java:61) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: io.digdag.spi.TemplateException: Failed to evaluate JavaScript code: ${td.last_results.job_count} ,source-file,Using ${td.last_results.job_count} causes an error with --dry-run  $ digdag run -d  2016-03-11 21:27:04 +0900 [ERROR] (0021@+main+tfidf): Task failed java.lang.RuntimeException: Failed to process task config templates at io.digdag.core.agent.OperatorManager.runWithArchive(OperatorManager.java:156) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$1(OperatorManager.java:128) at io.digdag.core.agent.CurrentDirectoryArchiveManager.withExtractedArchive(CurrentDirectoryArchiveManager.java:20) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:127) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:106) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:567) at io.digdag.core.agent.LocalAgent.lambda$run$0(LocalAgent.java:61) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: io.digdag.spi.TemplateException: Failed to evaluate JavaScript code: ${td.last_results.job_count}  source-file,no-bug,0.9
1545,digdag,https://github.com/treasure-data/digdag/issues/1545,NoSuchMethodError occurs in gcs_wait operator from version 0.10.0,"NoSuchMethodError occurs in gcs_wait operator from version 0.10.0. And continue to wait after an error occurs. ## Sample sample.dig  +gcs_wait: gcs_wait>: digdag-gcs_wait-test/tmp.txt  run with 0.9.42  $ digdag --version 0.9.42 $ digdag run sample.dig --rerun 2021-03-12 11:17:17 +0900: Digdag v0.9.42 2021-03-12 11:17:19 +0900 [WARN] (main): Reusing the last session time 2021-03-12T00:00:00+00:00. 2021-03-12 11:17:19 +0900 [INFO] (main): Using session /Users/katsuya.tajima/ghq/github.com/katsuyan/digdag-example/gcs_wait/.digdag/status/20210312T000000+0000. 2021-03-12 11:17:19 +0900 [INFO] (main): Starting a new session project id=1 workflow name=sample session_time=2021-03-12T00:00:00+00:00 2021-03-12 11:17:20 +0900 [INFO] (0018@[0:default]+sample+gcs_wait): gcs_wait>: digdag-gcs_wait-test/tmp.txt Success. Task state is saved at /Users/katsuya.tajima/ghq/github.com/katsuyan/digdag-example/gcs_wait/.digdag/status/20210312T000000+0000 directory. * Use --session <daily | hourly | ""yyyy-MM-dd[ HH:mm:ss]""> to not reuse the last session time. * Use --rerun, --start +NAME, or --goal +NAME argument to rerun skipped tasks.  run with 0.10.0  $ digdag --version 0.10.0 $ digdag run sample.dig --rerun 2021-03-12 11:17:30 +0900: Digdag v0.10.0 2021-03-12 11:17:32 +0900 [WARN] (main): Reusing the last session time 2021-03-12T00:00:00+00:00. 2021-03-12 11:17:32 +0900 [INFO] (main): Using session /Users/katsuya.tajima/ghq/github.com/katsuyan/digdag-example/gcs_wait/.digdag/status/20210312T000000+0000. 2021-03-12 11:17:32 +0900 [INFO] (main): Starting a new session project id=1 workflow name=sample session_time=2021-03-12T00:00:00+00:00 2021-03-12 11:17:32 +0900 [INFO] (0018@[0:default]+sample+gcs_wait): gcs_wait>: digdag-gcs_wait-test/tmp.txt 2021-03-12 11:17:32 +0900 [ERROR] (0018@[0:default]+sample+gcs_wait): Task failed with unexpected error: com.google.api.client.googleapis.services.json.AbstractGoogleJsonClient$Builder.setBatchPath(Ljava/lang/String;)Lcom/google/api/client/googleapis/services/AbstractGoogleClient$Builder; java.lang.NoSuchMethodError: com.google.api.client.googleapis.services.json.AbstractGoogleJsonClient$Builder.setBatchPath(Ljava/lang/String;)Lcom/google/api/client/googleapis/services/AbstractGoogleClient$Builder; at com.google.api.services.storage.Storage$Builder.setBatchPath(Storage.java:11057) at com.google.api.services.storage.Storage$Builder.<init>(Storage.java:11036) at io.digdag.standards.operator.gcp.GcsClient.client(GcsClient.java:38) at io.digdag.standards.operator.gcp.GcsClient.client(GcsClient.java:21) at io.digdag.standards.operator.gcp.BaseGcpClient.<init>(BaseGcpClient.java:30) at io.digdag.standards.operator.gcp.GcsClient.<init>(GcsClient.java:28) at io.digdag.standards.operator.gcp.GcsClient$Factory.create(GcsClient.java:70) at io.digdag.standards.operator.gcp.BaseGcsOperator.run(BaseGcsOperator.java:27) at io.digdag.standards.operator.gcp.BaseGcpOperator.runTask(BaseGcpOperator.java:25) at io.digdag.util.BaseOperator.run(BaseOperator.java:35) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:365) at io.digdag.cli.Run$OperatorManagerWithSkip.callExecutor(Run.java:707) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:298) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$2(OperatorManager.java:151) at io.digdag.core.agent.LocalWorkspaceManager.withExtractedArchive(LocalWorkspaceManager.java:25) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:149) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:132) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:689) at io.digdag.core.agent.MultiThreadAgent.lambda$null$0(MultiThreadAgent.java:132) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ^C% ",config-file,"NoSuchMethodError occurs in gcs_wait operator from version 0.10.0 NoSuchMethodError occurs in gcs_wait operator from version 0.10.0. And continue to wait after an error occurs. ## Sample sample.dig  +gcs_wait: gcs_wait>: digdag-gcs_wait-test/tmp.txt  run with 0.9.42  $ digdag --version 0.9.42 $ digdag run sample.dig --rerun 2021-03-12 11:17:17 +0900: Digdag v0.9.42 2021-03-12 11:17:19 +0900 [WARN] (main): Reusing the last session time 2021-03-12T00:00:00+00:00. 2021-03-12 11:17:19 +0900 [INFO] (main): Using session /Users/katsuya.tajima/ghq/github.com/katsuyan/digdag-example/gcs_wait/.digdag/status/20210312T000000+0000. 2021-03-12 11:17:19 +0900 [INFO] (main): Starting a new session project id=1 workflow name=sample session_time=2021-03-12T00:00:00+00:00 2021-03-12 11:17:20 +0900 [INFO] (0018@[0:default]+sample+gcs_wait): gcs_wait>: digdag-gcs_wait-test/tmp.txt Success. Task state is saved at /Users/katsuya.tajima/ghq/github.com/katsuyan/digdag-example/gcs_wait/.digdag/status/20210312T000000+0000 directory. * Use --session <daily | hourly | ""yyyy-MM-dd[ HH:mm:ss]""> to not reuse the last session time. * Use --rerun, --start +NAME, or --goal +NAME argument to rerun skipped tasks.  run with 0.10.0  $ digdag --version 0.10.0 $ digdag run sample.dig --rerun 2021-03-12 11:17:30 +0900: Digdag v0.10.0 2021-03-12 11:17:32 +0900 [WARN] (main): Reusing the last session time 2021-03-12T00:00:00+00:00. 2021-03-12 11:17:32 +0900 [INFO] (main): Using session /Users/katsuya.tajima/ghq/github.com/katsuyan/digdag-example/gcs_wait/.digdag/status/20210312T000000+0000. 2021-03-12 11:17:32 +0900 [INFO] (main): Starting a new session project id=1 workflow name=sample session_time=2021-03-12T00:00:00+00:00 2021-03-12 11:17:32 +0900 [INFO] (0018@[0:default]+sample+gcs_wait): gcs_wait>: digdag-gcs_wait-test/tmp.txt 2021-03-12 11:17:32 +0900 [ERROR] (0018@[0:default]+sample+gcs_wait): Task failed with unexpected error: com.google.api.client.googleapis.services.json.AbstractGoogleJsonClient$Builder.setBatchPath(Ljava/lang/String;)Lcom/google/api/client/googleapis/services/AbstractGoogleClient$Builder; java.lang.NoSuchMethodError: com.google.api.client.googleapis.services.json.AbstractGoogleJsonClient$Builder.setBatchPath(Ljava/lang/String;)Lcom/google/api/client/googleapis/services/AbstractGoogleClient$Builder; at com.google.api.services.storage.Storage$Builder.setBatchPath(Storage.java:11057) at com.google.api.services.storage.Storage$Builder.<init>(Storage.java:11036) at io.digdag.standards.operator.gcp.GcsClient.client(GcsClient.java:38) at io.digdag.standards.operator.gcp.GcsClient.client(GcsClient.java:21) at io.digdag.standards.operator.gcp.BaseGcpClient.<init>(BaseGcpClient.java:30) at io.digdag.standards.operator.gcp.GcsClient.<init>(GcsClient.java:28) at io.digdag.standards.operator.gcp.GcsClient$Factory.create(GcsClient.java:70) at io.digdag.standards.operator.gcp.BaseGcsOperator.run(BaseGcsOperator.java:27) at io.digdag.standards.operator.gcp.BaseGcpOperator.runTask(BaseGcpOperator.java:25) at io.digdag.util.BaseOperator.run(BaseOperator.java:35) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:365) at io.digdag.cli.Run$OperatorManagerWithSkip.callExecutor(Run.java:707) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:298) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$2(OperatorManager.java:151) at io.digdag.core.agent.LocalWorkspaceManager.withExtractedArchive(LocalWorkspaceManager.java:25) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:149) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:132) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:689) at io.digdag.core.agent.MultiThreadAgent.lambda$null$0(MultiThreadAgent.java:132) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ^C%  config-file",no-bug,0.9
129,digdag,https://github.com/treasure-data/digdag/issues/129,digdag init check numeric verification.,"Please add validation `digdag init` when project name start with a numeric.  digdag init 01_hoge 2016-06-17 22:25:01 +0900: Digdag v0.8.1 Creating 01_hoge/.gitignore Creating 01_hoge/tasks/shell_sample.sh Creating 01_hoge/tasks/repeat_hello.sh Creating 01_hoge/tasks/__init__.py Creating 01_hoge/01_hoge.dig Done. Type `cd 01_hoge` and then `digdag run 01_hoge.dig` to run the workflow. Enjoy!   cd 01_hoge/ digdag run 01_hoge.dig 2016-06-17 22:25:16 +0900: Digdag v0.8.1 error: Validating project failed workflow /private/tmp/01_hoge/01_hoge.dig Validating workflow failed name can't start with a numeric digit (0-9): ""01_hoge"" ",source-file,"digdag init check numeric verification. Please add validation `digdag init` when project name start with a numeric.  digdag init 01_hoge 2016-06-17 22:25:01 +0900: Digdag v0.8.1 Creating 01_hoge/.gitignore Creating 01_hoge/tasks/shell_sample.sh Creating 01_hoge/tasks/repeat_hello.sh Creating 01_hoge/tasks/__init__.py Creating 01_hoge/01_hoge.dig Done. Type `cd 01_hoge` and then `digdag run 01_hoge.dig` to run the workflow. Enjoy!   cd 01_hoge/ digdag run 01_hoge.dig 2016-06-17 22:25:16 +0900: Digdag v0.8.1 error: Validating project failed workflow /private/tmp/01_hoge/01_hoge.dig Validating workflow failed name can't start with a numeric digit (0-9): ""01_hoge""  source-file",no-bug,0.9
11,digdag,https://github.com/treasure-data/digdag/issues/11,td-ddl: empty_tables should be renamed to replace_table,empty is adjective while create and drop_table is verb.,documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file,td-ddl: empty_tables should be renamed to replace_table empty is adjective while create and drop_table is verb. documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file source-file source-file source-file,no-bug,0.8
6,digdag,https://github.com/treasure-data/digdag/issues/6,Reading td api key from TD_API_KEY env or .td/td.conf file,"We would like to avoid embedding td-api-key in the digdag yml file. How about making apikey parameter optional for td executor? Even if API key is not given, td-client-java can read TD_API_KEY or .td/td.conf file.",source-file | source-file | source-file | source-file,"Reading td api key from TD_API_KEY env or .td/td.conf file We would like to avoid embedding td-api-key in the digdag yml file. How about making apikey parameter optional for td executor? Even if API key is not given, td-client-java can read TD_API_KEY or .td/td.conf file. source-file source-file source-file source-file",no-bug,0.9
1352,digdag,https://github.com/treasure-data/digdag/issues/1352,"When clicking a workflow in web UI, once displayed, it turns white. from v0.9.40","When clicking a workflow in web UI, once the content displayed, it turns white page. from v0.9.40 This issue does not happen in v0.9.39. If there is anything I can do, please let me know.",other-file,"When clicking a workflow in web UI, once displayed, it turns white. from v0.9.40 When clicking a workflow in web UI, once the content displayed, it turns white page. from v0.9.40 This issue does not happen in v0.9.39. If there is anything I can do, please let me know. other-file",no-bug,0.9
47,digdag,https://github.com/treasure-data/digdag/issues/47,Server should not require clients to include precompiled .digdag.yml in project archive,"Now, client needs to include .digdag.yml file in a project archive file (tar.gz). But it makes it difficult to create client libraries such as ruby clients. Idea here is that server reads digdag.yml (not .digdag.yml) as described at #45. Clients just create a tar.gz including digdag.yml and send it to server. Archiving is like this command:  tar -C $(basedir path/to/digdag.yml) --exclude="".*"" -czvf - .  Client must include digdag.yml in the archive.",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file,"Server should not require clients to include precompiled .digdag.yml in project archive Now, client needs to include .digdag.yml file in a project archive file (tar.gz). But it makes it difficult to create client libraries such as ruby clients. Idea here is that server reads digdag.yml (not .digdag.yml) as described at #45. Clients just create a tar.gz including digdag.yml and send it to server. Archiving is like this command:  tar -C $(basedir path/to/digdag.yml) --exclude="".*"" -czvf - .  Client must include digdag.yml in the archive. source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file",no-bug,0.8
654,digdag,https://github.com/treasure-data/digdag/issues/654,Proposal for for_range operator,"We have a ETL workflow that needs to process approximately 300 million rows at a time. It is too big to process using one statement. Therefore, we want to divide it into multiple tasks using workflow. Our current approach is this: yaml # Fetch statistics of the source table +count: td>: {data: select min(time) as min_time, max(time) as max_time, count(*) as count from src} store_last_results: true # Divide it into 1 million rows +loop: loop>: ${Math.ceil(td.last_results.count / 1000000.0)} _do: td>: etl_with_time_range.sql time_width: ${Math.max((max_time - min_time) / Math.ceil(td.last_results.count / 1000000.0), 1)} time_cond: | ${td.last_results.min_time + i * parseInt(time_width)} <= time AND time <${td.last_results.min_time + (i + 1) * parseInt(time_width)}  There are several problems with this approach. * Workflow becomes complicated. It's hard to read. * Calculation of ceil and max are tricky and easily lead bugs. * If count is big, this loop creates too many tasks. It should add limitation on number of loop but adding such limitation makes the workflow even more complicated. Proposal here is adding another operator to make it easy. It can make the workflow much simpler: yaml # Fetch statistics of the source table +count: td>: {data: select min(time) as min_time, max(time) as max_time, count(*) as count from src} store_last_results: true # Divide it into 1 million rows +loop: # for the range of [min_time, max_time) for_range>: from: ${td.last_results.min_time} until: ${td.last_results.max_time + 1} # slice the range into this many chunks: count: ${Math.min(td.last_results.count / 1000000, 200)} # at most 200 tasks # it loops for each chunk with `range.from` and `range.until` parameters set _do: td>: etl_with_time_range.sql time_cond: | ${range.from} <= time AND ${range.until} < time ",source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file,"Proposal for for_range operator We have a ETL workflow that needs to process approximately 300 million rows at a time. It is too big to process using one statement. Therefore, we want to divide it into multiple tasks using workflow. Our current approach is this: yaml # Fetch statistics of the source table +count: td>: {data: select min(time) as min_time, max(time) as max_time, count(*) as count from src} store_last_results: true # Divide it into 1 million rows +loop: loop>: ${Math.ceil(td.last_results.count / 1000000.0)} _do: td>: etl_with_time_range.sql time_width: ${Math.max((max_time - min_time) / Math.ceil(td.last_results.count / 1000000.0), 1)} time_cond: | ${td.last_results.min_time + i * parseInt(time_width)} <= time AND time <${td.last_results.min_time + (i + 1) * parseInt(time_width)}  There are several problems with this approach. * Workflow becomes complicated. It's hard to read. * Calculation of ceil and max are tricky and easily lead bugs. * If count is big, this loop creates too many tasks. It should add limitation on number of loop but adding such limitation makes the workflow even more complicated. Proposal here is adding another operator to make it easy. It can make the workflow much simpler: yaml # Fetch statistics of the source table +count: td>: {data: select min(time) as min_time, max(time) as max_time, count(*) as count from src} store_last_results: true # Divide it into 1 million rows +loop: # for the range of [min_time, max_time) for_range>: from: ${td.last_results.min_time} until: ${td.last_results.max_time + 1} # slice the range into this many chunks: count: ${Math.min(td.last_results.count / 1000000, 200)} # at most 200 tasks # it loops for each chunk with `range.from` and `range.until` parameters set _do: td>: etl_with_time_range.sql time_cond: | ${range.from} <= time AND ${range.until} < time  source-file test-file test-file test-file test-file test-file test-file test-file",no-bug,0.95
16,digdag,https://github.com/treasure-data/digdag/issues/16,td:create_table option for WITH statement doesn't work for Hive,`INSERT OVERWRITE TABLE (table) WITH ` is not supported in Hive.,source-file,td:create_table option for WITH statement doesn't work for Hive `INSERT OVERWRITE TABLE (table) WITH ` is not supported in Hive. source-file,no-bug,0.8
1079,digdag,https://github.com/treasure-data/digdag/issues/1079,Authenticator as a System Plugin,"Hi, We need to integrate Digdag within a system using oAuth2. Before starting working on a PR we'd like to have feedback on our plans: 1. Making the Authenticator interface a system plugin and move it to the SPIs. 2. Make the JwtAuthenticator the default plugin to keep things as they are. 3. Create an oAuth2 ""Authenticator"" plugin that could be enabled and configured by properties. 4. Make the WebUI able to do oAuth2 Implicit Grant auth when configured to do so. 1 and 2 would be one PR while 3 and 4 another one. Let me know of any issue with this plan. Thanks, Pierre",source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | documentation-file | source-file | source-file | source-file | source-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file,"Authenticator as a System Plugin Hi, We need to integrate Digdag within a system using oAuth2. Before starting working on a PR we'd like to have feedback on our plans: 1. Making the Authenticator interface a system plugin and move it to the SPIs. 2. Make the JwtAuthenticator the default plugin to keep things as they are. 3. Create an oAuth2 ""Authenticator"" plugin that could be enabled and configured by properties. 4. Make the WebUI able to do oAuth2 Implicit Grant auth when configured to do so. 1 and 2 would be one PR while 3 and 4 another one. Let me know of any issue with this plan. Thanks, Pierre source-file documentation-file documentation-file source-file source-file source-file source-file test-file test-file source-file source-file source-file source-file source-file source-file other-file source-file documentation-file documentation-file source-file source-file source-file source-file test-file test-file source-file source-file source-file source-file source-file source-file other-file source-file documentation-file documentation-file source-file source-file source-file source-file test-file test-file source-file source-file source-file source-file source-file source-file other-file source-file documentation-file source-file source-file source-file source-file test-file test-file source-file source-file source-file source-file source-file source-file other-file",no-bug,0.9
651,digdag,https://github.com/treasure-data/digdag/issues/651,digdag-ui can't show tasklog when giving hash parameter to for_each>:,When giving hash parameter: yaml +step1: echo>: start +step2: +repeat: for_each>: param: - key: p1 - key: p2 - key: p3 _do: +substep: echo>: ${param.key} +step3: echo>: end  Tasks success but digdag-ui fails showing tasklog: ![image](https://user-images.githubusercontent.com/191684/30897780-09de0516-a393-11e7-9561-60f2feddb8fb.png),source-file | source-file,digdag-ui can't show tasklog when giving hash parameter to for_each>: When giving hash parameter: yaml +step1: echo>: start +step2: +repeat: for_each>: param: - key: p1 - key: p2 - key: p3 _do: +substep: echo>: ${param.key} +step3: echo>: end  Tasks success but digdag-ui fails showing tasklog: ![image](https://user-images.githubusercontent.com/191684/30897780-09de0516-a393-11e7-9561-60f2feddb8fb.png) source-file source-file,no-bug,0.8
132,digdag,https://github.com/treasure-data/digdag/issues/132,digdag scheduler -c option can't set database.type parameter,Hello Can I use config path for scheduler? `digdag server -c config.yml` work correctly. But `digdag server -c config.yml` does not work as I expected. config.yml  yaml database.type: postgresql database.user: username database.password: password database.host: 127.0.0.1 database.port: 5432 database.database: digdagdb  `digdag scheduler --project . -l debug -c config.conf` It use h2:mem db  digdag scheduler --project . -l debug -c test.conf 2016-06-18 23:46:54 +0900: Digdag v0.8.1 2016-06-18 23:46:55 +0900 [DEBUG] (main) io.digdag.core.database.DataSourceProvider: Using database URL jdbc:h2:mem:digdag-95ef3df8-241b-4136-9597-61dc7b125cbb 2016-06-18 23:46:55 +0900 [DEBUG] (main) io.digdag.core.database.DataSourceProvider: Using database URL jdbc:h2:mem:digdag-5d9bc29f-a800-4cee-90db-423bc40bc815 2016-06-18 23:46:56 +0900 [INFO] (main): Added new revision 1 2016-06-18 23:46:56 +0900 [DEBUG] (main) org.jboss.logging.LoggerProviders: Logging Provider: org.jboss.logging.Slf4jLoggerProvider ,source-file | config-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | other-file | source-file | source-file | source-file | config-file | config-file | config-file | config-file,digdag scheduler -c option can't set database.type parameter Hello Can I use config path for scheduler? `digdag server -c config.yml` work correctly. But `digdag server -c config.yml` does not work as I expected. config.yml  yaml database.type: postgresql database.user: username database.password: password database.host: 127.0.0.1 database.port: 5432 database.database: digdagdb  `digdag scheduler --project . -l debug -c config.conf` It use h2:mem db  digdag scheduler --project . -l debug -c test.conf 2016-06-18 23:46:54 +0900: Digdag v0.8.1 2016-06-18 23:46:55 +0900 [DEBUG] (main) io.digdag.core.database.DataSourceProvider: Using database URL jdbc:h2:mem:digdag-95ef3df8-241b-4136-9597-61dc7b125cbb 2016-06-18 23:46:55 +0900 [DEBUG] (main) io.digdag.core.database.DataSourceProvider: Using database URL jdbc:h2:mem:digdag-5d9bc29f-a800-4cee-90db-423bc40bc815 2016-06-18 23:46:56 +0900 [INFO] (main): Added new revision 1 2016-06-18 23:46:56 +0900 [DEBUG] (main) org.jboss.logging.LoggerProviders: Logging Provider: org.jboss.logging.Slf4jLoggerProvider  source-file config-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file other-file source-file source-file source-file config-file config-file config-file config-file,no-bug,0.9
915,digdag,https://github.com/treasure-data/digdag/issues/915,retry limit value cannot be set using an _export parameter,"When I set a retry limit value with an _export paramter on the digdag server of version 0.9.31, I got an error of 'Invalid _retry format (config)'. The project file that I used is as follows,  timezone: Asia/Tokyo _export: MAX_RETRY_COUNT: 2 +task1: _retry: ${MAX_RETRY_COUNT} sh>: scripts/fail.sh  On the digdag server of version 0.9.20, which we use for a production environment, I never get the error.",source-file | test-file | source-file | test-file | source-file,"retry limit value cannot be set using an _export parameter When I set a retry limit value with an _export paramter on the digdag server of version 0.9.31, I got an error of 'Invalid _retry format (config)'. The project file that I used is as follows,  timezone: Asia/Tokyo _export: MAX_RETRY_COUNT: 2 +task1: _retry: ${MAX_RETRY_COUNT} sh>: scripts/fail.sh  On the digdag server of version 0.9.20, which we use for a production environment, I never get the error. source-file test-file source-file test-file source-file",no-bug,0.8
643,digdag,https://github.com/treasure-data/digdag/issues/643,symbolic link causes NoSuchFileException,"after creating a symbolic link(no job is calling symbolic link, seems might be somewhere with verifying files), in Web UI, getting errors below: > 2017-09-06 12:16:10.169 +1000 [ERROR] (0088@[0:testing]+testing+testing+job1) io.digdag.core.agent.OperatorManager: Task failed with unexpected error: /tmp/digdag-tempdir1542790697699403398/workspace/+testing+testing+job1_1760033870796559295/0.sh > java.nio.file.NoSuchFileException: /tmp/digdag-tempdir1542790697699403398/workspace/+testing+testing+job1_1760033870796559295/0.sh > at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) > at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) > at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) > at sun.nio.fs.UnixFileAttributeViews$Posix.setMode(UnixFileAttributeViews.java:238) > at sun.nio.fs.UnixFileAttributeViews$Posix.setPermissions(UnixFileAttributeViews.java:260) > at java.nio.file.Files.setPosixFilePermissions(Files.java:2045) > at io.digdag.core.archive.ProjectArchives.extractArchive(ProjectArchives.java:82) > at io.digdag.core.archive.ProjectArchives.extractTarArchive(ProjectArchives.java:40) > at io.digdag.core.archive.ProjectArchives.extractTarArchive(ProjectArchives.java:33) > at io.digdag.core.agent.ExtractArchiveWorkspaceManager.withExtractedArchive(ExtractArchiveWorkspaceManager.java:34) > at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:135) > at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:119) > at io.digdag.core.agent.MultiThreadAgent.lambda$null$0(MultiThreadAgent.java:127) > at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) > at java.util.concurrent.FutureTask.run(FutureTask.java:266) > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) > at java.lang.Thread.run(Thread.java:748) steps to replicate this issue: mkdir testing cd testing touch 1.sh ln -s ./1.sh 0.sh testing.dig  _export: workflow_name: workflow1 schedule: minutes_interval>: 5 skip_on_overtime: true +testing: +job1: sh>: ./1.sh  digdag push testing go to web UI, run it. digdag download also fails: > digdag download testing > 2017-09-06 12:19:25 +1000: Digdag v0.9.13 > testing/testing.dig > testing/0.sh -> 1.sh > error: /tmp/t2/testing/0.sh (no such file)",source-file,"symbolic link causes NoSuchFileException after creating a symbolic link(no job is calling symbolic link, seems might be somewhere with verifying files), in Web UI, getting errors below: > 2017-09-06 12:16:10.169 +1000 [ERROR] (0088@[0:testing]+testing+testing+job1) io.digdag.core.agent.OperatorManager: Task failed with unexpected error: /tmp/digdag-tempdir1542790697699403398/workspace/+testing+testing+job1_1760033870796559295/0.sh > java.nio.file.NoSuchFileException: /tmp/digdag-tempdir1542790697699403398/workspace/+testing+testing+job1_1760033870796559295/0.sh > at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) > at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) > at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) > at sun.nio.fs.UnixFileAttributeViews$Posix.setMode(UnixFileAttributeViews.java:238) > at sun.nio.fs.UnixFileAttributeViews$Posix.setPermissions(UnixFileAttributeViews.java:260) > at java.nio.file.Files.setPosixFilePermissions(Files.java:2045) > at io.digdag.core.archive.ProjectArchives.extractArchive(ProjectArchives.java:82) > at io.digdag.core.archive.ProjectArchives.extractTarArchive(ProjectArchives.java:40) > at io.digdag.core.archive.ProjectArchives.extractTarArchive(ProjectArchives.java:33) > at io.digdag.core.agent.ExtractArchiveWorkspaceManager.withExtractedArchive(ExtractArchiveWorkspaceManager.java:34) > at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:135) > at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:119) > at io.digdag.core.agent.MultiThreadAgent.lambda$null$0(MultiThreadAgent.java:127) > at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) > at java.util.concurrent.FutureTask.run(FutureTask.java:266) > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) > at java.lang.Thread.run(Thread.java:748) steps to replicate this issue: mkdir testing cd testing touch 1.sh ln -s ./1.sh 0.sh testing.dig  _export: workflow_name: workflow1 schedule: minutes_interval>: 5 skip_on_overtime: true +testing: +job1: sh>: ./1.sh  digdag push testing go to web UI, run it. digdag download also fails: > digdag download testing > 2017-09-06 12:19:25 +1000: Digdag v0.9.13 > testing/testing.dig > testing/0.sh -> 1.sh > error: /tmp/t2/testing/0.sh (no such file) source-file",no-bug,0.9
658,digdag,https://github.com/treasure-data/digdag/issues/658,digdag-ui can't show task log when use of HTTP Basic Authentication,"Hi. digdag-ui can't show task log when connected through a proxy server. Using Nginx and restricting access with HTTP Basic Authentication. In that situation, we always get a `401 (Unauthorized)` Error. See below. (Left: Normal, Right: Through Proxy) ![2017-10-02 21 04 39](https://user-images.githubusercontent.com/4961885/31076499-5ada6272-a7b5-11e7-9b88-6bedb734ef64.png)",source-file,"digdag-ui can't show task log when use of HTTP Basic Authentication Hi. digdag-ui can't show task log when connected through a proxy server. Using Nginx and restricting access with HTTP Basic Authentication. In that situation, we always get a `401 (Unauthorized)` Error. See below. (Left: Normal, Right: Through Proxy) ![2017-10-02 21 04 39](https://user-images.githubusercontent.com/4961885/31076499-5ada6272-a7b5-11e7-9b88-6bedb734ef64.png) source-file",bug,0.9
1663,digdag,https://github.com/treasure-data/digdag/issues/1663,SELinux with docker needs :Z volume mount option,"When using SELinux with docker, volumes should be mounted with the `:Z` [mount option](https://docs.docker.com/storage/bind-mounts/#configure-the-selinux-label). This is set here: https://github.com/treasure-data/digdag/blob/07c11785b6f5c8c2469a94ec764e1d1a448be667/digdag-standards/src/main/java/io/digdag/standards/command/DockerCommandExecutor.java#L115-L116 When using SELinux, this should be: java command.add(""-v"").add(String.format(ENGLISH, ""%s:%s:rw,Z"", projectPath, projectPath)); // use projectPath to keep pb.directory() valid  Maybe a config option could be introduced: yaml _export: docker: image: ""python:3.8-slim"" pull_always: true selinux: true ",source-file | documentation-file | source-file,"SELinux with docker needs :Z volume mount option When using SELinux with docker, volumes should be mounted with the `:Z` [mount option](https://docs.docker.com/storage/bind-mounts/#configure-the-selinux-label). This is set here: https://github.com/treasure-data/digdag/blob/07c11785b6f5c8c2469a94ec764e1d1a448be667/digdag-standards/src/main/java/io/digdag/standards/command/DockerCommandExecutor.java#L115-L116 When using SELinux, this should be: java command.add(""-v"").add(String.format(ENGLISH, ""%s:%s:rw,Z"", projectPath, projectPath)); // use projectPath to keep pb.directory() valid  Maybe a config option could be introduced: yaml _export: docker: image: ""python:3.8-slim"" pull_always: true selinux: true  source-file documentation-file source-file",no-bug,0.9
65,digdag,https://github.com/treasure-data/digdag/issues/65,Fail when '_' is not used as a prefix as required for certain parameters,"When a user tries using a parameter that requires a '_' at as a prefix, but doesn't include it, it should fail and mentioned the the '_' is required. For example, if a user doesn't include the '_' as a prefix in '_parallel', then the workflow is executed sequentially, but it's not necessarily obvious that it's not operating as the user expected.",source-file | test-file | test-file,"Fail when '_' is not used as a prefix as required for certain parameters When a user tries using a parameter that requires a '_' at as a prefix, but doesn't include it, it should fail and mentioned the the '_' is required. For example, if a user doesn't include the '_' as a prefix in '_parallel', then the workflow is executed sequentially, but it's not necessarily obvious that it's not operating as the user expected. source-file test-file test-file",bug,0.85
25,digdag,https://github.com/treasure-data/digdag/issues/25,for_each parameterization capability is limited,"`for_each>` parameters can be parameterized like below by utilizing YAML anchors:  yaml run: +parameterized_for_each _export: foos: &FOOS - 1 - 2 +parameterized_for_each: for_each>: foo: *FOOS _do: sh>: ""echo hello ${foo}""  But it might be useful to allow for parameterizing `for_each>` using actual digdag parameters. E.g.  yaml run: +parameterized_for_each _export: foos: - 1 - 2 +parameterized_for_each: for_each>: foo: @foos _do: sh>: ""echo hello ${foo}""  And using parameters explicitly set by e.g. a `py>` task:  yaml run: +main +main: +export_foos: py>: tasks.export_foos +parameterized_for_each: for_each>: foo: ${foos} _do: sh>: ""echo hello ${foo}""   python import digdag def export_foos(): digdag.env.store({""foos"": [1, 2]})  Attempting to parameterize `for_each>` like this currently fails with the below error:  2016-03-30 10:18:17 +0900: Digdag v0.4.2 2016-03-30 10:18:18 +0900 [WARN] (main): Reusing the last session time 2016-03-29T00:00:00+09:00. 2016-03-30 10:18:18 +0900 [INFO] (main): Using session digdag.status/20160329T000000+0900. 2016-03-30 10:18:18 +0900 [INFO] (main): Starting a new session repository id=1 workflow name=+main session_time=2016-03-29T00:00:00+09:00 2016-03-30 10:18:19 +0900 [INFO] (0020@+main+export_foos): py>: tasks.export_foos 2016-03-30 10:18:19 +0900 [INFO] (0020@+main+parameterized_for_each): for_each>: {foo=[1,2]} 2016-03-30 10:18:19 +0900 [ERROR] (0020@+main+parameterized_for_each): Task failed io.digdag.client.config.ConfigException: Expected array type for key 'foo' but got ""[1,2]"" (string) at io.digdag.client.config.Config.propagateConvertException(Config.java:400) at io.digdag.client.config.Config.readObject(Config.java:391) at io.digdag.client.config.Config.get(Config.java:235) at io.digdag.client.config.Config.getList(Config.java:277) at io.digdag.standards.operator.ForEachOperatorFactory$ForEachOperator.runTask(ForEachOperatorFactory.java:67) at io.digdag.standards.operator.BaseOperator.run(BaseOperator.java:49) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:238) at io.digdag.cli.Run$OperatorManagerWithSkip.callExecutor(Run.java:653) at io.digdag.core.agent.OperatorManager.runWithArchive(OperatorManager.java:193) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$1(OperatorManager.java:130) at io.digdag.core.agent.CurrentDirectoryArchiveManager.withExtractedArchive(CurrentDirectoryArchiveManager.java:20) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:129) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:107) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:635) at io.digdag.core.agent.LocalAgent.lambda$run$0(LocalAgent.java:61) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: com.fasterxml.jackson.databind.JsonMappingException: Can not deserialize instance of java.util.ArrayList out of VALUE_STRING token at [Source: N/A; line: -1, column: -1] at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148) at com.fasterxml.jackson.databind.DeserializationContext.mappingException(DeserializationContext.java:854) at com.fasterxml.jackson.databind.DeserializationContext.mappingException(DeserializationContext.java:850) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.handleNonArray(CollectionDeserializer.java:292) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:227) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:217) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:25) at com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:3703) at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2072) at io.digdag.client.config.Config.readObject(Config.java:388)  18 common frames omitted error: * +main+parameterized_for_each: Expected array type for key 'foo' but got ""[1,2]"" (string) Task state is saved at digdag.status/20160329T000000+0900 directory. Run command with --session '2016-03-29 00:00:00' argument to retry failed tasks.  I'm uncertain whether the best approach would be to make `${}` expansion more intelligent about types of parameters and not coerce everything to a string, or if it would make more sense to introduce another syntax.",source-file | source-file | source-file | test-file,"for_each parameterization capability is limited `for_each>` parameters can be parameterized like below by utilizing YAML anchors:  yaml run: +parameterized_for_each _export: foos: &FOOS - 1 - 2 +parameterized_for_each: for_each>: foo: *FOOS _do: sh>: ""echo hello ${foo}""  But it might be useful to allow for parameterizing `for_each>` using actual digdag parameters. E.g.  yaml run: +parameterized_for_each _export: foos: - 1 - 2 +parameterized_for_each: for_each>: foo: @foos _do: sh>: ""echo hello ${foo}""  And using parameters explicitly set by e.g. a `py>` task:  yaml run: +main +main: +export_foos: py>: tasks.export_foos +parameterized_for_each: for_each>: foo: ${foos} _do: sh>: ""echo hello ${foo}""   python import digdag def export_foos(): digdag.env.store({""foos"": [1, 2]})  Attempting to parameterize `for_each>` like this currently fails with the below error:  2016-03-30 10:18:17 +0900: Digdag v0.4.2 2016-03-30 10:18:18 +0900 [WARN] (main): Reusing the last session time 2016-03-29T00:00:00+09:00. 2016-03-30 10:18:18 +0900 [INFO] (main): Using session digdag.status/20160329T000000+0900. 2016-03-30 10:18:18 +0900 [INFO] (main): Starting a new session repository id=1 workflow name=+main session_time=2016-03-29T00:00:00+09:00 2016-03-30 10:18:19 +0900 [INFO] (0020@+main+export_foos): py>: tasks.export_foos 2016-03-30 10:18:19 +0900 [INFO] (0020@+main+parameterized_for_each): for_each>: {foo=[1,2]} 2016-03-30 10:18:19 +0900 [ERROR] (0020@+main+parameterized_for_each): Task failed io.digdag.client.config.ConfigException: Expected array type for key 'foo' but got ""[1,2]"" (string) at io.digdag.client.config.Config.propagateConvertException(Config.java:400) at io.digdag.client.config.Config.readObject(Config.java:391) at io.digdag.client.config.Config.get(Config.java:235) at io.digdag.client.config.Config.getList(Config.java:277) at io.digdag.standards.operator.ForEachOperatorFactory$ForEachOperator.runTask(ForEachOperatorFactory.java:67) at io.digdag.standards.operator.BaseOperator.run(BaseOperator.java:49) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:238) at io.digdag.cli.Run$OperatorManagerWithSkip.callExecutor(Run.java:653) at io.digdag.core.agent.OperatorManager.runWithArchive(OperatorManager.java:193) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$1(OperatorManager.java:130) at io.digdag.core.agent.CurrentDirectoryArchiveManager.withExtractedArchive(CurrentDirectoryArchiveManager.java:20) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:129) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:107) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:635) at io.digdag.core.agent.LocalAgent.lambda$run$0(LocalAgent.java:61) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: com.fasterxml.jackson.databind.JsonMappingException: Can not deserialize instance of java.util.ArrayList out of VALUE_STRING token at [Source: N/A; line: -1, column: -1] at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:148) at com.fasterxml.jackson.databind.DeserializationContext.mappingException(DeserializationContext.java:854) at com.fasterxml.jackson.databind.DeserializationContext.mappingException(DeserializationContext.java:850) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.handleNonArray(CollectionDeserializer.java:292) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:227) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:217) at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:25) at com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:3703) at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2072) at io.digdag.client.config.Config.readObject(Config.java:388)  18 common frames omitted error: * +main+parameterized_for_each: Expected array type for key 'foo' but got ""[1,2]"" (string) Task state is saved at digdag.status/20160329T000000+0900 directory. Run command with --session '2016-03-29 00:00:00' argument to retry failed tasks.  I'm uncertain whether the best approach would be to make `${}` expansion more intelligent about types of parameters and not coerce everything to a string, or if it would make more sense to introduce another syntax. source-file source-file source-file test-file",no-bug,0.9
1748,digdag,https://github.com/treasure-data/digdag/issues/1748,node-sass is deprecated,Moving onto dart-sass is recommended. https://sass-lang.com/blog/libsass-is-deprecated Major drawbacks of node-sass: - No new features will be added. - `npm install` fails on macOS. https://medium.com/macoclock/fix-broken-node-gyp-issue-for-mac-os-596e4a8bcffd,other-file | source-file | documentation-file | documentation-file | source-file | other-file | source-file | documentation-file | documentation-file | test-file | source-file,node-sass is deprecated Moving onto dart-sass is recommended. https://sass-lang.com/blog/libsass-is-deprecated Major drawbacks of node-sass: - No new features will be added. - `npm install` fails on macOS. https://medium.com/macoclock/fix-broken-node-gyp-issue-for-mac-os-596e4a8bcffd other-file source-file documentation-file documentation-file source-file other-file source-file documentation-file documentation-file test-file source-file,no-bug,0.95
811,digdag,https://github.com/treasure-data/digdag/issues/811,How about add a link to docs.digdag.io to README,I tried to search the way installing digdag on Windows10 around https://github.com/treasure-data/digdag and could not find the way for a while. I think that the link to www.digdag.io in https://github.com/treasure-data/digdag#Documentation is ambiguous if you are beginner and you want to install digdag first time. To link docs.digdag.io directly will make the instruction more clear.,documentation-file | documentation-file,How about add a link to docs.digdag.io to README I tried to search the way installing digdag on Windows10 around https://github.com/treasure-data/digdag and could not find the way for a while. I think that the link to www.digdag.io in https://github.com/treasure-data/digdag#Documentation is ambiguous if you are beginner and you want to install digdag first time. To link docs.digdag.io directly will make the instruction more clear. documentation-file documentation-file,no-bug,0.95
21,digdag,https://github.com/treasure-data/digdag/issues/21,digdag should fail on multiple operators within a task,"I.e., the below workflow definition should fail instead of silently just executing `echo bar`.  yaml run: +foo +foo: sh>: echo bar py>: baz  Also, it might be worth considering if multiple identical keys should fail. In the below workflow definition the second `sh>` operator implicitly overrides the first operator when the yaml is parsed because yaml.  yaml run: +foo +foo: sh>: echo bar sh>: echo baz ",source-file | test-file | test-file | test-file | source-file | source-file | test-file | source-file,"digdag should fail on multiple operators within a task I.e., the below workflow definition should fail instead of silently just executing `echo bar`.  yaml run: +foo +foo: sh>: echo bar py>: baz  Also, it might be worth considering if multiple identical keys should fail. In the below workflow definition the second `sh>` operator implicitly overrides the first operator when the yaml is parsed because yaml.  yaml run: +foo +foo: sh>: echo bar sh>: echo baz  source-file test-file test-file test-file source-file source-file test-file source-file",no-bug,0.9
162,digdag,https://github.com/treasure-data/digdag/issues/162,Long file error while archiving logs,"I faced with the error when I tried to update my workflow using `digdag push default -r ""$(git show --pretty=format:'%T' | head -n 1)""` The error is as below:  2016-07-07 17:18:39 +0900: Digdag v0.8.3 Creating .digdag/tmp/archive-4904269853204965434.tar.gz Archiving log/2016-07-06/0.1daily_batch@20160707T000000+0900_bd59fcda-cda6-4e6e-a6d8-dbeb840d9fdb/+daily_batch+vac uum_reindex_and_cache_update+vacuum_reindex+vacuum_reindex_end_notify@577debe02d1ee740.8483@xxxxxxx.log.gz error: file name 'log/2016-07-06/0.1daily_batch@20160707T000000+0900_bd59fcda-cda6-4e6e-a6d8-dbeb840d9fdb/+daily_bat ch+vacuum_reindex_and_cache_update+vacuum_reindex+vacuum_reindex_end_notify@577debe02d1ee740.8483@xxxxxxx.log.gz' is too long ( > 100 bytes)  Finally, I shut down the digdag server and cleared build-in db files and log files, restarted server and re-tried pushing new workflow, fixed the problem. I wonder if anything wrong around TarArchiveOutputStream. Any comments would be appreciated. Thanks.",source-file | source-file,"Long file error while archiving logs I faced with the error when I tried to update my workflow using `digdag push default -r ""$(git show --pretty=format:'%T' | head -n 1)""` The error is as below:  2016-07-07 17:18:39 +0900: Digdag v0.8.3 Creating .digdag/tmp/archive-4904269853204965434.tar.gz Archiving log/2016-07-06/0.1daily_batch@20160707T000000+0900_bd59fcda-cda6-4e6e-a6d8-dbeb840d9fdb/+daily_batch+vac uum_reindex_and_cache_update+vacuum_reindex+vacuum_reindex_end_notify@577debe02d1ee740.8483@xxxxxxx.log.gz error: file name 'log/2016-07-06/0.1daily_batch@20160707T000000+0900_bd59fcda-cda6-4e6e-a6d8-dbeb840d9fdb/+daily_bat ch+vacuum_reindex_and_cache_update+vacuum_reindex+vacuum_reindex_end_notify@577debe02d1ee740.8483@xxxxxxx.log.gz' is too long ( > 100 bytes)  Finally, I shut down the digdag server and cleared build-in db files and log files, restarted server and re-tried pushing new workflow, fixed the problem. I wonder if anything wrong around TarArchiveOutputStream. Any comments would be appreciated. Thanks. source-file source-file",no-bug,0.95
7,digdag,https://github.com/treasure-data/digdag/issues/7,create_table option doesn't work for Hive,INSERT INTO OVERWRITE xxxx AS (query) is not supported yet in TD.,source-file | source-file,create_table option doesn't work for Hive INSERT INTO OVERWRITE xxxx AS (query) is not supported yet in TD. source-file source-file,no-bug,0.7
548,digdag,https://github.com/treasure-data/digdag/issues/548,!include requires whitespace before colon,"I'm trying to include a bunch of secret keys into my dig file. When i do !include: 'secrets.dig' It fails, but when I add the (seemingly) extra space, it works: !include : 'secrets.dig' This seems to defy convention for all other examples.",documentation-file,"!include requires whitespace before colon I'm trying to include a bunch of secret keys into my dig file. When i do !include: 'secrets.dig' It fails, but when I add the (seemingly) extra space, it works: !include : 'secrets.dig' This seems to defy convention for all other examples. documentation-file",no-bug,0.9
356,digdag,https://github.com/treasure-data/digdag/issues/356,SMTP auth always fails,"I want to use Google Apps's SMPT server for sending mails. For this i have an own domain and it is hosted with Google Apps e.g.: example.com When using now a user with the right password and a username like admin@example.com, the sending fails with Google Apps reporting an Authentication Error with username or password wrong. This was tested with: 1)mail.dig _export: mail.host: smtp.gmail.com mail.port: 587 mail.from: admin@example.com mail.username: admin@example.com mail.password: PASSWORD mail.debug: true +digdag: mail>: mail.txt subject: test mail to: admin@example.com 2.) digdag run mail.dig Here on debug the error was given: DEBUG SMTP: protocolConnect login, host=smtp.gmail.com, user=admin@example.com, password=<non-null> DEBUG SMTP: Attempt to authenticate using mechanisms: LOGIN PLAIN DIGEST-MD5 NTLM XOAUTH2 DEBUG SMTP: Using mechanism LOGIN DEBUG SMTP: AUTH LOGIN command trace suppressed DEBUG SMTP: AUTH LOGIN failed",documentation-file,"SMTP auth always fails I want to use Google Apps's SMPT server for sending mails. For this i have an own domain and it is hosted with Google Apps e.g.: example.com When using now a user with the right password and a username like admin@example.com, the sending fails with Google Apps reporting an Authentication Error with username or password wrong. This was tested with: 1)mail.dig _export: mail.host: smtp.gmail.com mail.port: 587 mail.from: admin@example.com mail.username: admin@example.com mail.password: PASSWORD mail.debug: true +digdag: mail>: mail.txt subject: test mail to: admin@example.com 2.) digdag run mail.dig Here on debug the error was given: DEBUG SMTP: protocolConnect login, host=smtp.gmail.com, user=admin@example.com, password=<non-null> DEBUG SMTP: Attempt to authenticate using mechanisms: LOGIN PLAIN DIGEST-MD5 NTLM XOAUTH2 DEBUG SMTP: Using mechanism LOGIN DEBUG SMTP: AUTH LOGIN command trace suppressed DEBUG SMTP: AUTH LOGIN failed documentation-file",no-bug,0.9
35,digdag,https://github.com/treasure-data/digdag/issues/35,digdag sometimes uses local ./digdag.yml scope and sometimes global scope," digdag schedules 2016-04-11 15:04:37 -0700: Digdag v0.5.9 Schedules: id: 2 repository: dano-churn-prediction-poc workflow: +main next session time: 2016-04-13 00:00:00 +0900 next runs at: 2016-04-12 15:00:00 -0700 (23h 55m 21s later) 1 entries. Use `digdag workflows +NAME` to show workflow details.  and  digdag check 2016-04-11 15:04:54 -0700: Digdag v0.5.9 System default timezone: America/Los_Angeles Definitions (1 workflows): +main (1 tasks) Parameters: timezone: ""America/Los_Angeles"" Schedules (0 entries):  As a user I'm really confused about how scheduling works =)",config-file | source-file | test-file | source-file | source-file | source-file | test-file | config-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | source-file | source-file | source-file | test-file | config-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | source-file | source-file | source-file | test-file | config-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | source-file | source-file | source-file | test-file | test-file | config-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | source-file | source-file | test-file | test-file | test-file | test-file,"digdag sometimes uses local ./digdag.yml scope and sometimes global scope  digdag schedules 2016-04-11 15:04:37 -0700: Digdag v0.5.9 Schedules: id: 2 repository: dano-churn-prediction-poc workflow: +main next session time: 2016-04-13 00:00:00 +0900 next runs at: 2016-04-12 15:00:00 -0700 (23h 55m 21s later) 1 entries. Use `digdag workflows +NAME` to show workflow details.  and  digdag check 2016-04-11 15:04:54 -0700: Digdag v0.5.9 System default timezone: America/Los_Angeles Definitions (1 workflows): +main (1 tasks) Parameters: timezone: ""America/Los_Angeles"" Schedules (0 entries):  As a user I'm really confused about how scheduling works =) config-file source-file test-file source-file source-file source-file test-file config-file source-file source-file source-file source-file test-file test-file test-file test-file test-file config-file source-file test-file source-file source-file source-file test-file config-file source-file source-file source-file source-file test-file test-file test-file test-file test-file config-file source-file test-file source-file source-file source-file test-file config-file source-file source-file source-file source-file test-file test-file test-file test-file test-file config-file source-file test-file source-file source-file source-file test-file test-file config-file source-file source-file source-file source-file source-file test-file test-file source-file source-file test-file test-file test-file test-file",no-bug,0.9
10,digdag,https://github.com/treasure-data/digdag/issues/10,ImmutableSessionAttempt class is missing,Forget to push?,source-file | test-file,ImmutableSessionAttempt class is missing Forget to push? source-file test-file,no-bug,0.3
1117,digdag,https://github.com/treasure-data/digdag/issues/1117,Session status filter can't return correct list at failure and pending status.,"Hi. I'm using digdag-ui at version 0.9.36. I found following out that part of session status filter return incorrect list. <img width=""1099"" src=""https://user-images.githubusercontent.com/1973465/58151136-be8dfa80-7ca3-11e9-89b9-d28162ddb653.png""> <img width=""1093"" src=""https://user-images.githubusercontent.com/1973465/58151145-c5b50880-7ca3-11e9-9d1e-7c36fd35cd48.png""> <img width=""1083"" src=""https://user-images.githubusercontent.com/1973465/58151150-c9488f80-7ca3-11e9-860f-1f7878eb4213.png""> Failure and pending filtered list has contain different status sessions. I think this bug is occurring from following code. https://github.com/treasure-data/digdag/blob/master/digdag-ui/console.jsx#L664",other-file,"Session status filter can't return correct list at failure and pending status. Hi. I'm using digdag-ui at version 0.9.36. I found following out that part of session status filter return incorrect list. <img width=""1099"" src=""https://user-images.githubusercontent.com/1973465/58151136-be8dfa80-7ca3-11e9-89b9-d28162ddb653.png""> <img width=""1093"" src=""https://user-images.githubusercontent.com/1973465/58151145-c5b50880-7ca3-11e9-9d1e-7c36fd35cd48.png""> <img width=""1083"" src=""https://user-images.githubusercontent.com/1973465/58151150-c9488f80-7ca3-11e9-860f-1f7878eb4213.png""> Failure and pending filtered list has contain different status sessions. I think this bug is occurring from following code. https://github.com/treasure-data/digdag/blob/master/digdag-ui/console.jsx#L664 other-file",no-bug,0.8
676,digdag,https://github.com/treasure-data/digdag/issues/676,"`--session ""yyyy-MM-dd HH:mm:ss""` is fail on Windows","I try digdag on windows. `--session ""yyyy-MM-dd HH:mm:ss""` is fail. Environment  - OS: Windows 10 Pro 64bit - Java: 1.8.0.151 - digdag: 0.9.20 Work log  powershell PS> # create test directory PS> cd ~/tmp PS> mkdir digdagtest PS> cd digdagtest PS> # get digdag PS> Invoke-WebRequest http://dl.digdag.io/digdag-latest.jar -OutFile digdag.bat PS> # run sample with session PS> .\digdag.bat init mydag PS> cd mydag PS> ..\digdag.bat run .\mydag.dig --session ""2017-11-01 00:00:00"" 2017-11-01 02:18:50 +0900: Digdag v0.9.20 error: Task pattern '00:00:00' doesn't match with any tasks. (no match)  Cause  Process removed session value's double quote in this line. https://github.com/treasure-data/digdag/blob/8b8aa86bed9947e389a2c82fef7673f4efedc233/digdag-cli/src/main/sh/selfrun.sh#L49-L54 Need re-quote arg in this line. https://github.com/treasure-data/digdag/blob/8b8aa86bed9947e389a2c82fef7673f4efedc233/digdag-cli/src/main/sh/selfrun.sh#L57",config-file,"`--session ""yyyy-MM-dd HH:mm:ss""` is fail on Windows I try digdag on windows. `--session ""yyyy-MM-dd HH:mm:ss""` is fail. Environment  - OS: Windows 10 Pro 64bit - Java: 1.8.0.151 - digdag: 0.9.20 Work log  powershell PS> # create test directory PS> cd ~/tmp PS> mkdir digdagtest PS> cd digdagtest PS> # get digdag PS> Invoke-WebRequest http://dl.digdag.io/digdag-latest.jar -OutFile digdag.bat PS> # run sample with session PS> .\digdag.bat init mydag PS> cd mydag PS> ..\digdag.bat run .\mydag.dig --session ""2017-11-01 00:00:00"" 2017-11-01 02:18:50 +0900: Digdag v0.9.20 error: Task pattern '00:00:00' doesn't match with any tasks. (no match)  Cause  Process removed session value's double quote in this line. https://github.com/treasure-data/digdag/blob/8b8aa86bed9947e389a2c82fef7673f4efedc233/digdag-cli/src/main/sh/selfrun.sh#L49-L54 Need re-quote arg in this line. https://github.com/treasure-data/digdag/blob/8b8aa86bed9947e389a2c82fef7673f4efedc233/digdag-cli/src/main/sh/selfrun.sh#L57 config-file",no-bug,0.9
536,digdag,https://github.com/treasure-data/digdag/issues/536,Skip backfill,"Hi, When the digdag server restarts it tries to backfill/catchup all the schedules for which digdag was not running. This is fine most of time, but, with heavy operations, backfill can slow the server down considerably. I suggest that a new option be added (say disable_backfill) to schedule config that will disable backfill. e.g,  schedule: minutes_interval>: 1 disable_backfill: true ",source-file | test-file | source-file | test-file | source-file | test-file | source-file | test-file,"Skip backfill Hi, When the digdag server restarts it tries to backfill/catchup all the schedules for which digdag was not running. This is fine most of time, but, with heavy operations, backfill can slow the server down considerably. I suggest that a new option be added (say disable_backfill) to schedule config that will disable backfill. e.g,  schedule: minutes_interval>: 1 disable_backfill: true  source-file test-file source-file test-file source-file test-file source-file test-file",no-bug,0.9
70,digdag,https://github.com/treasure-data/digdag/issues/70,project structure and workflow suffix changes,"- no `digdag.yml` project file - workflow file suffixes change from `.yml` to `.dig` - all `.dig` files in project directory are picked up automatically by digdag - digdag cli gains a `--project` option to specify the project directory - `digdag run` `-f` option is removed - `digdag init` creates: - `.digdag` directory in project directory - `.digdag/config` file - digdag cli looks for `.digdag` directory, starting in current directory and then recurses into parent directories (like git). - `digdag push` pushes entire project (as identified by `.digdag`) in some parent dir, even if executed in a subdir. - filenames referenced by a `.dig` file are resolved relative to the path of the `.dig` file so that given a project with:  .digdag/config subdir/workflow.dig queries/foo.sql  Then a user can both be in the project dir and do:  digdag run subdir/workflow  Or  cd subdir/ digdag run workflow  with the same result. The `subdir/workflow.dig` file must reference the query file as `../queries/foo.sql`. - digdag does not change ""CWD"" when executing a workflow. - `digdag run` `<workflow name>` parameter can include `.dig` suffix so that the below invocation semantics are identical: - `digdag run foo` - `digdag run foo.dig`",source-file | source-file | source-file | source-file | source-file | documentation-file | test-file | test-file | test-file,"project structure and workflow suffix changes - no `digdag.yml` project file - workflow file suffixes change from `.yml` to `.dig` - all `.dig` files in project directory are picked up automatically by digdag - digdag cli gains a `--project` option to specify the project directory - `digdag run` `-f` option is removed - `digdag init` creates: - `.digdag` directory in project directory - `.digdag/config` file - digdag cli looks for `.digdag` directory, starting in current directory and then recurses into parent directories (like git). - `digdag push` pushes entire project (as identified by `.digdag`) in some parent dir, even if executed in a subdir. - filenames referenced by a `.dig` file are resolved relative to the path of the `.dig` file so that given a project with:  .digdag/config subdir/workflow.dig queries/foo.sql  Then a user can both be in the project dir and do:  digdag run subdir/workflow  Or  cd subdir/ digdag run workflow  with the same result. The `subdir/workflow.dig` file must reference the query file as `../queries/foo.sql`. - digdag does not change ""CWD"" when executing a workflow. - `digdag run` `<workflow name>` parameter can include `.dig` suffix so that the below invocation semantics are identical: - `digdag run foo` - `digdag run foo.dig` source-file source-file source-file source-file source-file documentation-file test-file test-file test-file",no-bug,0.9
32,digdag,https://github.com/treasure-data/digdag/issues/32,digdag schedules output timezones confusing,"`next session time` and `next runs at` timezones are different, which is a bit confusing.  $ digdag schedules 2016-04-11 14:46:04 -0700: Digdag v0.5.9 Schedules: id: 2 repository: dano-churn-prediction-poc workflow: +main next session time: 2016-04-13 00:00:00 +0900 next runs at: 2016-04-12 15:00:00 -0700 (24h 13m 55s later) 1 entries. Use `digdag workflows +NAME` to show workflow details. ",source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file,"digdag schedules output timezones confusing `next session time` and `next runs at` timezones are different, which is a bit confusing.  $ digdag schedules 2016-04-11 14:46:04 -0700: Digdag v0.5.9 Schedules: id: 2 repository: dano-churn-prediction-poc workflow: +main next session time: 2016-04-13 00:00:00 +0900 next runs at: 2016-04-12 15:00:00 -0700 (24h 13m 55s later) 1 entries. Use `digdag workflows +NAME` to show workflow details.  source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file",no-bug,0.85
59,digdag,https://github.com/treasure-data/digdag/issues/59,--session should accept iso8601," digdag start test foobar --session ""2016-04-19T17:22:58Z"" 2016-04-19 10:23:11 -0700: Digdag v0.6.0 error: --session must be hourly, daily, now, ""yyyy-MM-dd"", or ""yyyy-MM-dd HH:mm:SS"" format: 2016-04-19T17:22:58Z ",source-file | source-file,"--session should accept iso8601  digdag start test foobar --session ""2016-04-19T17:22:58Z"" 2016-04-19 10:23:11 -0700: Digdag v0.6.0 error: --session must be hourly, daily, now, ""yyyy-MM-dd"", or ""yyyy-MM-dd HH:mm:SS"" format: 2016-04-19T17:22:58Z  source-file source-file",no-bug,0.8
149,digdag,https://github.com/treasure-data/digdag/issues/149,Set next runs at after 24+ hours when set non-UTC timezone," console $ digdag init test $ cd test  edit test.dig  $ LANG=C date Wed Jun 29 00:30:29 JST 2016 $ cat test.dig timezone: Asia/Tokyo schedule: daily>: 03:00:00 _export: hello: ""Hello, world!"" +step1: sh>: tasks/shell_sample.sh +step2: _parallel: true +worker1: sh>: tasks/repeat_hello.sh +worker2: sh>: tasks/repeat_hello.sh +step3: # defined at tasks/__init__.py py>: tasks.MyWorkflow.step3 $ digdag push sched -c /dev/null 2016-06-29 00:32:14 +0900: Digdag v0.8.2 Creating .digdag/tmp/archive-3637571230657586954.tar.gz Archiving test.dig Archiving tasks/__init__.py Archiving tasks/repeat_hello.sh Archiving tasks/shell_sample.sh Workflows: test Uploaded: id: 1 name: sched revision: b27d99c4-1984-4d06-9f31-9c8cc86c5760 archive type: db project created at: 2016-06-28T15:32:15Z revision updated at: 2016-06-28T15:32:15Z Use `digdag workflows` to show all workflows. $ digdag schedules -c /dev/null 2016-06-29 00:32:19 +0900: Digdag v0.8.2 Schedules: id: 1 project: sched workflow: test next session time: 2016-06-30 00:00:00 +0900 next runs at: 2016-06-30 03:00:00 +0900 (26h 27m 40s later) 1 entries. Use `digdag workflows [project-name] [workflow-name]` to show workflow details.  I want to schedule at 2.5 hours later instead of 26.5 hours later.",source-file | source-file | source-file | source-file | documentation-file | source-file | source-file | source-file | test-file | test-file | test-file,"Set next runs at after 24+ hours when set non-UTC timezone  console $ digdag init test $ cd test  edit test.dig  $ LANG=C date Wed Jun 29 00:30:29 JST 2016 $ cat test.dig timezone: Asia/Tokyo schedule: daily>: 03:00:00 _export: hello: ""Hello, world!"" +step1: sh>: tasks/shell_sample.sh +step2: _parallel: true +worker1: sh>: tasks/repeat_hello.sh +worker2: sh>: tasks/repeat_hello.sh +step3: # defined at tasks/__init__.py py>: tasks.MyWorkflow.step3 $ digdag push sched -c /dev/null 2016-06-29 00:32:14 +0900: Digdag v0.8.2 Creating .digdag/tmp/archive-3637571230657586954.tar.gz Archiving test.dig Archiving tasks/__init__.py Archiving tasks/repeat_hello.sh Archiving tasks/shell_sample.sh Workflows: test Uploaded: id: 1 name: sched revision: b27d99c4-1984-4d06-9f31-9c8cc86c5760 archive type: db project created at: 2016-06-28T15:32:15Z revision updated at: 2016-06-28T15:32:15Z Use `digdag workflows` to show all workflows. $ digdag schedules -c /dev/null 2016-06-29 00:32:19 +0900: Digdag v0.8.2 Schedules: id: 1 project: sched workflow: test next session time: 2016-06-30 00:00:00 +0900 next runs at: 2016-06-30 03:00:00 +0900 (26h 27m 40s later) 1 entries. Use `digdag workflows [project-name] [workflow-name]` to show workflow details.  I want to schedule at 2.5 hours later instead of 26.5 hours later. source-file source-file source-file source-file documentation-file source-file source-file source-file test-file test-file test-file",no-bug,0.9
3,digdag,https://github.com/treasure-data/digdag/issues/3,shell_sample.sh is evaluated as JavaScript?,"Observed the following error:  leo@weaver:~/work/td/2016-03-09> digdag new query-analysis [9:13:26 Mar 07 2016] 2016-03-09 09:13:36 +0900: Digdag v0.3.4 Creating query-analysis/digdag Creating query-analysis/.digdag-wrapper/digdag.jar Creating query-analysis/.gitignore Creating query-analysis/tasks/shell_sample.sh Creating query-analysis/tasks/repeat_hello.sh Creating query-analysis/tasks/__init__.py Creating query-analysis/digdag.yml Done. Type `cd query-analysis` and `./digdag r` to run the workflow. Enjoy! leo@weaver:~/work/td/2016-03-09> cd query-analysis [9:13:36 Mar 07 2016] leo@weaver:~/work/td/2016-03-09/query-analysis> ls [9:13:38 Mar 07 2016] digdag digdag.yml tasks leo@weaver:~/work/td/2016-03-09/query-analysis> digdag run [9:13:38 Mar 07 2016] 2016-03-09 09:13:49 +0900: Digdag v0.3.4 2016-03-09 09:13:50 +0900 [WARN] (main): --session-time argument, --hour argument, or _schedule in yaml file is not set. Using today's 00:00:00 as ${session_time}. 2016-03-09 09:13:50 +0900 [INFO] (main): Using state files at digdag.status/20160309T000000+0900. 2016-03-09 09:13:50 +0900 [INFO] (main): Starting a new session repository id=1 workflow name=+main session_time=2016-03-09T00:00:00+09:00 2016-03-09 09:13:50 +0900 [ERROR] (0021@+main+step1): Task failed java.lang.RuntimeException: Failed to process task config templates at io.digdag.core.agent.OperatorManager.runWithArchive(OperatorManager.java:154) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$1(OperatorManager.java:128) at io.digdag.core.agent.OperatorManager$$Lambda$114/2102246737.run(Unknown Source) at io.digdag.core.agent.CurrentDirectoryArchiveManager.withExtractedArchive(CurrentDirectoryArchiveManager.java:20) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:127) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:106) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:567) at io.digdag.core.agent.LocalAgent.lambda$run$0(LocalAgent.java:61) at io.digdag.core.agent.LocalAgent$$Lambda$112/1884150000.run(Unknown Source) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: io.digdag.spi.TemplateException: Failed to evaluate JavaScript code: tasks/shell_sample.sh at io.digdag.core.agent.ConfigEvalEngine.invokeTemplate(ConfigEvalEngine.java:91) at io.digdag.core.agent.ConfigEvalEngine.access$200(ConfigEvalEngine.java:31) at io.digdag.core.agent.ConfigEvalEngine$Context.evalValue(ConfigEvalEngine.java:170) at io.digdag.core.agent.ConfigEvalEngine$Context.evalObjectRecursive(ConfigEvalEngine.java:128) at io.digdag.core.agent.ConfigEvalEngine$Context.access$000(ConfigEvalEngine.java:95) at io.digdag.core.agent.ConfigEvalEngine.eval(ConfigEvalEngine.java:62) at io.digdag.core.agent.OperatorManager.runWithArchive(OperatorManager.java:151)  13 common frames omitted Caused by: javax.script.ScriptException: String index out of range: 72 at jdk.nashorn.api.scripting.NashornScriptEngine.throwAsScriptException(NashornScriptEngine.java:455) at jdk.nashorn.api.scripting.NashornScriptEngine.invokeImpl(NashornScriptEngine.java:387) at jdk.nashorn.api.scripting.NashornScriptEngine.invokeFunction(NashornScriptEngine.java:187) at io.digdag.core.agent.ConfigEvalEngine.invokeTemplate(ConfigEvalEngine.java:88)  19 common frames omitted Caused by: jdk.nashorn.internal.runtime.ParserException: String index out of range: 72 at jdk.nashorn.internal.runtime.Context$ThrowErrorManager.error(Context.java:419) at jdk.nashorn.internal.parser.Parser.recover(Parser.java:413) at jdk.nashorn.internal.parser.Parser.sourceElements(Parser.java:831) at jdk.nashorn.internal.parser.Parser.program(Parser.java:711) at jdk.nashorn.internal.parser.Parser.parse(Parser.java:284) at jdk.nashorn.internal.runtime.RecompilableScriptFunctionData.reparse(RecompilableScriptFunctionData.java:386) at jdk.nashorn.internal.runtime.RecompilableScriptFunctionData.compileTypeSpecialization(RecompilableScriptFunctionData.java:511) at jdk.nashorn.internal.runtime.RecompilableScriptFunctionData.getBest(RecompilableScriptFunctionData.java:730) at jdk.nashorn.internal.runtime.ScriptFunctionData.getBestInvoker(ScriptFunctionData.java:232) at jdk.nashorn.internal.runtime.ScriptFunction.findCallMethod(ScriptFunction.java:586) at jdk.nashorn.internal.runtime.ScriptObject.lookup(ScriptObject.java:1872) at jdk.nashorn.internal.runtime.linker.NashornLinker.getGuardedInvocation(NashornLinker.java:100) at jdk.nashorn.internal.runtime.linker.NashornLinker.getGuardedInvocation(NashornLinker.java:94) at jdk.internal.dynalink.support.CompositeTypeBasedGuardingDynamicLinker.getGuardedInvocation(CompositeTypeBasedGuardingDynamicLinker.java:176) at jdk.internal.dynalink.support.CompositeGuardingDynamicLinker.getGuardedInvocation(CompositeGuardingDynamicLinker.java:124) at jdk.internal.dynalink.support.LinkerServicesImpl.getGuardedInvocation(LinkerServicesImpl.java:149) at jdk.internal.dynalink.DynamicLinker.relink(DynamicLinker.java:233) at jdk.nashorn.internal.objects.NativeRegExp.callReplaceValue(NativeRegExp.java:819) at jdk.nashorn.internal.objects.NativeRegExp.replace(NativeRegExp.java:696) at jdk.nashorn.internal.objects.NativeString.replace(NativeString.java:809) at jdk.nashorn.internal.scripts.Script$Recompilation$1$62AA$\^eval\_.template(<eval>:26) at jdk.nashorn.internal.runtime.ScriptFunctionData.invoke(ScriptFunctionData.java:640) at jdk.nashorn.internal.runtime.ScriptFunction.invoke(ScriptFunction.java:229) at jdk.nashorn.internal.runtime.ScriptRuntime.apply(ScriptRuntime.java:387) at jdk.nashorn.api.scripting.ScriptObjectMirror.callMember(ScriptObjectMirror.java:192) at jdk.nashorn.api.scripting.NashornScriptEngine.invokeImpl(NashornScriptEngine.java:381)  21 common frames omitted error: * +main+step1: Failed to process task config templates Task state is saved at digdag.status/20160309T000000+0900 directory. Run command with --session-time '2016-03-09 00:00:00' argument to retry failed tasks. ",other-file | source-file | test-file | other-file,"shell_sample.sh is evaluated as JavaScript? Observed the following error:  leo@weaver:~/work/td/2016-03-09> digdag new query-analysis [9:13:26 Mar 07 2016] 2016-03-09 09:13:36 +0900: Digdag v0.3.4 Creating query-analysis/digdag Creating query-analysis/.digdag-wrapper/digdag.jar Creating query-analysis/.gitignore Creating query-analysis/tasks/shell_sample.sh Creating query-analysis/tasks/repeat_hello.sh Creating query-analysis/tasks/__init__.py Creating query-analysis/digdag.yml Done. Type `cd query-analysis` and `./digdag r` to run the workflow. Enjoy! leo@weaver:~/work/td/2016-03-09> cd query-analysis [9:13:36 Mar 07 2016] leo@weaver:~/work/td/2016-03-09/query-analysis> ls [9:13:38 Mar 07 2016] digdag digdag.yml tasks leo@weaver:~/work/td/2016-03-09/query-analysis> digdag run [9:13:38 Mar 07 2016] 2016-03-09 09:13:49 +0900: Digdag v0.3.4 2016-03-09 09:13:50 +0900 [WARN] (main): --session-time argument, --hour argument, or _schedule in yaml file is not set. Using today's 00:00:00 as ${session_time}. 2016-03-09 09:13:50 +0900 [INFO] (main): Using state files at digdag.status/20160309T000000+0900. 2016-03-09 09:13:50 +0900 [INFO] (main): Starting a new session repository id=1 workflow name=+main session_time=2016-03-09T00:00:00+09:00 2016-03-09 09:13:50 +0900 [ERROR] (0021@+main+step1): Task failed java.lang.RuntimeException: Failed to process task config templates at io.digdag.core.agent.OperatorManager.runWithArchive(OperatorManager.java:154) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$1(OperatorManager.java:128) at io.digdag.core.agent.OperatorManager$$Lambda$114/2102246737.run(Unknown Source) at io.digdag.core.agent.CurrentDirectoryArchiveManager.withExtractedArchive(CurrentDirectoryArchiveManager.java:20) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:127) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:106) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:567) at io.digdag.core.agent.LocalAgent.lambda$run$0(LocalAgent.java:61) at io.digdag.core.agent.LocalAgent$$Lambda$112/1884150000.run(Unknown Source) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: io.digdag.spi.TemplateException: Failed to evaluate JavaScript code: tasks/shell_sample.sh at io.digdag.core.agent.ConfigEvalEngine.invokeTemplate(ConfigEvalEngine.java:91) at io.digdag.core.agent.ConfigEvalEngine.access$200(ConfigEvalEngine.java:31) at io.digdag.core.agent.ConfigEvalEngine$Context.evalValue(ConfigEvalEngine.java:170) at io.digdag.core.agent.ConfigEvalEngine$Context.evalObjectRecursive(ConfigEvalEngine.java:128) at io.digdag.core.agent.ConfigEvalEngine$Context.access$000(ConfigEvalEngine.java:95) at io.digdag.core.agent.ConfigEvalEngine.eval(ConfigEvalEngine.java:62) at io.digdag.core.agent.OperatorManager.runWithArchive(OperatorManager.java:151)  13 common frames omitted Caused by: javax.script.ScriptException: String index out of range: 72 at jdk.nashorn.api.scripting.NashornScriptEngine.throwAsScriptException(NashornScriptEngine.java:455) at jdk.nashorn.api.scripting.NashornScriptEngine.invokeImpl(NashornScriptEngine.java:387) at jdk.nashorn.api.scripting.NashornScriptEngine.invokeFunction(NashornScriptEngine.java:187) at io.digdag.core.agent.ConfigEvalEngine.invokeTemplate(ConfigEvalEngine.java:88)  19 common frames omitted Caused by: jdk.nashorn.internal.runtime.ParserException: String index out of range: 72 at jdk.nashorn.internal.runtime.Context$ThrowErrorManager.error(Context.java:419) at jdk.nashorn.internal.parser.Parser.recover(Parser.java:413) at jdk.nashorn.internal.parser.Parser.sourceElements(Parser.java:831) at jdk.nashorn.internal.parser.Parser.program(Parser.java:711) at jdk.nashorn.internal.parser.Parser.parse(Parser.java:284) at jdk.nashorn.internal.runtime.RecompilableScriptFunctionData.reparse(RecompilableScriptFunctionData.java:386) at jdk.nashorn.internal.runtime.RecompilableScriptFunctionData.compileTypeSpecialization(RecompilableScriptFunctionData.java:511) at jdk.nashorn.internal.runtime.RecompilableScriptFunctionData.getBest(RecompilableScriptFunctionData.java:730) at jdk.nashorn.internal.runtime.ScriptFunctionData.getBestInvoker(ScriptFunctionData.java:232) at jdk.nashorn.internal.runtime.ScriptFunction.findCallMethod(ScriptFunction.java:586) at jdk.nashorn.internal.runtime.ScriptObject.lookup(ScriptObject.java:1872) at jdk.nashorn.internal.runtime.linker.NashornLinker.getGuardedInvocation(NashornLinker.java:100) at jdk.nashorn.internal.runtime.linker.NashornLinker.getGuardedInvocation(NashornLinker.java:94) at jdk.internal.dynalink.support.CompositeTypeBasedGuardingDynamicLinker.getGuardedInvocation(CompositeTypeBasedGuardingDynamicLinker.java:176) at jdk.internal.dynalink.support.CompositeGuardingDynamicLinker.getGuardedInvocation(CompositeGuardingDynamicLinker.java:124) at jdk.internal.dynalink.support.LinkerServicesImpl.getGuardedInvocation(LinkerServicesImpl.java:149) at jdk.internal.dynalink.DynamicLinker.relink(DynamicLinker.java:233) at jdk.nashorn.internal.objects.NativeRegExp.callReplaceValue(NativeRegExp.java:819) at jdk.nashorn.internal.objects.NativeRegExp.replace(NativeRegExp.java:696) at jdk.nashorn.internal.objects.NativeString.replace(NativeString.java:809) at jdk.nashorn.internal.scripts.Script$Recompilation$1$62AA$\^eval\_.template(<eval>:26) at jdk.nashorn.internal.runtime.ScriptFunctionData.invoke(ScriptFunctionData.java:640) at jdk.nashorn.internal.runtime.ScriptFunction.invoke(ScriptFunction.java:229) at jdk.nashorn.internal.runtime.ScriptRuntime.apply(ScriptRuntime.java:387) at jdk.nashorn.api.scripting.ScriptObjectMirror.callMember(ScriptObjectMirror.java:192) at jdk.nashorn.api.scripting.NashornScriptEngine.invokeImpl(NashornScriptEngine.java:381)  21 common frames omitted error: * +main+step1: Failed to process task config templates Task state is saved at digdag.status/20160309T000000+0900 directory. Run command with --session-time '2016-03-09 00:00:00' argument to retry failed tasks.  other-file source-file test-file other-file",no-bug,0.9
1739,digdag,https://github.com/treasure-data/digdag/issues/1739,docker build environment fails to build,"The docker build environment currently fails to build: https://github.com/treasure-data/digdag/blob/8b21fdcb6e103104848b3f067ea03afc82c4a03e/docker/bootstrap/dependencies.sh#L23 This currently fails with: sh #7 77.02 + apt-get -y install postgresql-11 postgresql-client-11 #7 77.07 Reading package lists #7 78.37 Building dependency tree #7 78.61 Reading state information #7 78.71 Some packages could not be installed. This may mean that you have #7 78.71 requested an impossible situation or if you are using the unstable #7 78.71 distribution that some required packages have not yet been created #7 78.71 or been moved out of Incoming. #7 78.71 The following information may help to resolve the situation: #7 78.71 #7 78.71 The following packages have unmet dependencies: #7 78.84 postgresql-11 : Depends: libicu60 (>= 60.1-1~) but it is not installable #7 78.84 Recommends: sysstat but it is not going to be installed #7 78.91 E: Unable to correct problems, you have held broken packages. ",config-file | config-file,"docker build environment fails to build The docker build environment currently fails to build: https://github.com/treasure-data/digdag/blob/8b21fdcb6e103104848b3f067ea03afc82c4a03e/docker/bootstrap/dependencies.sh#L23 This currently fails with: sh #7 77.02 + apt-get -y install postgresql-11 postgresql-client-11 #7 77.07 Reading package lists #7 78.37 Building dependency tree #7 78.61 Reading state information #7 78.71 Some packages could not be installed. This may mean that you have #7 78.71 requested an impossible situation or if you are using the unstable #7 78.71 distribution that some required packages have not yet been created #7 78.71 or been moved out of Incoming. #7 78.71 The following information may help to resolve the situation: #7 78.71 #7 78.71 The following packages have unmet dependencies: #7 78.84 postgresql-11 : Depends: libicu60 (>= 60.1-1~) but it is not installable #7 78.84 Recommends: sysstat but it is not going to be installed #7 78.91 E: Unable to correct problems, you have held broken packages.  config-file config-file",no-bug,0.95
1271,digdag,https://github.com/treasure-data/digdag/issues/1271,[Feature request] Update Node.js version,"As 8.x will be deprecated(EOL) in 2019 December, digdag-ui should support Node.js 10 and 12 and drop 8.x support in the near future.",documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file,"[Feature request] Update Node.js version As 8.x will be deprecated(EOL) in 2019 December, digdag-ui should support Node.js 10 and 12 and drop 8.x support in the near future. documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file",no-bug,0.95
1089,digdag,https://github.com/treasure-data/digdag/issues/1089,Badge for displaying packaging status,[Repology](https://repology.org/project/digdag/versions) is a great website for comparing package versions over different GNU/Linux distros and the various BSDs. There is also an API that that provides nice [badges](https://repology.org/project/digdag/badges) to show versions in all known repositories like so: [![Packaging status](https://repology.org/badge/vertical-allrepos/digdag.svg)](https://repology.org/project/digdag/versions) I think this might be a nice addition to [README.md](https://github.com/treasure-data/digdag/blob/master/README.md).,documentation-file,Badge for displaying packaging status [Repology](https://repology.org/project/digdag/versions) is a great website for comparing package versions over different GNU/Linux distros and the various BSDs. There is also an API that that provides nice [badges](https://repology.org/project/digdag/badges) to show versions in all known repositories like so: [![Packaging status](https://repology.org/badge/vertical-allrepos/digdag.svg)](https://repology.org/project/digdag/versions) I think this might be a nice addition to [README.md](https://github.com/treasure-data/digdag/blob/master/README.md). documentation-file,no-bug,0.9
27,digdag,https://github.com/treasure-data/digdag/issues/27,Failed to run workflow on Digdag server,"When I ran a workflow on Digdag server running in localhost, this error occurred.  2016-04-05 11:21:09 +0900 [INFO] (0037@+main+step1): sh>: ./tasks/bin/enc-tool -e development-ec2 -a 1 -c ENCRYPT -t 200 /bin/sh: ./tasks/bin/enc-tool: Permission denied 2016-04-05 11:21:09 +0900 [ERROR] (0037@+main+step1): Task failed java.lang.RuntimeException: Command failed with code 126 at io.digdag.standards.operator.ShOperatorFactory$ShOperator.runTask(ShOperatorFactory.java:115) at io.digdag.standards.operator.BaseOperator.run(BaseOperator.java:49) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:241) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:196) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$1(OperatorManager.java:133) at io.digdag.core.agent.LocalWorkspaceManager.withExtractedArchive(LocalWorkspaceManager.java:63) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:132) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:109) at io.digdag.core.agent.LocalAgent.lambda$run$0(LocalAgent.java:61) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)  It works in local mode.",source-file | test-file,"Failed to run workflow on Digdag server When I ran a workflow on Digdag server running in localhost, this error occurred.  2016-04-05 11:21:09 +0900 [INFO] (0037@+main+step1): sh>: ./tasks/bin/enc-tool -e development-ec2 -a 1 -c ENCRYPT -t 200 /bin/sh: ./tasks/bin/enc-tool: Permission denied 2016-04-05 11:21:09 +0900 [ERROR] (0037@+main+step1): Task failed java.lang.RuntimeException: Command failed with code 126 at io.digdag.standards.operator.ShOperatorFactory$ShOperator.runTask(ShOperatorFactory.java:115) at io.digdag.standards.operator.BaseOperator.run(BaseOperator.java:49) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:241) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:196) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$1(OperatorManager.java:133) at io.digdag.core.agent.LocalWorkspaceManager.withExtractedArchive(LocalWorkspaceManager.java:63) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:132) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:109) at io.digdag.core.agent.LocalAgent.lambda$run$0(LocalAgent.java:61) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)  It works in local mode. source-file test-file",no-bug,0.95
1092,digdag,https://github.com/treasure-data/digdag/issues/1092,[feature request] ignore file configuration like .gitignore,"This is a feature request to have a ignore file for a project. **Expected use case:** When developing a workflow with Python, Python generates `__pycache__` files. In Python development, it is a popular way to create a virtual environment under the project directory and sometimes it could be pushed to digdag server or could fail to push due to the large size of library dirs. It is a usual way to develop workflow locally and push to server for production. So I'd be happy if we could have a ignore file configuration.",config-file | source-file | source-file | test-file | test-file | config-file | source-file | source-file | test-file,"[feature request] ignore file configuration like .gitignore This is a feature request to have a ignore file for a project. **Expected use case:** When developing a workflow with Python, Python generates `__pycache__` files. In Python development, it is a popular way to create a virtual environment under the project directory and sometimes it could be pushed to digdag server or could fail to push due to the large size of library dirs. It is a usual way to develop workflow locally and push to server for production. So I'd be happy if we could have a ignore file configuration. config-file source-file source-file test-file test-file config-file source-file source-file test-file",no-bug,0.95
427,digdag,https://github.com/treasure-data/digdag/issues/427,link to Digdag server document is broken,"here: http://docs.digdag.io/command_reference.html#server broken link: under: -c, --config PATH [Digdag server] http://docs.digdag.io/digdag_server.html",documentation-file | documentation-file,"link to Digdag server document is broken here: http://docs.digdag.io/command_reference.html#server broken link: under: -c, --config PATH [Digdag server] http://docs.digdag.io/digdag_server.html documentation-file documentation-file",no-bug,0.95
8,digdag,https://github.com/treasure-data/digdag/issues/8,Using pre-defined variables inside _export block,It would be useful if we can use pre-defined variables within _export:  _export: query_start: 2016-03-01 query_end: ${session_date} ,config-file | other-file | config-file | documentation-file | config-file | config-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | config-file | source-file | other-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | other-file | documentation-file | documentation-file | documentation-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | config-file | source-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | config-file | source-file | source-file | source-file | other-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | other-file | other-file | documentation-file | documentation-file | source-file | other-file | source-file | config-file | config-file,Using pre-defined variables inside _export block It would be useful if we can use pre-defined variables within _export:  _export: query_start: 2016-03-01 query_end: ${session_date}  config-file other-file config-file documentation-file config-file config-file config-file source-file source-file source-file source-file source-file source-file source-file test-file config-file source-file other-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file other-file documentation-file documentation-file documentation-file source-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file config-file source-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file config-file source-file source-file source-file other-file test-file test-file test-file test-file test-file test-file test-file test-file test-file other-file other-file documentation-file documentation-file source-file other-file source-file config-file config-file,no-bug,0.8
582,digdag,https://github.com/treasure-data/digdag/issues/582,[digdag-ui] Can't display file contents if the sh> operator has an argument.,"* Digdag: 0.9.12 * macOS: 10.12.5 The `sh>: ./script/c.sh a` operator in the `+task3` has an argument `a` In this case, digdag-ui does not show the file contents. If I remove the argument `a`. digdag-ui show the contents of `./script/c.sh` ![digdag](https://user-images.githubusercontent.com/767650/27278683-0ca9c308-551d-11e7-9a62-0f02ef40d167.png)  Server setting  digdag server -m   Client setting  . |-- files.dig `-- scripts |-- a.sh |-- b.sh `-- c.sh   digdag push  yaml timezone: ""Asia/Tokyo"" +file_list: +task1: sh>: ./scripts/a.sh +task2: sh>: ./scripts/b.sh +task3: sh>: ./scripts/c.sh a  ## Reference Reported-by : [kamekoopa](https://twitter.com/kamekoopa/status/875653206977544192) (written in Japanese)",other-file | other-file,"[digdag-ui] Can't display file contents if the sh> operator has an argument. * Digdag: 0.9.12 * macOS: 10.12.5 The `sh>: ./script/c.sh a` operator in the `+task3` has an argument `a` In this case, digdag-ui does not show the file contents. If I remove the argument `a`. digdag-ui show the contents of `./script/c.sh` ![digdag](https://user-images.githubusercontent.com/767650/27278683-0ca9c308-551d-11e7-9a62-0f02ef40d167.png)  Server setting  digdag server -m   Client setting  . |-- files.dig `-- scripts |-- a.sh |-- b.sh `-- c.sh   digdag push  yaml timezone: ""Asia/Tokyo"" +file_list: +task1: sh>: ./scripts/a.sh +task2: sh>: ./scripts/b.sh +task3: sh>: ./scripts/c.sh a  ## Reference Reported-by : [kamekoopa](https://twitter.com/kamekoopa/status/875653206977544192) (written in Japanese) other-file other-file",no-bug,0.9
510,digdag,https://github.com/treasure-data/digdag/issues/510,"""./gradlew tasks"" failed",Related #509 The `./gradlew tasks` does not work properly.  commit 248f7a68dd86106ce84c086c5b7730065a01b9eb Merge: 47d74e0 6926ec0 Author: Sadayuki Furuhashi <frsyuki@users.sourceforge.jp> Date: Mon Mar 13 11:48:08 2017 -0700 Merge pull request #496 from treasure-data/max-threads-overacquire-fix Fix over acquiring of tasks in MultiThreadAgent   /gradlew tasks --all The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead. at build_erdkm0el29ihheeaqmsipw6av$_run_closure2.doCall(/Users/hsato/OpenProjects/digdag/digdag/build.gradle:243) Publication mavenJava not found in project :. Publication mavenJava not found in project :digdag-docs. Publication mavenJava not found in project :digdag-ui. :tasks  All tasks runnable from root project  :tasks FAILED FAILURE: Build failed with an exception. * What went wrong: Execution failed for task ':tasks'. > Could not determine the dependencies of task ':digdag-plugin-utils:bintrayUpload'. * Try: Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED Total time: 1.253 secs ,config-file | config-file | config-file | config-file,"""./gradlew tasks"" failed Related #509 The `./gradlew tasks` does not work properly.  commit 248f7a68dd86106ce84c086c5b7730065a01b9eb Merge: 47d74e0 6926ec0 Author: Sadayuki Furuhashi <frsyuki@users.sourceforge.jp> Date: Mon Mar 13 11:48:08 2017 -0700 Merge pull request #496 from treasure-data/max-threads-overacquire-fix Fix over acquiring of tasks in MultiThreadAgent   /gradlew tasks --all The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead. at build_erdkm0el29ihheeaqmsipw6av$_run_closure2.doCall(/Users/hsato/OpenProjects/digdag/digdag/build.gradle:243) Publication mavenJava not found in project :. Publication mavenJava not found in project :digdag-docs. Publication mavenJava not found in project :digdag-ui. :tasks  All tasks runnable from root project  :tasks FAILED FAILURE: Build failed with an exception. * What went wrong: Execution failed for task ':tasks'. > Could not determine the dependencies of task ':digdag-plugin-utils:bintrayUpload'. * Try: Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED Total time: 1.253 secs  config-file config-file config-file config-file",no-bug,0.9
46,digdag,https://github.com/treasure-data/digdag/issues/46,plural REST API with ?name= parameter replaces singular REST API,"There are 3 singular REST API: - `GET /api/project?name=<name>` - `GET /api/projects/{id}/workflow?name=name[&revision=name]` - `GET /api/workflow?project=<name>&name=<name>[&revision=<name>]` Replace them by adding following REST API: - `GET /api/projects?name=<name>` - `GET /api/projects/{id}/workflows?name=<name>[&revision=name]` - (no replacement for `/api/workflow`. Use `/api/projects?name=<name` and `/api/projects/{id}/workflows` instead) If a resource exist, these new REST API return an array with one element in it. If a resource doesn't exist, it returns an empty array (not 404 Not Found). With this way, client can tell that a project doesn't exist when `GET /api/projects/{id}/workflows?name=<name>[&revision=name]` returns 404 Not Found because it returns an empty array if a project exists but workflow doesn't exist.",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file,"plural REST API with ?name= parameter replaces singular REST API There are 3 singular REST API: - `GET /api/project?name=<name>` - `GET /api/projects/{id}/workflow?name=name[&revision=name]` - `GET /api/workflow?project=<name>&name=<name>[&revision=<name>]` Replace them by adding following REST API: - `GET /api/projects?name=<name>` - `GET /api/projects/{id}/workflows?name=<name>[&revision=name]` - (no replacement for `/api/workflow`. Use `/api/projects?name=<name` and `/api/projects/{id}/workflows` instead) If a resource exist, these new REST API return an array with one element in it. If a resource doesn't exist, it returns an empty array (not 404 Not Found). With this way, client can tell that a project doesn't exist when `GET /api/projects/{id}/workflows?name=<name>[&revision=name]` returns 404 Not Found because it returns an empty array if a project exists but workflow doesn't exist. source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file",no-bug,0.85
33,digdag,https://github.com/treasure-data/digdag/issues/33,digdag should offer weekly scheduling,"E.g.  _schedule: weekly>: 6, 07:00:00 ",test-file,"digdag should offer weekly scheduling E.g.  _schedule: weekly>: 6, 07:00:00  test-file",no-bug,0.9
15,digdag,https://github.com/treasure-data/digdag/issues/15,td: create_table option should be renamed to result_table,Since create_table doesn't indicate the query result will be stored to a table.,source-file | source-file,td: create_table option should be renamed to result_table Since create_table doesn't indicate the query result will be stored to a table. source-file source-file,no-bug,0.8
849,digdag,https://github.com/treasure-data/digdag/issues/849,Group retry does not work in call> operator,"Group retry does not work when the digdag script called from call> operator.  Environment Digdag version: 0.9.27 Mode: local  Procedure for reproducing * parent.dig  +call_child: call>: ./child.dig  * child.dig  _retry: 2 +run: sh>: sleep 3; exit 1  * Run  $ digdag run --rerun ./parent.dig  * Result Only executed one time, not retried.  2018-08-27 12:16:06 +0900: Digdag v0.9.27 2018-08-27 12:16:07 +0900 [WARN] (main): Reusing the last session time 2018-08-23T00:00:00+00:00. 2018-08-27 12:16:07 +0900 [INFO] (main): Using session /home//.digdag/status/20180823T000000+0000. 2018-08-27 12:16:07 +0900 [INFO] (main): Starting a new session project id=1 workflow name=parent session_time=2018-08-23T00:00:00+00:00 2018-08-27 12:16:08 +0900 [INFO] (0017@[0:default]+parent+call_child): call>: ./child.dig 2018-08-27 12:16:09 +0900 [INFO] (0017@[0:default]+parent+call_child^sub+run): sh>: sleep 3; exit 1 2018-08-27 12:16:12 +0900 [ERROR] (0017@[0:default]+parent+call_child^sub+run): Task failed with unexpected error: Command failed with code 1 java.lang.RuntimeException: Command failed with code 1 at io.digdag.standards.operator.ShOperatorFactory$ShOperator.runTask(ShOperatorFactory.java:143) at io.digdag.util.BaseOperator.run(BaseOperator.java:35) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:312) at io.digdag.cli.Run$OperatorManagerWithSkip.callExecutor(Run.java:694) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:254) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$2(OperatorManager.java:137) at io.digdag.core.agent.LocalWorkspaceManager.withExtractedArchive(LocalWorkspaceManager.java:25) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:135) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:119) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:676) at io.digdag.core.agent.MultiThreadAgent.lambda$null$0(MultiThreadAgent.java:127) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 2018-08-27 12:16:12 +0900 [INFO] (0017@[0:default]+parent^failure-alert): type: notify error: * +parent+call_child^sub+run: Command failed with code 1 (runtime) Task state is saved at /home//.digdag/status/20180823T000000+0000 directory. * Use --session <daily | hourly | ""yyyy-MM-dd[ HH:mm:ss]""> to not reuse the last session time. * Use --rerun, --start +NAME, or --goal +NAME argument to rerun skipped tasks.   Cause of problem When added tasks to retry, INITIAL_TASK is not set to state_flags, so the tasks are not picked up in DatabaseSessionStoreManager.copyInitialTasksForRetry(). If I added INITIAL_TASK in TaskControl.addGeneratedSubtasks(), it looks like working correctly. But I don't know the role of INITIAL_TASK, so it may not correct way.",source-file | source-file | source-file | test-file | test-file | test-file | test-file,"Group retry does not work in call> operator Group retry does not work when the digdag script called from call> operator.  Environment Digdag version: 0.9.27 Mode: local  Procedure for reproducing * parent.dig  +call_child: call>: ./child.dig  * child.dig  _retry: 2 +run: sh>: sleep 3; exit 1  * Run  $ digdag run --rerun ./parent.dig  * Result Only executed one time, not retried.  2018-08-27 12:16:06 +0900: Digdag v0.9.27 2018-08-27 12:16:07 +0900 [WARN] (main): Reusing the last session time 2018-08-23T00:00:00+00:00. 2018-08-27 12:16:07 +0900 [INFO] (main): Using session /home//.digdag/status/20180823T000000+0000. 2018-08-27 12:16:07 +0900 [INFO] (main): Starting a new session project id=1 workflow name=parent session_time=2018-08-23T00:00:00+00:00 2018-08-27 12:16:08 +0900 [INFO] (0017@[0:default]+parent+call_child): call>: ./child.dig 2018-08-27 12:16:09 +0900 [INFO] (0017@[0:default]+parent+call_child^sub+run): sh>: sleep 3; exit 1 2018-08-27 12:16:12 +0900 [ERROR] (0017@[0:default]+parent+call_child^sub+run): Task failed with unexpected error: Command failed with code 1 java.lang.RuntimeException: Command failed with code 1 at io.digdag.standards.operator.ShOperatorFactory$ShOperator.runTask(ShOperatorFactory.java:143) at io.digdag.util.BaseOperator.run(BaseOperator.java:35) at io.digdag.core.agent.OperatorManager.callExecutor(OperatorManager.java:312) at io.digdag.cli.Run$OperatorManagerWithSkip.callExecutor(Run.java:694) at io.digdag.core.agent.OperatorManager.runWithWorkspace(OperatorManager.java:254) at io.digdag.core.agent.OperatorManager.lambda$runWithHeartbeat$2(OperatorManager.java:137) at io.digdag.core.agent.LocalWorkspaceManager.withExtractedArchive(LocalWorkspaceManager.java:25) at io.digdag.core.agent.OperatorManager.runWithHeartbeat(OperatorManager.java:135) at io.digdag.core.agent.OperatorManager.run(OperatorManager.java:119) at io.digdag.cli.Run$OperatorManagerWithSkip.run(Run.java:676) at io.digdag.core.agent.MultiThreadAgent.lambda$null$0(MultiThreadAgent.java:127) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 2018-08-27 12:16:12 +0900 [INFO] (0017@[0:default]+parent^failure-alert): type: notify error: * +parent+call_child^sub+run: Command failed with code 1 (runtime) Task state is saved at /home//.digdag/status/20180823T000000+0000 directory. * Use --session <daily | hourly | ""yyyy-MM-dd[ HH:mm:ss]""> to not reuse the last session time. * Use --rerun, --start +NAME, or --goal +NAME argument to rerun skipped tasks.   Cause of problem When added tasks to retry, INITIAL_TASK is not set to state_flags, so the tasks are not picked up in DatabaseSessionStoreManager.copyInitialTasksForRetry(). If I added INITIAL_TASK in TaskControl.addGeneratedSubtasks(), it looks like working correctly. But I don't know the role of INITIAL_TASK, so it may not correct way. source-file source-file source-file test-file test-file test-file test-file",no-bug,0.9
