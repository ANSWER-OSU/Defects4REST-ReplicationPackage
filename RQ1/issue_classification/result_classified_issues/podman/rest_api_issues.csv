issue_no,repo,issue_url,title,description,patched_file_types,text_for_topic_modeling,prediction,confidence
15720,podman,https://github.com/containers/podman/issues/15720,REST API /system/df UsageData.RefCount on volumes is always 1,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** In order to see who is using a volume, I'm using the `RefCount` attribute of `UsageData` field But even if you create a volume without using it, it returns 1. And if you create multiple containers using that volume, it's still using 1 **Steps to reproduce the issue:** 1. Create a volume without using it bash $ dummyVolume=$(podman volume create)  2. Use REST api to see refCount  $ curl --silent --unix-socket /Users/benoitf/.local/share/containers/podman/machine/podman-machine-default/podman.sock ""http:/v1.41/system/df"" | jq --arg volumeName $dummyVolume '.Volumes[] | select(.Name == ($volumeName))' { ""Driver"": """", ""Labels"": {}, ""Mountpoint"": """", ""Name"": ""413f47a2d1656a4ec6e876543dc8a0caa5256c670a425638eff7be9730da0229"", ""Options"": null, ""Scope"": ""local"", ""UsageData"": { ""RefCount"": 1, ""Size"": 0 } }  It's already invalid 3. Now, try to start containers using that volume bash $ podman run -d --mount ""type=volume,src=$dummyVolume,target=/foo"" docker.io/library/httpd $ podman run -d --mount ""type=volume,src=$dummyVolume,target=/foo"" docker.io/library/httpd  4. Rest API is still invalid bash $ curl --silent --unix-socket /Users/benoitf/.local/share/containers/podman/machine/podman-machine-default/podman.sock ""http:/v1.41/system/df"" | jq --arg volumeName $dummyVolume '.Volumes[] | select(.Name == ($volumeName))' { ""Driver"": """", ""Labels"": {}, ""Mountpoint"": """", ""Name"": ""413f47a2d1656a4ec6e876543dc8a0caa5256c670a425638eff7be9730da0229"", ""Options"": null, ""Scope"": ""local"", ""UsageData"": { ""RefCount"": 1, ""Size"": 0 } }  And we still have our container using it bash $ podman inspect 18e7216f21ab0eddb5fab99c388ba04030809b6870cd0c595e7bc84008c7fb3d | jq '.[0].Mounts' [ { ""Type"": ""volume"", ""Name"": ""413f47a2d1656a4ec6e876543dc8a0caa5256c670a425638eff7be9730da0229"", ""Source"": ""/var/home/core/.local/share/containers/storage/volumes/413f47a2d1656a4ec6e876543dc8a0caa5256c670a425638eff7be9730da0229/_data"", ""Destination"": ""/foo"", ""Driver"": ""local"", ""Mode"": """", ""Options"": [ ""nosuid"", ""nodev"", ""rbind"" ], ""RW"": true, ""Propagation"": ""rprivate"" } ]  **Describe the results you received:** RefCount = 1 **Describe the results you expected:** Valid RefCount **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  4.2.1  **Output of `podman info`:**  (paste your output here)  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  (paste your output here)  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes/No **Additional environment details (AWS, VirtualBox, physical, etc.):**",source-file | source-file | test-file | test-file,"REST API /system/df UsageData.RefCount on volumes is always 1 <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** In order to see who is using a volume, I'm using the `RefCount` attribute of `UsageData` field But even if you create a volume without using it, it returns 1. And if you create multiple containers using that volume, it's still using 1 **Steps to reproduce the issue:** 1. Create a volume without using it bash $ dummyVolume=$(podman volume create)  2. Use REST api to see refCount  $ curl --silent --unix-socket /Users/benoitf/.local/share/containers/podman/machine/podman-machine-default/podman.sock ""http:/v1.41/system/df"" | jq --arg volumeName $dummyVolume '.Volumes[] | select(.Name == ($volumeName))' { ""Driver"": """", ""Labels"": {}, ""Mountpoint"": """", ""Name"": ""413f47a2d1656a4ec6e876543dc8a0caa5256c670a425638eff7be9730da0229"", ""Options"": null, ""Scope"": ""local"", ""UsageData"": { ""RefCount"": 1, ""Size"": 0 } }  It's already invalid 3. Now, try to start containers using that volume bash $ podman run -d --mount ""type=volume,src=$dummyVolume,target=/foo"" docker.io/library/httpd $ podman run -d --mount ""type=volume,src=$dummyVolume,target=/foo"" docker.io/library/httpd  4. Rest API is still invalid bash $ curl --silent --unix-socket /Users/benoitf/.local/share/containers/podman/machine/podman-machine-default/podman.sock ""http:/v1.41/system/df"" | jq --arg volumeName $dummyVolume '.Volumes[] | select(.Name == ($volumeName))' { ""Driver"": """", ""Labels"": {}, ""Mountpoint"": """", ""Name"": ""413f47a2d1656a4ec6e876543dc8a0caa5256c670a425638eff7be9730da0229"", ""Options"": null, ""Scope"": ""local"", ""UsageData"": { ""RefCount"": 1, ""Size"": 0 } }  And we still have our container using it bash $ podman inspect 18e7216f21ab0eddb5fab99c388ba04030809b6870cd0c595e7bc84008c7fb3d | jq '.[0].Mounts' [ { ""Type"": ""volume"", ""Name"": ""413f47a2d1656a4ec6e876543dc8a0caa5256c670a425638eff7be9730da0229"", ""Source"": ""/var/home/core/.local/share/containers/storage/volumes/413f47a2d1656a4ec6e876543dc8a0caa5256c670a425638eff7be9730da0229/_data"", ""Destination"": ""/foo"", ""Driver"": ""local"", ""Mode"": """", ""Options"": [ ""nosuid"", ""nodev"", ""rbind"" ], ""RW"": true, ""Propagation"": ""rprivate"" } ]  **Describe the results you received:** RefCount = 1 **Describe the results you expected:** Valid RefCount **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  4.2.1  **Output of `podman info`:**  (paste your output here)  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  (paste your output here)  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes/No **Additional environment details (AWS, VirtualBox, physical, etc.):** source-file source-file test-file test-file",bug,0.95
18424,podman,https://github.com/containers/podman/issues/18424,Podman ExecIDs report inaccurate Running state.," Issue Description It seems that podman ExecIDs are persisting and reporting as ""running"" for ~5 minutes before being removed. I've only tested this using the podman Golang bindings. Here is a demonstration program: https://gist.github.com/AndroidKitKat/2e1233b17316d96173fe1cf9f3e8aa48  Steps to reproduce the issue Steps to reproduce the issue 1. Create a new container  podman create --name alpine-test --tty alpine:latest  2. Start the container  podman start alpine-test  3. Start an exec in the container using the REST API. I did this using the Go program referenced in the GitHub Gist above I compiled it by doing:  go mod init inspectbug curl ""https://gist.githubusercontent.com/AndroidKitKat/2e1233b17316d96173fe1cf9f3e8aa48/raw/40c2b071a53275d0e270d71aee34051140094e46/main.go"" > main.go go get go build ./inspectbug   Describe the results you received In that file, I have an Exec with the command `sleep 5` and I have a loop running checking the status of the Exec every 1 second. For a duration of 5 seconds (the duration of the sleep command) + 5 minutes, the `inspectResult` struct's `Running` member is `true`, until eventually the program crashes due to no error handling when checking `containers.ExecInspect` because the ExecID seems to no longer exist at all. Here's the output of the program:  [developer@guthix inspectbug]$ ./inspectbug 2023/05/02 14:12:21 Exec ID: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76 2023/05/02 14:12:21 Still running: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76  2023/05/02 14:17:23 Still running: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76 2023/05/02 14:17:24 Still running: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76 2023/05/02 14:17:25 Still running: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76 panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x43 pc=0xe79322]   Describe the results you expected After 5 seconds, the Exec shows as ""Not running"" due to sleep exiting.  podman info output yaml [developer@guthix inspectbug]$ podman info host: arch: amd64 buildahVersion: 1.27.3 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.4-1.module+el8.7.0+1154+147ffa21.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.4, commit: ddbeffc1e2a247aef04a1be0bc9b1b5ef5f1cd09' cpuUtilization: idlePercent: 99.9 systemPercent: 0.04 userPercent: 0.06 cpus: 8 distribution: distribution: '""rocky""' version: ""8.7"" eventLogger: file hostname: guthix idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 4.18.0-425.19.2.el8_7.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 27185180672 memTotal: 33146671104 networkBackend: cni ociRuntime: name: runc package: runc-1.1.4-1.module+el8.7.0+1154+147ffa21.x86_64 path: /usr/bin/runc version: |- runc version 1.1.4 spec: 1.0.2-dev go: go1.18.9 libseccomp: 2.5.2 os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-2.module+el8.7.0+1154+147ffa21.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 16741560320 swapTotal: 16741560320 uptime: 100h 45m 33.00s (Approximately 4.17 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.access.redhat.com - registry.redhat.io - docker.io store: configFile: /home/developer/.config/containers/storage.conf containerStore: number: 7 paused: 0 running: 7 stopped: 0 graphDriverName: overlay graphOptions: overlay.mount_program: Executable: /usr/bin/fuse-overlayfs Package: fuse-overlayfs-1.9-1.module+el8.7.0+1154+147ffa21.x86_64 Version: |- fusermount3 version: 3.3.0 fuse-overlayfs: version 1.9 FUSE library version 3.3.0 using FUSE kernel interface version 7.26 graphRoot: /storage/containers/storage graphRootAllocated: 502921392128 graphRootUsed: 1801228288 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 9 runRoot: /run/user/1000 volumePath: /storage/containers/storage/volumes version: APIVersion: 4.2.0 Built: 1677003394 BuiltTime: Tue Feb 21 13:16:34 2023 GitCommit: """" GoVersion: go1.18.9 Os: linux OsArch: linux/amd64 Version: 4.2.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release No  Additional environment details Running on bare metal on Intel NUCs with 11th gen Intel processors. I am accessing Podman using the golang bindings.  Additional information I verified that the sleep command exits by looking at the process list of the container.",source-file | test-file | source-file | test-file,"Podman ExecIDs report inaccurate Running state.  Issue Description It seems that podman ExecIDs are persisting and reporting as ""running"" for ~5 minutes before being removed. I've only tested this using the podman Golang bindings. Here is a demonstration program: https://gist.github.com/AndroidKitKat/2e1233b17316d96173fe1cf9f3e8aa48  Steps to reproduce the issue Steps to reproduce the issue 1. Create a new container  podman create --name alpine-test --tty alpine:latest  2. Start the container  podman start alpine-test  3. Start an exec in the container using the REST API. I did this using the Go program referenced in the GitHub Gist above I compiled it by doing:  go mod init inspectbug curl ""https://gist.githubusercontent.com/AndroidKitKat/2e1233b17316d96173fe1cf9f3e8aa48/raw/40c2b071a53275d0e270d71aee34051140094e46/main.go"" > main.go go get go build ./inspectbug   Describe the results you received In that file, I have an Exec with the command `sleep 5` and I have a loop running checking the status of the Exec every 1 second. For a duration of 5 seconds (the duration of the sleep command) + 5 minutes, the `inspectResult` struct's `Running` member is `true`, until eventually the program crashes due to no error handling when checking `containers.ExecInspect` because the ExecID seems to no longer exist at all. Here's the output of the program:  [developer@guthix inspectbug]$ ./inspectbug 2023/05/02 14:12:21 Exec ID: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76 2023/05/02 14:12:21 Still running: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76  2023/05/02 14:17:23 Still running: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76 2023/05/02 14:17:24 Still running: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76 2023/05/02 14:17:25 Still running: 20ed50eb719cd691259d66de361be5793ac1f6d70179d5ef6978fbb72adf0c76 panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x43 pc=0xe79322]   Describe the results you expected After 5 seconds, the Exec shows as ""Not running"" due to sleep exiting.  podman info output yaml [developer@guthix inspectbug]$ podman info host: arch: amd64 buildahVersion: 1.27.3 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.4-1.module+el8.7.0+1154+147ffa21.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.4, commit: ddbeffc1e2a247aef04a1be0bc9b1b5ef5f1cd09' cpuUtilization: idlePercent: 99.9 systemPercent: 0.04 userPercent: 0.06 cpus: 8 distribution: distribution: '""rocky""' version: ""8.7"" eventLogger: file hostname: guthix idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 4.18.0-425.19.2.el8_7.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 27185180672 memTotal: 33146671104 networkBackend: cni ociRuntime: name: runc package: runc-1.1.4-1.module+el8.7.0+1154+147ffa21.x86_64 path: /usr/bin/runc version: |- runc version 1.1.4 spec: 1.0.2-dev go: go1.18.9 libseccomp: 2.5.2 os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-2.module+el8.7.0+1154+147ffa21.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 16741560320 swapTotal: 16741560320 uptime: 100h 45m 33.00s (Approximately 4.17 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.access.redhat.com - registry.redhat.io - docker.io store: configFile: /home/developer/.config/containers/storage.conf containerStore: number: 7 paused: 0 running: 7 stopped: 0 graphDriverName: overlay graphOptions: overlay.mount_program: Executable: /usr/bin/fuse-overlayfs Package: fuse-overlayfs-1.9-1.module+el8.7.0+1154+147ffa21.x86_64 Version: |- fusermount3 version: 3.3.0 fuse-overlayfs: version 1.9 FUSE library version 3.3.0 using FUSE kernel interface version 7.26 graphRoot: /storage/containers/storage graphRootAllocated: 502921392128 graphRootUsed: 1801228288 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 9 runRoot: /run/user/1000 volumePath: /storage/containers/storage/volumes version: APIVersion: 4.2.0 Built: 1677003394 BuiltTime: Tue Feb 21 13:16:34 2023 GitCommit: """" GoVersion: go1.18.9 Os: linux OsArch: linux/amd64 Version: 4.2.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release No  Additional environment details Running on bare metal on Intel NUCs with 11th gen Intel processors. I am accessing Podman using the golang bindings.  Additional information I verified that the sleep command exits by looking at the process list of the container. source-file test-file source-file test-file",bug,0.95
22989,podman,https://github.com/containers/podman/issues/22989,remote: pod start empty pod: error without message,"console $ bin/podman-remote pod create --infra=false --name=foo a0cb944bf836e95ca0a83f98e1af151879552ac4fc424e29031c5f214227bdf6 $ bin/podman-remote pod start foo Error:  (That's it. Just ""Error colon space""). The expected error message (from podman-local) is:  Error: no containers in pod a0cb944bf836e95ca0a83f98e1af151879552ac4fc424e29031c5f214227bdf6 have no dependencies, cannot start pod: no such container ",source-file | test-file,"remote: pod start empty pod: error without message console $ bin/podman-remote pod create --infra=false --name=foo a0cb944bf836e95ca0a83f98e1af151879552ac4fc424e29031c5f214227bdf6 $ bin/podman-remote pod start foo Error:  (That's it. Just ""Error colon space""). The expected error message (from podman-local) is:  Error: no containers in pod a0cb944bf836e95ca0a83f98e1af151879552ac4fc424e29031c5f214227bdf6 have no dependencies, cannot start pod: no such container  source-file test-file",bug,0.9
25026,podman,https://github.com/containers/podman/issues/25026,podman-remote: create command is wrong," Issue Description When checking the `CreateCommand` of a container created through the api we are getting some weird results.  Steps to reproduce the issue Steps to reproduce the issue 1. Create a container using the api (E.g. using Podman-Desktop) 2. Inspect the container Config.CreateCommand 3. assert command is wrong  Describe the results you received  $: podman inspect hello-container | jq .[0].Config.CreateCommand [ ""podman"", ""system"", ""service"", ""--time=0"" ]   Describe the results you expected Empty array or nil.  podman info output yaml host: arch: amd64 buildahVersion: 1.38.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.12-3.fc41.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.12, commit: ' cpuUtilization: idlePercent: 94.31 systemPercent: 0.82 userPercent: 4.87 cpus: 16 databaseBackend: sqlite distribution: distribution: fedora variant: workstation version: ""41"" eventLogger: journald freeLocks: 2031 hostname: fedora idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 kernel: 6.12.8-200.fc41.x86_64 linkmode: dynamic logDriver: journald memFree: 19032514560 memTotal: 67107631104 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.13.1-1.fc41.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.13.1 package: netavark-1.13.1-1.fc41.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.13.1 ociRuntime: name: crun package: crun-1.19.1-1.fc41.x86_64 path: /usr/bin/crun version: |- crun version 1.19.1 commit: 3e32a70c93f5aa5fea69b50256cca7fd4aa23c80 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20241211.g09478d5-1.fc41.x86_64 version: | pasta 0^20241211.g09478d5-1.fc41.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/user/1000/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: """" package: """" version: """" swapFree: 8589930496 swapTotal: 8589930496 uptime: 52h 2m 21.00s (Approximately 2.17 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /home/axel7083/.config/containers/storage.conf containerStore: number: 10 paused: 0 running: 0 stopped: 10 graphDriverName: overlay graphOptions: {} graphRoot: /home/axel7083/.local/share/containers/storage graphRootAllocated: 1022505254912 graphRootUsed: 396294459392 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 670 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/axel7083/.local/share/containers/storage/volumes version: APIVersion: 5.3.1 Built: 1732147200 BuiltTime: Thu Nov 21 01:00:00 2024 GitCommit: """" GoVersion: go1.23.3 Os: linux OsArch: linux/amd64 Version: 5.3.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details _No response_  Additional information Linked issues - https://github.com/containers/podman/issues/6649 - https://github.com/containers/podman/pull/6896",source-file | source-file | source-file | source-file | test-file,"podman-remote: create command is wrong  Issue Description When checking the `CreateCommand` of a container created through the api we are getting some weird results.  Steps to reproduce the issue Steps to reproduce the issue 1. Create a container using the api (E.g. using Podman-Desktop) 2. Inspect the container Config.CreateCommand 3. assert command is wrong  Describe the results you received  $: podman inspect hello-container | jq .[0].Config.CreateCommand [ ""podman"", ""system"", ""service"", ""--time=0"" ]   Describe the results you expected Empty array or nil.  podman info output yaml host: arch: amd64 buildahVersion: 1.38.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.12-3.fc41.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.12, commit: ' cpuUtilization: idlePercent: 94.31 systemPercent: 0.82 userPercent: 4.87 cpus: 16 databaseBackend: sqlite distribution: distribution: fedora variant: workstation version: ""41"" eventLogger: journald freeLocks: 2031 hostname: fedora idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 kernel: 6.12.8-200.fc41.x86_64 linkmode: dynamic logDriver: journald memFree: 19032514560 memTotal: 67107631104 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.13.1-1.fc41.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.13.1 package: netavark-1.13.1-1.fc41.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.13.1 ociRuntime: name: crun package: crun-1.19.1-1.fc41.x86_64 path: /usr/bin/crun version: |- crun version 1.19.1 commit: 3e32a70c93f5aa5fea69b50256cca7fd4aa23c80 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20241211.g09478d5-1.fc41.x86_64 version: | pasta 0^20241211.g09478d5-1.fc41.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/user/1000/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: """" package: """" version: """" swapFree: 8589930496 swapTotal: 8589930496 uptime: 52h 2m 21.00s (Approximately 2.17 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /home/axel7083/.config/containers/storage.conf containerStore: number: 10 paused: 0 running: 0 stopped: 10 graphDriverName: overlay graphOptions: {} graphRoot: /home/axel7083/.local/share/containers/storage graphRootAllocated: 1022505254912 graphRootUsed: 396294459392 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 670 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/axel7083/.local/share/containers/storage/volumes version: APIVersion: 5.3.1 Built: 1732147200 BuiltTime: Thu Nov 21 01:00:00 2024 GitCommit: """" GoVersion: go1.23.3 Os: linux OsArch: linux/amd64 Version: 5.3.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details _No response_  Additional information Linked issues - https://github.com/containers/podman/issues/6649 - https://github.com/containers/podman/pull/6896 source-file source-file source-file source-file test-file",bug,0.9
18597,podman,https://github.com/containers/podman/issues/18597,API endpoint /images/create seems to ignore the tag parameter?," Issue Description Hey there. This issue originated when trying to use Podman as a backend for GitLab Runner. The issue presented itself when GitLab Runner was attempting to pull images for performing builds as can be seen by some of the log entries:  podman[400972]: Trying to pull registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66 podman[400972]: @ - - [17/May/2023:10:50:23 +1000] ""POST /v1.40/images/create?fromImage=registry.gitlab.com%2Fgitlab-org%2Fgitlab-runner%2Fgitlab-runner-helper&tag=x86_64-dcfb4b66 HTTP/1.1"" 200 284 """" ""Go-http-client/1.1"" podman[400972]: time=""2023-05-17T10:50:23+10:00"" level=info msg=""Request Failed(Not Found): failed to find image registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: No such image"" podman[400972]: @ - - [17/May/2023:10:50:23 +1000] ""GET /v1.40/images/registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66/json HTTP/1.1"" 404 441 """" ""Go-http-client/1.1"" gitlab-runner[400957]: WARNING: Failed to pull image with policy ""if-not-present"": Error response from daemon: failed to find image registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: No such image (manager.go:241:0s) job=639118 project=39435 runner=r3KQ8vBa gitlab-runner[400957]: WARNING: Preparation failed: failed to pull image ""registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66"" with specified policies [if-not-present]: Error response from daemon: failed to find image registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: No such image (manager.go:241:0s) job=639118 project=39435 runner=r3KQ8vBa  After investigating this further, I can actually reproduce a difference in behaviour between Docker and the Podman Docker API which I believe is resulting in the issue. When hitting the `/images/create` API endpoint with a repo and tag parameter, Docker creates an image with the chosen name and tag while Podman seems to only use the name and set the tag to `latest`. More information is shown below with a repro.  Steps to reproduce the issue Steps to reproduce the issue 1. Ensure you have a system with both Podman and Docker available 2. Create an empty image using each and inspect the results as follows: bash me@myserver:~$ # Create an empty image with Docker. me@myserver:~$ curl -v -XPOST --unix-socket /run/docker.sock 'http:api/images/create?fromSrc=-&repo=myimage&tag=mytag' * Trying /run/docker.sock:0 * Connected to api (/run/docker.sock) port 80 (#0) > POST /images/create?fromSrc=-&repo=myimage&tag=mytag HTTP/1.1 > Host: api > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Docker-Experimental: false < Ostype: linux < Server: Docker/20.10.21 (linux) < Date: Wed, 17 May 2023 06:51:16 GMT < Transfer-Encoding: chunked < {""status"":""sha256:bb944196d717548f774e7d64da0c3b1ab9b567dcef3577d5db11c712b0eb8df1""} * Connection #0 to host api left intact me@myserver:~$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE myimage mytag bb944196d717 6 seconds ago 0B me@myserver:~$ curl -v --unix-socket /run/docker.sock 'http://api/images/myimage:mytag/json' * Trying /run/docker.sock:0 * Connected to api (/run/docker.sock) port 80 (#0) > GET /images/myimage:mytag/json HTTP/1.1 > Host: api > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Docker-Experimental: false < Ostype: linux < Server: Docker/20.10.21 (linux) < Date: Wed, 17 May 2023 06:51:26 GMT < Content-Length: 1424 < {""Id"":""sha256:bb944196d717548f774e7d64da0c3b1ab9b567dcef3577d5db11c712b0eb8df1"",""RepoTags"":[""myimage:mytag""],""RepoDigests"":[],""Parent"":"""",""Comment"":""Imported from -"",""Created"":""2023-05-17T06:51:16.354773741Z"",""Container"":"""",""ContainerConfig"":{""Hostname"":"""",""Domainname"":"""",""User"":"""",""AttachStdin"":false,""AttachStdout"":false,""AttachStderr"":false,""Tty"":false,""OpenStdin"":false,""StdinOnce"":false,""Env"":null,""Cmd"":null,""Image"":"""",""Volumes"":null,""WorkingDir"":"""",""Entrypoint"":null,""OnBuild"":null,""Labels"":null},""DockerVersion"":""20.10.21"",""Author"":"""",""Config"":{""Hostname"":"""",""Domainname"":"""",""User"":"""",""AttachStdin"":false,""AttachStdout"":false,""AttachStderr"":false,""Tty"":false,""OpenStdin"":false,""StdinOnce"":false,""Env"":null,""Cmd"":null,""Image"":"""",""Volumes"":null,""WorkingDir"":"""",""Entrypoint"":null,""OnBuild"":null,""Labels"":null},""Architecture"":""amd64"",""Os"":""linux"",""Size"":0,""VirtualSize"":0,""GraphDriver"":{""Data"":{""MergedDir"":""/var/lib/docker/overlay2/669ba8d7a9fcdf7b1b321ec814400ec4df2964cfdc11c65a0d320afb7e288114/merged"",""UpperDir"":""/var/lib/docker/overlay2/669ba8d7a9fcdf7b1b321ec814400ec4df2964cfdc11c65a0d320afb7e288114/diff"",""WorkDir"":""/var/lib/docker/overlay2/669ba8d7a9fcdf7b1b321ec814400ec4df2964cfdc11c65a0d320afb7e288114/work""},""Name"":""overlay2""},""RootFS"":{""Type"":""layers"",""Layers"":[""sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855""]},""Metadata"":{""LastTagTime"":""2023-05-17T16:51:16.357888207+10:00""}} * Connection #0 to host api left intact me@myserver:~$ # Create an empty image with Podman. me@myserver:~$ curl -v -XPOST --unix-socket /run/user/1000/podman/podman.sock 'http:api/images/create?fromSrc=-&repo=myimage&tag=mytag' * Trying /run/user/1000/podman/podman.sock:0 * Connected to api (/run/user/1000/podman/podman.sock) port 80 (#0) > POST /images/create?fromSrc=-&repo=myimage&tag=mytag HTTP/1.1 > Host: api > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Libpod-Api-Version: 4.5.0 < Server: Libpod/4.5.0 (linux) < X-Reference-Id: 0xc0000c0490 < Date: Wed, 17 May 2023 06:51:45 GMT < Content-Length: 198 < {""status"":""sha256:b27eea98c26530caa6fbb4754d1dd08d401f5c839e0ae4068fff65be1668ddb1"",""progress"":"""",""progressDetail"":{},""id"":""sha256:b27eea98c26530caa6fbb4754d1dd08d401f5c839e0ae4068fff65be1668ddb1""} * Connection #0 to host api left intact me@myserver:~$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/myimage latest b27eea98c265 3 seconds ago 1.09 kB me@myserver:~$ curl -v --unix-socket /run/user/1000/podman/podman.sock 'http://api/images/myimage:mytag/json' * Trying /run/user/1000/podman/podman.sock:0 * Connected to api (/run/user/1000/podman/podman.sock) port 80 (#0) > GET /images/myimage:mytag/json HTTP/1.1 > Host: api > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 404 Not Found < Api-Version: 1.41 < Content-Type: application/json < Libpod-Api-Version: 4.5.0 < Server: Libpod/4.5.0 (linux) < X-Reference-Id: 0xc0000c92a0 < Date: Wed, 17 May 2023 06:51:59 GMT < Content-Length: 205 < {""cause"":""failed to find image myimage:mytag: docker.io/library/myimage:mytag: No such image"",""message"":""failed to find image myimage:mytag: docker.io/library/myimage:mytag: No such image"",""response"":404} * Connection #0 to host api left intact   Describe the results you received As you can see, Docker creates the image `myimage:mytag` while Podman creates the image `docker.io/library/myimage:latest` where the tag has not been respected by Podman.  Describe the results you expected I expected Podman to create the image with the tag `mytag` as Docker does.  podman info output yaml host: arch: amd64 buildahVersion: 1.30.0 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon_100:2.1.2~0_amd64 path: /usr/libexec/podman/conmon version: 'conmon version 2.1.2, commit: ' cpuUtilization: idlePercent: 99.08 systemPercent: 0.23 userPercent: 0.69 cpus: 4 databaseBackend: boltdb distribution: codename: jammy distribution: ubuntu version: ""22.04"" eventLogger: journald hostname: myserver.mydomain idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.15.0-71-generic linkmode: dynamic logDriver: journald memFree: 7083307008 memTotal: 16753397760 networkBackend: cni ociRuntime: name: runc package: runc_1.1.0-0ubuntu1_amd64 path: /usr/sbin/runc version: |- runc version 1.1.0-0ubuntu1 spec: 1.0.2-dev go: go1.17.3 libseccomp: 2.5.3 os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns_1.0.1-2_amd64 version: |- slirp4netns version 1.0.1 commit: 6a7b16babc95b6a3056b33fb45b74a6f62262dd4 libslirp: 4.6.1 swapFree: 4294963200 swapTotal: 4294963200 uptime: 314h 16m 0.00s (Approximately 13.08 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io - quay.io store: configFile: /home/me/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/me/.local/share/containers/storage graphRootAllocated: 103240073216 graphRootUsed: 48463527936 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/me/.local/share/containers/storage/volumes version: APIVersion: 4.5.0 Built: 0 BuiltTime: Thu Jan 1 10:00:00 1970 GitCommit: """" GoVersion: go1.18.1 Os: linux OsArch: linux/amd64 Version: 4.5.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details I've replicated this problem on various versions of Podman ranging from 3.4.4 to the latest on both Ubuntu 22.04 and AlmaLinux 9.  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",source-file | test-file | test-file | source-file | test-file | test-file,"API endpoint /images/create seems to ignore the tag parameter?  Issue Description Hey there. This issue originated when trying to use Podman as a backend for GitLab Runner. The issue presented itself when GitLab Runner was attempting to pull images for performing builds as can be seen by some of the log entries:  podman[400972]: Trying to pull registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66 podman[400972]: @ - - [17/May/2023:10:50:23 +1000] ""POST /v1.40/images/create?fromImage=registry.gitlab.com%2Fgitlab-org%2Fgitlab-runner%2Fgitlab-runner-helper&tag=x86_64-dcfb4b66 HTTP/1.1"" 200 284 """" ""Go-http-client/1.1"" podman[400972]: time=""2023-05-17T10:50:23+10:00"" level=info msg=""Request Failed(Not Found): failed to find image registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: No such image"" podman[400972]: @ - - [17/May/2023:10:50:23 +1000] ""GET /v1.40/images/registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66/json HTTP/1.1"" 404 441 """" ""Go-http-client/1.1"" gitlab-runner[400957]: WARNING: Failed to pull image with policy ""if-not-present"": Error response from daemon: failed to find image registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: No such image (manager.go:241:0s) job=639118 project=39435 runner=r3KQ8vBa gitlab-runner[400957]: WARNING: Preparation failed: failed to pull image ""registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66"" with specified policies [if-not-present]: Error response from daemon: failed to find image registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: registry.gitlab.com/gitlab-org/gitlab-runner/gitlab-runner-helper:x86_64-dcfb4b66: No such image (manager.go:241:0s) job=639118 project=39435 runner=r3KQ8vBa  After investigating this further, I can actually reproduce a difference in behaviour between Docker and the Podman Docker API which I believe is resulting in the issue. When hitting the `/images/create` API endpoint with a repo and tag parameter, Docker creates an image with the chosen name and tag while Podman seems to only use the name and set the tag to `latest`. More information is shown below with a repro.  Steps to reproduce the issue Steps to reproduce the issue 1. Ensure you have a system with both Podman and Docker available 2. Create an empty image using each and inspect the results as follows: bash me@myserver:~$ # Create an empty image with Docker. me@myserver:~$ curl -v -XPOST --unix-socket /run/docker.sock 'http:api/images/create?fromSrc=-&repo=myimage&tag=mytag' * Trying /run/docker.sock:0 * Connected to api (/run/docker.sock) port 80 (#0) > POST /images/create?fromSrc=-&repo=myimage&tag=mytag HTTP/1.1 > Host: api > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Docker-Experimental: false < Ostype: linux < Server: Docker/20.10.21 (linux) < Date: Wed, 17 May 2023 06:51:16 GMT < Transfer-Encoding: chunked < {""status"":""sha256:bb944196d717548f774e7d64da0c3b1ab9b567dcef3577d5db11c712b0eb8df1""} * Connection #0 to host api left intact me@myserver:~$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE myimage mytag bb944196d717 6 seconds ago 0B me@myserver:~$ curl -v --unix-socket /run/docker.sock 'http://api/images/myimage:mytag/json' * Trying /run/docker.sock:0 * Connected to api (/run/docker.sock) port 80 (#0) > GET /images/myimage:mytag/json HTTP/1.1 > Host: api > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Docker-Experimental: false < Ostype: linux < Server: Docker/20.10.21 (linux) < Date: Wed, 17 May 2023 06:51:26 GMT < Content-Length: 1424 < {""Id"":""sha256:bb944196d717548f774e7d64da0c3b1ab9b567dcef3577d5db11c712b0eb8df1"",""RepoTags"":[""myimage:mytag""],""RepoDigests"":[],""Parent"":"""",""Comment"":""Imported from -"",""Created"":""2023-05-17T06:51:16.354773741Z"",""Container"":"""",""ContainerConfig"":{""Hostname"":"""",""Domainname"":"""",""User"":"""",""AttachStdin"":false,""AttachStdout"":false,""AttachStderr"":false,""Tty"":false,""OpenStdin"":false,""StdinOnce"":false,""Env"":null,""Cmd"":null,""Image"":"""",""Volumes"":null,""WorkingDir"":"""",""Entrypoint"":null,""OnBuild"":null,""Labels"":null},""DockerVersion"":""20.10.21"",""Author"":"""",""Config"":{""Hostname"":"""",""Domainname"":"""",""User"":"""",""AttachStdin"":false,""AttachStdout"":false,""AttachStderr"":false,""Tty"":false,""OpenStdin"":false,""StdinOnce"":false,""Env"":null,""Cmd"":null,""Image"":"""",""Volumes"":null,""WorkingDir"":"""",""Entrypoint"":null,""OnBuild"":null,""Labels"":null},""Architecture"":""amd64"",""Os"":""linux"",""Size"":0,""VirtualSize"":0,""GraphDriver"":{""Data"":{""MergedDir"":""/var/lib/docker/overlay2/669ba8d7a9fcdf7b1b321ec814400ec4df2964cfdc11c65a0d320afb7e288114/merged"",""UpperDir"":""/var/lib/docker/overlay2/669ba8d7a9fcdf7b1b321ec814400ec4df2964cfdc11c65a0d320afb7e288114/diff"",""WorkDir"":""/var/lib/docker/overlay2/669ba8d7a9fcdf7b1b321ec814400ec4df2964cfdc11c65a0d320afb7e288114/work""},""Name"":""overlay2""},""RootFS"":{""Type"":""layers"",""Layers"":[""sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855""]},""Metadata"":{""LastTagTime"":""2023-05-17T16:51:16.357888207+10:00""}} * Connection #0 to host api left intact me@myserver:~$ # Create an empty image with Podman. me@myserver:~$ curl -v -XPOST --unix-socket /run/user/1000/podman/podman.sock 'http:api/images/create?fromSrc=-&repo=myimage&tag=mytag' * Trying /run/user/1000/podman/podman.sock:0 * Connected to api (/run/user/1000/podman/podman.sock) port 80 (#0) > POST /images/create?fromSrc=-&repo=myimage&tag=mytag HTTP/1.1 > Host: api > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Libpod-Api-Version: 4.5.0 < Server: Libpod/4.5.0 (linux) < X-Reference-Id: 0xc0000c0490 < Date: Wed, 17 May 2023 06:51:45 GMT < Content-Length: 198 < {""status"":""sha256:b27eea98c26530caa6fbb4754d1dd08d401f5c839e0ae4068fff65be1668ddb1"",""progress"":"""",""progressDetail"":{},""id"":""sha256:b27eea98c26530caa6fbb4754d1dd08d401f5c839e0ae4068fff65be1668ddb1""} * Connection #0 to host api left intact me@myserver:~$ podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/myimage latest b27eea98c265 3 seconds ago 1.09 kB me@myserver:~$ curl -v --unix-socket /run/user/1000/podman/podman.sock 'http://api/images/myimage:mytag/json' * Trying /run/user/1000/podman/podman.sock:0 * Connected to api (/run/user/1000/podman/podman.sock) port 80 (#0) > GET /images/myimage:mytag/json HTTP/1.1 > Host: api > User-Agent: curl/7.81.0 > Accept: */* > * Mark bundle as not supporting multiuse < HTTP/1.1 404 Not Found < Api-Version: 1.41 < Content-Type: application/json < Libpod-Api-Version: 4.5.0 < Server: Libpod/4.5.0 (linux) < X-Reference-Id: 0xc0000c92a0 < Date: Wed, 17 May 2023 06:51:59 GMT < Content-Length: 205 < {""cause"":""failed to find image myimage:mytag: docker.io/library/myimage:mytag: No such image"",""message"":""failed to find image myimage:mytag: docker.io/library/myimage:mytag: No such image"",""response"":404} * Connection #0 to host api left intact   Describe the results you received As you can see, Docker creates the image `myimage:mytag` while Podman creates the image `docker.io/library/myimage:latest` where the tag has not been respected by Podman.  Describe the results you expected I expected Podman to create the image with the tag `mytag` as Docker does.  podman info output yaml host: arch: amd64 buildahVersion: 1.30.0 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon_100:2.1.2~0_amd64 path: /usr/libexec/podman/conmon version: 'conmon version 2.1.2, commit: ' cpuUtilization: idlePercent: 99.08 systemPercent: 0.23 userPercent: 0.69 cpus: 4 databaseBackend: boltdb distribution: codename: jammy distribution: ubuntu version: ""22.04"" eventLogger: journald hostname: myserver.mydomain idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.15.0-71-generic linkmode: dynamic logDriver: journald memFree: 7083307008 memTotal: 16753397760 networkBackend: cni ociRuntime: name: runc package: runc_1.1.0-0ubuntu1_amd64 path: /usr/sbin/runc version: |- runc version 1.1.0-0ubuntu1 spec: 1.0.2-dev go: go1.17.3 libseccomp: 2.5.3 os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns_1.0.1-2_amd64 version: |- slirp4netns version 1.0.1 commit: 6a7b16babc95b6a3056b33fb45b74a6f62262dd4 libslirp: 4.6.1 swapFree: 4294963200 swapTotal: 4294963200 uptime: 314h 16m 0.00s (Approximately 13.08 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io - quay.io store: configFile: /home/me/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/me/.local/share/containers/storage graphRootAllocated: 103240073216 graphRootUsed: 48463527936 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/me/.local/share/containers/storage/volumes version: APIVersion: 4.5.0 Built: 0 BuiltTime: Thu Jan 1 10:00:00 1970 GitCommit: """" GoVersion: go1.18.1 Os: linux OsArch: linux/amd64 Version: 4.5.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details I've replicated this problem on various versions of Podman ranging from 3.4.4 to the latest on both Ubuntu 22.04 and AlmaLinux 9.  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting source-file test-file test-file source-file test-file test-file",bug,0.95
20375,podman,https://github.com/containers/podman/issues/20375,"REST API: `/v1.24/images/${img}/history` contains `""Size"":0` where docker has `""Size"":7458929`"," Issue Description REST API: Result from `/v1.24/images/${img}/history` contains `""Size"":0` where docker has `""Size"":7458929`  Steps to reproduce the issue On Fedora CoreOS 39.20231006.1.0 (aarch64) VM perform the following steps: 1. `sudo -i` 2. `systemctl start docker` 3. `useradd test` 4. `usermod -aG docker` 5. `machinectl shell --uid test` 6. `img=$(podman pull alpine:3.17.2)` 7. `docker pull alpine:3.17.2` 8. `systemctl --user start podman.socket` 9. `curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history > docker-result.txt` 10. `curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history > podman-result.txt` 11. `cat result-docker.txt | jq '.[1].Size'` 12. `cat result-podman.txt | jq '.[1].Size'`  Describe the results you received Step 11:  $ cat result-docker.txt | jq '.[1].Size' 7458929  Step 12:  $ cat result-podman.txt | jq '.[1].Size' 0   Describe the results you expected I expected to see the same number in both __Step 11__ and __Step 12__  podman info output yaml host: arch: arm64 buildahVersion: 1.32.0 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.7-3.fc39.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 97.6 systemPercent: 1.37 userPercent: 1.03 cpus: 1 databaseBackend: boltdb distribution: distribution: fedora variant: coreos version: ""39"" eventLogger: journald freeLocks: 2048 hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1011 size: 1 - container_id: 1 host_id: 1245184 size: 65536 uidmap: - container_id: 0 host_id: 1011 size: 1 - container_id: 1 host_id: 1245184 size: 65536 kernel: 6.5.5-300.fc39.aarch64 linkmode: dynamic logDriver: journald memFree: 1244913664 memTotal: 1980260352 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.7.0-2.fc39.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.7.0 package: netavark-1.7.0-2.fc39.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.7.0 ociRuntime: name: crun package: crun-1.9.2-1.fc39.aarch64 path: /usr/bin/crun version: |- crun version 1.9.2 commit: 35274d346d2e9ffeacb22cc11590b0266a23d634 rundir: /run/user/1011/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20230908.g05627dc-1.fc39.aarch64 version: | pasta 0^20230908.g05627dc-1.fc39.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/user/1011/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.1-1.fc39.aarch64 version: |- slirp4netns version 1.2.1 commit: 09e31e92fa3d2a1d3ca261adaeb012c8d75a8194 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 0h 27m 34.00s plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /var/home/test11/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/test11/.local/share/containers/storage graphRootAllocated: 24091013120 graphRootUsed: 7668953088 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/user/1011/containers transientStore: false volumePath: /var/home/test11/.local/share/containers/storage/volumes version: APIVersion: 4.7.0 Built: 1695838660 BuiltTime: Wed Sep 27 18:17:40 2023 GitCommit: """" GoVersion: go1.21.1 Os: linux OsArch: linux/arm64 Version: 4.7.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details  $ rpm -q podman podman-4.7.0-1.fc39.aarch64 $ rpm -q moby-engine moby-engine-24.0.5-1.fc39.aarch64 $   Additional information The whole result from the REST API calls above.  $ cat result-docker.txt | jq . [ { ""Comment"": """", ""Created"": 1676064248, ""CreatedBy"": ""/bin/sh -c #(nop) CMD [\""/bin/sh\""]"", ""Id"": ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"", ""Size"": 0, ""Tags"": [ ""alpine:3.17.2"" ] }, { ""Comment"": """", ""Created"": 1676064248, ""CreatedBy"": ""/bin/sh -c #(nop) ADD file:9bd9ea42a9f3bdc769e80c6b8a4b117d65f73ae68e155a6172a1184e7ac8bcc1 in / "", ""Id"": ""<missing>"", ""Size"": 7458929, ""Tags"": null } ]   $ cat result-podman.txt | jq . [ { ""Id"": ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"", ""Created"": 1676064248, ""CreatedBy"": ""/bin/sh -c #(nop) CMD [\""/bin/sh\""]"", ""Tags"": [ ""docker.io/library/alpine:3.17.2"" ], ""Size"": 0, ""Comment"": """" }, { ""Id"": ""sha256:<missing>"", ""Created"": 1676064248, ""CreatedBy"": ""/bin/sh -c #(nop) ADD file:9bd9ea42a9f3bdc769e80c6b8a4b117d65f73ae68e155a6172a1184e7ac8bcc1 in / "", ""Tags"": null, ""Size"": 0, ""Comment"": """" } ] ",other-file | other-file | test-file | source-file | other-file,"REST API: `/v1.24/images/${img}/history` contains `""Size"":0` where docker has `""Size"":7458929`  Issue Description REST API: Result from `/v1.24/images/${img}/history` contains `""Size"":0` where docker has `""Size"":7458929`  Steps to reproduce the issue On Fedora CoreOS 39.20231006.1.0 (aarch64) VM perform the following steps: 1. `sudo -i` 2. `systemctl start docker` 3. `useradd test` 4. `usermod -aG docker` 5. `machinectl shell --uid test` 6. `img=$(podman pull alpine:3.17.2)` 7. `docker pull alpine:3.17.2` 8. `systemctl --user start podman.socket` 9. `curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history > docker-result.txt` 10. `curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history > podman-result.txt` 11. `cat result-docker.txt | jq '.[1].Size'` 12. `cat result-podman.txt | jq '.[1].Size'`  Describe the results you received Step 11:  $ cat result-docker.txt | jq '.[1].Size' 7458929  Step 12:  $ cat result-podman.txt | jq '.[1].Size' 0   Describe the results you expected I expected to see the same number in both __Step 11__ and __Step 12__  podman info output yaml host: arch: arm64 buildahVersion: 1.32.0 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.7-3.fc39.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 97.6 systemPercent: 1.37 userPercent: 1.03 cpus: 1 databaseBackend: boltdb distribution: distribution: fedora variant: coreos version: ""39"" eventLogger: journald freeLocks: 2048 hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1011 size: 1 - container_id: 1 host_id: 1245184 size: 65536 uidmap: - container_id: 0 host_id: 1011 size: 1 - container_id: 1 host_id: 1245184 size: 65536 kernel: 6.5.5-300.fc39.aarch64 linkmode: dynamic logDriver: journald memFree: 1244913664 memTotal: 1980260352 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.7.0-2.fc39.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.7.0 package: netavark-1.7.0-2.fc39.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.7.0 ociRuntime: name: crun package: crun-1.9.2-1.fc39.aarch64 path: /usr/bin/crun version: |- crun version 1.9.2 commit: 35274d346d2e9ffeacb22cc11590b0266a23d634 rundir: /run/user/1011/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20230908.g05627dc-1.fc39.aarch64 version: | pasta 0^20230908.g05627dc-1.fc39.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/user/1011/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.1-1.fc39.aarch64 version: |- slirp4netns version 1.2.1 commit: 09e31e92fa3d2a1d3ca261adaeb012c8d75a8194 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 0h 27m 34.00s plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /var/home/test11/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/test11/.local/share/containers/storage graphRootAllocated: 24091013120 graphRootUsed: 7668953088 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/user/1011/containers transientStore: false volumePath: /var/home/test11/.local/share/containers/storage/volumes version: APIVersion: 4.7.0 Built: 1695838660 BuiltTime: Wed Sep 27 18:17:40 2023 GitCommit: """" GoVersion: go1.21.1 Os: linux OsArch: linux/arm64 Version: 4.7.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details  $ rpm -q podman podman-4.7.0-1.fc39.aarch64 $ rpm -q moby-engine moby-engine-24.0.5-1.fc39.aarch64 $   Additional information The whole result from the REST API calls above.  $ cat result-docker.txt | jq . [ { ""Comment"": """", ""Created"": 1676064248, ""CreatedBy"": ""/bin/sh -c #(nop) CMD [\""/bin/sh\""]"", ""Id"": ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"", ""Size"": 0, ""Tags"": [ ""alpine:3.17.2"" ] }, { ""Comment"": """", ""Created"": 1676064248, ""CreatedBy"": ""/bin/sh -c #(nop) ADD file:9bd9ea42a9f3bdc769e80c6b8a4b117d65f73ae68e155a6172a1184e7ac8bcc1 in / "", ""Id"": ""<missing>"", ""Size"": 7458929, ""Tags"": null } ]   $ cat result-podman.txt | jq . [ { ""Id"": ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"", ""Created"": 1676064248, ""CreatedBy"": ""/bin/sh -c #(nop) CMD [\""/bin/sh\""]"", ""Tags"": [ ""docker.io/library/alpine:3.17.2"" ], ""Size"": 0, ""Comment"": """" }, { ""Id"": ""sha256:<missing>"", ""Created"": 1676064248, ""CreatedBy"": ""/bin/sh -c #(nop) ADD file:9bd9ea42a9f3bdc769e80c6b8a4b117d65f73ae68e155a6172a1184e7ac8bcc1 in / "", ""Tags"": null, ""Size"": 0, ""Comment"": """" } ]  other-file other-file test-file source-file other-file",bug,0.95
14291,podman,https://github.com/containers/podman/issues/14291,Podman's Docker-compatible REST API and previously pulled images.,"/kind bug **Description** Since podman 4, when using the Docker-compatible REST API, trying to start a container that was previously pulled fails because the container does not exist on Docker Hub. More precisely I get a 404 for docker.io/library/NAME:TAG. This issue is fixed by setting `compat_api_enforce_docker_hub` to `false`. I can also report that Docker is perfectly happy with starting the containers with the same API request. This was originally discussed [here](https://github.com/containers/podman/issues/12320#issuecomment-1131537455) **Steps to reproduce the issue:** 1. Login to a private registry 2. Pull a container 3. Logout from the registry 4. Try to start the container using the Docker-compatible REST API **Describe the results you received:** 404 on `docker.io/library/NAME:TAG` Please note that I removed the environment variables sections of the requests.  # podman --version podman version 4.0.2 # podman --log-level=debug system service -t 0 tcp:0.0.0.0:2376 INFO[0000] podman filtering at log level debug DEBU[0000] Called service.PersistentPreRunE(podman --log-level=debug system service -t 0 tcp:0.0.0.0:2376) DEBU[0000] Merged system config ""/usr/share/containers/containers.conf"" DEBU[0000] Merged system config ""/etc/containers/containers.conf"" DEBU[0000] Using conmon: ""/usr/bin/conmon"" DEBU[0000] Initializing boltdb state at /var/lib/containers/storage/libpod/bolt_state.db DEBU[0000] Using graph driver overlay DEBU[0000] Using graph root /var/lib/containers/storage DEBU[0000] Using run root /run/containers/storage DEBU[0000] Using static dir /var/lib/containers/storage/libpod DEBU[0000] Using tmp dir /run/libpod DEBU[0000] Using volume path /var/lib/containers/storage/volumes DEBU[0000] Set libpod namespace to """" DEBU[0000] [graphdriver] trying provided driver ""overlay"" DEBU[0000] Cached value indicated that overlay is supported DEBU[0000] Cached value indicated that metacopy is being used DEBU[0000] Cached value indicated that native-diff is not being used INFO[0000] Not using native diff for overlay, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled DEBU[0000] backingFs=xfs, projectQuotaSupported=false, useNativeDiff=false, usingMetacopy=true DEBU[0000] Initializing event backend file DEBU[0000] Configured OCI runtime runsc initialization failed: no valid executable found for OCI runtime runsc: invalid argument DEBU[0000] Configured OCI runtime krun initialization failed: no valid executable found for OCI runtime krun: invalid argument DEBU[0000] Configured OCI runtime kata initialization failed: no valid executable found for OCI runtime kata: invalid argument DEBU[0000] Using OCI runtime ""/usr/bin/runc"" INFO[0000] Setting parallel job count to 25 DEBU[0000] registered SIGHUP watcher for config DEBU[0000] CORS Headers were not set INFO[0000] API service listening on ""[::]:2376"" DEBU[0000] waiting for SIGHUP to reload configuration DEBU[0000] API service(s) shutting down, idle for 0s DEBU[0000] API service shutdown request ignored as timeout Duration is UnlimitedService DEBU[0020] IdleTracker:new 0m+0h/0t connection(s) X-Reference-Id=0xc000011318 DEBU[0020] IdleTracker:new 0m+0h/1t connection(s) X-Reference-Id=0xc000011320 DEBU[0020] IdleTracker:active 0m+0h/2t connection(s) X-Reference-Id=0xc000011320 DEBU[0020] IdleTracker:active 1m+0h/2t connection(s) X-Reference-Id=0xc000011318 DEBU[0020] Looking up image ""NAME:TAG"" in local containers storage DEBU[0020] Normalized platform linux/amd64 to {amd64 linux [] } DEBU[0020] Trying ""NAME:TAG""  DEBU[0020] Loading registries configuration ""/etc/containers/registries.conf"" DEBU[0020] Looking up image ""NAME:TAG"" in local containers storage DEBU[0020] Normalized platform linux/amd64 to {amd64 linux [] } DEBU[0020] Trying ""NAME:TAG""  DEBU[0020] Loading registries configuration ""/etc/containers/registries.conf.d/000-shortnames.conf"" DEBU[0020] Loading registries configuration ""/etc/containers/registries.conf.d/001-rhel-shortnames.conf"" DEBU[0020] Loading registries configuration ""/etc/containers/registries.conf.d/002-rhel-shortnames-overrides.conf"" DEBU[0020] Trying ""localhost/NAME:TAG""  DEBU[0020] Trying ""registry.fedoraproject.org/NAME:TAG""  DEBU[0020] Trying ""localhost/NAME:TAG""  DEBU[0020] Trying ""registry.access.redhat.com/NAME:TAG""  DEBU[0020] Trying ""registry.fedoraproject.org/NAME:TAG""  DEBU[0020] Trying ""registry.centos.org/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""registry.access.redhat.com/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""registry.centos.org/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""REGISTRY/PATH/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] parsed reference into ""[overlay@/var/lib/containers/storage+/run/containers/storage:overlay.mountopt=nodev,metacopy=on]@5fb70826285b943f31f0c4b0dc0247d29177941c6e01a99b188bdda633244892"" DEBU[0020] Found image ""NAME:TAG"" as ""REGISTRY/PATH/NAME:TAG"" in local containers storage DEBU[0020] Trying ""REGISTRY/PATH/NAME:TAG""  DEBU[0020] parsed reference into ""[overlay@/var/lib/containers/storage+/run/containers/storage:overlay.mountopt=nodev,metacopy=on]@5fb70826285b943f31f0c4b0dc0247d29177941c6e01a99b188bdda633244892"" DEBU[0020] Found image ""NAME:TAG"" as ""REGISTRY/PATH/NAME:TAG"" in local containers storage DEBU[0020] Found image ""NAME:TAG"" as ""REGISTRY/PATH/NAME:TAG"" in local containers storage ([overlay@/var/lib/containers/storage+/run/containers/storage:overlay.mountopt=nodev,metacopy=on]@5fb70826285b943f31f0c4b0dc0247d29177941c6e01a99b188bdda633244892) DEBU[0020] Looking up image ""docker.io/library/NAME:TAG"" in local containers storage DEBU[0020] Normalized platform linux/amd64 to {amd64 linux [] } DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Found image ""NAME:TAG"" as ""REGISTRY/PATH/NAME:TAG"" in local containers storage ([overlay@/var/lib/containers/storage+/run/containers/storage:overlay.mountopt=nodev,metacopy=on]@5fb70826285b943f31f0c4b0dc0247d29177941c6e01a99b188bdda633244892) DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Looking up image ""docker.io/library/NAME:TAG"" in local containers storage DEBU[0020] Normalized platform linux/amd64 to {amd64 linux [] } DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  INFO[0020] Request Failed(Not Found): No such image: docker.io/library/NAME:TAG: image not known DEBU[0020] Trying ""docker.io/library/NAME:TAG""  INFO[0020] Request Failed(Not Found): No such image: docker.io/library/NAME:TAG: image not known 34.74.239.56 - - [19/May/2022:11:31:19 +0000] ""POST /containers/create?Image=NAME%3ATAG&Tty=true&Cmd=%5B%22%2Fopt%2Fbin%2Fentry_point.sh%22%5D&HostConfig=%7B%22Privileged%22%3Atrue%2C%22Binds%22%3A%5B%22%2Fdev%2Fshm%3A%2Fdev%2Fshm%22%2C%22%2Fdata%2Fvideo%3A%2Fvideo%22%2C%22%2Fdata%2Fbrowsers%3A%2Fbrowsers%22%5D%7D&name=overconfident-wish-2&Env=REMOVED_BY_PETER HTTP/1.1"" 404 133 """" """" 34.74.239.56 - - [19/May/2022:11:31:19 +0000] ""POST /containers/create?Image=NAME%3ATAG&Tty=true&Cmd=%5B%22%2Fopt%2Fbin%2Fentry_point.sh%22%5D&HostConfig=%7B%22Privileged%22%3Atrue%2C%22Binds%22%3A%5B%22%2Fdev%2Fshm%3A%2Fdev%2Fshm%22%2C%22%2Fdata%2Fvideo%3A%2Fvideo%22%2C%22%2Fdata%2Fbrowsers%3A%2Fbrowsers%22%5D%2C%22PortBindings%22%3A%7B%225900%2Ftcp%22%3A%5B%7B%22HostPort%22%3A%225900%22%7D%5D%2C%229229%2Ftcp%22%3A%5B%7B%22HostPort%22%3A%229229%22%7D%5D%7D%7D&name=overconfident-wish-1&Env=REMOVED_BY_PETER HTTP/1.1"" 404 133 """" """" DEBU[0020] IdleTracker:closed 2m+0h/2t connection(s) X-Reference-Id=0xc000011320 DEBU[0020] IdleTracker:closed 1m+0h/2t connection(s) X-Reference-Id=0xc000011318  **Describe the results you expected:** The container should start as it is found on local storage. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  podman version 4.0.2  **Output of `podman info --debug`:**  host: arch: amd64 buildahVersion: 1.24.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-1.module_el8.6.0+2877+8e437bf5.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: edfc4e28654b9f8e3597bb8f87c6af099a50261f' cpus: 8 distribution: distribution: '""spearlineos""' version: ""8.6"" eventLogger: file hostname: peter-from-image1 idMappings: gidmap: null uidmap: null kernel: 4.18.0-372.9.1.el8.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 4423311360 memTotal: 8340979712 networkBackend: cni ociRuntime: name: runc package: runc-1.0.3-1.module_el8.6.0+2877+8e437bf5.x86_64 path: /usr/bin/runc version: |- runc version 1.0.3 spec: 1.0.2-dev go: go1.17.7 libseccomp: 2.5.2 os: linux remoteSocket: path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.1.8-2.module_el8.6.0+2877+8e437bf5.x86_64 version: |- slirp4netns version 1.1.8 commit: d361001f495417b880f20329121e3aa431a8f90f libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 2209345536 swapTotal: 2209345536 uptime: 1h 18m 47.63s (Approximately 0.04 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - registry.centos.org - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 4 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.0.2 Built: 1652194338 BuiltTime: Tue May 10 14:52:18 2022 GitCommit: """" GoVersion: go1.17.7 OsArch: linux/amd64 Version: 4.0.2  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman-4.0.2-5.module_el8.6.0+2877+8e437bf5.x86_64  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** No **Additional environment details (AWS, VirtualBox, physical, etc.):** GCP",source-file | test-file | source-file | test-file | source-file | test-file,"Podman's Docker-compatible REST API and previously pulled images. /kind bug **Description** Since podman 4, when using the Docker-compatible REST API, trying to start a container that was previously pulled fails because the container does not exist on Docker Hub. More precisely I get a 404 for docker.io/library/NAME:TAG. This issue is fixed by setting `compat_api_enforce_docker_hub` to `false`. I can also report that Docker is perfectly happy with starting the containers with the same API request. This was originally discussed [here](https://github.com/containers/podman/issues/12320#issuecomment-1131537455) **Steps to reproduce the issue:** 1. Login to a private registry 2. Pull a container 3. Logout from the registry 4. Try to start the container using the Docker-compatible REST API **Describe the results you received:** 404 on `docker.io/library/NAME:TAG` Please note that I removed the environment variables sections of the requests.  # podman --version podman version 4.0.2 # podman --log-level=debug system service -t 0 tcp:0.0.0.0:2376 INFO[0000] podman filtering at log level debug DEBU[0000] Called service.PersistentPreRunE(podman --log-level=debug system service -t 0 tcp:0.0.0.0:2376) DEBU[0000] Merged system config ""/usr/share/containers/containers.conf"" DEBU[0000] Merged system config ""/etc/containers/containers.conf"" DEBU[0000] Using conmon: ""/usr/bin/conmon"" DEBU[0000] Initializing boltdb state at /var/lib/containers/storage/libpod/bolt_state.db DEBU[0000] Using graph driver overlay DEBU[0000] Using graph root /var/lib/containers/storage DEBU[0000] Using run root /run/containers/storage DEBU[0000] Using static dir /var/lib/containers/storage/libpod DEBU[0000] Using tmp dir /run/libpod DEBU[0000] Using volume path /var/lib/containers/storage/volumes DEBU[0000] Set libpod namespace to """" DEBU[0000] [graphdriver] trying provided driver ""overlay"" DEBU[0000] Cached value indicated that overlay is supported DEBU[0000] Cached value indicated that metacopy is being used DEBU[0000] Cached value indicated that native-diff is not being used INFO[0000] Not using native diff for overlay, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled DEBU[0000] backingFs=xfs, projectQuotaSupported=false, useNativeDiff=false, usingMetacopy=true DEBU[0000] Initializing event backend file DEBU[0000] Configured OCI runtime runsc initialization failed: no valid executable found for OCI runtime runsc: invalid argument DEBU[0000] Configured OCI runtime krun initialization failed: no valid executable found for OCI runtime krun: invalid argument DEBU[0000] Configured OCI runtime kata initialization failed: no valid executable found for OCI runtime kata: invalid argument DEBU[0000] Using OCI runtime ""/usr/bin/runc"" INFO[0000] Setting parallel job count to 25 DEBU[0000] registered SIGHUP watcher for config DEBU[0000] CORS Headers were not set INFO[0000] API service listening on ""[::]:2376"" DEBU[0000] waiting for SIGHUP to reload configuration DEBU[0000] API service(s) shutting down, idle for 0s DEBU[0000] API service shutdown request ignored as timeout Duration is UnlimitedService DEBU[0020] IdleTracker:new 0m+0h/0t connection(s) X-Reference-Id=0xc000011318 DEBU[0020] IdleTracker:new 0m+0h/1t connection(s) X-Reference-Id=0xc000011320 DEBU[0020] IdleTracker:active 0m+0h/2t connection(s) X-Reference-Id=0xc000011320 DEBU[0020] IdleTracker:active 1m+0h/2t connection(s) X-Reference-Id=0xc000011318 DEBU[0020] Looking up image ""NAME:TAG"" in local containers storage DEBU[0020] Normalized platform linux/amd64 to {amd64 linux [] } DEBU[0020] Trying ""NAME:TAG""  DEBU[0020] Loading registries configuration ""/etc/containers/registries.conf"" DEBU[0020] Looking up image ""NAME:TAG"" in local containers storage DEBU[0020] Normalized platform linux/amd64 to {amd64 linux [] } DEBU[0020] Trying ""NAME:TAG""  DEBU[0020] Loading registries configuration ""/etc/containers/registries.conf.d/000-shortnames.conf"" DEBU[0020] Loading registries configuration ""/etc/containers/registries.conf.d/001-rhel-shortnames.conf"" DEBU[0020] Loading registries configuration ""/etc/containers/registries.conf.d/002-rhel-shortnames-overrides.conf"" DEBU[0020] Trying ""localhost/NAME:TAG""  DEBU[0020] Trying ""registry.fedoraproject.org/NAME:TAG""  DEBU[0020] Trying ""localhost/NAME:TAG""  DEBU[0020] Trying ""registry.access.redhat.com/NAME:TAG""  DEBU[0020] Trying ""registry.fedoraproject.org/NAME:TAG""  DEBU[0020] Trying ""registry.centos.org/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""registry.access.redhat.com/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""registry.centos.org/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""REGISTRY/PATH/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] parsed reference into ""[overlay@/var/lib/containers/storage+/run/containers/storage:overlay.mountopt=nodev,metacopy=on]@5fb70826285b943f31f0c4b0dc0247d29177941c6e01a99b188bdda633244892"" DEBU[0020] Found image ""NAME:TAG"" as ""REGISTRY/PATH/NAME:TAG"" in local containers storage DEBU[0020] Trying ""REGISTRY/PATH/NAME:TAG""  DEBU[0020] parsed reference into ""[overlay@/var/lib/containers/storage+/run/containers/storage:overlay.mountopt=nodev,metacopy=on]@5fb70826285b943f31f0c4b0dc0247d29177941c6e01a99b188bdda633244892"" DEBU[0020] Found image ""NAME:TAG"" as ""REGISTRY/PATH/NAME:TAG"" in local containers storage DEBU[0020] Found image ""NAME:TAG"" as ""REGISTRY/PATH/NAME:TAG"" in local containers storage ([overlay@/var/lib/containers/storage+/run/containers/storage:overlay.mountopt=nodev,metacopy=on]@5fb70826285b943f31f0c4b0dc0247d29177941c6e01a99b188bdda633244892) DEBU[0020] Looking up image ""docker.io/library/NAME:TAG"" in local containers storage DEBU[0020] Normalized platform linux/amd64 to {amd64 linux [] } DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Found image ""NAME:TAG"" as ""REGISTRY/PATH/NAME:TAG"" in local containers storage ([overlay@/var/lib/containers/storage+/run/containers/storage:overlay.mountopt=nodev,metacopy=on]@5fb70826285b943f31f0c4b0dc0247d29177941c6e01a99b188bdda633244892) DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Looking up image ""docker.io/library/NAME:TAG"" in local containers storage DEBU[0020] Normalized platform linux/amd64 to {amd64 linux [] } DEBU[0020] Trying ""docker.io/library/NAME:TAG""  DEBU[0020] Trying ""docker.io/library/NAME:TAG""  INFO[0020] Request Failed(Not Found): No such image: docker.io/library/NAME:TAG: image not known DEBU[0020] Trying ""docker.io/library/NAME:TAG""  INFO[0020] Request Failed(Not Found): No such image: docker.io/library/NAME:TAG: image not known 34.74.239.56 - - [19/May/2022:11:31:19 +0000] ""POST /containers/create?Image=NAME%3ATAG&Tty=true&Cmd=%5B%22%2Fopt%2Fbin%2Fentry_point.sh%22%5D&HostConfig=%7B%22Privileged%22%3Atrue%2C%22Binds%22%3A%5B%22%2Fdev%2Fshm%3A%2Fdev%2Fshm%22%2C%22%2Fdata%2Fvideo%3A%2Fvideo%22%2C%22%2Fdata%2Fbrowsers%3A%2Fbrowsers%22%5D%7D&name=overconfident-wish-2&Env=REMOVED_BY_PETER HTTP/1.1"" 404 133 """" """" 34.74.239.56 - - [19/May/2022:11:31:19 +0000] ""POST /containers/create?Image=NAME%3ATAG&Tty=true&Cmd=%5B%22%2Fopt%2Fbin%2Fentry_point.sh%22%5D&HostConfig=%7B%22Privileged%22%3Atrue%2C%22Binds%22%3A%5B%22%2Fdev%2Fshm%3A%2Fdev%2Fshm%22%2C%22%2Fdata%2Fvideo%3A%2Fvideo%22%2C%22%2Fdata%2Fbrowsers%3A%2Fbrowsers%22%5D%2C%22PortBindings%22%3A%7B%225900%2Ftcp%22%3A%5B%7B%22HostPort%22%3A%225900%22%7D%5D%2C%229229%2Ftcp%22%3A%5B%7B%22HostPort%22%3A%229229%22%7D%5D%7D%7D&name=overconfident-wish-1&Env=REMOVED_BY_PETER HTTP/1.1"" 404 133 """" """" DEBU[0020] IdleTracker:closed 2m+0h/2t connection(s) X-Reference-Id=0xc000011320 DEBU[0020] IdleTracker:closed 1m+0h/2t connection(s) X-Reference-Id=0xc000011318  **Describe the results you expected:** The container should start as it is found on local storage. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  podman version 4.0.2  **Output of `podman info --debug`:**  host: arch: amd64 buildahVersion: 1.24.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-1.module_el8.6.0+2877+8e437bf5.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: edfc4e28654b9f8e3597bb8f87c6af099a50261f' cpus: 8 distribution: distribution: '""spearlineos""' version: ""8.6"" eventLogger: file hostname: peter-from-image1 idMappings: gidmap: null uidmap: null kernel: 4.18.0-372.9.1.el8.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 4423311360 memTotal: 8340979712 networkBackend: cni ociRuntime: name: runc package: runc-1.0.3-1.module_el8.6.0+2877+8e437bf5.x86_64 path: /usr/bin/runc version: |- runc version 1.0.3 spec: 1.0.2-dev go: go1.17.7 libseccomp: 2.5.2 os: linux remoteSocket: path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.1.8-2.module_el8.6.0+2877+8e437bf5.x86_64 version: |- slirp4netns version 1.1.8 commit: d361001f495417b880f20329121e3aa431a8f90f libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 2209345536 swapTotal: 2209345536 uptime: 1h 18m 47.63s (Approximately 0.04 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - registry.centos.org - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 4 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.0.2 Built: 1652194338 BuiltTime: Tue May 10 14:52:18 2022 GitCommit: """" GoVersion: go1.17.7 OsArch: linux/amd64 Version: 4.0.2  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman-4.0.2-5.module_el8.6.0+2877+8e437bf5.x86_64  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** No **Additional environment details (AWS, VirtualBox, physical, etc.):** GCP source-file test-file source-file test-file source-file test-file",bug,0.95
15476,podman,https://github.com/containers/podman/issues/15476,build from API or `podman-remote build` does not supports `--userns=auto` while it works with CLI,"User trying this implementation in Podman 4.2.0. Seems its working via CLI, but API call does not work. **Dockerfile**  cat << EOF > Dockerfile FROM alpine RUN cat /proc/self/uid_map EOF  Works correctly via CLI  podman build -t test --userns=auto .  Does not work via API  tar -czf context.tar.gz Dockerfile curl -s --unix-socket /run/podman/podman.sock -X POST -H ""Content-Type:application/tar"" --data-binary ""@context.tar.gz"" 'http://d/v4.2.0/libpod/build?userns=auto'  _Originally posted by @lukasmrtvy in https://github.com/containers/buildah/issues/4060#issuecomment-1226050854_",source-file | source-file | test-file | test-file,"build from API or `podman-remote build` does not supports `--userns=auto` while it works with CLI User trying this implementation in Podman 4.2.0. Seems its working via CLI, but API call does not work. **Dockerfile**  cat << EOF > Dockerfile FROM alpine RUN cat /proc/self/uid_map EOF  Works correctly via CLI  podman build -t test --userns=auto .  Does not work via API  tar -czf context.tar.gz Dockerfile curl -s --unix-socket /run/podman/podman.sock -X POST -H ""Content-Type:application/tar"" --data-binary ""@context.tar.gz"" 'http://d/v4.2.0/libpod/build?userns=auto'  _Originally posted by @lukasmrtvy in https://github.com/containers/buildah/issues/4060#issuecomment-1226050854_ source-file source-file test-file test-file",bug,0.95
18092,podman,https://github.com/containers/podman/issues/18092,Image Filtering Incompatability with Docker API," Issue Description Attempting to filter images by label value via the compat HTTP API doesn't find images that definitely exist with the set labels. The same requests with the docker socket (after the same set of calls to create a labeled image) do return images. There seems to be something about a new and old format for filters, this seems to only apply to how the new filters are handled.  Steps to reproduce the issue 1. run `podman image pull docker.io/ubuntu:22.04` 2. run:  curl --unix-socket /run/user/1000/podman/podman.sock --get 'http://_/images/json' \ --data-urlencode 'filters={""label"": [""org.opencontainers.image.ref.name=ubuntu"", ""org.opencontainers.image.version=22.04""]}'   Describe the results you received This returns `[]`.  Describe the results you expected I would expect this to return the same thing as:  curl --unix-socket /run/user/1000/podman/podman.sock --get 'http://_/images/json' \ --data-urlencode 'filters={""label"": {""org.opencontainers.image.ref.name=ubuntu"":true, ""org.opencontainers.image.version=22.04"": true}}'  Which returns:  [ { ""Id"": ""sha256:08d22c0ceb150ddeb2237c5fa3129c0183f3cc6f5eeb2e7aa4016da3ad02140a"", ""ParentId"": """", ""RepoTags"": [ ""docker.io/library/ubuntu:22.04"", ""docker.io/library/ubuntu:latest"" ], ""RepoDigests"": [ ""docker.io/library/ubuntu@sha256:67211c14fa74f070d27cc59d69a7fa9aeff8e28ea118ef3babc295a0428a6d21"", ""docker.io/library/ubuntu@sha256:7a57c69fe1e9d5b97c5fe649849e79f2cfc3bf11d10bbd5218b4eb61716aebe6"" ], ""Created"": 1678250667, ""Size"": 80337591, ""SharedSize"": 0, ""VirtualSize"": 80337591, ""Labels"": { ""org.opencontainers.image.ref.name"": ""ubuntu"", ""org.opencontainers.image.version"": ""22.04"" }, ""Containers"": 496, ""Names"": [ ""docker.io/library/ubuntu:22.04"", ""docker.io/library/ubuntu:latest"" ], ""Digest"": ""sha256:67211c14fa74f070d27cc59d69a7fa9aeff8e28ea118ef3babc295a0428a6d21"", ""History"": [ ""docker.io/library/ubuntu:latest"", ""docker.io/library/ubuntu:22.04"" ] } ]   podman info output yaml Client: Podman Engine Version: 4.4.2 API Version: 4.4.2 Go Version: go1.19.6 Built: Wed Mar 1 05:22:59 2023 OS/Arch: linux/amd64   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release No  Additional environment details _No response_  Additional information When the above commands are run, but for the docker command/socket, both curl calls result in the same output.",source-file | test-file | source-file | test-file,"Image Filtering Incompatability with Docker API  Issue Description Attempting to filter images by label value via the compat HTTP API doesn't find images that definitely exist with the set labels. The same requests with the docker socket (after the same set of calls to create a labeled image) do return images. There seems to be something about a new and old format for filters, this seems to only apply to how the new filters are handled.  Steps to reproduce the issue 1. run `podman image pull docker.io/ubuntu:22.04` 2. run:  curl --unix-socket /run/user/1000/podman/podman.sock --get 'http://_/images/json' \ --data-urlencode 'filters={""label"": [""org.opencontainers.image.ref.name=ubuntu"", ""org.opencontainers.image.version=22.04""]}'   Describe the results you received This returns `[]`.  Describe the results you expected I would expect this to return the same thing as:  curl --unix-socket /run/user/1000/podman/podman.sock --get 'http://_/images/json' \ --data-urlencode 'filters={""label"": {""org.opencontainers.image.ref.name=ubuntu"":true, ""org.opencontainers.image.version=22.04"": true}}'  Which returns:  [ { ""Id"": ""sha256:08d22c0ceb150ddeb2237c5fa3129c0183f3cc6f5eeb2e7aa4016da3ad02140a"", ""ParentId"": """", ""RepoTags"": [ ""docker.io/library/ubuntu:22.04"", ""docker.io/library/ubuntu:latest"" ], ""RepoDigests"": [ ""docker.io/library/ubuntu@sha256:67211c14fa74f070d27cc59d69a7fa9aeff8e28ea118ef3babc295a0428a6d21"", ""docker.io/library/ubuntu@sha256:7a57c69fe1e9d5b97c5fe649849e79f2cfc3bf11d10bbd5218b4eb61716aebe6"" ], ""Created"": 1678250667, ""Size"": 80337591, ""SharedSize"": 0, ""VirtualSize"": 80337591, ""Labels"": { ""org.opencontainers.image.ref.name"": ""ubuntu"", ""org.opencontainers.image.version"": ""22.04"" }, ""Containers"": 496, ""Names"": [ ""docker.io/library/ubuntu:22.04"", ""docker.io/library/ubuntu:latest"" ], ""Digest"": ""sha256:67211c14fa74f070d27cc59d69a7fa9aeff8e28ea118ef3babc295a0428a6d21"", ""History"": [ ""docker.io/library/ubuntu:latest"", ""docker.io/library/ubuntu:22.04"" ] } ]   podman info output yaml Client: Podman Engine Version: 4.4.2 API Version: 4.4.2 Go Version: go1.19.6 Built: Wed Mar 1 05:22:59 2023 OS/Arch: linux/amd64   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release No  Additional environment details _No response_  Additional information When the above commands are run, but for the docker command/socket, both curl calls result in the same output. source-file test-file source-file test-file",bug,0.95
14458,podman,https://github.com/containers/podman/issues/14458,podman logs missing last line from json stdout,"/kind bug **Description** <!-- Using the docker-style API or podman logs command the JSON output from a container is missing the last line leading to malformed JSON and decoding errors. This is present in podman version 4.2.0-dev and 3.4.2 --> **Steps to reproduce the issue:** 1. podman pull docker.io/fkiecad/cwe_checker:stable 2. podman run fkiecad/cwe_checker:stable /bin/echo --json ERROR: instr_001022f0_2: Call target at 00102050 does not exist ERROR: instr_001022f5_2: Call target at 00102050 does not exist ERROR: instr_001022fa_2: Call target at 00102050 does not exist ERROR: instr_001022ff_2: Call target at 00102050 does not exist ERROR: instr_00102304_2: Call target at 00102050 does not exist ERROR: instr_00102309_2: Call target at 00102050 does not exist ERROR: instr_0010230e_2: Call target at 00102050 does not exist ERROR: instr_00102313_2: Call target at 00102050 does not exist ERROR: instr_00102318_2: Call target at 00102050 does not exist ERROR: instr_0010231d_2: Call target at 00102050 does not exist ERROR: instr_00102322_2: Call target at 00102050 does not exist ERROR: instr_00103070_2: Call target at 00102050 does not exist ERROR: instr_001050b0_0: Jump target at 00102327 does not exist ERROR: instr_001050b9_0: Jump target at 00102327 does not exist ERROR: instr_00105b70_2: Call target at 00102050 does not exist DEBUG: Pointer Inference @ instr_001042b2_2: Address not contained in runtime memory image DEBUG: Pointer Inference @ instr_00105963_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104291_0: Address not contained in runtime memory image DEBUG: Pointer Inference @ instr_00105ce4_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_00104d42_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001058a0_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00105c02_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_00105778_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001043d7_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001044fe_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104882_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_001059e8_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104867_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_001048a8_2: Free on a non-pointer value called. DEBUG: Pointer Inference: Adding 4 entry points DEBUG: Pointer Inference: Blocks with state: 14 / 1109 DEBUG: Pointer Inference: Adding 60 speculative entry points DEBUG: Pointer Inference: Blocks with state: 1040 / 1109 DEBUG: Pointer Inference: Adding 0 speculative entry points DEBUG: Pointer Inference: Blocks with state: 1040 / 1109 [ { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""001032a9"" ], ""tids"": [ ""instr_001032a9_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (001032a9) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""0010335c"" ], ""tids"": [ ""instr_0010335c_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (0010335c) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00103398"" ], ""tids"": [ ""instr_00103398_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""memcmp"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (00103398) -> memcmp"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00103c9e"" ], ""tids"": [ ""instr_00103c9e_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (00103c9e) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00104485"" ], ""tids"": [ ""instr_00104485_2"" ], ""symbols"": [ ""FUN_001043f0"" ], ""other"": [ [ ""dangerous_function"", ""memset"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_001043f0 (00104485) -> memset"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105aef"" ], ""tids"": [ ""instr_00105aef_0"" ], ""symbols"": [ ""FUN_00105ac0"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105ac0 (00105aef) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105b0a"" ], ""tids"": [ ""instr_00105b0a_2"" ], ""symbols"": [ ""FUN_00105b00"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105b00 (00105b0a) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105b30"" ], ""tids"": [ ""instr_00105b30_0"" ], ""symbols"": [ ""FUN_00105b00"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105b00 (00105b30) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105d52"" ], ""tids"": [ ""instr_00105d52_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105d52) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105d85"" ], ""tids"": [ ""instr_00105d85_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105d85) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105daa"" ], ""tids"": [ ""instr_00105daa_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105daa) -> memcpy"" }, { ""name"": ""CWE125"", ""version"": ""0.2"", ""addresses"": [ ""001042b2"" ], ""tids"": [ ""instr_001042b2_2"" ], ""symbols"": [], ""other"": [], ""description"": ""(Out-of-bounds Read) Memory load at 001042b2 may be out of bounds"" }, { ""name"": ""CWE125"", ""version"": ""0.2"", ""addresses"": [ ""00104291"" ], ""tids"": [ ""instr_00104291_0"" ], ""symbols"": [], ""other"": [], ""description"": ""(Out-of-bounds Read) Memory load at 00104291 may be out of bounds"" } ] 3. podman logs 90613051c45e ERROR: instr_001022f0_2: Call target at 00102050 does not exist ERROR: instr_001022f5_2: Call target at 00102050 does not exist ERROR: instr_001022fa_2: Call target at 00102050 does not exist ERROR: instr_001022ff_2: Call target at 00102050 does not exist ERROR: instr_00102304_2: Call target at 00102050 does not exist ERROR: instr_00102309_2: Call target at 00102050 does not exist ERROR: instr_0010230e_2: Call target at 00102050 does not exist ERROR: instr_00102313_2: Call target at 00102050 does not exist ERROR: instr_00102318_2: Call target at 00102050 does not exist ERROR: instr_0010231d_2: Call target at 00102050 does not exist ERROR: instr_00102322_2: Call target at 00102050 does not exist ERROR: instr_00103070_2: Call target at 00102050 does not exist ERROR: instr_001050b0_0: Jump target at 00102327 does not exist ERROR: instr_001050b9_0: Jump target at 00102327 does not exist ERROR: instr_00105b70_2: Call target at 00102050 does not exist DEBUG: Pointer Inference @ instr_001042b2_2: Address not contained in runtime memory image DEBUG: Pointer Inference @ instr_00105963_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104291_0: Address not contained in runtime memory image DEBUG: Pointer Inference @ instr_00105ce4_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_00104d42_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001058a0_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00105c02_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_00105778_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001043d7_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001044fe_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104882_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_001059e8_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104867_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_001048a8_2: Free on a non-pointer value called. DEBUG: Pointer Inference: Adding 4 entry points DEBUG: Pointer Inference: Blocks with state: 14 / 1109 DEBUG: Pointer Inference: Adding 60 speculative entry points DEBUG: Pointer Inference: Blocks with state: 1040 / 1109 DEBUG: Pointer Inference: Adding 0 speculative entry points DEBUG: Pointer Inference: Blocks with state: 1040 / 1109 [ { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""001032a9"" ], ""tids"": [ ""instr_001032a9_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (001032a9) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""0010335c"" ], ""tids"": [ ""instr_0010335c_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (0010335c) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00103398"" ], ""tids"": [ ""instr_00103398_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""memcmp"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (00103398) -> memcmp"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00103c9e"" ], ""tids"": [ ""instr_00103c9e_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (00103c9e) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00104485"" ], ""tids"": [ ""instr_00104485_2"" ], ""symbols"": [ ""FUN_001043f0"" ], ""other"": [ [ ""dangerous_function"", ""memset"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_001043f0 (00104485) -> memset"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105aef"" ], ""tids"": [ ""instr_00105aef_0"" ], ""symbols"": [ ""FUN_00105ac0"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105ac0 (00105aef) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105b0a"" ], ""tids"": [ ""instr_00105b0a_2"" ], ""symbols"": [ ""FUN_00105b00"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105b00 (00105b0a) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105b30"" ], ""tids"": [ ""instr_00105b30_0"" ], ""symbols"": [ ""FUN_00105b00"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105b00 (00105b30) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105d52"" ], ""tids"": [ ""instr_00105d52_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105d52) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105d85"" ], ""tids"": [ ""instr_00105d85_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105d85) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105daa"" ], ""tids"": [ ""instr_00105daa_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105daa) -> memcpy"" }, { ""name"": ""CWE125"", ""version"": ""0.2"", ""addresses"": [ ""001042b2"" ], ""tids"": [ ""instr_001042b2_2"" ], ""symbols"": [], ""other"": [], ""description"": ""(Out-of-bounds Read) Memory load at 001042b2 may be out of bounds"" }, { ""name"": ""CWE125"", ""version"": ""0.2"", ""addresses"": [ ""00104291"" ], ""tids"": [ ""instr_00104291_0"" ], ""symbols"": [], ""other"": [], ""description"": ""(Out-of-bounds Read) Memory load at 00104291 may be out of bounds"" } **Describe the results you received:** The stdout from the run command shows the last line with the ""]"", however the logs command is missing the ""]"". If I do the same thing with docker the logs command is not missing the last line. **Describe the results you expected:** I expected the logs command to provide the last line of the stdout. **Additional information you deem important (e.g. issue happens only occasionally):** Issue is always reproducible. **Output of `podman version`:**  podman --version podman version 3.4.2 Also: podman --version podman version 4.2.0-dev followed the source install method on podman.io for 4.2.0. Reverted back to 3.4.2 after seeing same results on latest source.  **Output of `podman info --debug`:**  podman info --debug host: arch: amd64 buildahVersion: 1.23.1 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: 'conmon: /usr/libexec/podman/conmon' path: /usr/libexec/podman/conmon version: 'conmon version 2.0.30, commit: ' cpus: 8 distribution: codename: focal distribution: ubuntu version: ""20.04"" eventLogger: journald hostname: bryce-VirtualBox idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.13.0-44-generic linkmode: dynamic logDriver: journald memFree: 25507426304 memTotal: 39464538112 ociRuntime: name: crun package: 'crun: /usr/bin/crun' path: /usr/bin/crun version: |- crun version UNKNOWN commit: ea1fe3938eefa14eb707f1d22adff4db670645d6 spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: 'slirp4netns: /usr/bin/slirp4netns' version: |- slirp4netns version 1.1.8 commit: unknown libslirp: 4.3.1-git SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.4.3 swapFree: 2147479552 swapTotal: 2147479552 uptime: 1h 10m 20.19s (Approximately 0.04 days) plugins: log: - k8s-file - none - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /home/bryce/.config/containers/storage.conf containerStore: number: 11 paused: 0 running: 3 stopped: 8 graphDriverName: overlay graphOptions: {} graphRoot: /home/bryce/.local/share/containers/storage graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageStore: number: 59 runRoot: /run/user/1000/containers volumePath: /home/bryce/.local/share/containers/storage/volumes version: APIVersion: 3.4.2 Built: 0 BuiltTime: Wed Dec 31 18:00:00 1969 GitCommit: """" GoVersion: go1.15.2 OsArch: linux/amd64 Version: 3.4.2  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  apt list podman Listing Done podman/unknown,now 100:3.4.2-5 amd64 [installed] podman/unknown 100:3.4.2-5 arm64 podman/unknown 100:3.4.2-5 armhf podman/unknown 100:3.4.2-5 s390x  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** VirtualBox 6.1",source-file | test-file | source-file | test-file,"podman logs missing last line from json stdout /kind bug **Description** <!-- Using the docker-style API or podman logs command the JSON output from a container is missing the last line leading to malformed JSON and decoding errors. This is present in podman version 4.2.0-dev and 3.4.2 --> **Steps to reproduce the issue:** 1. podman pull docker.io/fkiecad/cwe_checker:stable 2. podman run fkiecad/cwe_checker:stable /bin/echo --json ERROR: instr_001022f0_2: Call target at 00102050 does not exist ERROR: instr_001022f5_2: Call target at 00102050 does not exist ERROR: instr_001022fa_2: Call target at 00102050 does not exist ERROR: instr_001022ff_2: Call target at 00102050 does not exist ERROR: instr_00102304_2: Call target at 00102050 does not exist ERROR: instr_00102309_2: Call target at 00102050 does not exist ERROR: instr_0010230e_2: Call target at 00102050 does not exist ERROR: instr_00102313_2: Call target at 00102050 does not exist ERROR: instr_00102318_2: Call target at 00102050 does not exist ERROR: instr_0010231d_2: Call target at 00102050 does not exist ERROR: instr_00102322_2: Call target at 00102050 does not exist ERROR: instr_00103070_2: Call target at 00102050 does not exist ERROR: instr_001050b0_0: Jump target at 00102327 does not exist ERROR: instr_001050b9_0: Jump target at 00102327 does not exist ERROR: instr_00105b70_2: Call target at 00102050 does not exist DEBUG: Pointer Inference @ instr_001042b2_2: Address not contained in runtime memory image DEBUG: Pointer Inference @ instr_00105963_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104291_0: Address not contained in runtime memory image DEBUG: Pointer Inference @ instr_00105ce4_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_00104d42_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001058a0_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00105c02_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_00105778_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001043d7_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001044fe_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104882_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_001059e8_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104867_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_001048a8_2: Free on a non-pointer value called. DEBUG: Pointer Inference: Adding 4 entry points DEBUG: Pointer Inference: Blocks with state: 14 / 1109 DEBUG: Pointer Inference: Adding 60 speculative entry points DEBUG: Pointer Inference: Blocks with state: 1040 / 1109 DEBUG: Pointer Inference: Adding 0 speculative entry points DEBUG: Pointer Inference: Blocks with state: 1040 / 1109 [ { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""001032a9"" ], ""tids"": [ ""instr_001032a9_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (001032a9) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""0010335c"" ], ""tids"": [ ""instr_0010335c_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (0010335c) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00103398"" ], ""tids"": [ ""instr_00103398_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""memcmp"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (00103398) -> memcmp"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00103c9e"" ], ""tids"": [ ""instr_00103c9e_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (00103c9e) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00104485"" ], ""tids"": [ ""instr_00104485_2"" ], ""symbols"": [ ""FUN_001043f0"" ], ""other"": [ [ ""dangerous_function"", ""memset"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_001043f0 (00104485) -> memset"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105aef"" ], ""tids"": [ ""instr_00105aef_0"" ], ""symbols"": [ ""FUN_00105ac0"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105ac0 (00105aef) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105b0a"" ], ""tids"": [ ""instr_00105b0a_2"" ], ""symbols"": [ ""FUN_00105b00"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105b00 (00105b0a) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105b30"" ], ""tids"": [ ""instr_00105b30_0"" ], ""symbols"": [ ""FUN_00105b00"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105b00 (00105b30) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105d52"" ], ""tids"": [ ""instr_00105d52_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105d52) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105d85"" ], ""tids"": [ ""instr_00105d85_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105d85) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105daa"" ], ""tids"": [ ""instr_00105daa_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105daa) -> memcpy"" }, { ""name"": ""CWE125"", ""version"": ""0.2"", ""addresses"": [ ""001042b2"" ], ""tids"": [ ""instr_001042b2_2"" ], ""symbols"": [], ""other"": [], ""description"": ""(Out-of-bounds Read) Memory load at 001042b2 may be out of bounds"" }, { ""name"": ""CWE125"", ""version"": ""0.2"", ""addresses"": [ ""00104291"" ], ""tids"": [ ""instr_00104291_0"" ], ""symbols"": [], ""other"": [], ""description"": ""(Out-of-bounds Read) Memory load at 00104291 may be out of bounds"" } ] 3. podman logs 90613051c45e ERROR: instr_001022f0_2: Call target at 00102050 does not exist ERROR: instr_001022f5_2: Call target at 00102050 does not exist ERROR: instr_001022fa_2: Call target at 00102050 does not exist ERROR: instr_001022ff_2: Call target at 00102050 does not exist ERROR: instr_00102304_2: Call target at 00102050 does not exist ERROR: instr_00102309_2: Call target at 00102050 does not exist ERROR: instr_0010230e_2: Call target at 00102050 does not exist ERROR: instr_00102313_2: Call target at 00102050 does not exist ERROR: instr_00102318_2: Call target at 00102050 does not exist ERROR: instr_0010231d_2: Call target at 00102050 does not exist ERROR: instr_00102322_2: Call target at 00102050 does not exist ERROR: instr_00103070_2: Call target at 00102050 does not exist ERROR: instr_001050b0_0: Jump target at 00102327 does not exist ERROR: instr_001050b9_0: Jump target at 00102327 does not exist ERROR: instr_00105b70_2: Call target at 00102050 does not exist DEBUG: Pointer Inference @ instr_001042b2_2: Address not contained in runtime memory image DEBUG: Pointer Inference @ instr_00105963_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104291_0: Address not contained in runtime memory image DEBUG: Pointer Inference @ instr_00105ce4_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_00104d42_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001058a0_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00105c02_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_00105778_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001043d7_0_r: Unexpected stack register value on return DEBUG: Pointer Inference @ instr_001044fe_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104882_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_001059e8_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_00104867_2: Free on a non-pointer value called. DEBUG: Pointer Inference @ instr_001048a8_2: Free on a non-pointer value called. DEBUG: Pointer Inference: Adding 4 entry points DEBUG: Pointer Inference: Blocks with state: 14 / 1109 DEBUG: Pointer Inference: Adding 60 speculative entry points DEBUG: Pointer Inference: Blocks with state: 1040 / 1109 DEBUG: Pointer Inference: Adding 0 speculative entry points DEBUG: Pointer Inference: Blocks with state: 1040 / 1109 [ { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""001032a9"" ], ""tids"": [ ""instr_001032a9_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (001032a9) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""0010335c"" ], ""tids"": [ ""instr_0010335c_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (0010335c) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00103398"" ], ""tids"": [ ""instr_00103398_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""memcmp"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (00103398) -> memcmp"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00103c9e"" ], ""tids"": [ ""instr_00103c9e_2"" ], ""symbols"": [ ""FUN_00103160"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00103160 (00103c9e) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00104485"" ], ""tids"": [ ""instr_00104485_2"" ], ""symbols"": [ ""FUN_001043f0"" ], ""other"": [ [ ""dangerous_function"", ""memset"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_001043f0 (00104485) -> memset"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105aef"" ], ""tids"": [ ""instr_00105aef_0"" ], ""symbols"": [ ""FUN_00105ac0"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105ac0 (00105aef) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105b0a"" ], ""tids"": [ ""instr_00105b0a_2"" ], ""symbols"": [ ""FUN_00105b00"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105b00 (00105b0a) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105b30"" ], ""tids"": [ ""instr_00105b30_0"" ], ""symbols"": [ ""FUN_00105b00"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105b00 (00105b30) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105d52"" ], ""tids"": [ ""instr_00105d52_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""strlen"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105d52) -> strlen"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105d85"" ], ""tids"": [ ""instr_00105d85_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105d85) -> memcpy"" }, { ""name"": ""CWE676"", ""version"": ""0.1"", ""addresses"": [ ""00105daa"" ], ""tids"": [ ""instr_00105daa_2"" ], ""symbols"": [ ""FUN_00105d30"" ], ""other"": [ [ ""dangerous_function"", ""memcpy"" ] ], ""description"": ""(Use of Potentially Dangerous Function) FUN_00105d30 (00105daa) -> memcpy"" }, { ""name"": ""CWE125"", ""version"": ""0.2"", ""addresses"": [ ""001042b2"" ], ""tids"": [ ""instr_001042b2_2"" ], ""symbols"": [], ""other"": [], ""description"": ""(Out-of-bounds Read) Memory load at 001042b2 may be out of bounds"" }, { ""name"": ""CWE125"", ""version"": ""0.2"", ""addresses"": [ ""00104291"" ], ""tids"": [ ""instr_00104291_0"" ], ""symbols"": [], ""other"": [], ""description"": ""(Out-of-bounds Read) Memory load at 00104291 may be out of bounds"" } **Describe the results you received:** The stdout from the run command shows the last line with the ""]"", however the logs command is missing the ""]"". If I do the same thing with docker the logs command is not missing the last line. **Describe the results you expected:** I expected the logs command to provide the last line of the stdout. **Additional information you deem important (e.g. issue happens only occasionally):** Issue is always reproducible. **Output of `podman version`:**  podman --version podman version 3.4.2 Also: podman --version podman version 4.2.0-dev followed the source install method on podman.io for 4.2.0. Reverted back to 3.4.2 after seeing same results on latest source.  **Output of `podman info --debug`:**  podman info --debug host: arch: amd64 buildahVersion: 1.23.1 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: 'conmon: /usr/libexec/podman/conmon' path: /usr/libexec/podman/conmon version: 'conmon version 2.0.30, commit: ' cpus: 8 distribution: codename: focal distribution: ubuntu version: ""20.04"" eventLogger: journald hostname: bryce-VirtualBox idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.13.0-44-generic linkmode: dynamic logDriver: journald memFree: 25507426304 memTotal: 39464538112 ociRuntime: name: crun package: 'crun: /usr/bin/crun' path: /usr/bin/crun version: |- crun version UNKNOWN commit: ea1fe3938eefa14eb707f1d22adff4db670645d6 spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: 'slirp4netns: /usr/bin/slirp4netns' version: |- slirp4netns version 1.1.8 commit: unknown libslirp: 4.3.1-git SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.4.3 swapFree: 2147479552 swapTotal: 2147479552 uptime: 1h 10m 20.19s (Approximately 0.04 days) plugins: log: - k8s-file - none - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /home/bryce/.config/containers/storage.conf containerStore: number: 11 paused: 0 running: 3 stopped: 8 graphDriverName: overlay graphOptions: {} graphRoot: /home/bryce/.local/share/containers/storage graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageStore: number: 59 runRoot: /run/user/1000/containers volumePath: /home/bryce/.local/share/containers/storage/volumes version: APIVersion: 3.4.2 Built: 0 BuiltTime: Wed Dec 31 18:00:00 1969 GitCommit: """" GoVersion: go1.15.2 OsArch: linux/amd64 Version: 3.4.2  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  apt list podman Listing Done podman/unknown,now 100:3.4.2-5 amd64 [installed] podman/unknown 100:3.4.2-5 arm64 podman/unknown 100:3.4.2-5 armhf podman/unknown 100:3.4.2-5 s390x  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** VirtualBox 6.1 source-file test-file source-file test-file",bug,0.95
25881,podman,https://github.com/containers/podman/issues/25881,Podman Restful API in rootless mode ignores ulimits," Issue Description Hello, when I use the podman in rootless mode, and want to specify ulimit for cpu through the rest api, the limit gets ignored. When I use `--ulimit cpu=10:10 ` in rootless mode on the CLI, the limit gets applied correctly. In rootful mode for rest api the ulimit gets applied. This leads me to believe the issue is connected with rootless and rest api.  Steps to reproduce the issue I am running ubuntu 24.04 and podman v 4.9.3. To see if it happens on latest podman, these steps are using podman in container, but the same behaviour is observed on my system with just rootless podman. This example shows failing use case:  podman run --privileged -u podman -it quay.io/podman/stable bash podman $LOGGING system service -t 0 tcp:0.0.0.0:8081 & # you might need to hit enter for the cmd prompt to appear podman --url tcp://127.0.0.1:8081 image pull alpine curl -X POST ""http://localhost:8081/containers/create?name=test"" \ -H ""Content-Type: application/json"" \ -d '{ ""Image"": ""alpine"", ""Name"": ""test"", ""Cmd"": [""sh"", ""-c"", ""ulimit -Ht""], ""HostConfig"": { ""Ulimits"": [ { ""Name"": ""cpu"", ""Soft"": 1, ""Hard"": 2 } ] } }' podman --url tcp://127.0.0.1:8081 inspect test #""Ulimits"": [],  As seen from the inspect command, Ulimits are empty. If I run this command instead of doing the http request `podman --url tcp://127.0.0.1:8081 run -it --ulimit cpu=1:2 alpine sh -c ""ulimit -Ht""` or start the podman as root (ie. without `-u podman`) `podman run --privileged -it quay.io/podman/stable bash` and then do the http request, the ulimits are applied correctly  ""Ulimits"": [ { ""Name"": ""RLIMIT_CPU"", ""Soft"": 1, ""Hard"": 2 }   Describe the results you received Using rootless mode and rest api, the ulimits are not applied  Describe the results you expected Using rootless mode and rest api, the cpu limit should be applied the same way when passed on CLI `--ulimit `  podman info output yaml Output from my **local** podman with v4.9.3 host: arch: amd64 buildahVersion: 1.33.7 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: Unknown path: /usr/local/libexec/podman/conmon version: 'conmon version 2.1.13, commit: 82de887596ed8ee6d9b2ee85e4f167f307bb569b' cpuUtilization: idlePercent: 99.87 systemPercent: 0.05 userPercent: 0.08 cpus: 4 databaseBackend: sqlite distribution: codename: noble distribution: ubuntu version: ""24.04"" eventLogger: journald freeLocks: 2030 hostname: server idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.11.0-1012-azure linkmode: dynamic logDriver: journald memFree: 11403198464 memTotal: 16713166848 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns_1.4.0-5_amd64 path: /usr/lib/podman/aardvark-dns version: aardvark-dns 1.4.0 package: netavark_1.4.0-4_amd64 path: /usr/lib/podman/netavark version: netavark 1.4.0 ociRuntime: name: crun package: crun_1.14.1-1_amd64 path: /usr/bin/crun version: |- crun version 1.14.1 commit: de537a7965bfbe9992e2cfae0baeb56a08128171 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt_0.0~git20240220.1e6f92b-1_amd64 version: | pasta unknown version Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: false path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns_1.2.1-1build2_amd64 version: |- slirp4netns version 1.2.1 commit: 09e31e92fa3d2a1d3ca261adaeb012c8d75a8194 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.5 swapFree: 0 swapTotal: 0 uptime: 183h 57m 22.00s (Approximately 7.62 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: {} store: configFile: /home/jan/.config/containers/storage.conf containerStore: number: 10 paused: 0 running: 0 stopped: 10 graphDriverName: overlay graphOptions: {} graphRoot: /home/jan/.local/share/containers/storage graphRootAllocated: 30084825088 graphRootUsed: 7610572800 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 2 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/jan/.local/share/containers/storage/volumes version: APIVersion: 4.9.3 Built: 0 BuiltTime: Thu Jan 1 00:00:00 1970 GitCommit: """" GoVersion: go1.22.2 Os: linux OsArch: linux/amd64 Version: 4.9.3 podman info from **podman in container** host: arch: amd64 buildahVersion: 1.39.4 cgroupControllers: - cpu - memory - pids cgroupManager: cgroupfs cgroupVersion: v2 conmon: package: conmon-2.1.13-1.fc41.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.13, commit: ' cpuUtilization: idlePercent: 99.87 systemPercent: 0.06 userPercent: 0.08 cpus: 4 databaseBackend: sqlite distribution: distribution: fedora variant: container version: ""41"" eventLogger: file freeLocks: 2048 hostname: 001ec30a88b2 idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 1 size: 999 - container_id: 1000 host_id: 1001 size: 64535 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 1 size: 999 - container_id: 1000 host_id: 1001 size: 64535 kernel: 6.11.0-1012-azure linkmode: dynamic logDriver: k8s-file memFree: 11347324928 memTotal: 16713166848 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.14.0-1.fc41.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.14.0 package: netavark-1.14.1-1.fc41.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.14.1 ociRuntime: name: crun package: crun-1.21-1.fc41.x86_64 path: /usr/bin/crun version: |- crun version 1.21 commit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88 rundir: /tmp/storage-run-1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20250320.g32f6212-2.fc41.x86_64 version: """" remoteSocket: exists: true path: /tmp/storage-run-1000/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: """" package: """" version: """" swapFree: 0 swapTotal: 0 uptime: 188h 5m 28.00s (Approximately 7.83 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /home/podman/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/podman/.local/share/containers/storage graphRootAllocated: 30084825088 graphRootUsed: 7633362944 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /tmp/storage-run-1000/containers transientStore: false volumePath: /home/podman/.local/share/containers/storage/volumes version: APIVersion: 5.4.2 BuildOrigin: Fedora Project Built: 1743552000 BuiltTime: Wed Apr 2 00:00:00 2025 GitCommit: be85287fcf4590961614ee37be65eeb315e5d9ff GoVersion: go1.23.7 Os: linux OsArch: linux/amd64 Version: 5.4.2   Podman in a container Yes  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details This happens both on my local machine and Azure portal VM running ubuntu 24.04. On my machines, I have podman version 4.9.2. The reproduction is using stable podman image with podman v5.4.2  Additional information _No response_",source-file | test-file,"Podman Restful API in rootless mode ignores ulimits  Issue Description Hello, when I use the podman in rootless mode, and want to specify ulimit for cpu through the rest api, the limit gets ignored. When I use `--ulimit cpu=10:10 ` in rootless mode on the CLI, the limit gets applied correctly. In rootful mode for rest api the ulimit gets applied. This leads me to believe the issue is connected with rootless and rest api.  Steps to reproduce the issue I am running ubuntu 24.04 and podman v 4.9.3. To see if it happens on latest podman, these steps are using podman in container, but the same behaviour is observed on my system with just rootless podman. This example shows failing use case:  podman run --privileged -u podman -it quay.io/podman/stable bash podman $LOGGING system service -t 0 tcp:0.0.0.0:8081 & # you might need to hit enter for the cmd prompt to appear podman --url tcp://127.0.0.1:8081 image pull alpine curl -X POST ""http://localhost:8081/containers/create?name=test"" \ -H ""Content-Type: application/json"" \ -d '{ ""Image"": ""alpine"", ""Name"": ""test"", ""Cmd"": [""sh"", ""-c"", ""ulimit -Ht""], ""HostConfig"": { ""Ulimits"": [ { ""Name"": ""cpu"", ""Soft"": 1, ""Hard"": 2 } ] } }' podman --url tcp://127.0.0.1:8081 inspect test #""Ulimits"": [],  As seen from the inspect command, Ulimits are empty. If I run this command instead of doing the http request `podman --url tcp://127.0.0.1:8081 run -it --ulimit cpu=1:2 alpine sh -c ""ulimit -Ht""` or start the podman as root (ie. without `-u podman`) `podman run --privileged -it quay.io/podman/stable bash` and then do the http request, the ulimits are applied correctly  ""Ulimits"": [ { ""Name"": ""RLIMIT_CPU"", ""Soft"": 1, ""Hard"": 2 }   Describe the results you received Using rootless mode and rest api, the ulimits are not applied  Describe the results you expected Using rootless mode and rest api, the cpu limit should be applied the same way when passed on CLI `--ulimit `  podman info output yaml Output from my **local** podman with v4.9.3 host: arch: amd64 buildahVersion: 1.33.7 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: Unknown path: /usr/local/libexec/podman/conmon version: 'conmon version 2.1.13, commit: 82de887596ed8ee6d9b2ee85e4f167f307bb569b' cpuUtilization: idlePercent: 99.87 systemPercent: 0.05 userPercent: 0.08 cpus: 4 databaseBackend: sqlite distribution: codename: noble distribution: ubuntu version: ""24.04"" eventLogger: journald freeLocks: 2030 hostname: server idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.11.0-1012-azure linkmode: dynamic logDriver: journald memFree: 11403198464 memTotal: 16713166848 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns_1.4.0-5_amd64 path: /usr/lib/podman/aardvark-dns version: aardvark-dns 1.4.0 package: netavark_1.4.0-4_amd64 path: /usr/lib/podman/netavark version: netavark 1.4.0 ociRuntime: name: crun package: crun_1.14.1-1_amd64 path: /usr/bin/crun version: |- crun version 1.14.1 commit: de537a7965bfbe9992e2cfae0baeb56a08128171 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt_0.0~git20240220.1e6f92b-1_amd64 version: | pasta unknown version Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: false path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns_1.2.1-1build2_amd64 version: |- slirp4netns version 1.2.1 commit: 09e31e92fa3d2a1d3ca261adaeb012c8d75a8194 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.5 swapFree: 0 swapTotal: 0 uptime: 183h 57m 22.00s (Approximately 7.62 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: {} store: configFile: /home/jan/.config/containers/storage.conf containerStore: number: 10 paused: 0 running: 0 stopped: 10 graphDriverName: overlay graphOptions: {} graphRoot: /home/jan/.local/share/containers/storage graphRootAllocated: 30084825088 graphRootUsed: 7610572800 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 2 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/jan/.local/share/containers/storage/volumes version: APIVersion: 4.9.3 Built: 0 BuiltTime: Thu Jan 1 00:00:00 1970 GitCommit: """" GoVersion: go1.22.2 Os: linux OsArch: linux/amd64 Version: 4.9.3 podman info from **podman in container** host: arch: amd64 buildahVersion: 1.39.4 cgroupControllers: - cpu - memory - pids cgroupManager: cgroupfs cgroupVersion: v2 conmon: package: conmon-2.1.13-1.fc41.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.13, commit: ' cpuUtilization: idlePercent: 99.87 systemPercent: 0.06 userPercent: 0.08 cpus: 4 databaseBackend: sqlite distribution: distribution: fedora variant: container version: ""41"" eventLogger: file freeLocks: 2048 hostname: 001ec30a88b2 idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 1 size: 999 - container_id: 1000 host_id: 1001 size: 64535 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 1 size: 999 - container_id: 1000 host_id: 1001 size: 64535 kernel: 6.11.0-1012-azure linkmode: dynamic logDriver: k8s-file memFree: 11347324928 memTotal: 16713166848 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.14.0-1.fc41.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.14.0 package: netavark-1.14.1-1.fc41.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.14.1 ociRuntime: name: crun package: crun-1.21-1.fc41.x86_64 path: /usr/bin/crun version: |- crun version 1.21 commit: 10269840aa07fb7e6b7e1acff6198692d8ff5c88 rundir: /tmp/storage-run-1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20250320.g32f6212-2.fc41.x86_64 version: """" remoteSocket: exists: true path: /tmp/storage-run-1000/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: """" package: """" version: """" swapFree: 0 swapTotal: 0 uptime: 188h 5m 28.00s (Approximately 7.83 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /home/podman/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/podman/.local/share/containers/storage graphRootAllocated: 30084825088 graphRootUsed: 7633362944 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /tmp/storage-run-1000/containers transientStore: false volumePath: /home/podman/.local/share/containers/storage/volumes version: APIVersion: 5.4.2 BuildOrigin: Fedora Project Built: 1743552000 BuiltTime: Wed Apr 2 00:00:00 2025 GitCommit: be85287fcf4590961614ee37be65eeb315e5d9ff GoVersion: go1.23.7 Os: linux OsArch: linux/amd64 Version: 5.4.2   Podman in a container Yes  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details This happens both on my local machine and Azure portal VM running ubuntu 24.04. On my machines, I have podman version 4.9.2. The reproduction is using stable podman image with podman v5.4.2  Additional information _No response_ source-file test-file",bug,0.95
17542,podman,https://github.com/containers/podman/issues/17542,Docker compatible `/containers/<id or name>/stop` does not support negative timeouts," Issue Description The Docker compatible `/containers/<id or name>/stop` endpoint does not support using a negative timeout. Docker's [spec](https://github.com/moby/moby/blob/master/api/types/container/config.go#L24-L32) for this endpoint states that you can ""use '-1' to wait indefinitely."" rather then having a set timeout after which the container will be killed. Podman's [implementation](https://github.com/containers/podman/blob/main/pkg/api/handlers/compat/containers_stop.go#L27) of this endpoint utilizes a `uint` rather then an `int`, causing a error to be thrown when using negative values.  Steps to reproduce the issue Steps to reproduce the issue 1. Run a container 2. Send stop request with a negative (indefinite) timeout value of `-1` (if using the Docker Go SDK this will need to be `-1 * time.Second`) 3. Notice the error  Describe the results you received Request returns an error and the container continues to run.  failed to parse parameters for /v1.41/containers/<ID>/stop?t=-1: schema: error converting value for ""t""   Describe the results you expected Container is sent a stop signal and is not terminated immediately or after a default timeout.  podman info output yaml host: arch: amd64 buildahVersion: 1.29.0-dev cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.5-3.fc38.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: ' cpuUtilization: idlePercent: 95.57 systemPercent: 1.1 userPercent: 3.33 cpus: 24 distribution: distribution: fedora variant: silverblue version: ""38"" eventLogger: journald hostname: matthew-desktop.local idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.2.0-0.rc7.20230208git0983f6bf2bfc.52.fc38.x86_64 linkmode: dynamic logDriver: journald memFree: 72184705024 memTotal: 134972575744 networkBackend: cni ociRuntime: name: crun package: crun-1.8-1.fc38.x86_64 path: /usr/bin/crun version: |- crun version 1.8 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-12.fc38.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 8589930496 swapTotal: 8589930496 uptime: 5h 45m 24.00s (Approximately 0.21 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: default-route-openshift-image-registry.apps-crc.testing: Blocked: false Insecure: true Location: default-route-openshift-image-registry.apps-crc.testing MirrorByDigestOnly: false Mirrors: null Prefix: default-route-openshift-image-registry.apps-crc.testing PullFromMirror: """" search: - docker.io - quay.io - registry.fedoraproject.org - registry.access.redhat.com store: configFile: /var/home/matthew/.config/containers/storage.conf containerStore: number: 10 paused: 0 running: 8 stopped: 2 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/matthew/.local/share/containers/storage graphRootAllocated: 998483427328 graphRootUsed: 696580554752 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 329 runRoot: /run/user/1000/containers transientStore: false volumePath: /var/home/matthew/.local/share/containers/storage/volumes version: APIVersion: 4.4.0-rc2 Built: 1674134600 BuiltTime: Thu Jan 19 06:23:20 2023 GitCommit: """" GoVersion: go1.20rc3 Os: linux OsArch: linux/amd64 Version: 4.4.0-rc2   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_",source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file,"Docker compatible `/containers/<id or name>/stop` does not support negative timeouts  Issue Description The Docker compatible `/containers/<id or name>/stop` endpoint does not support using a negative timeout. Docker's [spec](https://github.com/moby/moby/blob/master/api/types/container/config.go#L24-L32) for this endpoint states that you can ""use '-1' to wait indefinitely."" rather then having a set timeout after which the container will be killed. Podman's [implementation](https://github.com/containers/podman/blob/main/pkg/api/handlers/compat/containers_stop.go#L27) of this endpoint utilizes a `uint` rather then an `int`, causing a error to be thrown when using negative values.  Steps to reproduce the issue Steps to reproduce the issue 1. Run a container 2. Send stop request with a negative (indefinite) timeout value of `-1` (if using the Docker Go SDK this will need to be `-1 * time.Second`) 3. Notice the error  Describe the results you received Request returns an error and the container continues to run.  failed to parse parameters for /v1.41/containers/<ID>/stop?t=-1: schema: error converting value for ""t""   Describe the results you expected Container is sent a stop signal and is not terminated immediately or after a default timeout.  podman info output yaml host: arch: amd64 buildahVersion: 1.29.0-dev cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.5-3.fc38.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: ' cpuUtilization: idlePercent: 95.57 systemPercent: 1.1 userPercent: 3.33 cpus: 24 distribution: distribution: fedora variant: silverblue version: ""38"" eventLogger: journald hostname: matthew-desktop.local idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.2.0-0.rc7.20230208git0983f6bf2bfc.52.fc38.x86_64 linkmode: dynamic logDriver: journald memFree: 72184705024 memTotal: 134972575744 networkBackend: cni ociRuntime: name: crun package: crun-1.8-1.fc38.x86_64 path: /usr/bin/crun version: |- crun version 1.8 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-12.fc38.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 8589930496 swapTotal: 8589930496 uptime: 5h 45m 24.00s (Approximately 0.21 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: default-route-openshift-image-registry.apps-crc.testing: Blocked: false Insecure: true Location: default-route-openshift-image-registry.apps-crc.testing MirrorByDigestOnly: false Mirrors: null Prefix: default-route-openshift-image-registry.apps-crc.testing PullFromMirror: """" search: - docker.io - quay.io - registry.fedoraproject.org - registry.access.redhat.com store: configFile: /var/home/matthew/.config/containers/storage.conf containerStore: number: 10 paused: 0 running: 8 stopped: 2 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/matthew/.local/share/containers/storage graphRootAllocated: 998483427328 graphRootUsed: 696580554752 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 329 runRoot: /run/user/1000/containers transientStore: false volumePath: /var/home/matthew/.local/share/containers/storage/volumes version: APIVersion: 4.4.0-rc2 Built: 1674134600 BuiltTime: Thu Jan 19 06:23:20 2023 GitCommit: """" GoVersion: go1.20rc3 Os: linux OsArch: linux/amd64 Version: 4.4.0-rc2   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_ source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file documentation-file documentation-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file",bug,0.95
15765,podman,https://github.com/containers/podman/issues/15765,memory Limit is returning invalid result for `/container/stats` REST API on macOS,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** The field `memory_stats.limit` is returning an invalid value **Steps to reproduce the issue:** 1. Start a dummy container like fedora bash $ MY_DUMMY_CONTAINER=$(podman run -d fedora sleep 20000)  2. Display the memory of the container from inside the container bash $ podman exec -ti $MY_DUMMY_CONTAINER cat /proc/meminfo | head -3 MemTotal: 2018452 kB MemFree: 860084 kB MemAvailable: 1556036 kB  3. Now, read the value returned by the REST API bash curl --silent --unix-socket /Users/benoitf/.local/share/containers/podman/machine/podman-machine-default/podman.sock ""http:/v1.41/containers/$MY_DUMMY_CONTAINER/stats?stream=false&one-shot=true"" | jq .memory_stats { ""usage"": 94208, ""max_usage"": 18446744073709552000, ""limit"": 18446744073709552000 }  **Describe the results you received:**  ""limit"": 18446744073709552000  it seems this value is like the maximum value of the int/long/etc **Describe the results you expected:** something displaying around 2GB / 2Gi because it's the size of the machine.  ""limit"": 2066894848  and based from https://docs.docker.com/engine/api/v1.41/#tag/Container/operation/ContainerStats  used_memory = memory_stats.usage - memory_stats.stats.cache available_memory = memory_stats.limit Memory usage % = (used_memory / available_memory) * 100.0  **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  4.2.1  **Output of `podman info`:**  4.2.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  (paste your output here)  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes/No **Additional environment details (AWS, VirtualBox, physical, etc.):**",source-file | test-file | test-file,"memory Limit is returning invalid result for `/container/stats` REST API on macOS <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** The field `memory_stats.limit` is returning an invalid value **Steps to reproduce the issue:** 1. Start a dummy container like fedora bash $ MY_DUMMY_CONTAINER=$(podman run -d fedora sleep 20000)  2. Display the memory of the container from inside the container bash $ podman exec -ti $MY_DUMMY_CONTAINER cat /proc/meminfo | head -3 MemTotal: 2018452 kB MemFree: 860084 kB MemAvailable: 1556036 kB  3. Now, read the value returned by the REST API bash curl --silent --unix-socket /Users/benoitf/.local/share/containers/podman/machine/podman-machine-default/podman.sock ""http:/v1.41/containers/$MY_DUMMY_CONTAINER/stats?stream=false&one-shot=true"" | jq .memory_stats { ""usage"": 94208, ""max_usage"": 18446744073709552000, ""limit"": 18446744073709552000 }  **Describe the results you received:**  ""limit"": 18446744073709552000  it seems this value is like the maximum value of the int/long/etc **Describe the results you expected:** something displaying around 2GB / 2Gi because it's the size of the machine.  ""limit"": 2066894848  and based from https://docs.docker.com/engine/api/v1.41/#tag/Container/operation/ContainerStats  used_memory = memory_stats.usage - memory_stats.stats.cache available_memory = memory_stats.limit Memory usage % = (used_memory / available_memory) * 100.0  **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  4.2.1  **Output of `podman info`:**  4.2.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  (paste your output here)  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes/No **Additional environment details (AWS, VirtualBox, physical, etc.):** source-file test-file test-file",bug,0.95
19219,podman,https://github.com/containers/podman/issues/19219,Multiple filter options do not act as logical AND for volume ls," Issue Description Proximately the same issue as was described for `ps` in containers/podman#1341 Using multiple `--filter` option on volumes is an `OR` but should be `AND`.  Steps to reproduce the issue Steps to reproduce the issue: 1. Create test volumes using the follow commands 2. Query volumes using multiple filters  [root@localhost tmp]# docker volume create --label a=b --label b=a podman-test-a Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-a [root@localhost tmp]# docker volume create --label a=b --label c=d podman-test-b Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-b [root@localhost tmp]# docker volume create --label c=d --label e=f podman-test-c Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-c [root@localhost tmp]#  Query with compound filters:  [root@localhost tmp]# docker volume ls -q --filter label=a=b --filter label=c=d Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-a podman-test-b podman-test-c [root@localhost tmp]#   Describe the results you received Lists all volumes matching either label  [root@localhost tmp]# docker volume ls -q --filter label=a=b --filter label=c=d Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-a podman-test-b podman-test-c [root@localhost tmp]#   Describe the results you expected Lists only the volumes matching _all_ the specified filters:  [root@localhost tmp]# docker volume ls -q --filter label=a=b --filter label=c=d Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-b [root@localhost tmp]#   podman info output yaml host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.5-1.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: ' cpuUtilization: idlePercent: 99.78 systemPercent: 0.02 userPercent: 0.19 cpus: 8 distribution: distribution: fedora variant: container version: ""36"" eventLogger: file hostname: localhost idMappings: gidmap: null uidmap: null kernel: 5.10.102.1-microsoft-standard-WSL2 linkmode: dynamic logDriver: k8s-file memFree: 1144238080 memTotal: 8190472192 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.1-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.8.1 commit: f8a096be060b22ccd3d5f3ebe44108517fbf6c30 rundir: /root/.run/containers/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +WASM:wasmedge +YAJL os: linux remoteSocket: path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 2147483648 swapTotal: 2147483648 uptime: 166h 12m 0.00s (Approximately 6.92 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /etc/containers/storage.conf containerStore: number: 4 paused: 0 running: 2 stopped: 2 graphDriverName: overlay graphOptions: overlay.mount_program: Executable: /usr/bin/fuse-overlayfs Package: fuse-overlayfs-1.9-6.fc36.x86_64 Version: |- fusermount3 version: 3.10.5 fuse-overlayfs: version 1.9 FUSE library version 3.10.5 using FUSE kernel interface version 7.31 overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 269490393088 graphRootUsed: 14522699776 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 5 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.4.1 Built: 1676629882 BuiltTime: Fri Feb 17 02:31:22 2023 GitCommit: """" GoVersion: go1.18.10 Os: linux OsArch: linux/amd64 Version: 4.4.1   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details WSL environment created using podman-machine  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",other-file | source-file | source-file | source-file | source-file | test-file | other-file | source-file | source-file | source-file | source-file | test-file,"Multiple filter options do not act as logical AND for volume ls  Issue Description Proximately the same issue as was described for `ps` in containers/podman#1341 Using multiple `--filter` option on volumes is an `OR` but should be `AND`.  Steps to reproduce the issue Steps to reproduce the issue: 1. Create test volumes using the follow commands 2. Query volumes using multiple filters  [root@localhost tmp]# docker volume create --label a=b --label b=a podman-test-a Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-a [root@localhost tmp]# docker volume create --label a=b --label c=d podman-test-b Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-b [root@localhost tmp]# docker volume create --label c=d --label e=f podman-test-c Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-c [root@localhost tmp]#  Query with compound filters:  [root@localhost tmp]# docker volume ls -q --filter label=a=b --filter label=c=d Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-a podman-test-b podman-test-c [root@localhost tmp]#   Describe the results you received Lists all volumes matching either label  [root@localhost tmp]# docker volume ls -q --filter label=a=b --filter label=c=d Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-a podman-test-b podman-test-c [root@localhost tmp]#   Describe the results you expected Lists only the volumes matching _all_ the specified filters:  [root@localhost tmp]# docker volume ls -q --filter label=a=b --filter label=c=d Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. podman-test-b [root@localhost tmp]#   podman info output yaml host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.5-1.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: ' cpuUtilization: idlePercent: 99.78 systemPercent: 0.02 userPercent: 0.19 cpus: 8 distribution: distribution: fedora variant: container version: ""36"" eventLogger: file hostname: localhost idMappings: gidmap: null uidmap: null kernel: 5.10.102.1-microsoft-standard-WSL2 linkmode: dynamic logDriver: k8s-file memFree: 1144238080 memTotal: 8190472192 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.1-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.8.1 commit: f8a096be060b22ccd3d5f3ebe44108517fbf6c30 rundir: /root/.run/containers/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +WASM:wasmedge +YAJL os: linux remoteSocket: path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 2147483648 swapTotal: 2147483648 uptime: 166h 12m 0.00s (Approximately 6.92 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /etc/containers/storage.conf containerStore: number: 4 paused: 0 running: 2 stopped: 2 graphDriverName: overlay graphOptions: overlay.mount_program: Executable: /usr/bin/fuse-overlayfs Package: fuse-overlayfs-1.9-6.fc36.x86_64 Version: |- fusermount3 version: 3.10.5 fuse-overlayfs: version 1.9 FUSE library version 3.10.5 using FUSE kernel interface version 7.31 overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 269490393088 graphRootUsed: 14522699776 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 5 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.4.1 Built: 1676629882 BuiltTime: Fri Feb 17 02:31:22 2023 GitCommit: """" GoVersion: go1.18.10 Os: linux OsArch: linux/amd64 Version: 4.4.1   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details WSL environment created using podman-machine  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting other-file source-file source-file source-file source-file test-file other-file source-file source-file source-file source-file test-file",bug,0.9
19280,podman,https://github.com/containers/podman/issues/19280,Why am I receiving random bytes over podman.socket," Issue Description I sometimes receive a sequence like this: `\SOH\NUL\NUL\NUL\NUL\NUL\NULb` which breaks the json that my script in the exec is outputting. My first workaround was to throw away the first 8 bytes but this then reoccurs later in the stream.  Steps to reproduce the issue Steps to reproduce the issue 1. send  POST /containers/{cid}/exec HTTP/1.1 Host: localhost User-Agent: something Content-Type: application/json Content-Length: 196 {""AttachStderr"":true,""AttachStdin"":false,""AttachStdout"":true,""Cmd"":[""/bin/bash"",""-c"",""ls""],""DetachKeys"":""ctrl-a"",""Logs"":false,""Stream"":true,""Tty"":false,""User"":""root"",""WorkingDir"":""/tmp/compiling""}  2. send  POST /exec/{responseIdFromStep1}/start HTTP/1.1 Host: localhost User-Agent: esystant Content-Type: application/json Content-Length: 27 {""Detach"":false,""Tty"":true}  3. Observe weird bytes prepended. 4. I've also noticed that after reading a lot of bytes, this re-occurs.  Describe the results you received This is what I get: [file1.txt](https://github.com/containers/podman/files/12089248/file1.txt)  Describe the results you expected This is what I expect to get: [file1.txt](https://github.com/containers/podman/files/12089250/file1.txt)  podman info output yaml host: arch: amd64 buildahVersion: 1.30.0 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: /usr/bin/conmon is owned by conmon 1:2.1.7-1 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: f633919178f6c8ee4fb41b848a056ec33f8d707d' cpuUtilization: idlePercent: 97.19 systemPercent: 0.4 userPercent: 2.41 cpus: 12 databaseBackend: boltdb distribution: distribution: arch version: unknown eventLogger: journald hostname: arch idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.4.3-arch1-2 linkmode: dynamic logDriver: journald memFree: 10285826048 memTotal: 33556062208 networkBackend: netavark ociRuntime: name: crun package: /usr/bin/crun is owned by crun 1.8.5-1 path: /usr/bin/crun version: |- crun version 1.8.5 commit: b6f80f766c9a89eb7b1440c0a70ab287434b17ed rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: /usr/bin/slirp4netns is owned by slirp4netns 1.2.0-1 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 8589668352 swapTotal: 8589930496 uptime: 11h 28m 53.00s (Approximately 0.46 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /home/merlijn/.config/containers/storage.conf containerStore: number: 5 paused: 0 running: 2 stopped: 3 graphDriverName: overlay graphOptions: {} graphRoot: /home/merlijn/.local/share/containers/storage graphRootAllocated: 973837332480 graphRootUsed: 832484134912 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 33 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/merlijn/.local/share/containers/storage/volumes version: APIVersion: 4.5.1 Built: 1685139594 BuiltTime: Sat May 27 00:19:54 2023 GitCommit: 9eef30051c83f62816a1772a743e5f1271b196d7-dirty GoVersion: go1.20.4 Os: linux OsArch: linux/amd64 Version: 4.5.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release No  Additional environment details Arch linux  Additional information I used `nc -U /run/user/1000/podman/podman.sock > file1.txt 2>&1` to send the http command and create the output files",source-file | source-file,"Why am I receiving random bytes over podman.socket  Issue Description I sometimes receive a sequence like this: `\SOH\NUL\NUL\NUL\NUL\NUL\NULb` which breaks the json that my script in the exec is outputting. My first workaround was to throw away the first 8 bytes but this then reoccurs later in the stream.  Steps to reproduce the issue Steps to reproduce the issue 1. send  POST /containers/{cid}/exec HTTP/1.1 Host: localhost User-Agent: something Content-Type: application/json Content-Length: 196 {""AttachStderr"":true,""AttachStdin"":false,""AttachStdout"":true,""Cmd"":[""/bin/bash"",""-c"",""ls""],""DetachKeys"":""ctrl-a"",""Logs"":false,""Stream"":true,""Tty"":false,""User"":""root"",""WorkingDir"":""/tmp/compiling""}  2. send  POST /exec/{responseIdFromStep1}/start HTTP/1.1 Host: localhost User-Agent: esystant Content-Type: application/json Content-Length: 27 {""Detach"":false,""Tty"":true}  3. Observe weird bytes prepended. 4. I've also noticed that after reading a lot of bytes, this re-occurs.  Describe the results you received This is what I get: [file1.txt](https://github.com/containers/podman/files/12089248/file1.txt)  Describe the results you expected This is what I expect to get: [file1.txt](https://github.com/containers/podman/files/12089250/file1.txt)  podman info output yaml host: arch: amd64 buildahVersion: 1.30.0 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: /usr/bin/conmon is owned by conmon 1:2.1.7-1 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: f633919178f6c8ee4fb41b848a056ec33f8d707d' cpuUtilization: idlePercent: 97.19 systemPercent: 0.4 userPercent: 2.41 cpus: 12 databaseBackend: boltdb distribution: distribution: arch version: unknown eventLogger: journald hostname: arch idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.4.3-arch1-2 linkmode: dynamic logDriver: journald memFree: 10285826048 memTotal: 33556062208 networkBackend: netavark ociRuntime: name: crun package: /usr/bin/crun is owned by crun 1.8.5-1 path: /usr/bin/crun version: |- crun version 1.8.5 commit: b6f80f766c9a89eb7b1440c0a70ab287434b17ed rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: /usr/bin/slirp4netns is owned by slirp4netns 1.2.0-1 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 8589668352 swapTotal: 8589930496 uptime: 11h 28m 53.00s (Approximately 0.46 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /home/merlijn/.config/containers/storage.conf containerStore: number: 5 paused: 0 running: 2 stopped: 3 graphDriverName: overlay graphOptions: {} graphRoot: /home/merlijn/.local/share/containers/storage graphRootAllocated: 973837332480 graphRootUsed: 832484134912 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 33 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/merlijn/.local/share/containers/storage/volumes version: APIVersion: 4.5.1 Built: 1685139594 BuiltTime: Sat May 27 00:19:54 2023 GitCommit: 9eef30051c83f62816a1772a743e5f1271b196d7-dirty GoVersion: go1.20.4 Os: linux OsArch: linux/amd64 Version: 4.5.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release No  Additional environment details Arch linux  Additional information I used `nc -U /run/user/1000/podman/podman.sock > file1.txt 2>&1` to send the http command and create the output files source-file source-file",bug,0.9
15828,podman,https://github.com/containers/podman/issues/15828,Create image compat endpoint does not return 404,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** The create image compatibility endpoint does not behave the same as the docker engine endpoint or the documentation https://docs.podman.io/en/latest/_static/api.html#tag/images-(compat) When provided with an image that does not exist on the query parameter `fromImage` the endpoint returns a status code 200 instead of 404. Response body contains the expected error text. <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. cURL the endpoint with the command ` curl -XPOST --unix-socket /var/run/docker.sock -v 'http://d/images/create?fromImage=NON_EXISTING_IMAGE' --header 'X-Registry-Auth: AUTH_TOKEN'` **Describe the results you received:** Received status code 200 OK  * Trying /var/run/docker.sock:0 * Connected to d (/run/user/podman/podman.sock) port 80 (#0) > POST /images/create?fromImage=NON_EXISTING_IMAGE HTTP/1.1 > Host: d > User-Agent: curl/7.83.1 > Accept: */* > X-Registry-Auth: AUTH_TOKEN > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.40 < Libpod-Api-Version: 4.1.0 < Server: Libpod/4.1.0 (linux) < X-Reference-Id: 0xc000010008 < Date: Wed, 14 Sep 2022 08:53:00 GMT < Transfer-Encoding: chunked < {""progressDetail"":{},""error"":""initializing source NON_EXISTING_IMAGE: reading manifest latest in IMAGE: manifest unknown: manifest unknown""} * Connection #0 to host d left intact  **Describe the results you expected:** Expected status code 404 NOT FOUND  * Trying /var/run/docker.sock:0 * Connected to d (/run/user/podman/podman.sock) port 80 (#0) > POST /images/create?fromImage=NON_EXISTING_IMAGE HTTP/1.1 > Host: d > User-Agent: curl/7.83.1 > Accept: */* > X-Registry-Auth: AUTH_TOKEN > * Mark bundle as not supporting multiuse < HTTP/1.1 404 Not Found < Api-Version: 1.40 < Libpod-Api-Version: 4.1.0 < Server: Libpod/4.1.0 (linux) < X-Reference-Id: 0xc000010008 < Date: Wed, 14 Sep 2022 08:53:00 GMT < Transfer-Encoding: chunked < {""progressDetail"":{},""error"":""initializing source NON_EXISTING_IMAGE: reading manifest latest in IMAGE: manifest unknown: manifest unknown""} * Connection #0 to host d left intact  **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Client: Podman Engine Version: 4.1.0 API Version: 4.1.0 Go Version: go1.18.5 Git Commit: c5de69cd3da571c8ebf1c03aa49d07c1e35da67c Built: Tue Aug 2 10:39:40 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  host: arch: amd64 buildahVersion: 1.26.1 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.2-r0 path: /usr/bin/conmon version: 'conmon version 2.1.2, commit: f25214cda836296fe8695c2e582ca79d2246053a' cpuUtilization: idlePercent: 86.72 systemPercent: 4.6 userPercent: 8.68 cpus: 6 distribution: distribution: alpine version: 3.16.2 eventLogger: file hostname: image-builder-0 idMappings: gidmap: null uidmap: null kernel: 5.4.0-121-generic linkmode: dynamic logDriver: k8s-file memFree: 6932779008 memTotal: 50502590464 networkBackend: netavark ociRuntime: name: crun package: crun-1.4.5-r0 path: /usr/bin/crun version: |- crun version 1.4.5 commit: c381048530aa750495cf502ddb7181f2ded5b400 spec: 1.0.0 +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-r0 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.2 swapFree: 0 swapTotal: 0 uptime: 1762h 3m 18.39s (Approximately 73.42 days) plugins: log: - k8s-file - none - passthrough network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev graphRoot: /var/lib/containers/storage graphRootAllocated: 20507869184 graphRootUsed: 46239744 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.1.0 Built: 1659436780 BuiltTime: Tue Aug 2 10:39:40 2022 GitCommit: c5de69cd3da571c8ebf1c03aa49d07c1e35da67c GoVersion: go1.18.5 Os: linux OsArch: linux/amd64 Version: 4.1.0  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman-4.1.0-r3 x86_64 {podman} (Apache-2.0) [installed] podman-4.1.0-r4 x86_64 {podman} (Apache-2.0) [upgradable from: podman-4.1.0-r3]  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** Running on K8S",other-file | source-file | test-file,"Create image compat endpoint does not return 404 <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** The create image compatibility endpoint does not behave the same as the docker engine endpoint or the documentation https://docs.podman.io/en/latest/_static/api.html#tag/images-(compat) When provided with an image that does not exist on the query parameter `fromImage` the endpoint returns a status code 200 instead of 404. Response body contains the expected error text. <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. cURL the endpoint with the command ` curl -XPOST --unix-socket /var/run/docker.sock -v 'http://d/images/create?fromImage=NON_EXISTING_IMAGE' --header 'X-Registry-Auth: AUTH_TOKEN'` **Describe the results you received:** Received status code 200 OK  * Trying /var/run/docker.sock:0 * Connected to d (/run/user/podman/podman.sock) port 80 (#0) > POST /images/create?fromImage=NON_EXISTING_IMAGE HTTP/1.1 > Host: d > User-Agent: curl/7.83.1 > Accept: */* > X-Registry-Auth: AUTH_TOKEN > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.40 < Libpod-Api-Version: 4.1.0 < Server: Libpod/4.1.0 (linux) < X-Reference-Id: 0xc000010008 < Date: Wed, 14 Sep 2022 08:53:00 GMT < Transfer-Encoding: chunked < {""progressDetail"":{},""error"":""initializing source NON_EXISTING_IMAGE: reading manifest latest in IMAGE: manifest unknown: manifest unknown""} * Connection #0 to host d left intact  **Describe the results you expected:** Expected status code 404 NOT FOUND  * Trying /var/run/docker.sock:0 * Connected to d (/run/user/podman/podman.sock) port 80 (#0) > POST /images/create?fromImage=NON_EXISTING_IMAGE HTTP/1.1 > Host: d > User-Agent: curl/7.83.1 > Accept: */* > X-Registry-Auth: AUTH_TOKEN > * Mark bundle as not supporting multiuse < HTTP/1.1 404 Not Found < Api-Version: 1.40 < Libpod-Api-Version: 4.1.0 < Server: Libpod/4.1.0 (linux) < X-Reference-Id: 0xc000010008 < Date: Wed, 14 Sep 2022 08:53:00 GMT < Transfer-Encoding: chunked < {""progressDetail"":{},""error"":""initializing source NON_EXISTING_IMAGE: reading manifest latest in IMAGE: manifest unknown: manifest unknown""} * Connection #0 to host d left intact  **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Client: Podman Engine Version: 4.1.0 API Version: 4.1.0 Go Version: go1.18.5 Git Commit: c5de69cd3da571c8ebf1c03aa49d07c1e35da67c Built: Tue Aug 2 10:39:40 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  host: arch: amd64 buildahVersion: 1.26.1 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.2-r0 path: /usr/bin/conmon version: 'conmon version 2.1.2, commit: f25214cda836296fe8695c2e582ca79d2246053a' cpuUtilization: idlePercent: 86.72 systemPercent: 4.6 userPercent: 8.68 cpus: 6 distribution: distribution: alpine version: 3.16.2 eventLogger: file hostname: image-builder-0 idMappings: gidmap: null uidmap: null kernel: 5.4.0-121-generic linkmode: dynamic logDriver: k8s-file memFree: 6932779008 memTotal: 50502590464 networkBackend: netavark ociRuntime: name: crun package: crun-1.4.5-r0 path: /usr/bin/crun version: |- crun version 1.4.5 commit: c381048530aa750495cf502ddb7181f2ded5b400 spec: 1.0.0 +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-r0 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.2 swapFree: 0 swapTotal: 0 uptime: 1762h 3m 18.39s (Approximately 73.42 days) plugins: log: - k8s-file - none - passthrough network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev graphRoot: /var/lib/containers/storage graphRootAllocated: 20507869184 graphRootUsed: 46239744 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.1.0 Built: 1659436780 BuiltTime: Tue Aug 2 10:39:40 2022 GitCommit: c5de69cd3da571c8ebf1c03aa49d07c1e35da67c GoVersion: go1.18.5 Os: linux OsArch: linux/amd64 Version: 4.1.0  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman-4.1.0-r3 x86_64 {podman} (Apache-2.0) [installed] podman-4.1.0-r4 x86_64 {podman} (Apache-2.0) [upgradable from: podman-4.1.0-r3]  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** Running on K8S other-file source-file test-file",bug,0.95
14498,podman,https://github.com/containers/podman/issues/14498,container state improper 409 - via stats endpoint for container in created state,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** - Its not possible to attach to stats API endpoint via `/container/{id}/stats` if the container is created via `/containers/create` endpoint and is in created/initialized state. Same applies for CLI (`podman stats {id}`). - Its working for Docker 20.10.13 <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. `containerid=$(curl -s --unix-socket /run/podman/podman.sock -X POST -H ""Content-Type: application/json"" -d @payload.json ""http://d/containers/create"" | jq -r .Id)` 2. `curl -s --unix-socket /run/podman/podman.sock -X GET http://d/containers/$containerid/stats` **Describe the results you received:** Response from step2 `{""cause"":""container state improper"",""message"":""container state improper"",""response"":409}` **Describe the results you expected:** Empty stats are streamed   {""read"":""0001-01-01T00:00:00Z"",""preread"":""0001-01-01T00:00:00Z"",""pids_stats"":{},""blkio_stats"":{""io_service_bytes_recursive"":null,""io_serviced_recursive"":null,""io_queue_recursive"":null,""io_service_time_recursive"":null,""io_wait_time_recursive"":null,""io_merged_recursive"":null,""io_time_recursive"":null,""sectors_recursive"":null},""num_procs"":0,""storage_stats"":{},""cpu_stats"":{""cpu_usage"":{""total_usage"":0,""usage_in_kernelmode"":0,""usage_in_usermode"":0},""throttling_data"":{""periods"":0,""throttled_periods"":0,""throttled_time"":0}},""precpu_stats"":{""cpu_usage"":{""total_usage"":0,""usage_in_kernelmode"":0,""usage_in_usermode"":0},""throttling_data"":{""periods"":0,""throttled_periods"":0,""throttled_time"":0}},""memory_stats"":{},""name"":""/pensive_sanderson"",""id"":""3fb3108ac3ed8e0d41062889e6f442bc79b03b73d78f6a93d0a469324acbdabf""} {""read"":""0001-01-01T00:00:00Z"",""preread"":""0001-01-01T00:00:00Z"",""pids_stats"":{},""blkio_stats"":{""io_service_bytes_recursive"":null,""io_serviced_recursive"":null,""io_queue_recursive"":null,""io_service_time_recursive"":null,""io_wait_time_recursive"":null,""io_merged_recursive"":null,""io_time_recursive"":null,""sectors_recursive"":null},""num_procs"":0,""storage_stats"":{},""cpu_stats"":{""cpu_usage"":{""total_usage"":0,""usage_in_kernelmode"":0,""usage_in_usermode"":0},""throttling_data"":{""periods"":0,""throttled_periods"":0,""throttled_time"":0}},""precpu_stats"":{""cpu_usage"":{""total_usage"":0,""usage_in_kernelmode"":0,""usage_in_usermode"":0},""throttling_data"":{""periods"":0,""throttled_periods"":0,""throttled_time"":0}},""memory_stats"":{},""name"":""/pensive_sanderson"",""id"":""3fb3108ac3ed8e0d41062889e6f442bc79b03b73d78f6a93d0a469324acbdabf""}   **Additional information you deem important (e.g. issue happens only occasionally):** - Podman CLI returns:  podman stats $containerid Error: entities.ContainerStatsReport.Error: decode non empty interface: can not unmarshal into nil, error found in #9 byte of |{""Error"":{},""Stats""|, bigger context |{""Error"":{},""Stats"":null}  **Output of `podman version`:**  Client: Podman Engine Version: 4.0.2 API Version: 4.0.2 Go Version: go1.18beta2 Built: Thu Mar 3 14:56:09 2022 OS/Arch: linux/amd64  **Output of `podman info --debug`:**  (paste your output here)host: arch: amd64 buildahVersion: 1.24.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-2.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: ' cpus: 8 distribution: distribution: fedora variant: coreos version: ""36"" eventLogger: journald hostname: ip-10-2-17-77 idMappings: gidmap: null uidmap: null kernel: 5.17.5-300.fc36.x86_64 linkmode: dynamic logDriver: journald memFree: 13253181440 memTotal: 32935444480 networkBackend: netavark ociRuntime: name: crun package: crun-1.4.4-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.4.4 commit: 6521fcc5806f20f6187eb933f9f45130c86da230 spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 96h 0m 0.13s (Approximately 4.00 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 12 paused: 0 running: 3 stopped: 9 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 26 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.0.2 Built: 1646319369 BuiltTime: Thu Mar 3 14:56:09 2022 GitCommit: """" GoVersion: go1.18beta2 OsArch: linux/amd64 Version: 4.0.2  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman-4.0.2-1.fc36.x86_64  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** No **Additional environment details (AWS, VirtualBox, physical, etc.):** - AWS - Fedora CoreOS 36.20220505.3.2",source-file | source-file | source-file | test-file | source-file | source-file | source-file | test-file | test-file | source-file | source-file | source-file | test-file | test-file,"container state improper 409 - via stats endpoint for container in created state <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** - Its not possible to attach to stats API endpoint via `/container/{id}/stats` if the container is created via `/containers/create` endpoint and is in created/initialized state. Same applies for CLI (`podman stats {id}`). - Its working for Docker 20.10.13 <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. `containerid=$(curl -s --unix-socket /run/podman/podman.sock -X POST -H ""Content-Type: application/json"" -d @payload.json ""http://d/containers/create"" | jq -r .Id)` 2. `curl -s --unix-socket /run/podman/podman.sock -X GET http://d/containers/$containerid/stats` **Describe the results you received:** Response from step2 `{""cause"":""container state improper"",""message"":""container state improper"",""response"":409}` **Describe the results you expected:** Empty stats are streamed   {""read"":""0001-01-01T00:00:00Z"",""preread"":""0001-01-01T00:00:00Z"",""pids_stats"":{},""blkio_stats"":{""io_service_bytes_recursive"":null,""io_serviced_recursive"":null,""io_queue_recursive"":null,""io_service_time_recursive"":null,""io_wait_time_recursive"":null,""io_merged_recursive"":null,""io_time_recursive"":null,""sectors_recursive"":null},""num_procs"":0,""storage_stats"":{},""cpu_stats"":{""cpu_usage"":{""total_usage"":0,""usage_in_kernelmode"":0,""usage_in_usermode"":0},""throttling_data"":{""periods"":0,""throttled_periods"":0,""throttled_time"":0}},""precpu_stats"":{""cpu_usage"":{""total_usage"":0,""usage_in_kernelmode"":0,""usage_in_usermode"":0},""throttling_data"":{""periods"":0,""throttled_periods"":0,""throttled_time"":0}},""memory_stats"":{},""name"":""/pensive_sanderson"",""id"":""3fb3108ac3ed8e0d41062889e6f442bc79b03b73d78f6a93d0a469324acbdabf""} {""read"":""0001-01-01T00:00:00Z"",""preread"":""0001-01-01T00:00:00Z"",""pids_stats"":{},""blkio_stats"":{""io_service_bytes_recursive"":null,""io_serviced_recursive"":null,""io_queue_recursive"":null,""io_service_time_recursive"":null,""io_wait_time_recursive"":null,""io_merged_recursive"":null,""io_time_recursive"":null,""sectors_recursive"":null},""num_procs"":0,""storage_stats"":{},""cpu_stats"":{""cpu_usage"":{""total_usage"":0,""usage_in_kernelmode"":0,""usage_in_usermode"":0},""throttling_data"":{""periods"":0,""throttled_periods"":0,""throttled_time"":0}},""precpu_stats"":{""cpu_usage"":{""total_usage"":0,""usage_in_kernelmode"":0,""usage_in_usermode"":0},""throttling_data"":{""periods"":0,""throttled_periods"":0,""throttled_time"":0}},""memory_stats"":{},""name"":""/pensive_sanderson"",""id"":""3fb3108ac3ed8e0d41062889e6f442bc79b03b73d78f6a93d0a469324acbdabf""}   **Additional information you deem important (e.g. issue happens only occasionally):** - Podman CLI returns:  podman stats $containerid Error: entities.ContainerStatsReport.Error: decode non empty interface: can not unmarshal into nil, error found in #9 byte of |{""Error"":{},""Stats""|, bigger context |{""Error"":{},""Stats"":null}  **Output of `podman version`:**  Client: Podman Engine Version: 4.0.2 API Version: 4.0.2 Go Version: go1.18beta2 Built: Thu Mar 3 14:56:09 2022 OS/Arch: linux/amd64  **Output of `podman info --debug`:**  (paste your output here)host: arch: amd64 buildahVersion: 1.24.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-2.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: ' cpus: 8 distribution: distribution: fedora variant: coreos version: ""36"" eventLogger: journald hostname: ip-10-2-17-77 idMappings: gidmap: null uidmap: null kernel: 5.17.5-300.fc36.x86_64 linkmode: dynamic logDriver: journald memFree: 13253181440 memTotal: 32935444480 networkBackend: netavark ociRuntime: name: crun package: crun-1.4.4-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.4.4 commit: 6521fcc5806f20f6187eb933f9f45130c86da230 spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 96h 0m 0.13s (Approximately 4.00 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 12 paused: 0 running: 3 stopped: 9 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 26 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.0.2 Built: 1646319369 BuiltTime: Thu Mar 3 14:56:09 2022 GitCommit: """" GoVersion: go1.18beta2 OsArch: linux/amd64 Version: 4.0.2  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman-4.0.2-1.fc36.x86_64  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** No **Additional environment details (AWS, VirtualBox, physical, etc.):** - AWS - Fedora CoreOS 36.20220505.3.2 source-file source-file source-file test-file source-file source-file source-file test-file test-file source-file source-file source-file test-file test-file",bug,0.95
15430,podman,https://github.com/containers/podman/issues/15430,podman system service uses different syntax to podman-remote for tcp connections,"/kind bug **Description** `podman system service` uses an inconsistent syntax for specifying the listen port compared to `podman --remote --connection` and does not error about it properly. **Steps to reproduce the issue:** 1. Try starting a podman system service on a tcp port with the syntax one would expect from the `--connection` syntax: `podman system service tcp://localhost:2375` **Describe the results you received:** The command appears to succeed, but no listening port on `2375` or any other is created. **Describe the results you expected:** The command should either error, explaining that the syntax is invalid, or better, actually work since the same pseudo-path like syntax is supported for remote connections. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Version: 3.4.4 API Version: 3.4.4 Go Version: go1.17.3 Built: Thu Jan 1 10:00:00 1970 OS/Arch: linux/amd64  **Output of `podman info`:**  host: arch: amd64 buildahVersion: 1.23.1 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: 'conmon: /usr/bin/conmon' path: /usr/bin/conmon version: 'conmon version 2.0.25, commit: unknown' cpus: 16 distribution: codename: jammy distribution: ubuntu version: ""22.04"" eventLogger: file hostname: will-desktop idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.15.0-27-generic linkmode: dynamic logDriver: k8s-file memFree: 34020188160 memTotal: 67362914304 ociRuntime: name: runc package: 'runc: /usr/sbin/runc' path: /usr/sbin/runc version: |- runc version 1.1.0-0ubuntu1 spec: 1.0.2-dev go: go1.17.3 libseccomp: 2.5.3 os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: 'slirp4netns: /usr/bin/slirp4netns' version: |- slirp4netns version 1.0.1 commit: 6a7b16babc95b6a3056b33fb45b74a6f62262dd4 libslirp: 4.6.1 swapFree: 524283904 swapTotal: 524283904 uptime: 69h 14m 23.56s (Approximately 2.88 days) plugins: log: - k8s-file - none - journald network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /home/will/.config/containers/storage.conf containerStore: number: 43 paused: 0 running: 2 stopped: 41 graphDriverName: overlay graphOptions: overlay.mount_program: Executable: /usr/bin/fuse-overlayfs Package: 'fuse-overlayfs: /usr/bin/fuse-overlayfs' Version: |- fusermount3 version: 3.10.5 fuse-overlayfs: version 1.7.1 FUSE library version 3.10.5 using FUSE kernel interface version 7.31 graphRoot: /home/will/.local/share/containers/storage graphStatus: Backing Filesystem: zfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageStore: number: 524 runRoot: /run/user/1000/containers volumePath: /home/will/.local/share/containers/storage/volumes version: APIVersion: 3.4.4 Built: 0 BuiltTime: Thu Jan 1 10:00:00 1970 GitCommit: """" GoVersion: go1.17.3 OsArch: linux/amd64 Version: 3.4.4  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman/jammy,now 3.4.4+ds1-1ubuntu1 amd64 [installed]  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes. The command exits, but still displays no error message or explanation as to why. **Additional environment details (AWS, VirtualBox, physical, etc.):** Physical",documentation-file | test-file | test-file | test-file | test-file,"podman system service uses different syntax to podman-remote for tcp connections /kind bug **Description** `podman system service` uses an inconsistent syntax for specifying the listen port compared to `podman --remote --connection` and does not error about it properly. **Steps to reproduce the issue:** 1. Try starting a podman system service on a tcp port with the syntax one would expect from the `--connection` syntax: `podman system service tcp://localhost:2375` **Describe the results you received:** The command appears to succeed, but no listening port on `2375` or any other is created. **Describe the results you expected:** The command should either error, explaining that the syntax is invalid, or better, actually work since the same pseudo-path like syntax is supported for remote connections. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Version: 3.4.4 API Version: 3.4.4 Go Version: go1.17.3 Built: Thu Jan 1 10:00:00 1970 OS/Arch: linux/amd64  **Output of `podman info`:**  host: arch: amd64 buildahVersion: 1.23.1 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: 'conmon: /usr/bin/conmon' path: /usr/bin/conmon version: 'conmon version 2.0.25, commit: unknown' cpus: 16 distribution: codename: jammy distribution: ubuntu version: ""22.04"" eventLogger: file hostname: will-desktop idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.15.0-27-generic linkmode: dynamic logDriver: k8s-file memFree: 34020188160 memTotal: 67362914304 ociRuntime: name: runc package: 'runc: /usr/sbin/runc' path: /usr/sbin/runc version: |- runc version 1.1.0-0ubuntu1 spec: 1.0.2-dev go: go1.17.3 libseccomp: 2.5.3 os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: 'slirp4netns: /usr/bin/slirp4netns' version: |- slirp4netns version 1.0.1 commit: 6a7b16babc95b6a3056b33fb45b74a6f62262dd4 libslirp: 4.6.1 swapFree: 524283904 swapTotal: 524283904 uptime: 69h 14m 23.56s (Approximately 2.88 days) plugins: log: - k8s-file - none - journald network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /home/will/.config/containers/storage.conf containerStore: number: 43 paused: 0 running: 2 stopped: 41 graphDriverName: overlay graphOptions: overlay.mount_program: Executable: /usr/bin/fuse-overlayfs Package: 'fuse-overlayfs: /usr/bin/fuse-overlayfs' Version: |- fusermount3 version: 3.10.5 fuse-overlayfs: version 1.7.1 FUSE library version 3.10.5 using FUSE kernel interface version 7.31 graphRoot: /home/will/.local/share/containers/storage graphStatus: Backing Filesystem: zfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageStore: number: 524 runRoot: /run/user/1000/containers volumePath: /home/will/.local/share/containers/storage/volumes version: APIVersion: 3.4.4 Built: 0 BuiltTime: Thu Jan 1 10:00:00 1970 GitCommit: """" GoVersion: go1.17.3 OsArch: linux/amd64 Version: 3.4.4  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman/jammy,now 3.4.4+ds1-1ubuntu1 amd64 [installed]  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes. The command exits, but still displays no error message or explanation as to why. **Additional environment details (AWS, VirtualBox, physical, etc.):** Physical documentation-file test-file test-file test-file test-file",bug,0.9
24910,podman,https://github.com/containers/podman/issues/24910,Container inspect endpoint is not Docker-API compatible," Issue Description I am currently playing with `podman compose` utilizing the upstream `docker-compose` as backend. It always recreates the container because the network config is not as it should be. I added some debug prints to the `docker-compose` code and it cannot match the networks because the `NetworkID` returned by podman is wrong. Running something along the lines of `podman inspect 4cb3ce78cde3 --format=""{{ json .NetworkSettings.Networks }}""|jq` returns: json { ""tmp_default"": { ""EndpointID"": """", ""Gateway"": ""10.89.0.1"", ""IPAddress"": ""10.89.0.3"", ""IPPrefixLen"": 24, ""IPv6Gateway"": """", ""GlobalIPv6Address"": """", ""GlobalIPv6PrefixLen"": 0, ""MacAddress"": ""c2:79:d5:75:e5:09"", ""NetworkID"": ""tmp_default"", ""DriverOpts"": null, ""IPAMConfig"": null, ""Links"": null, ""Aliases"": [ ""tmp-traefik-1"", ""traefik"", ""4cb3ce78cde3"" ] } }  Here we can see that `NetworkID` is the network name and not the id of the network (`podman network inspect tmp_default`): json [ { ""name"": ""tmp_default"", ""id"": ""675bea81a6d49e9f1ae909c58a2269524a9a9c910f91bce12a85b05b882ab5cb"", ""driver"": ""bridge"", ""network_interface"": ""podman1"", ""created"": ""2024-12-28T20:54:28.320995381+01:00"", ""subnets"": [ { ""subnet"": ""10.89.0.0/24"", ""gateway"": ""10.89.0.1"" } ], ""ipv6_enabled"": false, ""internal"": false, ""dns_enabled"": true, ""labels"": { ""com.docker.compose.config-hash"": ""4bd7e5f2ecc11ab238c4eb5931f77d1e2196619bcc8daeeb80428562a7210c37"", ""com.docker.compose.network"": ""default"", ""com.docker.compose.project"": ""tmp"", ""com.docker.compose.version"": ""2.32.1"" }, ""options"": { ""isolate"": ""true"" }, ""ipam_options"": { ""driver"": ""host-local"" }, ""containers"": { ""4cb3ce78cde33f763bdd6abe5b19a5d93f5bb30043c94fa489114a48da5d4290"": { ""name"": ""tmp-traefik-1"", ""interfaces"": { ""eth0"": { ""subnets"": [ { ""ipnet"": ""10.89.0.3/24"", ""gateway"": ""10.89.0.1"" } ], ""mac_address"": ""c2:79:d5:75:e5:09"" } } } } } ]   Steps to reproduce the issue Steps to reproduce the issue 1. `podman run --rm -it --name testing ubuntu:latest sleep infinity` 2. `podman inspect testing --format=""{{ json .NetworkSettings.Networks.podman }}""|jq` 3. Observe that the `NetworkID` is the network name  Describe the results you received I would have expected the actual `NetworkID` to be shown  Describe the results you expected The network name is shown as ID  podman info output yaml host: arch: amd64 buildahVersion: 1.38.0 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.12-3.fc41.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.12, commit: ' cpuUtilization: idlePercent: 97.27 systemPercent: 0.8 userPercent: 1.93 cpus: 16 databaseBackend: sqlite distribution: distribution: fedora variant: workstation version: ""41"" eventLogger: journald freeLocks: 2046 hostname: apollo13 idMappings: gidmap: null uidmap: null kernel: 6.12.4-200.fc41.x86_64 linkmode: dynamic logDriver: journald memFree: 6023499776 memTotal: 32395685888 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.13.1-1.fc41.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.13.1 package: netavark-1.13.1-1.fc41.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.13.1 ociRuntime: name: crun package: crun-1.19.1-1.fc41.x86_64 path: /usr/bin/crun version: |- crun version 1.19.1 commit: 3e32a70c93f5aa5fea69b50256cca7fd4aa23c80 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20241211.g09478d5-1.fc41.x86_64 version: | pasta 0^20241211.g09478d5-1.fc41.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.3.1-1.fc41.x86_64 version: |- slirp4netns version 1.3.1 commit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236 libslirp: 4.8.0 SLIRP_CONFIG_VERSION_MAX: 5 libseccomp: 2.5.5 swapFree: 8589930496 swapTotal: 8589930496 uptime: 11h 16m 31.00s (Approximately 0.46 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 2 paused: 0 running: 2 stopped: 0 graphDriverName: overlay graphOptions: overlay.imagestore: /usr/lib/containers/storage overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 536854134784 graphRootUsed: 403665657856 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 5 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.3.1 Built: 1732147200 BuiltTime: Thu Nov 21 01:00:00 2024 GitCommit: """" GoVersion: go1.23.3 Os: linux OsArch: linux/amd64 Version: 5.3.1   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details _No response_  Additional information _No response_",source-file | source-file | config-file | source-file | test-file | test-file | config-file | source-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file,"Container inspect endpoint is not Docker-API compatible  Issue Description I am currently playing with `podman compose` utilizing the upstream `docker-compose` as backend. It always recreates the container because the network config is not as it should be. I added some debug prints to the `docker-compose` code and it cannot match the networks because the `NetworkID` returned by podman is wrong. Running something along the lines of `podman inspect 4cb3ce78cde3 --format=""{{ json .NetworkSettings.Networks }}""|jq` returns: json { ""tmp_default"": { ""EndpointID"": """", ""Gateway"": ""10.89.0.1"", ""IPAddress"": ""10.89.0.3"", ""IPPrefixLen"": 24, ""IPv6Gateway"": """", ""GlobalIPv6Address"": """", ""GlobalIPv6PrefixLen"": 0, ""MacAddress"": ""c2:79:d5:75:e5:09"", ""NetworkID"": ""tmp_default"", ""DriverOpts"": null, ""IPAMConfig"": null, ""Links"": null, ""Aliases"": [ ""tmp-traefik-1"", ""traefik"", ""4cb3ce78cde3"" ] } }  Here we can see that `NetworkID` is the network name and not the id of the network (`podman network inspect tmp_default`): json [ { ""name"": ""tmp_default"", ""id"": ""675bea81a6d49e9f1ae909c58a2269524a9a9c910f91bce12a85b05b882ab5cb"", ""driver"": ""bridge"", ""network_interface"": ""podman1"", ""created"": ""2024-12-28T20:54:28.320995381+01:00"", ""subnets"": [ { ""subnet"": ""10.89.0.0/24"", ""gateway"": ""10.89.0.1"" } ], ""ipv6_enabled"": false, ""internal"": false, ""dns_enabled"": true, ""labels"": { ""com.docker.compose.config-hash"": ""4bd7e5f2ecc11ab238c4eb5931f77d1e2196619bcc8daeeb80428562a7210c37"", ""com.docker.compose.network"": ""default"", ""com.docker.compose.project"": ""tmp"", ""com.docker.compose.version"": ""2.32.1"" }, ""options"": { ""isolate"": ""true"" }, ""ipam_options"": { ""driver"": ""host-local"" }, ""containers"": { ""4cb3ce78cde33f763bdd6abe5b19a5d93f5bb30043c94fa489114a48da5d4290"": { ""name"": ""tmp-traefik-1"", ""interfaces"": { ""eth0"": { ""subnets"": [ { ""ipnet"": ""10.89.0.3/24"", ""gateway"": ""10.89.0.1"" } ], ""mac_address"": ""c2:79:d5:75:e5:09"" } } } } } ]   Steps to reproduce the issue Steps to reproduce the issue 1. `podman run --rm -it --name testing ubuntu:latest sleep infinity` 2. `podman inspect testing --format=""{{ json .NetworkSettings.Networks.podman }}""|jq` 3. Observe that the `NetworkID` is the network name  Describe the results you received I would have expected the actual `NetworkID` to be shown  Describe the results you expected The network name is shown as ID  podman info output yaml host: arch: amd64 buildahVersion: 1.38.0 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.12-3.fc41.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.12, commit: ' cpuUtilization: idlePercent: 97.27 systemPercent: 0.8 userPercent: 1.93 cpus: 16 databaseBackend: sqlite distribution: distribution: fedora variant: workstation version: ""41"" eventLogger: journald freeLocks: 2046 hostname: apollo13 idMappings: gidmap: null uidmap: null kernel: 6.12.4-200.fc41.x86_64 linkmode: dynamic logDriver: journald memFree: 6023499776 memTotal: 32395685888 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.13.1-1.fc41.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.13.1 package: netavark-1.13.1-1.fc41.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.13.1 ociRuntime: name: crun package: crun-1.19.1-1.fc41.x86_64 path: /usr/bin/crun version: |- crun version 1.19.1 commit: 3e32a70c93f5aa5fea69b50256cca7fd4aa23c80 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20241211.g09478d5-1.fc41.x86_64 version: | pasta 0^20241211.g09478d5-1.fc41.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.3.1-1.fc41.x86_64 version: |- slirp4netns version 1.3.1 commit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236 libslirp: 4.8.0 SLIRP_CONFIG_VERSION_MAX: 5 libseccomp: 2.5.5 swapFree: 8589930496 swapTotal: 8589930496 uptime: 11h 16m 31.00s (Approximately 0.46 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 2 paused: 0 running: 2 stopped: 0 graphDriverName: overlay graphOptions: overlay.imagestore: /usr/lib/containers/storage overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 536854134784 graphRootUsed: 403665657856 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 5 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.3.1 Built: 1732147200 BuiltTime: Thu Nov 21 01:00:00 2024 GitCommit: """" GoVersion: go1.23.3 Os: linux OsArch: linux/amd64 Version: 5.3.1   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details _No response_  Additional information _No response_ source-file source-file config-file source-file test-file test-file config-file source-file test-file test-file config-file source-file test-file test-file test-file config-file source-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file test-file config-file source-file test-file test-file test-file test-file test-file test-file test-file test-file",bug,0.9
22616,podman,https://github.com/containers/podman/issues/22616,podman-remote volume rm ambiguous-name: unmarshalling error,"console $ bin/podman-remote volume create myvol1 myvol1 $ bin/podman-remote volume create myvol2 myvol2 $ bin/podman-remote volume rm myv Error: unmarshalling error into &errorhandling.ErrorModel{Because:"""", Message:"""", ResponseCode:0}, data ""{\""cause\"":\""volume already exists\"",\""message\"":\""more than one result for volume name myv: volume already exists\"",\""response\"":500}\n{\""cause\"":\""volume already exists\"",\""message\"":\""more than one result for volume name myv: volume already exists\"",\""response\"":404}\n"": invalid character '{' after top-level value ",source-file | test-file,"podman-remote volume rm ambiguous-name: unmarshalling error console $ bin/podman-remote volume create myvol1 myvol1 $ bin/podman-remote volume create myvol2 myvol2 $ bin/podman-remote volume rm myv Error: unmarshalling error into &errorhandling.ErrorModel{Because:"""", Message:"""", ResponseCode:0}, data ""{\""cause\"":\""volume already exists\"",\""message\"":\""more than one result for volume name myv: volume already exists\"",\""response\"":500}\n{\""cause\"":\""volume already exists\"",\""message\"":\""more than one result for volume name myv: volume already exists\"",\""response\"":404}\n"": invalid character '{' after top-level value  source-file test-file",bug,0.9
13824,podman,https://github.com/containers/podman/issues/13824,podman stats as root broken,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. Install podman on Arch Linux (4.0.3), Centos 8 Stream (4.0.2) or use Fedora Coreos (4.0.2) 2. log in as root 3. `podman stats` **Describe the results you received:** It just prints `Error: Link not found` **Describe the results you expected:** not that **Additional information you deem important (e.g. issue happens only occasionally):** `podman stats` seems to work when run as non-root, tested on Arch Linux, but I get the same error there when run as root. Using the socket API: `curl --unix-socket /run/podman/podman.sock 'http://d/v4.0.0/libpod/containers/stats'` {""Error"":{},""Stats"":null} **Output of `podman version`:** 4.0.3 and 4.0.2 ** podman info -D ** From coreos:  host: arch: amd64 buildahVersion: 1.24.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-2.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: ' cpus: 4 distribution: distribution: fedora variant: coreos version: ""36"" eventLogger: journald hostname: dro-1 idMappings: gidmap: null uidmap: null kernel: 5.17.0-300.fc36.x86_64 linkmode: dynamic logDriver: journald memFree: 14985670656 memTotal: 16773632000 networkBackend: netavark ociRuntime: name: crun package: crun-1.4.3-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.4.3 commit: 61c9600d1335127eba65632731e2d72bc3f0b9e8 spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 40h 43m 52.05s (Approximately 1.67 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 11 paused: 0 running: 11 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 8 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.0.2 Built: 1646319369 BuiltTime: Thu Mar 3 14:56:09 2022 GitCommit: """" GoVersion: go1.18beta2 OsArch: linux/amd64 Version: 4.0.2 ",source-file | test-file | source-file | test-file,"podman stats as root broken <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. Install podman on Arch Linux (4.0.3), Centos 8 Stream (4.0.2) or use Fedora Coreos (4.0.2) 2. log in as root 3. `podman stats` **Describe the results you received:** It just prints `Error: Link not found` **Describe the results you expected:** not that **Additional information you deem important (e.g. issue happens only occasionally):** `podman stats` seems to work when run as non-root, tested on Arch Linux, but I get the same error there when run as root. Using the socket API: `curl --unix-socket /run/podman/podman.sock 'http://d/v4.0.0/libpod/containers/stats'` {""Error"":{},""Stats"":null} **Output of `podman version`:** 4.0.3 and 4.0.2 ** podman info -D ** From coreos:  host: arch: amd64 buildahVersion: 1.24.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-2.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: ' cpus: 4 distribution: distribution: fedora variant: coreos version: ""36"" eventLogger: journald hostname: dro-1 idMappings: gidmap: null uidmap: null kernel: 5.17.0-300.fc36.x86_64 linkmode: dynamic logDriver: journald memFree: 14985670656 memTotal: 16773632000 networkBackend: netavark ociRuntime: name: crun package: crun-1.4.3-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.4.3 commit: 61c9600d1335127eba65632731e2d72bc3f0b9e8 spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 40h 43m 52.05s (Approximately 1.67 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 11 paused: 0 running: 11 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 8 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.0.2 Built: 1646319369 BuiltTime: Thu Mar 3 14:56:09 2022 GitCommit: """" GoVersion: go1.18beta2 OsArch: linux/amd64 Version: 4.0.2  source-file test-file source-file test-file",bug,0.9
17204,podman,https://github.com/containers/podman/issues/17204,APIv2 tests: python something != something-podman,"Seeing this occasionally since Dec 30:  ________________________ ContainerTestCase.test_attach _________________________ self = <python.rest_api.test_v2_0_0_container.ContainerTestCase testMethod=test_attach> def test_attach(self): r = requests.post( self.podman_url + ""/v1.40/containers/create?name=topcontainer"", json={""Cmd"": [""sh"", ""-c"", ""echo podman; sleep 100""], ""Image"": ""alpine:latest""}, ) self.assertEqual(r.status_code, 201, r.text) payload = r.json() r = requests.post( self.podman_url + f""/v1.40/containers/{payload['Id']}/start"" ) self.assertEqual(r.status_code, 204, r.text) r = requests.post( self.podman_url + f""/v1.40/containers/{payload['Id']}/attach?logs=true&stream=false"" ) self.assertIn(r.status_code, (101, 200), r.text) # see the attach format docs, stdout = 1, length = 7, message = podman\n > self.assertEqual(r.content, b""\x01\x00\x00\x00\x00\x00\x00\x07podman\n"", r.text) E AssertionError: b'' != b'\x01\x00\x00\x00\x00\x00\x00\x07podman\n' : test/apiv2/python/rest_api/test_v2_0_0_container.py:155: AssertionError  short test summary info  FAILED test/apiv2/python/rest_api/test_v2_0_0_container.py::ContainerTestCase::test_attach  1 failed, 43 passed in 78.37s (0:01:18)   If I read the URL correctly, this has something to do with `logs`, and ISTR other flakes in podman-remote logs but can't find the issue number. * fedora-37 : APIv2 test on fedora-37 (root) * PR #17165 * [01-19 06:53](https://api.cirrus-ci.com/v1/task/5030036521091072/logs/main.log) * PR #16997 * [01-05 03:10](https://api.cirrus-ci.com/v1/task/4694463075844096/logs/main.log) * PR #16978 * [01-04 03:14](https://api.cirrus-ci.com/v1/task/4749243169112064/logs/main.log) * PR #16810 * [12-30 10:43](https://api.cirrus-ci.com/v1/task/5556295575535616/logs/main.log) * [12-30 10:30](https://api.cirrus-ci.com/v1/task/5214991905718272/logs/main.log)",test-file,"APIv2 tests: python something != something-podman Seeing this occasionally since Dec 30:  ________________________ ContainerTestCase.test_attach _________________________ self = <python.rest_api.test_v2_0_0_container.ContainerTestCase testMethod=test_attach> def test_attach(self): r = requests.post( self.podman_url + ""/v1.40/containers/create?name=topcontainer"", json={""Cmd"": [""sh"", ""-c"", ""echo podman; sleep 100""], ""Image"": ""alpine:latest""}, ) self.assertEqual(r.status_code, 201, r.text) payload = r.json() r = requests.post( self.podman_url + f""/v1.40/containers/{payload['Id']}/start"" ) self.assertEqual(r.status_code, 204, r.text) r = requests.post( self.podman_url + f""/v1.40/containers/{payload['Id']}/attach?logs=true&stream=false"" ) self.assertIn(r.status_code, (101, 200), r.text) # see the attach format docs, stdout = 1, length = 7, message = podman\n > self.assertEqual(r.content, b""\x01\x00\x00\x00\x00\x00\x00\x07podman\n"", r.text) E AssertionError: b'' != b'\x01\x00\x00\x00\x00\x00\x00\x07podman\n' : test/apiv2/python/rest_api/test_v2_0_0_container.py:155: AssertionError  short test summary info  FAILED test/apiv2/python/rest_api/test_v2_0_0_container.py::ContainerTestCase::test_attach  1 failed, 43 passed in 78.37s (0:01:18)   If I read the URL correctly, this has something to do with `logs`, and ISTR other flakes in podman-remote logs but can't find the issue number. * fedora-37 : APIv2 test on fedora-37 (root) * PR #17165 * [01-19 06:53](https://api.cirrus-ci.com/v1/task/5030036521091072/logs/main.log) * PR #16997 * [01-05 03:10](https://api.cirrus-ci.com/v1/task/4694463075844096/logs/main.log) * PR #16978 * [01-04 03:14](https://api.cirrus-ci.com/v1/task/4749243169112064/logs/main.log) * PR #16810 * [12-30 10:43](https://api.cirrus-ci.com/v1/task/5556295575535616/logs/main.log) * [12-30 10:30](https://api.cirrus-ci.com/v1/task/5214991905718272/logs/main.log) test-file",bug,0.9
14674,podman,https://github.com/containers/podman/issues/14674,discrepancy in podman network inspect for netmask (compat API),"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** podman main: The netmask of secondaryIPAddresses/SecondaryIPv6Addresses in container network inspect compat API is always zero. podman v3: Wrong type used for secondaryIPAddresses/SecondaryIPv6Addresses (string instead of network.Address) results in broken container inspect compat API in presence of secondary network interfaces. **Steps to reproduce the issue:** Reproduction in **main** branch 1. create a named network namespace and setup two veth interface  ip netns add test ip netns exec test /bin/bash ip link add enp2s0 type veth peer name eth0 ip addr add 10.0.1.0/24 dev eth0 ip link set eth0 up ip link add enp2s1 type veth peer name eth1 ip addr add 10.0.2.0/24 dev eth1 ip link set eth1 up exit  2. run a container and join the created network namespace  podman run --net ns:/run/netns/test --name test -it alpine /bin/sh  3. start a podman service  podman system service tcp:localhost:20000 --log-level=debug --time=0  4. Check output of podman inspect test  curl -X GET 'http://127.0.0.1:20000/v3.0.0/containers/test/json'| jq  **Describe the results you received:** PrefixLen is **0**  ""NetworkSettings"": { ""Bridge"": """", ""SandboxID"": """", ""HairpinMode"": false, ""LinkLocalIPv6Address"": """", ""LinkLocalIPv6PrefixLen"": 0, ""Ports"": {}, ""SandboxKey"": """", ""SecondaryIPAddresses"": [ { ""Addr"": ""10.0.2.0"", ""PrefixLen"": 0 }  **Describe the results you expected:** PrefixLen should be **24** **Output of `podman version`:**  podman version 4.0.0-dev  **Output of `podman info --debug`:**  host: arch: amd64 buildahVersion: 1.24.1 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - pids cgroupManager: systemd cgroupVersion: v1 conmon: package: conmon-2.0.27+git0+3efab3e71c-r0.core2_64 path: /usr/bin/conmon version: 'conmon version 2.0.28-dev, commit: 3efab3e71c4c29f127cd7b8e8a5a885fc17dec88' cpus: 6 distribution: distribution: mbient version: ""1.0"" eventLogger: journald hostname: qemux86-64 idMappings: gidmap: null uidmap: null kernel: 5.10.30-yocto-standard linkmode: dynamic logDriver: journald memFree: 7883513856 memTotal: 22187085824 networkBackend: cni ociRuntime: name: crun package: crun-0.18+gitf302dd8c02c6fddd2c50d1685d82b7a19aae8afe-r0.core2_64 path: /usr/bin/crun version: |- crun version 0.19.5-f302-dirty commit: f302dd8c02c6fddd2c50d1685d82b7a19aae8afe spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: path: /run/podman/podman.sock security: apparmorEnabled: true capabilities: CAP_AUDIT_WRITE,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_MKNOD,CAP_NET_BIND_SERVICE,CAP_NET_RAW,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: """" selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-0.4.1-r0.core2_64 version: |- slirp4netns version 0.4.1 commit: unknown swapFree: 46133248 swapTotal: 46133248 uptime: 6h 6m 42.11s (Approximately 0.25 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io - registry.fedoraproject.org - quay.io - registry.access.redhat.com - registry.centos.org store: configFile: /etc/containers/storage.conf containerStore: number: 6 paused: 0 running: 3 stopped: 3 graphDriverName: overlay graphOptions: overlay.mountopt: nodev graphRoot: /mnt/systemdata/dynamic-contents/podman graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 2 runRoot: /var/run/containers/storage volumePath: /mnt/systemdata/dynamic-contents/podman/volumes version: APIVersion: 4.0.0-dev Built: 1655756421 BuiltTime: Mon Jun 20 22:20:21 2022 GitCommit: 192dea7d981443a8091596eb91e6c4274b8a6f85 GoVersion: go1.18.3 OsArch: linux/amd64 Version: 4.0.0-dev  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes ",source-file | source-file | source-file | test-file | test-file,"discrepancy in podman network inspect for netmask (compat API) <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** podman main: The netmask of secondaryIPAddresses/SecondaryIPv6Addresses in container network inspect compat API is always zero. podman v3: Wrong type used for secondaryIPAddresses/SecondaryIPv6Addresses (string instead of network.Address) results in broken container inspect compat API in presence of secondary network interfaces. **Steps to reproduce the issue:** Reproduction in **main** branch 1. create a named network namespace and setup two veth interface  ip netns add test ip netns exec test /bin/bash ip link add enp2s0 type veth peer name eth0 ip addr add 10.0.1.0/24 dev eth0 ip link set eth0 up ip link add enp2s1 type veth peer name eth1 ip addr add 10.0.2.0/24 dev eth1 ip link set eth1 up exit  2. run a container and join the created network namespace  podman run --net ns:/run/netns/test --name test -it alpine /bin/sh  3. start a podman service  podman system service tcp:localhost:20000 --log-level=debug --time=0  4. Check output of podman inspect test  curl -X GET 'http://127.0.0.1:20000/v3.0.0/containers/test/json'| jq  **Describe the results you received:** PrefixLen is **0**  ""NetworkSettings"": { ""Bridge"": """", ""SandboxID"": """", ""HairpinMode"": false, ""LinkLocalIPv6Address"": """", ""LinkLocalIPv6PrefixLen"": 0, ""Ports"": {}, ""SandboxKey"": """", ""SecondaryIPAddresses"": [ { ""Addr"": ""10.0.2.0"", ""PrefixLen"": 0 }  **Describe the results you expected:** PrefixLen should be **24** **Output of `podman version`:**  podman version 4.0.0-dev  **Output of `podman info --debug`:**  host: arch: amd64 buildahVersion: 1.24.1 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - pids cgroupManager: systemd cgroupVersion: v1 conmon: package: conmon-2.0.27+git0+3efab3e71c-r0.core2_64 path: /usr/bin/conmon version: 'conmon version 2.0.28-dev, commit: 3efab3e71c4c29f127cd7b8e8a5a885fc17dec88' cpus: 6 distribution: distribution: mbient version: ""1.0"" eventLogger: journald hostname: qemux86-64 idMappings: gidmap: null uidmap: null kernel: 5.10.30-yocto-standard linkmode: dynamic logDriver: journald memFree: 7883513856 memTotal: 22187085824 networkBackend: cni ociRuntime: name: crun package: crun-0.18+gitf302dd8c02c6fddd2c50d1685d82b7a19aae8afe-r0.core2_64 path: /usr/bin/crun version: |- crun version 0.19.5-f302-dirty commit: f302dd8c02c6fddd2c50d1685d82b7a19aae8afe spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: path: /run/podman/podman.sock security: apparmorEnabled: true capabilities: CAP_AUDIT_WRITE,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_MKNOD,CAP_NET_BIND_SERVICE,CAP_NET_RAW,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: """" selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-0.4.1-r0.core2_64 version: |- slirp4netns version 0.4.1 commit: unknown swapFree: 46133248 swapTotal: 46133248 uptime: 6h 6m 42.11s (Approximately 0.25 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io - registry.fedoraproject.org - quay.io - registry.access.redhat.com - registry.centos.org store: configFile: /etc/containers/storage.conf containerStore: number: 6 paused: 0 running: 3 stopped: 3 graphDriverName: overlay graphOptions: overlay.mountopt: nodev graphRoot: /mnt/systemdata/dynamic-contents/podman graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 2 runRoot: /var/run/containers/storage volumePath: /mnt/systemdata/dynamic-contents/podman/volumes version: APIVersion: 4.0.0-dev Built: 1655756421 BuiltTime: Mon Jun 20 22:20:21 2022 GitCommit: 192dea7d981443a8091596eb91e6c4274b8a6f85 GoVersion: go1.18.3 OsArch: linux/amd64 Version: 4.0.0-dev  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes  source-file source-file source-file test-file test-file",bug,0.95
16856,podman,https://github.com/containers/podman/issues/16856,non-tty logs/container attach format broken,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** Podman seems to have a bug in attach format which is described here https://docs.docker.com/engine/api/v1.41/#tag/Container/operation/ContainerAttach The bug only seems to appear for the log items produced *before* the attach command is executed, not after **Steps to reproduce the issue:** 1. Start a container which prints a 4-byte message on stdout each second during 10 seconds 2. Wait for 5 seconds 3. Attach to the container and save the output The same steps in form of a script:  sudo podman run -d --rm --name test alpine sh -c 'for i in $(seq 0 10); do printf ""%03d\n"" $i; sleep 1; done' sleep 5 sudo curl -X POST --unix /run/podman/podman.sock 'http://localhost/containers/test/attach?logs=true&stream=true&stdout=true' --output dump.bin  **Describe the results you received:** Looking at the `dump.bin` produced with the above reproduction procedure, one can clearly see how the `length` field is incorrect (indicates 3 bytes for a 4-byte payload) during the first 5 seconds, and correct (indicates 4 bytes) during the last 5 seconds.  h01:~$ xxd -c12 dump.bin 00000000: 0100 0000 0000 0003 3030 300a 000. 0000000c: 0100 0000 0000 0003 3030 310a 001. 00000018: 0100 0000 0000 0003 3030 320a 002. 00000024: 0100 0000 0000 0003 3030 330a 003. 00000030: 0100 0000 0000 0003 3030 340a 004. 0000003c: 0100 0000 0000 0003 3030 350a 005. 00000048: 0100 0000 0000 0004 3030 360a 006. 00000054: 0100 0000 0000 0004 3030 370a 007. 00000060: 0100 0000 0000 0004 3030 380a 008. 0000006c: 0100 0000 0000 0004 3030 390a 009. 00000078: 0100 0000 0000 0004 3031 300a 010.  **Describe the results you expected:** I expected the `length` to be `4` for all messages. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  $ podman version Client: Podman Engine Version: 4.3.1 API Version: 4.3.1 Go Version: go1.19.4 Built: Tue Dec 13 02:00:33 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  $ podman info host: arch: amd64 buildahVersion: 1.28.0 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.5-r0 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: unknown' cpuUtilization: idlePercent: 99.74 systemPercent: 0.12 userPercent: 0.15 cpus: 1 distribution: distribution: alpine version: 3.17.0 eventLogger: file hostname: h01 idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 uidmap: - container_id: 0 host_id: 1000 size: 1 kernel: 5.15.82-0-virt linkmode: dynamic logDriver: k8s-file memFree: 1847091200 memTotal: 2087096320 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-r0 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /tmp/podman-run-1000/crun spec: 1.0.0 +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: path: /tmp/podman-run-1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-r0 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 0 swapTotal: 0 uptime: 0h 46m 48.00s plugins: authorization: null log: - k8s-file - none - passthrough network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /home/docker/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/docker/.local/share/containers/storage graphRootAllocated: 4226809856 graphRootUsed: 286416896 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /tmp/podman-run-1000/containers volumePath: /home/docker/.local/share/containers/storage/volumes version: APIVersion: 4.3.1 Built: 1670896833 BuiltTime: Tue Dec 13 02:00:33 2022 GitCommit: """" GoVersion: go1.19.4 Os: linux OsArch: linux/amd64 Version: 4.3.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman` or `brew info podman`):**  $ apk info podman podman-4.3.1-r1 description: Simple management tool for pods, containers and images podman-4.3.1-r1 webpage: https://podman.io/ podman-4.3.1-r1 installed size: 39 MiB  **Have you tested with the latest version of Podman and have you checked [the Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md)?** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):**",source-file | test-file | test-file,"non-tty logs/container attach format broken <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** Podman seems to have a bug in attach format which is described here https://docs.docker.com/engine/api/v1.41/#tag/Container/operation/ContainerAttach The bug only seems to appear for the log items produced *before* the attach command is executed, not after **Steps to reproduce the issue:** 1. Start a container which prints a 4-byte message on stdout each second during 10 seconds 2. Wait for 5 seconds 3. Attach to the container and save the output The same steps in form of a script:  sudo podman run -d --rm --name test alpine sh -c 'for i in $(seq 0 10); do printf ""%03d\n"" $i; sleep 1; done' sleep 5 sudo curl -X POST --unix /run/podman/podman.sock 'http://localhost/containers/test/attach?logs=true&stream=true&stdout=true' --output dump.bin  **Describe the results you received:** Looking at the `dump.bin` produced with the above reproduction procedure, one can clearly see how the `length` field is incorrect (indicates 3 bytes for a 4-byte payload) during the first 5 seconds, and correct (indicates 4 bytes) during the last 5 seconds.  h01:~$ xxd -c12 dump.bin 00000000: 0100 0000 0000 0003 3030 300a 000. 0000000c: 0100 0000 0000 0003 3030 310a 001. 00000018: 0100 0000 0000 0003 3030 320a 002. 00000024: 0100 0000 0000 0003 3030 330a 003. 00000030: 0100 0000 0000 0003 3030 340a 004. 0000003c: 0100 0000 0000 0003 3030 350a 005. 00000048: 0100 0000 0000 0004 3030 360a 006. 00000054: 0100 0000 0000 0004 3030 370a 007. 00000060: 0100 0000 0000 0004 3030 380a 008. 0000006c: 0100 0000 0000 0004 3030 390a 009. 00000078: 0100 0000 0000 0004 3031 300a 010.  **Describe the results you expected:** I expected the `length` to be `4` for all messages. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  $ podman version Client: Podman Engine Version: 4.3.1 API Version: 4.3.1 Go Version: go1.19.4 Built: Tue Dec 13 02:00:33 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  $ podman info host: arch: amd64 buildahVersion: 1.28.0 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.5-r0 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: unknown' cpuUtilization: idlePercent: 99.74 systemPercent: 0.12 userPercent: 0.15 cpus: 1 distribution: distribution: alpine version: 3.17.0 eventLogger: file hostname: h01 idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 uidmap: - container_id: 0 host_id: 1000 size: 1 kernel: 5.15.82-0-virt linkmode: dynamic logDriver: k8s-file memFree: 1847091200 memTotal: 2087096320 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-r0 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /tmp/podman-run-1000/crun spec: 1.0.0 +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: path: /tmp/podman-run-1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-r0 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 0 swapTotal: 0 uptime: 0h 46m 48.00s plugins: authorization: null log: - k8s-file - none - passthrough network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /home/docker/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/docker/.local/share/containers/storage graphRootAllocated: 4226809856 graphRootUsed: 286416896 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /tmp/podman-run-1000/containers volumePath: /home/docker/.local/share/containers/storage/volumes version: APIVersion: 4.3.1 Built: 1670896833 BuiltTime: Tue Dec 13 02:00:33 2022 GitCommit: """" GoVersion: go1.19.4 Os: linux OsArch: linux/amd64 Version: 4.3.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman` or `brew info podman`):**  $ apk info podman podman-4.3.1-r1 description: Simple management tool for pods, containers and images podman-4.3.1-r1 webpage: https://podman.io/ podman-4.3.1-r1 installed size: 39 MiB  **Have you tested with the latest version of Podman and have you checked [the Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md)?** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** source-file test-file test-file",bug,0.95
19159,podman,https://github.com/containers/podman/issues/19159,New podman pod remove API error message is less detailed," Issue Description In 4.6.0-rc1 the `libpods/pod` DELETE endpoint's error message changed to: ![image](https://github.com/containers/podman/assets/67428/09887439-128b-42be-92bb-7b9da3b92dcf) From: ![image](https://github.com/containers/podman/assets/67428/8dd91399-3241-451e-8ef2-c7ee13b38979) The new error message does not include anything about it being unable delete running/paused containers.  Steps to reproduce the issue Steps to reproduce the issue  [root@fedora-testing-127-0-0-2-2201 ~]# podman pod ps POD ID NAME STATUS CREATED INFRA ID # OF CONTAINERS 64ec75433156 brave_solomon Running 24 minutes ago c168e97d7e68 2 [root@fedora-testing-127-0-0-2-2201 ~]# curl -X DELETE -s -g --no-buffer --unix-socket /run/podman/podman.sock 'http://localhost/v1.12/libpod/pods/64ec75433156' {""cause"":""removing pod containers"",""message"":""not all containers could be removed from pod 64ec754331564082b0ccbf078268d701ad009a415ad1dfdaca64425cbe8d8821: removing pod containers"",""response"":500}   Describe the results you received See above :)  Describe the results you expected I'm wondering if the error should inform that the container is running/paused so requires force to be removed.  podman info output  Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",source-file | test-file | test-file | source-file | test-file | test-file,"New podman pod remove API error message is less detailed  Issue Description In 4.6.0-rc1 the `libpods/pod` DELETE endpoint's error message changed to: ![image](https://github.com/containers/podman/assets/67428/09887439-128b-42be-92bb-7b9da3b92dcf) From: ![image](https://github.com/containers/podman/assets/67428/8dd91399-3241-451e-8ef2-c7ee13b38979) The new error message does not include anything about it being unable delete running/paused containers.  Steps to reproduce the issue Steps to reproduce the issue  [root@fedora-testing-127-0-0-2-2201 ~]# podman pod ps POD ID NAME STATUS CREATED INFRA ID # OF CONTAINERS 64ec75433156 brave_solomon Running 24 minutes ago c168e97d7e68 2 [root@fedora-testing-127-0-0-2-2201 ~]# curl -X DELETE -s -g --no-buffer --unix-socket /run/podman/podman.sock 'http://localhost/v1.12/libpod/pods/64ec75433156' {""cause"":""removing pod containers"",""message"":""not all containers could be removed from pod 64ec754331564082b0ccbf078268d701ad009a415ad1dfdaca64425cbe8d8821: removing pod containers"",""response"":500}   Describe the results you received See above :)  Describe the results you expected I'm wondering if the error should inform that the container is running/paused so requires force to be removed.  podman info output  Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting source-file test-file test-file source-file test-file test-file",bug,0.9
20013,podman,https://github.com/containers/podman/issues/20013,docker rest api compatibility not honored for error responses.," Issue Description It seems that the compatibility with the api rest of docker is not met for error responses. According to the docker docs, the response should include a field ""message"" but podman has a different schema. This can be reproduced easily running a request to pull an image, for example. As consequence, clients like the official python library for docker doesn't work because the response has a different schema (the message is inside ""errordetail"") and the library expects to have a ""message"" key at the root level as is described in the docs. Docker docs ref: https://docs.docker.com/engine/api/v1.43/#tag/Image/operation/ImageCreate Example of podman response  curl -XPOST ""http://localhost:8888/images/create?fromImage=quay.io/idonotexist/idonotexist:dummy"" | jq { ""progressDetail"": {}, ""errorDetail"": { ""message"": ""initializing source docker://quay.io/idonotexist/idonotexist:dummy: reading manifest dummy in quay.io/idonotexist/idonotexist: unauthorized: access to the requested resource is not authorized"" }, ""error"": ""initializing source docker://quay.io/idonotexist/idonotexist:dummy: reading manifest dummy in quay.io/idonotexist/idonotexist: unauthorized: access to the requested resource is not authorized"" }  Tested with latest podman container 4.6.1  Steps to reproduce the issue  curl -XPOST ""http://localhost:8888/images/create?fromImage=quay.io/idonotexist/idonotexist:dummy""   Describe the results you received The response doesn't meet the spec of docker api response, don't have a ""message"" key in the root level  Describe the results you expected a ""message"" key in the root level with the reason of the error.  podman info output yaml podman v4.6.1   Podman in a container Yes  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_",source-file | test-file,"docker rest api compatibility not honored for error responses.  Issue Description It seems that the compatibility with the api rest of docker is not met for error responses. According to the docker docs, the response should include a field ""message"" but podman has a different schema. This can be reproduced easily running a request to pull an image, for example. As consequence, clients like the official python library for docker doesn't work because the response has a different schema (the message is inside ""errordetail"") and the library expects to have a ""message"" key at the root level as is described in the docs. Docker docs ref: https://docs.docker.com/engine/api/v1.43/#tag/Image/operation/ImageCreate Example of podman response  curl -XPOST ""http://localhost:8888/images/create?fromImage=quay.io/idonotexist/idonotexist:dummy"" | jq { ""progressDetail"": {}, ""errorDetail"": { ""message"": ""initializing source docker://quay.io/idonotexist/idonotexist:dummy: reading manifest dummy in quay.io/idonotexist/idonotexist: unauthorized: access to the requested resource is not authorized"" }, ""error"": ""initializing source docker://quay.io/idonotexist/idonotexist:dummy: reading manifest dummy in quay.io/idonotexist/idonotexist: unauthorized: access to the requested resource is not authorized"" }  Tested with latest podman container 4.6.1  Steps to reproduce the issue  curl -XPOST ""http://localhost:8888/images/create?fromImage=quay.io/idonotexist/idonotexist:dummy""   Describe the results you received The response doesn't meet the spec of docker api response, don't have a ""message"" key in the root level  Describe the results you expected a ""message"" key in the root level with the reason of the error.  podman info output yaml podman v4.6.1   Podman in a container Yes  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_ source-file test-file",bug,0.95
15952,podman,https://github.com/containers/podman/issues/15952,remote: manifest add --annotation failed to add annotations field,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> In help page, podman-remote manifest add --annotation option seems to be supported. However, --annotation option doesn't work.  # podman-remote manifest add --help Add images to a manifest list or image index < snip > Options: --all add all of the list's images if the image is a list --annotation annotation set an annotation for the specified image --arch architecture override the architecture of the specified image --authfile string path of the authentication file. Use REGISTRY_AUTH_FILE environment variable to override --creds [username[:password]] use [username[:password]] for accessing the registry --features features override the features of the specified image --os OS override the OS of the specified image --os-version version override the OS version of the specified image --tls-verify require HTTPS and verify certificates when accessing the registry (default true) --variant Variant override the Variant of the specified image  **Steps to reproduce the issue:**  # podman-remote manifest create test 44f340f882d25deee282e2d9abede61a030a922041d03105b02c22982f1fa8d6 # podman-remote manifest add --annotation foo=bar test quay.io/libpod/testimage:20220615 44f340f882d25deee282e2d9abede61a030a922041d03105b02c22982f1fa8d6  **Describe the results you received:**  # podman-remote manifest inspect test { ""schemaVersion"": 2, ""mediaType"": ""application/vnd.docker.distribution.manifest.list.v2+json"", ""manifests"": [ { ""mediaType"": ""application/vnd.oci.image.manifest.v1+json"", ""size"": 758, ""digest"": ""sha256:a07e678985cd67330fd30a4c8a008cefeb1ca917c25ab0f8468ea17c7b34beb2"", ""platform"": { ""architecture"": ""amd64"", ""os"": ""linux"" } } ] }  **Describe the results you expected:** 1. If --annotation option isn't supported, we need to hide --annotation option from help page. 2. If --annotation option is supported, add annotations field such as below.  # podman-remote manifest inspect test { ""schemaVersion"": 2, ""mediaType"": ""application/vnd.docker.distribution.manifest.list.v2+json"", ""manifests"": [ { ""mediaType"": ""application/vnd.oci.image.manifest.v1+json"", ""size"": 758, ""digest"": ""sha256:a07e678985cd67330fd30a4c8a008cefeb1ca917c25ab0f8468ea17c7b34beb2"", ""platform"": { ""architecture"": ""amd64"", ""os"": ""linux"" }, ""annotations"": { ""foo"": ""bar"" } } ] }  **Additional information you deem important (e.g. issue happens only occasionally):** Succeeded to add annotations field on local enviornment.  # podman manifest inspect test { ""schemaVersion"": 2, ""mediaType"": ""application/vnd.docker.distribution.manifest.list.v2+json"", ""manifests"": [ { ""mediaType"": ""application/vnd.oci.image.manifest.v1+json"", ""size"": 758, ""digest"": ""sha256:a07e678985cd67330fd30a4c8a008cefeb1ca917c25ab0f8468ea17c7b34beb2"", ""platform"": { ""architecture"": ""amd64"", ""os"": ""linux"" }, ""annotations"": { ""foo"": ""bar"" } } ] }  <details> <summary>podman-remote version</summary>  Client: Podman Engine Version: 4.3.0-dev API Version: 4.3.0-dev Go Version: go1.18.3 Git Commit: 98e26278844506e64e311810f97bffcc7efc0b8a Built: Tue Sep 27 13:55:08 2022 OS/Arch: linux/amd64 Server: Podman Engine Version: 4.3.0-dev API Version: 4.3.0-dev Go Version: go1.18.3 Git Commit: 98e26278844506e64e311810f97bffcc7efc0b8a Built: Tue Sep 27 13:54:58 2022 OS/Arch: linux/amd64  </details> <details> <summary>podman-remote info</summary>  host: arch: amd64 buildahVersion: 1.28.0-dev cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-2.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: ' cpuUtilization: idlePercent: 99.98 systemPercent: 0.01 userPercent: 0.01 cpus: 12 distribution: distribution: fedora variant: server version: ""36"" eventLogger: journald hostname: fedora36 idMappings: gidmap: null uidmap: null kernel: 5.18.11-200.fc36.x86_64 linkmode: dynamic logDriver: journald memFree: 6005047296 memTotal: 8326590464 networkBackend: netavark ociRuntime: name: crun package: crun-1.5-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.5 commit: 54ebb8ca8bf7e6ddae2eb919f5b82d1d96863dea spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: unix:run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 8325689344 swapTotal: 8325689344 uptime: 124h 49m 14.00s (Approximately 5.17 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 106285760512 graphRootUsed: 19557855232 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.3.0-dev Built: 1664254498 BuiltTime: Tue Sep 27 13:54:58 2022 GitCommit: 98e26278844506e64e311810f97bffcc7efc0b8a GoVersion: go1.18.3 Os: linux OsArch: linux/amd64 Version: 4.3.0-dev  </details> **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** KVM, fedora36",source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file,"remote: manifest add --annotation failed to add annotations field <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> In help page, podman-remote manifest add --annotation option seems to be supported. However, --annotation option doesn't work.  # podman-remote manifest add --help Add images to a manifest list or image index < snip > Options: --all add all of the list's images if the image is a list --annotation annotation set an annotation for the specified image --arch architecture override the architecture of the specified image --authfile string path of the authentication file. Use REGISTRY_AUTH_FILE environment variable to override --creds [username[:password]] use [username[:password]] for accessing the registry --features features override the features of the specified image --os OS override the OS of the specified image --os-version version override the OS version of the specified image --tls-verify require HTTPS and verify certificates when accessing the registry (default true) --variant Variant override the Variant of the specified image  **Steps to reproduce the issue:**  # podman-remote manifest create test 44f340f882d25deee282e2d9abede61a030a922041d03105b02c22982f1fa8d6 # podman-remote manifest add --annotation foo=bar test quay.io/libpod/testimage:20220615 44f340f882d25deee282e2d9abede61a030a922041d03105b02c22982f1fa8d6  **Describe the results you received:**  # podman-remote manifest inspect test { ""schemaVersion"": 2, ""mediaType"": ""application/vnd.docker.distribution.manifest.list.v2+json"", ""manifests"": [ { ""mediaType"": ""application/vnd.oci.image.manifest.v1+json"", ""size"": 758, ""digest"": ""sha256:a07e678985cd67330fd30a4c8a008cefeb1ca917c25ab0f8468ea17c7b34beb2"", ""platform"": { ""architecture"": ""amd64"", ""os"": ""linux"" } } ] }  **Describe the results you expected:** 1. If --annotation option isn't supported, we need to hide --annotation option from help page. 2. If --annotation option is supported, add annotations field such as below.  # podman-remote manifest inspect test { ""schemaVersion"": 2, ""mediaType"": ""application/vnd.docker.distribution.manifest.list.v2+json"", ""manifests"": [ { ""mediaType"": ""application/vnd.oci.image.manifest.v1+json"", ""size"": 758, ""digest"": ""sha256:a07e678985cd67330fd30a4c8a008cefeb1ca917c25ab0f8468ea17c7b34beb2"", ""platform"": { ""architecture"": ""amd64"", ""os"": ""linux"" }, ""annotations"": { ""foo"": ""bar"" } } ] }  **Additional information you deem important (e.g. issue happens only occasionally):** Succeeded to add annotations field on local enviornment.  # podman manifest inspect test { ""schemaVersion"": 2, ""mediaType"": ""application/vnd.docker.distribution.manifest.list.v2+json"", ""manifests"": [ { ""mediaType"": ""application/vnd.oci.image.manifest.v1+json"", ""size"": 758, ""digest"": ""sha256:a07e678985cd67330fd30a4c8a008cefeb1ca917c25ab0f8468ea17c7b34beb2"", ""platform"": { ""architecture"": ""amd64"", ""os"": ""linux"" }, ""annotations"": { ""foo"": ""bar"" } } ] }  <details> <summary>podman-remote version</summary>  Client: Podman Engine Version: 4.3.0-dev API Version: 4.3.0-dev Go Version: go1.18.3 Git Commit: 98e26278844506e64e311810f97bffcc7efc0b8a Built: Tue Sep 27 13:55:08 2022 OS/Arch: linux/amd64 Server: Podman Engine Version: 4.3.0-dev API Version: 4.3.0-dev Go Version: go1.18.3 Git Commit: 98e26278844506e64e311810f97bffcc7efc0b8a Built: Tue Sep 27 13:54:58 2022 OS/Arch: linux/amd64  </details> <details> <summary>podman-remote info</summary>  host: arch: amd64 buildahVersion: 1.28.0-dev cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-2.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: ' cpuUtilization: idlePercent: 99.98 systemPercent: 0.01 userPercent: 0.01 cpus: 12 distribution: distribution: fedora variant: server version: ""36"" eventLogger: journald hostname: fedora36 idMappings: gidmap: null uidmap: null kernel: 5.18.11-200.fc36.x86_64 linkmode: dynamic logDriver: journald memFree: 6005047296 memTotal: 8326590464 networkBackend: netavark ociRuntime: name: crun package: crun-1.5-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.5 commit: 54ebb8ca8bf7e6ddae2eb919f5b82d1d96863dea spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: unix:run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 8325689344 swapTotal: 8325689344 uptime: 124h 49m 14.00s (Approximately 5.17 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 106285760512 graphRootUsed: 19557855232 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.3.0-dev Built: 1664254498 BuiltTime: Tue Sep 27 13:54:58 2022 GitCommit: 98e26278844506e64e311810f97bffcc7efc0b8a GoVersion: go1.18.3 Os: linux OsArch: linux/amd64 Version: 4.3.0-dev  </details> **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** KVM, fedora36 source-file source-file source-file source-file source-file source-file source-file test-file source-file source-file source-file source-file source-file test-file test-file",bug,0.95
24632,podman,https://github.com/containers/podman/issues/24632,stats API endpoint crashing 500 when container has no cgroup," Issue Description hello! I found a surprising behavior via use of the podman-exporter to report metrics. In creating a pod via `kube play`, the pod is created fine, but the metrics endpoint is failing with 500:  curl -s localhost:5000/v5.2.5/libpod/containers/stats Request Failed(Internal Server Error): cannot run top on container 0c72.. as it did not create a cgroup: this container does not have a cgroup  error logged:  GET /v5.2.5/libpod/containers/stats?all=false&interval=1&stream=false HTTP/1.1"" 500 239 """" ""Go-http-client/1.1""  Here the container `0c72..` is a pause container. I would not expect the endpoint to crash 500 and deny all stat info. Notably podman is rootless mode and in k8s (ie like DnD). Not seeing the problem in native host mode. Unclear if `containers.conf` is the source of problem. Have not had time to extensively rtfm there, but a latest attempt looked like this (largely inherited from /etc version found at https://quay.io/repository/containers/podman)  [containers] netns=""host"" userns=""host"" ipcns=""host"" utsns=""host"" cgroupns=""host"" cgroups=""disabled"" log_driver = ""k8s-file"" volumes = [ ""/proc:/proc"", ] default_sysctls = [] [engine] cgroup_manager = ""cgroupfs"" events_logger=""file"" runtime=""crun""  Version info  $ podman version Client: Podman Engine Version: 5.2.5 API Version: 5.2.5 Go Version: go1.23.2 Built: Fri Oct 18 00:00:00 2024 OS/Arch: linux/amd64 ",source-file | test-file | source-file | test-file,"stats API endpoint crashing 500 when container has no cgroup  Issue Description hello! I found a surprising behavior via use of the podman-exporter to report metrics. In creating a pod via `kube play`, the pod is created fine, but the metrics endpoint is failing with 500:  curl -s localhost:5000/v5.2.5/libpod/containers/stats Request Failed(Internal Server Error): cannot run top on container 0c72.. as it did not create a cgroup: this container does not have a cgroup  error logged:  GET /v5.2.5/libpod/containers/stats?all=false&interval=1&stream=false HTTP/1.1"" 500 239 """" ""Go-http-client/1.1""  Here the container `0c72..` is a pause container. I would not expect the endpoint to crash 500 and deny all stat info. Notably podman is rootless mode and in k8s (ie like DnD). Not seeing the problem in native host mode. Unclear if `containers.conf` is the source of problem. Have not had time to extensively rtfm there, but a latest attempt looked like this (largely inherited from /etc version found at https://quay.io/repository/containers/podman)  [containers] netns=""host"" userns=""host"" ipcns=""host"" utsns=""host"" cgroupns=""host"" cgroups=""disabled"" log_driver = ""k8s-file"" volumes = [ ""/proc:/proc"", ] default_sysctls = [] [engine] cgroup_manager = ""cgroupfs"" events_logger=""file"" runtime=""crun""  Version info  $ podman version Client: Podman Engine Version: 5.2.5 API Version: 5.2.5 Go Version: go1.23.2 Built: Fri Oct 18 00:00:00 2024 OS/Arch: linux/amd64  source-file test-file source-file test-file",bug,0.95
18751,podman,https://github.com/containers/podman/issues/18751,docker API returns 200 on push even if it fails," Issue Description docker API returns 200 on push even if it fails  Steps to reproduce the issue Steps to reproduce the issue 1. try to push a image to a private repository 2. the repository has to reject the push 3. podman logs the rejection as error and retries  Describe the results you received 4. podman confirms the push on the API with status 200  Describe the results you expected 4. podman should error on the API with status range reflecting the push error (e.g. 403)  podman info output yaml host: arch: amd64 buildahVersion: 1.30.0 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.7-2.fc37.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 97.99 systemPercent: 0.4 userPercent: 1.6 cpus: 16 databaseBackend: boltdb distribution: distribution: fedora variant: kde version: ""37"" eventLogger: journald hostname: thinkpad idMappings: gidmap: null uidmap: null kernel: 6.3.0-63.fc39.x86_64 linkmode: dynamic logDriver: journald memFree: 1388929024 memTotal: 29224398848 networkBackend: cni ociRuntime: name: crun package: crun-1.8.4-1.fc37.x86_64 path: /usr/bin/crun version: |- crun version 1.8.4 commit: 5a8fa99a5e41facba2eda4af12fa26313918805b rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 7802028032 swapTotal: 8589930496 uptime: 235h 31m 12.00s (Approximately 9.79 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 201769652224 graphRootUsed: 99855040512 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 4 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.5.0 Built: 1681486976 BuiltTime: Fri Apr 14 17:42:56 2023 GitCommit: """" GoVersion: go1.19.7 Os: linux OsArch: linux/amd64 Version: 4.5.0  Output from `journalctl -u podman`  May 31 11:02:12 thinkpad podman[498506]: @ - - [31/May/2023:11:02:12 +0200] ""POST /build?t=my.private.registry.local%2Fmy.image%2Fmessenger%3A0.0.1&dockerfile=Containerfile&rm=true HTTP/1.1"" 200 1527 """" ""Apache-HttpClient/5.0.3 (Java/17.0.6)"" May 31 11:02:12 thinkpad podman[498506]: time=""2023-05-31T11:02:12+02:00"" level=warning msg=""Failed, retrying in 1s  (1/3). Error: writing blob: initiating layer upload to /v2/my.image/messenger/blobs/uploads/ in my.private.registry.local: unknown: Unable to upload into a virtual repository without default local deployment configured."" May 31 11:02:14 thinkpad podman[498506]: time=""2023-05-31T11:02:14+02:00"" level=warning msg=""Failed, retrying in 1s  (2/3). Error: writing blob: initiating layer upload to /v2/my.image/messenger/blobs/uploads/ in my.private.registry.local: unknown: Unable to upload into a virtual repository without default local deployment configured."" May 31 11:02:15 thinkpad podman[498506]: time=""2023-05-31T11:02:15+02:00"" level=warning msg=""Failed, retrying in 1s  (3/3). Error: writing blob: initiating layer upload to /v2/my.image/messenger/blobs/uploads/ in my.private.registry.local: unknown: Unable to upload into a virtual repository without default local deployment configured."" May 31 11:02:17 thinkpad podman[498506]: @ - - [31/May/2023:11:02:12 +0200] ""POST /images/my.private.registry.local%2Fmy.image%2Fmessenger:0.0.1/push HTTP/1.1"" 200 671 """" ""Apache-HttpClient/5.0.3 (Java/17.0.6)"" May 31 11:02:17 thinkpad podman[498506]: 2023-05-31 11:02:12.320654708 +0200 CEST m=+6.087204790 image push 1d8a277aac651f36677366edc062de55ca26db8821309c6cece21710418f7df0 my.private.registry.local/my.image/messenger:0.0.1  Output from `podman version` (Fedora 37):  [root@thinkpad hargut]# podman version Client: Podman Engine Version: 4.5.0 API Version: 4.5.0 Go Version: go1.19.7 Built: Fri Apr 14 17:42:56 2023 OS/Arch: linux/amd64 [root@thinkpad hargut]# rpm -q podman podman-4.5.0-1.fc37.x86_64 [root@thinkpad hargut]#   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",source-file | test-file | test-file,"docker API returns 200 on push even if it fails  Issue Description docker API returns 200 on push even if it fails  Steps to reproduce the issue Steps to reproduce the issue 1. try to push a image to a private repository 2. the repository has to reject the push 3. podman logs the rejection as error and retries  Describe the results you received 4. podman confirms the push on the API with status 200  Describe the results you expected 4. podman should error on the API with status range reflecting the push error (e.g. 403)  podman info output yaml host: arch: amd64 buildahVersion: 1.30.0 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.7-2.fc37.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 97.99 systemPercent: 0.4 userPercent: 1.6 cpus: 16 databaseBackend: boltdb distribution: distribution: fedora variant: kde version: ""37"" eventLogger: journald hostname: thinkpad idMappings: gidmap: null uidmap: null kernel: 6.3.0-63.fc39.x86_64 linkmode: dynamic logDriver: journald memFree: 1388929024 memTotal: 29224398848 networkBackend: cni ociRuntime: name: crun package: crun-1.8.4-1.fc37.x86_64 path: /usr/bin/crun version: |- crun version 1.8.4 commit: 5a8fa99a5e41facba2eda4af12fa26313918805b rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 7802028032 swapTotal: 8589930496 uptime: 235h 31m 12.00s (Approximately 9.79 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 201769652224 graphRootUsed: 99855040512 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 4 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.5.0 Built: 1681486976 BuiltTime: Fri Apr 14 17:42:56 2023 GitCommit: """" GoVersion: go1.19.7 Os: linux OsArch: linux/amd64 Version: 4.5.0  Output from `journalctl -u podman`  May 31 11:02:12 thinkpad podman[498506]: @ - - [31/May/2023:11:02:12 +0200] ""POST /build?t=my.private.registry.local%2Fmy.image%2Fmessenger%3A0.0.1&dockerfile=Containerfile&rm=true HTTP/1.1"" 200 1527 """" ""Apache-HttpClient/5.0.3 (Java/17.0.6)"" May 31 11:02:12 thinkpad podman[498506]: time=""2023-05-31T11:02:12+02:00"" level=warning msg=""Failed, retrying in 1s  (1/3). Error: writing blob: initiating layer upload to /v2/my.image/messenger/blobs/uploads/ in my.private.registry.local: unknown: Unable to upload into a virtual repository without default local deployment configured."" May 31 11:02:14 thinkpad podman[498506]: time=""2023-05-31T11:02:14+02:00"" level=warning msg=""Failed, retrying in 1s  (2/3). Error: writing blob: initiating layer upload to /v2/my.image/messenger/blobs/uploads/ in my.private.registry.local: unknown: Unable to upload into a virtual repository without default local deployment configured."" May 31 11:02:15 thinkpad podman[498506]: time=""2023-05-31T11:02:15+02:00"" level=warning msg=""Failed, retrying in 1s  (3/3). Error: writing blob: initiating layer upload to /v2/my.image/messenger/blobs/uploads/ in my.private.registry.local: unknown: Unable to upload into a virtual repository without default local deployment configured."" May 31 11:02:17 thinkpad podman[498506]: @ - - [31/May/2023:11:02:12 +0200] ""POST /images/my.private.registry.local%2Fmy.image%2Fmessenger:0.0.1/push HTTP/1.1"" 200 671 """" ""Apache-HttpClient/5.0.3 (Java/17.0.6)"" May 31 11:02:17 thinkpad podman[498506]: 2023-05-31 11:02:12.320654708 +0200 CEST m=+6.087204790 image push 1d8a277aac651f36677366edc062de55ca26db8821309c6cece21710418f7df0 my.private.registry.local/my.image/messenger:0.0.1  Output from `podman version` (Fedora 37):  [root@thinkpad hargut]# podman version Client: Podman Engine Version: 4.5.0 API Version: 4.5.0 Go Version: go1.19.7 Built: Fri Apr 14 17:42:56 2023 OS/Arch: linux/amd64 [root@thinkpad hargut]# rpm -q podman podman-4.5.0-1.fc37.x86_64 [root@thinkpad hargut]#   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting source-file test-file test-file",bug,0.95
17385,podman,https://github.com/containers/podman/issues/17385,[Bug]: network modes `none` and `host` should create entries in `NetworkSettings.Networks`," Issue Description When creating a container with network mode `host` or `none` using docker, the network settings contain entries with those names, respectively. With podman, the networks map is empty.  Steps to reproduce the issue Steps to reproduce the issue 1. run a container with network mode `host` or `none` (`podman run --network=host alpine sleep 1000`) 2. check the container's network settings (`podman inspect $id`)  Describe the results you received No entries in the networks map.  Describe the results you expected Examples from docker json ""Networks"": { ""none"": { ""IPAMConfig"": null, ""Links"": null, ""Aliases"": null, ""NetworkID"": ""840780b61c9fe68c120a0ba5b71158bfab274e9874ff8adba18381c962647e32"", ""EndpointID"": ""21b3555e8d0431ba777bd7e5e0f3e9f38525c5cf98cd9c3ead83d83bcaed15b2"", ""Gateway"": """", ""IPAddress"": """", ""IPPrefixLen"": 0, ""IPv6Gateway"": """", ""GlobalIPv6Address"": """", ""GlobalIPv6PrefixLen"": 0, ""MacAddress"": """", ""DriverOpts"": null } }  json ""Networks"": { ""host"": { ""IPAMConfig"": null, ""Links"": null, ""Aliases"": null, ""NetworkID"": ""7713bb616c867a4104cdd9c1f70f32c7c5b5f1714d5542410386fcb8b33d0ca9"", ""EndpointID"": ""158dea76994f883b58b373a56b4daef5a7b2b4e36cc041a6f09c73832738a2aa"", ""Gateway"": """", ""IPAddress"": """", ""IPPrefixLen"": 0, ""IPv6Gateway"": """", ""GlobalIPv6Address"": """", ""GlobalIPv6PrefixLen"": 0, ""MacAddress"": """", ""DriverOpts"": null } }   podman info output yaml host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.5-1.fc37.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: ' cpuUtilization: idlePercent: 96.34 systemPercent: 0.95 userPercent: 2.71 cpus: 16 distribution: distribution: fedora variant: workstation version: ""37"" eventLogger: journald hostname: honestmistake idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.1.9-200.fc37.x86_64 linkmode: dynamic logDriver: journald memFree: 21072125952 memTotal: 32835108864 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-3.fc37.x86_64 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 8589930496 swapTotal: 8589930496 uptime: 0h 34m 4.00s plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: localhost: Blocked: false Insecure: true Location: localhost MirrorByDigestOnly: false Mirrors: null Prefix: localhost PullFromMirror: """" search: - docker.io store: configFile: /home/jakob/.config/containers/storage.conf containerStore: number: 64 paused: 0 running: 5 stopped: 59 graphDriverName: overlay graphOptions: {} graphRoot: /home/jakob/.local/share/containers/storage graphRootAllocated: 510405902336 graphRootUsed: 358397812736 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 107 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/jakob/.local/share/containers/storage/volumes version: APIVersion: 4.4.0 Built: 1675341170 BuiltTime: Thu Feb 2 13:32:50 2023 GitCommit: """" GoVersion: go1.19.5 Os: linux OsArch: linux/amd64   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information This is required for full docker compatibility, to make the testcontainers-java test suite pass, see https://github.com/testcontainers/testcontainers-java/pull/6158 I'm not sure how big the scope of the required changes for this is, but I'd be happy to contribute a fix if someone can point me in the right direction.",source-file | test-file | test-file,"[Bug]: network modes `none` and `host` should create entries in `NetworkSettings.Networks`  Issue Description When creating a container with network mode `host` or `none` using docker, the network settings contain entries with those names, respectively. With podman, the networks map is empty.  Steps to reproduce the issue Steps to reproduce the issue 1. run a container with network mode `host` or `none` (`podman run --network=host alpine sleep 1000`) 2. check the container's network settings (`podman inspect $id`)  Describe the results you received No entries in the networks map.  Describe the results you expected Examples from docker json ""Networks"": { ""none"": { ""IPAMConfig"": null, ""Links"": null, ""Aliases"": null, ""NetworkID"": ""840780b61c9fe68c120a0ba5b71158bfab274e9874ff8adba18381c962647e32"", ""EndpointID"": ""21b3555e8d0431ba777bd7e5e0f3e9f38525c5cf98cd9c3ead83d83bcaed15b2"", ""Gateway"": """", ""IPAddress"": """", ""IPPrefixLen"": 0, ""IPv6Gateway"": """", ""GlobalIPv6Address"": """", ""GlobalIPv6PrefixLen"": 0, ""MacAddress"": """", ""DriverOpts"": null } }  json ""Networks"": { ""host"": { ""IPAMConfig"": null, ""Links"": null, ""Aliases"": null, ""NetworkID"": ""7713bb616c867a4104cdd9c1f70f32c7c5b5f1714d5542410386fcb8b33d0ca9"", ""EndpointID"": ""158dea76994f883b58b373a56b4daef5a7b2b4e36cc041a6f09c73832738a2aa"", ""Gateway"": """", ""IPAddress"": """", ""IPPrefixLen"": 0, ""IPv6Gateway"": """", ""GlobalIPv6Address"": """", ""GlobalIPv6PrefixLen"": 0, ""MacAddress"": """", ""DriverOpts"": null } }   podman info output yaml host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.5-1.fc37.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: ' cpuUtilization: idlePercent: 96.34 systemPercent: 0.95 userPercent: 2.71 cpus: 16 distribution: distribution: fedora variant: workstation version: ""37"" eventLogger: journald hostname: honestmistake idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.1.9-200.fc37.x86_64 linkmode: dynamic logDriver: journald memFree: 21072125952 memTotal: 32835108864 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-3.fc37.x86_64 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 8589930496 swapTotal: 8589930496 uptime: 0h 34m 4.00s plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: localhost: Blocked: false Insecure: true Location: localhost MirrorByDigestOnly: false Mirrors: null Prefix: localhost PullFromMirror: """" search: - docker.io store: configFile: /home/jakob/.config/containers/storage.conf containerStore: number: 64 paused: 0 running: 5 stopped: 59 graphDriverName: overlay graphOptions: {} graphRoot: /home/jakob/.local/share/containers/storage graphRootAllocated: 510405902336 graphRootUsed: 358397812736 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 107 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/jakob/.local/share/containers/storage/volumes version: APIVersion: 4.4.0 Built: 1675341170 BuiltTime: Thu Feb 2 13:32:50 2023 GitCommit: """" GoVersion: go1.19.5 Os: linux OsArch: linux/amd64   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information This is required for full docker compatibility, to make the testcontainers-java test suite pass, see https://github.com/testcontainers/testcontainers-java/pull/6158 I'm not sure how big the scope of the required changes for this is, but I'd be happy to contribute a fix if someone can point me in the right direction. source-file test-file test-file",bug,0.95
22986,podman,https://github.com/containers/podman/issues/22986,remote: pod top -eo invalid: unmarshalling error,"console $ bin/podman-remote run -d --pod new:foo quay.io/libpod/testimage:20240123 top b829985a6ffa9e8aa35114e48e44e1814469ddf8a801bc77bc3141d7c1065e47 $ bin/podman-remote pod top foo -eo bar Error: unmarshalling into &handlers.PodTopOKBody{ContainerTopOKBody:container.ContainerTopOKBody{Processes:[][]string(nil), Titles:[]string(nil)}}, data """": unexpected end of JSON input ",source-file | test-file | source-file | test-file,"remote: pod top -eo invalid: unmarshalling error console $ bin/podman-remote run -d --pod new:foo quay.io/libpod/testimage:20240123 top b829985a6ffa9e8aa35114e48e44e1814469ddf8a801bc77bc3141d7c1065e47 $ bin/podman-remote pod top foo -eo bar Error: unmarshalling into &handlers.PodTopOKBody{ContainerTopOKBody:container.ContainerTopOKBody{Processes:[][]string(nil), Titles:[]string(nil)}}, data """": unexpected end of JSON input  source-file test-file source-file test-file",bug,0.95
14676,podman,https://github.com/containers/podman/issues/14676,Wrong memory limit stats from podman's remote stats API," Description `podman` remote stats API reports wrong ""memory_limit"" stats for memory-limited container when the container is launched with `crun`. I am not sure whether the issue comes from `podman` or `crun`, but decided to report here because the issue is gone if I change the runtime to `runc`.  How to reproduce 1. Run **rootful** podman API daemon  podman system service -t 0 tcp:127.0.0.1:12345  2. Launch a container with latest `crun` runtime with memory limit  podman run --rm -it -m 512m --runtime=/crun-1.4.5-linux-amd64 --name test busybox  3. Check the output from `podman`'s remote stats API  curl http://127.0.0.1:12345/containers/test/stats?stream=false  4. Confirm that `memory_stats.limit` is wrongly reported.  { ""read"": ""2022-06-21T05:27:32.81008054Z"", ""preread"": ""0001-01-01T00:00:00Z"", ""pids_stats"": , ""blkio_stats"": , ""num_procs"": , ""storage_stats"": , ""cpu_stats"": ,, ""precpu_stats"": , ""memory_stats"": { ""usage"": 589824, ""max_usage"": 18446744073709551615, ""limit"": 18446744073709551615 }, ""name"": ""test"", ""Id"": , ""networks"":  }  5. Repeat the same procedure with `runc` runtime  podman run --rm -it -m 512m --runtime=/home/ubuntu/runc-1.1.3.amd64 --name test busybox  6. Confirm that `memory_stats.limit` is correctly reported.  { ""read"": ""2022-06-21T05:33:54.710659699Z"", ""preread"": ""0001-01-01T00:00:00Z"", ""pids_stats"": , ""blkio_stats"": , ""num_procs"": , ""storage_stats"": , ""cpu_stats"": ,, ""precpu_stats"": , ""memory_stats"": { ""usage"": 335872, ""max_usage"": 536870912, ""limit"": 536870912 }, ""name"": ""test"", ""Id"": , ""networks"":  }   Environment  root@machine:/home/ubuntu# podman info host: arch: amd64 buildahVersion: 1.23.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: 'conmon: /usr/bin/conmon' path: /usr/bin/conmon version: 'conmon version 2.0.25, commit: unknown' cpus: 16 distribution: codename: jammy distribution: ubuntu version: ""22.04"" eventLogger: journald hostname: {maksed} idMappings: gidmap: null uidmap: null kernel: 5.15.0-1004-aws linkmode: dynamic logDriver: journald memFree: 1502658560 memTotal: 67403698176 ociRuntime: name: crun package: 'crun: /usr/bin/crun' path: /usr/bin/crun version: |- crun version 0.17 commit: 0e9229ae34caaebcb86f1fde18de3acaf18c6d9a spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: true capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: 'slirp4netns: /usr/bin/slirp4netns' version: |- slirp4netns version 1.0.1 commit: 6a7b16babc95b6a3056b33fb45b74a6f62262dd4 libslirp: 4.6.1 swapFree: 0 swapTotal: 0 uptime: {maksed} plugins: log: - k8s-file - none - journald network: - bridge - macvlan volume: - local registries: {} store: configFile: /etc/containers/storage.conf containerStore: number: {maksed} paused: {maksed} running: {maksed} stopped: {maksed} graphDriverName: overlay graphOptions: {} graphRoot: /var/lib/containers/storage graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageStore: number: {maksed} runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 3.4.4 Built: 0 BuiltTime: Thu Jan 1 00:00:00 1970 GitCommit: """" GoVersion: go1.17.3 OsArch: linux/amd64 Version: 3.4.4 ",source-file | test-file,"Wrong memory limit stats from podman's remote stats API  Description `podman` remote stats API reports wrong ""memory_limit"" stats for memory-limited container when the container is launched with `crun`. I am not sure whether the issue comes from `podman` or `crun`, but decided to report here because the issue is gone if I change the runtime to `runc`.  How to reproduce 1. Run **rootful** podman API daemon  podman system service -t 0 tcp:127.0.0.1:12345  2. Launch a container with latest `crun` runtime with memory limit  podman run --rm -it -m 512m --runtime=/crun-1.4.5-linux-amd64 --name test busybox  3. Check the output from `podman`'s remote stats API  curl http://127.0.0.1:12345/containers/test/stats?stream=false  4. Confirm that `memory_stats.limit` is wrongly reported.  { ""read"": ""2022-06-21T05:27:32.81008054Z"", ""preread"": ""0001-01-01T00:00:00Z"", ""pids_stats"": , ""blkio_stats"": , ""num_procs"": , ""storage_stats"": , ""cpu_stats"": ,, ""precpu_stats"": , ""memory_stats"": { ""usage"": 589824, ""max_usage"": 18446744073709551615, ""limit"": 18446744073709551615 }, ""name"": ""test"", ""Id"": , ""networks"":  }  5. Repeat the same procedure with `runc` runtime  podman run --rm -it -m 512m --runtime=/home/ubuntu/runc-1.1.3.amd64 --name test busybox  6. Confirm that `memory_stats.limit` is correctly reported.  { ""read"": ""2022-06-21T05:33:54.710659699Z"", ""preread"": ""0001-01-01T00:00:00Z"", ""pids_stats"": , ""blkio_stats"": , ""num_procs"": , ""storage_stats"": , ""cpu_stats"": ,, ""precpu_stats"": , ""memory_stats"": { ""usage"": 335872, ""max_usage"": 536870912, ""limit"": 536870912 }, ""name"": ""test"", ""Id"": , ""networks"":  }   Environment  root@machine:/home/ubuntu# podman info host: arch: amd64 buildahVersion: 1.23.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: 'conmon: /usr/bin/conmon' path: /usr/bin/conmon version: 'conmon version 2.0.25, commit: unknown' cpus: 16 distribution: codename: jammy distribution: ubuntu version: ""22.04"" eventLogger: journald hostname: {maksed} idMappings: gidmap: null uidmap: null kernel: 5.15.0-1004-aws linkmode: dynamic logDriver: journald memFree: 1502658560 memTotal: 67403698176 ociRuntime: name: crun package: 'crun: /usr/bin/crun' path: /usr/bin/crun version: |- crun version 0.17 commit: 0e9229ae34caaebcb86f1fde18de3acaf18c6d9a spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: true capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: 'slirp4netns: /usr/bin/slirp4netns' version: |- slirp4netns version 1.0.1 commit: 6a7b16babc95b6a3056b33fb45b74a6f62262dd4 libslirp: 4.6.1 swapFree: 0 swapTotal: 0 uptime: {maksed} plugins: log: - k8s-file - none - journald network: - bridge - macvlan volume: - local registries: {} store: configFile: /etc/containers/storage.conf containerStore: number: {maksed} paused: {maksed} running: {maksed} stopped: {maksed} graphDriverName: overlay graphOptions: {} graphRoot: /var/lib/containers/storage graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageStore: number: {maksed} runRoot: /run/containers/storage volumePath: /var/lib/containers/storage/volumes version: APIVersion: 3.4.4 Built: 0 BuiltTime: Thu Jan 1 00:00:00 1970 GitCommit: """" GoVersion: go1.17.3 OsArch: linux/amd64 Version: 3.4.4  source-file test-file",bug,0.95
17778,podman,https://github.com/containers/podman/issues/17778,Build image `pull` field type mismatch with Docker," Issue Description Docker accepts any string value for the `pull` field to indicate the image should be pulled:https://docs.docker.com/engine/api/v1.42/#tag/Image/operation/ImageBuild Podman expects the value to equal either boolean `true` or string `true`: https://docs.podman.io/en/latest/_static/api.html?version=v4.4#tag/images/operation/ImageBuildLibpod Applications that align with Docker's spec are unable to switch to Podman as building an image with this field fails.  Steps to reproduce the issue 1. POST to the create image endpoint with `pull` set to a string, such as `always`.  Describe the results you received No response stream received  Describe the results you expected Expected a response stream  podman info output yaml host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon_2:2.1.7-0debian9999+obs15.6_amd64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 87.46 systemPercent: 5.69 userPercent: 6.84 cpus: 2 distribution: codename: jammy distribution: ubuntu version: ""22.04"" eventLogger: journald hostname: fv-az646-90 idMappings: gidmap: - container_id: 0 host_id: 123 size: 1 - container_id: 1 host_id: 165536 size: 65536 uidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 165536 size: 65536 kernel: 5.15.0-1034-azure linkmode: dynamic logDriver: journald memFree: 4857856000 memTotal: 7281278976 networkBackend: netavark ociRuntime: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/runner/.local/share/containers/storage graphRootAllocated: 89297309696 graphRootUsed: 58336636928 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /run/user/1001/containers transientStore: false volumePath: /home/runner/.local/share/containers/storage/volumes version: APIVersion: 4.4.2 Built: 0 BuiltTime: Thu Jan 1 00:00:00 1970 GitCommit: """" GoVersion: go1.19.6 Os: linux OsArch: linux/amd64 Version: 4.4.2   Podman in a container Yes  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details `DOCKER_HOST=unix:run/user/$(id -u)/podman/podman.sock`  Additional information _No response_",source-file | source-file | test-file | source-file | source-file | test-file,"Build image `pull` field type mismatch with Docker  Issue Description Docker accepts any string value for the `pull` field to indicate the image should be pulled:https://docs.docker.com/engine/api/v1.42/#tag/Image/operation/ImageBuild Podman expects the value to equal either boolean `true` or string `true`: https://docs.podman.io/en/latest/_static/api.html?version=v4.4#tag/images/operation/ImageBuildLibpod Applications that align with Docker's spec are unable to switch to Podman as building an image with this field fails.  Steps to reproduce the issue 1. POST to the create image endpoint with `pull` set to a string, such as `always`.  Describe the results you received No response stream received  Describe the results you expected Expected a response stream  podman info output yaml host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon_2:2.1.7-0debian9999+obs15.6_amd64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 87.46 systemPercent: 5.69 userPercent: 6.84 cpus: 2 distribution: codename: jammy distribution: ubuntu version: ""22.04"" eventLogger: journald hostname: fv-az646-90 idMappings: gidmap: - container_id: 0 host_id: 123 size: 1 - container_id: 1 host_id: 165536 size: 65536 uidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 165536 size: 65536 kernel: 5.15.0-1034-azure linkmode: dynamic logDriver: journald memFree: 4857856000 memTotal: 7281278976 networkBackend: netavark ociRuntime: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/runner/.local/share/containers/storage graphRootAllocated: 89297309696 graphRootUsed: 58336636928 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /run/user/1001/containers transientStore: false volumePath: /home/runner/.local/share/containers/storage/volumes version: APIVersion: 4.4.2 Built: 0 BuiltTime: Thu Jan 1 00:00:00 1970 GitCommit: """" GoVersion: go1.19.6 Os: linux OsArch: linux/amd64 Version: 4.4.2   Podman in a container Yes  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details `DOCKER_HOST=unix:run/user/$(id -u)/podman/podman.sock`  Additional information _No response_ source-file source-file test-file source-file source-file test-file",bug,0.95
22071,podman,https://github.com/containers/podman/issues/22071,Unable to build multi-arch via Podman CURL / API.," Issue Description When trying to pass in `--platform` equivalent within the API, I am getting this parsing error:  {""cause"":""invalid argument"",""message"":""failed to parse query parameter 'platform': \""linux/arm64,linux/amd64\"": invalid platform syntax for --platform=\""linux/arm64,linux/amd64\"": \""arm64,linux\"" is an invalid component of \""linux/arm64,linux/amd64\"": platform specifier component must match \""^[A-Za-z0-9_-]+$\"": invalid argument"",""response"":400}   Steps to reproduce the issue Steps to reproduce the issue 1. Package any Containerfile (`tar -czf context.tar.gz -C dir .`) 2. Use the below CURL command: sh curl --unix-socket ~/.local/share/containers/podman/machine/applehv/podman.sock -X POST \ -H ""Content-Type: application/tar"" \ -H ""Content-Encoding: gzip"" \ --data-binary ""@context.tar.gz"" \ ""http://d/v4.0.0/libpod/build?platform=linux/arm64,linux/amd64&t=quay.io/mytestcontainer"" {""cause"":""invalid argument"",""message"":""failed to parse query parameter 'platform': \""linux/arm64,linux/amd64\"": invalid platform syntax for --platform=\""linux/arm64,linux/amd64\"": \""arm64,linux\"" is an invalid component of \""linux/arm64,linux/amd64\"": platform specifier component must match \""^[A-Za-z0-9_-]+$\"": invalid argument"",""response"":400}   Describe the results you received Getting a platform error (see above).  Describe the results you expected Passing, similar to:  podman build --platform=""linux/arm64,linux/amd64"" -t mytestcontainer .   podman info output yaml  podman info host: arch: arm64 buildahVersion: 1.34.1-dev cgroupControllers: - cpuset - cpu - io - memory - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.10-1.fc39.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.10, commit: ' cpuUtilization: idlePercent: 98.84 systemPercent: 0.65 userPercent: 0.51 cpus: 5 databaseBackend: sqlite distribution: distribution: fedora variant: coreos version: ""39"" eventLogger: journald freeLocks: 2020 hostname: localhost.localdomain idMappings: gidmap: null uidmap: null kernel: 6.7.5-200.fc39.aarch64 linkmode: dynamic logDriver: journald memFree: 510242816 memTotal: 2047860736 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.10.0-1.20240229100444279141.main.16.g03ce519.fc39.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.11.0-dev package: netavark-1.10.1-1.20240229113356745230.main.40.g773fd54.fc39.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.11.0-dev ociRuntime: name: crun package: crun-1.14.3-1.20240229113428746398.main.10.g31aab34.fc39.aarch64 path: /usr/bin/crun version: |- crun version UNKNOWN commit: aea8fc0fc7d0aabdbcfd1462d7bf6ea0d1e5215b rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20240220.g1e6f92b-1.fc39.aarch64 version: | pasta 0^20240220.g1e6f92b-1.fc39.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-1.fc39.aarch64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 0h 32m 30.00s variant: v8 plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 2 paused: 0 running: 0 stopped: 2 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 106769133568 graphRootUsed: 21349634048 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 10 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.0.0-dev-96f9d0867 Built: 1709164800 BuiltTime: Wed Feb 28 19:00:00 2024 GitCommit: """" GoVersion: go1.21.7 Os: linux OsArch: linux/arm64 Version: 5.0.0-dev-96f9d0867    Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",source-file | source-file | test-file,"Unable to build multi-arch via Podman CURL / API.  Issue Description When trying to pass in `--platform` equivalent within the API, I am getting this parsing error:  {""cause"":""invalid argument"",""message"":""failed to parse query parameter 'platform': \""linux/arm64,linux/amd64\"": invalid platform syntax for --platform=\""linux/arm64,linux/amd64\"": \""arm64,linux\"" is an invalid component of \""linux/arm64,linux/amd64\"": platform specifier component must match \""^[A-Za-z0-9_-]+$\"": invalid argument"",""response"":400}   Steps to reproduce the issue Steps to reproduce the issue 1. Package any Containerfile (`tar -czf context.tar.gz -C dir .`) 2. Use the below CURL command: sh curl --unix-socket ~/.local/share/containers/podman/machine/applehv/podman.sock -X POST \ -H ""Content-Type: application/tar"" \ -H ""Content-Encoding: gzip"" \ --data-binary ""@context.tar.gz"" \ ""http://d/v4.0.0/libpod/build?platform=linux/arm64,linux/amd64&t=quay.io/mytestcontainer"" {""cause"":""invalid argument"",""message"":""failed to parse query parameter 'platform': \""linux/arm64,linux/amd64\"": invalid platform syntax for --platform=\""linux/arm64,linux/amd64\"": \""arm64,linux\"" is an invalid component of \""linux/arm64,linux/amd64\"": platform specifier component must match \""^[A-Za-z0-9_-]+$\"": invalid argument"",""response"":400}   Describe the results you received Getting a platform error (see above).  Describe the results you expected Passing, similar to:  podman build --platform=""linux/arm64,linux/amd64"" -t mytestcontainer .   podman info output yaml  podman info host: arch: arm64 buildahVersion: 1.34.1-dev cgroupControllers: - cpuset - cpu - io - memory - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.10-1.fc39.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.10, commit: ' cpuUtilization: idlePercent: 98.84 systemPercent: 0.65 userPercent: 0.51 cpus: 5 databaseBackend: sqlite distribution: distribution: fedora variant: coreos version: ""39"" eventLogger: journald freeLocks: 2020 hostname: localhost.localdomain idMappings: gidmap: null uidmap: null kernel: 6.7.5-200.fc39.aarch64 linkmode: dynamic logDriver: journald memFree: 510242816 memTotal: 2047860736 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.10.0-1.20240229100444279141.main.16.g03ce519.fc39.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.11.0-dev package: netavark-1.10.1-1.20240229113356745230.main.40.g773fd54.fc39.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.11.0-dev ociRuntime: name: crun package: crun-1.14.3-1.20240229113428746398.main.10.g31aab34.fc39.aarch64 path: /usr/bin/crun version: |- crun version UNKNOWN commit: aea8fc0fc7d0aabdbcfd1462d7bf6ea0d1e5215b rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20240220.g1e6f92b-1.fc39.aarch64 version: | pasta 0^20240220.g1e6f92b-1.fc39.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-1.fc39.aarch64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 0h 32m 30.00s variant: v8 plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 2 paused: 0 running: 0 stopped: 2 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 106769133568 graphRootUsed: 21349634048 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 10 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.0.0-dev-96f9d0867 Built: 1709164800 BuiltTime: Wed Feb 28 19:00:00 2024 GitCommit: """" GoVersion: go1.21.7 Os: linux OsArch: linux/arm64 Version: 5.0.0-dev-96f9d0867    Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting source-file source-file test-file",bug,0.95
18041,podman,https://github.com/containers/podman/issues/18041,"[apiv2] play/kube test: name is in use, pod already exists","Two slightly different variants. Possibly a race in the test itself:  not ok 1202 [80-kube] POST libpod/play/kube : status # expected: 200 # actual: 500 # response: {""cause"":""pod already exists"", ""message"":""playing YAML file: adding pod to state: name \""peacefulgoldberg-pod\"" is in use: pod already exists"", ""response"":500}  and  # response: {""cause"":""pod already exists"", ""message"":""playing YAML file: encountered while bringing up pod modestfeistel-pod-deployment-pod: adding pod to state: name \""modestfeistel-pod-deployment-pod\"" is in use: pod already exists"", ""response"":500}   [APIv2] [80-kube] POST libpod/play/kube : status * fedora-37 : APIv2 test on fedora-37 (rootless) * PR #17993 * [04-03 14:43](https://api.cirrus-ci.com/v1/task/6489412033314816/logs/main.log#t--01209) * PR #17925 * [03-25 06:39](https://api.cirrus-ci.com/v1/task/5563134824415232/logs/main.log#t--01202)",test-file | source-file | source-file | test-file,"[apiv2] play/kube test: name is in use, pod already exists Two slightly different variants. Possibly a race in the test itself:  not ok 1202 [80-kube] POST libpod/play/kube : status # expected: 200 # actual: 500 # response: {""cause"":""pod already exists"", ""message"":""playing YAML file: adding pod to state: name \""peacefulgoldberg-pod\"" is in use: pod already exists"", ""response"":500}  and  # response: {""cause"":""pod already exists"", ""message"":""playing YAML file: encountered while bringing up pod modestfeistel-pod-deployment-pod: adding pod to state: name \""modestfeistel-pod-deployment-pod\"" is in use: pod already exists"", ""response"":500}   [APIv2] [80-kube] POST libpod/play/kube : status * fedora-37 : APIv2 test on fedora-37 (rootless) * PR #17993 * [04-03 14:43](https://api.cirrus-ci.com/v1/task/6489412033314816/logs/main.log#t--01209) * PR #17925 * [03-25 06:39](https://api.cirrus-ci.com/v1/task/5563134824415232/logs/main.log#t--01202) test-file source-file source-file test-file",bug,0.9
14863,podman,https://github.com/containers/podman/issues/14863,Cirrus: new API clobbers past (flake) runs,"The new logformatter URL mechanism, after #14608, has a serious flaw: flake URLs are lost. Reason: the new URLs include the **BuildID**, and **TaskName**, but not the **TaskID**. TaskID is the crucial one for linking to a flake log. Without TaskID, we have something like `/build/12345/int this that`, but the `int this that` on a successful run clobbers the one of the flaked run. @cevich let's talk about this upon your return please.",other-file | other-file,"Cirrus: new API clobbers past (flake) runs The new logformatter URL mechanism, after #14608, has a serious flaw: flake URLs are lost. Reason: the new URLs include the **BuildID**, and **TaskName**, but not the **TaskID**. TaskID is the crucial one for linking to a flake log. Without TaskID, we have something like `/build/12345/int this that`, but the `int this that` on a successful run clobbers the one of the flaked run. @cevich let's talk about this upon your return please. other-file other-file",bug,0.85
18618,podman,https://github.com/containers/podman/issues/18618,"podman events --filter doesn't work for ""volume"" as a key"," Issue Description The command `podman events --filter volume=volume-name` does not print any events  Steps to reproduce the issue Steps to reproduce the issue Open two terminals: 1. podman events --filter volume=volume-name 2. podman volume create volume-name  Describe the results you received No event was created  Describe the results you expected An event should have been printed  podman info output yaml $ podman version Client: Podman Engine Version: 4.5.0 API Version: 4.5.0 Go Version: go1.20.2 Built: Fri Apr 14 21:12:22 2023 OS/Arch: linux/amd64   $ podman info host: arch: amd64 buildahVersion: 1.30.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.7-2.fc38.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 94.82 systemPercent: 1.74 userPercent: 3.44 cpus: 20 databaseBackend: boltdb distribution: distribution: fedora variant: workstation version: ""38"" eventLogger: journald hostname: fedora idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 kernel: 6.2.14-300.fc38.x86_64 linkmode: dynamic logDriver: journald memFree: 48852434944 memTotal: 67070021632 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.4-1.fc38.x86_64 path: /usr/bin/crun version: |- crun version 1.8.4 commit: 5a8fa99a5e41facba2eda4af12fa26313918805b rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-12.fc38.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 8589930496 swapTotal: 8589930496 uptime: 27h 41m 11.00s (Approximately 1.12 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /home/gvyas/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/gvyas/.local/share/containers/storage graphRootAllocated: 523214258176 graphRootUsed: 20388020224 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 53 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/gvyas/.local/share/containers/storage/volumes version: APIVersion: 4.5.0 Built: 1681486942 BuiltTime: Fri Apr 14 21:12:22 2023 GitCommit: """" GoVersion: go1.20.2 Os: linux OsArch: linux/amd64 Version: 4.5.0    Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_",source-file | test-file | test-file | test-file,"podman events --filter doesn't work for ""volume"" as a key  Issue Description The command `podman events --filter volume=volume-name` does not print any events  Steps to reproduce the issue Steps to reproduce the issue Open two terminals: 1. podman events --filter volume=volume-name 2. podman volume create volume-name  Describe the results you received No event was created  Describe the results you expected An event should have been printed  podman info output yaml $ podman version Client: Podman Engine Version: 4.5.0 API Version: 4.5.0 Go Version: go1.20.2 Built: Fri Apr 14 21:12:22 2023 OS/Arch: linux/amd64   $ podman info host: arch: amd64 buildahVersion: 1.30.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.7-2.fc38.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 94.82 systemPercent: 1.74 userPercent: 3.44 cpus: 20 databaseBackend: boltdb distribution: distribution: fedora variant: workstation version: ""38"" eventLogger: journald hostname: fedora idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 kernel: 6.2.14-300.fc38.x86_64 linkmode: dynamic logDriver: journald memFree: 48852434944 memTotal: 67070021632 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.4-1.fc38.x86_64 path: /usr/bin/crun version: |- crun version 1.8.4 commit: 5a8fa99a5e41facba2eda4af12fa26313918805b rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-12.fc38.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 8589930496 swapTotal: 8589930496 uptime: 27h 41m 11.00s (Approximately 1.12 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /home/gvyas/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/gvyas/.local/share/containers/storage graphRootAllocated: 523214258176 graphRootUsed: 20388020224 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 53 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/gvyas/.local/share/containers/storage/volumes version: APIVersion: 4.5.0 Built: 1681486942 BuiltTime: Fri Apr 14 21:12:22 2023 GitCommit: """" GoVersion: go1.20.2 Os: linux OsArch: linux/amd64 Version: 4.5.0    Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_ source-file test-file test-file test-file",bug,0.9
17869,podman,https://github.com/containers/podman/issues/17869,"Stats response has `""Id""` field instead of `""id""`"," Issue Description The stats API JSON response is inconsistent with Docker, because it incorrectly capitalises the `""id""` field. I discovered this when using the Rust crate [Bollard](https://github.com/fussybeaver/bollard), which reported that the `""id""` field was missing.  Steps to reproduce the issue 1. sh curl --unix-socket /var/run/docker.sock ""http://d/containers/$SOME_CONTAINER_ID/stats"" # {,""id"":"""",}  2. sh curl --unix-socket /run/user/1000/podman/podman.sock ""http://d/containers/$SOME_CONTAINER_ID/stats"" # {,""Id"":"""",}   Describe the results you received JSON response contains `""Id""` field, but no `""id""`.  Describe the results you expected JSON response should contain `""id""` field.  podman info output yaml host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: /usr/bin/conmon is owned by conmon 1:2.1.7-1 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: f633919178f6c8ee4fb41b848a056ec33f8d707d' cpuUtilization: idlePercent: 79.35 systemPercent: 5.16 userPercent: 15.48 cpus: 8 distribution: distribution: arch version: unknown eventLogger: journald hostname: REDACTED idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.2.6-arch1-1 linkmode: dynamic logDriver: journald memFree: 813850624 memTotal: 24844115968 networkBackend: netavark ociRuntime: name: crun package: /usr/bin/crun is owned by crun 1.8.1-1 path: /usr/bin/crun version: |- crun version 1.8.1 commit: f8a096be060b22ccd3d5f3ebe44108517fbf6c30 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: /usr/bin/slirp4netns is owned by slirp4netns 1.2.0-1 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 34215489536 swapTotal: 34359734272 uptime: 140h 28m 48.00s (Approximately 5.83 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: {} store: configFile: /home/REDACTED/.config/containers/storage.conf containerStore: number: 8 paused: 0 running: 8 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/REDACTED/.local/share/containers/storage graphRootAllocated: 476673212416 graphRootUsed: 276004855808 graphStatus: Backing Filesystem: f2fs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 202 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/REDACTED/.local/share/containers/storage/volumes version: APIVersion: 4.4.2 Built: 1677255177 BuiltTime: Fri Feb 24 17:12:57 2023 GitCommit: 74afe26887f814d1c39925a1624851ef3590e79c-dirty GoVersion: go1.20.1 Os: linux OsArch: linux/amd64 Version: 4.4.2   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_",source-file | source-file | source-file | test-file,"Stats response has `""Id""` field instead of `""id""`  Issue Description The stats API JSON response is inconsistent with Docker, because it incorrectly capitalises the `""id""` field. I discovered this when using the Rust crate [Bollard](https://github.com/fussybeaver/bollard), which reported that the `""id""` field was missing.  Steps to reproduce the issue 1. sh curl --unix-socket /var/run/docker.sock ""http://d/containers/$SOME_CONTAINER_ID/stats"" # {,""id"":"""",}  2. sh curl --unix-socket /run/user/1000/podman/podman.sock ""http://d/containers/$SOME_CONTAINER_ID/stats"" # {,""Id"":"""",}   Describe the results you received JSON response contains `""Id""` field, but no `""id""`.  Describe the results you expected JSON response should contain `""id""` field.  podman info output yaml host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - cpu - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: /usr/bin/conmon is owned by conmon 1:2.1.7-1 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: f633919178f6c8ee4fb41b848a056ec33f8d707d' cpuUtilization: idlePercent: 79.35 systemPercent: 5.16 userPercent: 15.48 cpus: 8 distribution: distribution: arch version: unknown eventLogger: journald hostname: REDACTED idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 6.2.6-arch1-1 linkmode: dynamic logDriver: journald memFree: 813850624 memTotal: 24844115968 networkBackend: netavark ociRuntime: name: crun package: /usr/bin/crun is owned by crun 1.8.1-1 path: /usr/bin/crun version: |- crun version 1.8.1 commit: f8a096be060b22ccd3d5f3ebe44108517fbf6c30 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: /usr/bin/slirp4netns is owned by slirp4netns 1.2.0-1 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 34215489536 swapTotal: 34359734272 uptime: 140h 28m 48.00s (Approximately 5.83 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: {} store: configFile: /home/REDACTED/.config/containers/storage.conf containerStore: number: 8 paused: 0 running: 8 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/REDACTED/.local/share/containers/storage graphRootAllocated: 476673212416 graphRootUsed: 276004855808 graphStatus: Backing Filesystem: f2fs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 202 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/REDACTED/.local/share/containers/storage/volumes version: APIVersion: 4.4.2 Built: 1677255177 BuiltTime: Fri Feb 24 17:12:57 2023 GitCommit: 74afe26887f814d1c39925a1624851ef3590e79c-dirty GoVersion: go1.20.1 Os: linux OsArch: linux/amd64 Version: 4.4.2   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_ source-file source-file source-file test-file",bug,0.95
23712,podman,https://github.com/containers/podman/issues/23712,Compat API endpoints for events and container logs with `follow=true` do not send response immediately like Docker," Issue Description It seems that the Podman compat API does not immediately return an HTTP response when querying `/events` or `/containers/${id}/logs?follow=true&stdout=true&stderr=true` if no events or logs are produced yet. In this case, the client cannot determine whether the request was successful until some data is received. For example, I use [Vector](https://github.com/vectordotdev/vector), which in turn uses the [Bollard](https://github.com/fussybeaver/bollard) library to interact with the Docker API, to collect logs from Docker and Podman. When Vector starts fetching system events and container logs using the Podman 5.2.1 compat API via the socket `unix:run/podman/podman.sock`, the requests fail with error `RequestTimeoutError` after a 2 minute timeout if no response is received from Podman, causing Vector to stop monitoring the container. At the same time, everything works fine with Docker. Vector logs:  vector[19350]: 2024-08-21T17:05:19.072757Z INFO source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::sources::docker_logs: Capturing logs from now on. now=2024-08-21T17:05:19.072742638+00:00 vector[19350]: 2024-08-21T17:05:19.072925Z INFO source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::sources::docker_logs: Listening to docker log events. vector[19350]: 2024-08-21T17:05:19.114233Z INFO source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::internal_events::docker_logs: Started watching for container logs. container_id=6c7b2459bcb71617e192003e88955527b738bb20dc4b6b5cbbc798112b45b357 vector[19350]: 2024-08-21T17:07:19.098385Z ERROR source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::internal_events::docker_logs: Error in communication with Docker daemon. error=RequestTimeoutError error_type=""connection_failed"" stage=""receiving"" container_id=None internal_log_rate_limit=true vector[19350]: 2024-08-21T17:07:19.115508Z ERROR source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::internal_events::docker_logs: Internal log [Error in communication with Docker daemon.] is being suppressed to avoid flooding. vector[19350]: 2024-08-21T17:07:19.115597Z INFO source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::internal_events::docker_logs: Stopped watching for container logs. container_id=6c7b2459bcb71617e192003e88955527b738bb20dc4b6b5cbbc798112b45b357   Steps to reproduce the issue 1. Run any container that does not produce logs: `id=$(podman run -d --rm alpine sleep infinity)` 2. Get events: `curl -v --unix-socket /run/podman/podman.sock ""http://d/events"" -o -` 3. Get container logs: `curl -v --unix-socket /run/podman/podman.sock ""http://d/containers/${id}/logs?follow=true&stdout=true&stderr=true"" -o -`  Describe the results you received There is no HTTP response if no events or logs are produced:  # curl -v --unix-socket /run/podman/podman.sock ""http://d/events"" -o - * Trying /run/podman/podman.sock:0 * Connected to d (/run/podman/podman.sock) port 80 > GET /events HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > ^C # curl -v --unix-socket /run/podman/podman.sock ""http://d/containers/${id}/logs?follow=true&stdout=true&stderr=true"" -o - * Trying /run/podman/podman.sock:0 * Connected to d (/run/podman/podman.sock) port 80 > GET /containers/6c7b2459bcb71617e192003e88955527b738bb20dc4b6b5cbbc798112b45b357/logs?follow=true&stdout=true&stderr=true HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > ^C   Describe the results you expected  For events endpoint Receive the HTTP response even if there are no logs or events. Like with Podman 5.0.3:  # curl -v --unix-socket /run/podman/podman.sock ""http://d/events"" -o - * Trying /run/podman/podman.sock:0 * Connected to d (/run/podman/podman.sock) port 80 > GET /events HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Libpod-Api-Version: 5.0.3 < Server: Libpod/5.0.3 (linux) < X-Reference-Id: 0xc00006a338 < Date: Wed, 21 Aug 2024 19:16:59 GMT < Transfer-Encoding: chunked < ^C  Like with Docker:  # curl -v --unix-socket /run/docker.sock ""http://d/events"" -o - * Trying /run/docker.sock:0 * Connected to d (/run/docker.sock) port 80 > GET /events HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > < HTTP/1.1 200 OK < Api-Version: 1.46 < Content-Type: application/json < Docker-Experimental: false < Ostype: linux < Server: Docker/27.1.2 (linux) < Date: Wed, 21 Aug 2024 20:08:13 GMT < Transfer-Encoding: chunked < ^C   For container logs endpoint Recevice the HTTP response even if there are no logs or events. Like with Docker:  # curl -v --unix-socket /run/docker.sock ""http://d/containers/${id}/logs?follow=true&stdout=true&stderr=true"" -o - * Trying /run/docker.sock:0 * Connected to d (/run/docker.sock) port 80 > GET /containers/c60d242591740394e4ef8f2dd9a4264a931dffd9f357d0adb2b1e53dd0024196/logs?follow=true&stdout=true&stderr=true HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > < HTTP/1.1 200 OK < Api-Version: 1.46 < Content-Type: application/vnd.docker.multiplexed-stream < Docker-Experimental: false < Ostype: linux < Server: Docker/27.1.2 (linux) < Date: Wed, 21 Aug 2024 20:10:30 GMT < Transfer-Encoding: chunked < ^C   podman info output yaml host: arch: amd64 buildahVersion: 1.37.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids cgroupManager: cgroupfs cgroupVersion: v2 conmon: package: conmon-2.1.10-1.fc40.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.10, commit: ' cpuUtilization: idlePercent: 99.44 systemPercent: 0.27 userPercent: 0.29 cpus: 2 databaseBackend: sqlite distribution: distribution: fedora variant: container version: ""40"" eventLogger: file freeLocks: 2048 hostname: 2659a687fd77 idMappings: gidmap: null uidmap: null kernel: 5.15.0-208.159.3.2.el9uek.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 1310531584 memTotal: 3624329216 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.12.1-1.fc40.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.12.1 package: netavark-1.12.1-1.fc40.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.12.1 ociRuntime: name: crun package: crun-1.15-1.fc40.x86_64 path: /usr/bin/crun version: |- crun version 1.15 commit: e6eacaf4034e84185fd8780ac9262bbf57082278 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20240814.g61c0b0d-1.fc40.x86_64 version: | pasta 0^20240814.g61c0b0d-1.fc40.x86_64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: false path: /run/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: """" package: """" version: """" swapFree: 2147479552 swapTotal: 2147479552 uptime: 27h 22m 32.00s (Approximately 1.12 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.imagestore: /usr/lib/containers/storage overlay.mount_program: Executable: /usr/bin/fuse-overlayfs Package: fuse-overlayfs-1.13-1.fc40.x86_64 Version: |- fusermount3 version: 3.16.2 fuse-overlayfs: version 1.13-dev FUSE library version 3.16.2 using FUSE kernel interface version 7.38 overlay.mountopt: nodev,fsync=0 graphRoot: /var/lib/containers/storage graphRootAllocated: 21027463168 graphRootUsed: 10851258368 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.2.1 Built: 1723593600 BuiltTime: Wed Aug 14 00:00:00 2024 GitCommit: """" GoVersion: go1.22.5 Os: linux OsArch: linux/amd64 Version: 5.2.1   Podman in a container Yes  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | test-file,"Compat API endpoints for events and container logs with `follow=true` do not send response immediately like Docker  Issue Description It seems that the Podman compat API does not immediately return an HTTP response when querying `/events` or `/containers/${id}/logs?follow=true&stdout=true&stderr=true` if no events or logs are produced yet. In this case, the client cannot determine whether the request was successful until some data is received. For example, I use [Vector](https://github.com/vectordotdev/vector), which in turn uses the [Bollard](https://github.com/fussybeaver/bollard) library to interact with the Docker API, to collect logs from Docker and Podman. When Vector starts fetching system events and container logs using the Podman 5.2.1 compat API via the socket `unix:run/podman/podman.sock`, the requests fail with error `RequestTimeoutError` after a 2 minute timeout if no response is received from Podman, causing Vector to stop monitoring the container. At the same time, everything works fine with Docker. Vector logs:  vector[19350]: 2024-08-21T17:05:19.072757Z INFO source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::sources::docker_logs: Capturing logs from now on. now=2024-08-21T17:05:19.072742638+00:00 vector[19350]: 2024-08-21T17:05:19.072925Z INFO source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::sources::docker_logs: Listening to docker log events. vector[19350]: 2024-08-21T17:05:19.114233Z INFO source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::internal_events::docker_logs: Started watching for container logs. container_id=6c7b2459bcb71617e192003e88955527b738bb20dc4b6b5cbbc798112b45b357 vector[19350]: 2024-08-21T17:07:19.098385Z ERROR source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::internal_events::docker_logs: Error in communication with Docker daemon. error=RequestTimeoutError error_type=""connection_failed"" stage=""receiving"" container_id=None internal_log_rate_limit=true vector[19350]: 2024-08-21T17:07:19.115508Z ERROR source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::internal_events::docker_logs: Internal log [Error in communication with Docker daemon.] is being suppressed to avoid flooding. vector[19350]: 2024-08-21T17:07:19.115597Z INFO source{component_kind=""source"" component_id=src_podman component_type=docker_logs}: vector::internal_events::docker_logs: Stopped watching for container logs. container_id=6c7b2459bcb71617e192003e88955527b738bb20dc4b6b5cbbc798112b45b357   Steps to reproduce the issue 1. Run any container that does not produce logs: `id=$(podman run -d --rm alpine sleep infinity)` 2. Get events: `curl -v --unix-socket /run/podman/podman.sock ""http://d/events"" -o -` 3. Get container logs: `curl -v --unix-socket /run/podman/podman.sock ""http://d/containers/${id}/logs?follow=true&stdout=true&stderr=true"" -o -`  Describe the results you received There is no HTTP response if no events or logs are produced:  # curl -v --unix-socket /run/podman/podman.sock ""http://d/events"" -o - * Trying /run/podman/podman.sock:0 * Connected to d (/run/podman/podman.sock) port 80 > GET /events HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > ^C # curl -v --unix-socket /run/podman/podman.sock ""http://d/containers/${id}/logs?follow=true&stdout=true&stderr=true"" -o - * Trying /run/podman/podman.sock:0 * Connected to d (/run/podman/podman.sock) port 80 > GET /containers/6c7b2459bcb71617e192003e88955527b738bb20dc4b6b5cbbc798112b45b357/logs?follow=true&stdout=true&stderr=true HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > ^C   Describe the results you expected  For events endpoint Receive the HTTP response even if there are no logs or events. Like with Podman 5.0.3:  # curl -v --unix-socket /run/podman/podman.sock ""http://d/events"" -o - * Trying /run/podman/podman.sock:0 * Connected to d (/run/podman/podman.sock) port 80 > GET /events HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Libpod-Api-Version: 5.0.3 < Server: Libpod/5.0.3 (linux) < X-Reference-Id: 0xc00006a338 < Date: Wed, 21 Aug 2024 19:16:59 GMT < Transfer-Encoding: chunked < ^C  Like with Docker:  # curl -v --unix-socket /run/docker.sock ""http://d/events"" -o - * Trying /run/docker.sock:0 * Connected to d (/run/docker.sock) port 80 > GET /events HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > < HTTP/1.1 200 OK < Api-Version: 1.46 < Content-Type: application/json < Docker-Experimental: false < Ostype: linux < Server: Docker/27.1.2 (linux) < Date: Wed, 21 Aug 2024 20:08:13 GMT < Transfer-Encoding: chunked < ^C   For container logs endpoint Recevice the HTTP response even if there are no logs or events. Like with Docker:  # curl -v --unix-socket /run/docker.sock ""http://d/containers/${id}/logs?follow=true&stdout=true&stderr=true"" -o - * Trying /run/docker.sock:0 * Connected to d (/run/docker.sock) port 80 > GET /containers/c60d242591740394e4ef8f2dd9a4264a931dffd9f357d0adb2b1e53dd0024196/logs?follow=true&stdout=true&stderr=true HTTP/1.1 > Host: d > User-Agent: curl/8.6.0 > Accept: */* > < HTTP/1.1 200 OK < Api-Version: 1.46 < Content-Type: application/vnd.docker.multiplexed-stream < Docker-Experimental: false < Ostype: linux < Server: Docker/27.1.2 (linux) < Date: Wed, 21 Aug 2024 20:10:30 GMT < Transfer-Encoding: chunked < ^C   podman info output yaml host: arch: amd64 buildahVersion: 1.37.1 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids cgroupManager: cgroupfs cgroupVersion: v2 conmon: package: conmon-2.1.10-1.fc40.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.10, commit: ' cpuUtilization: idlePercent: 99.44 systemPercent: 0.27 userPercent: 0.29 cpus: 2 databaseBackend: sqlite distribution: distribution: fedora variant: container version: ""40"" eventLogger: file freeLocks: 2048 hostname: 2659a687fd77 idMappings: gidmap: null uidmap: null kernel: 5.15.0-208.159.3.2.el9uek.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 1310531584 memTotal: 3624329216 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.12.1-1.fc40.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.12.1 package: netavark-1.12.1-1.fc40.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.12.1 ociRuntime: name: crun package: crun-1.15-1.fc40.x86_64 path: /usr/bin/crun version: |- crun version 1.15 commit: e6eacaf4034e84185fd8780ac9262bbf57082278 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20240814.g61c0b0d-1.fc40.x86_64 version: | pasta 0^20240814.g61c0b0d-1.fc40.x86_64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: false path: /run/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: """" package: """" version: """" swapFree: 2147479552 swapTotal: 2147479552 uptime: 27h 22m 32.00s (Approximately 1.12 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.imagestore: /usr/lib/containers/storage overlay.mount_program: Executable: /usr/bin/fuse-overlayfs Package: fuse-overlayfs-1.13-1.fc40.x86_64 Version: |- fusermount3 version: 3.16.2 fuse-overlayfs: version 1.13-dev FUSE library version 3.16.2 using FUSE kernel interface version 7.38 overlay.mountopt: nodev,fsync=0 graphRoot: /var/lib/containers/storage graphRootAllocated: 21027463168 graphRootUsed: 10851258368 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.2.1 Built: 1723593600 BuiltTime: Wed Aug 14 00:00:00 2024 GitCommit: """" GoVersion: go1.22.5 Os: linux OsArch: linux/amd64 Version: 5.2.1   Podman in a container Yes  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file source-file test-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file source-file test-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file source-file test-file",bug,0.95
16360,podman,https://github.com/containers/podman/issues/16360,Podman socket: Build API produces different results then Docker's Build API,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> As i am migrating from OL7 -> OL8 (RHEL based), i am wanting to keep my current code working - for that i am relying on podman socket to keep my docker-compose / docker py usage functional. Docker-compose seems fine but docker-py is not, there is an issue where the return values from the build API is different: https://github.com/containers/podman/blob/40e8bcb8482f2a1f60b93524ceda05770d20739e/pkg/api/handlers/compat/images_build.go#L803 The final response includes 2 lines rather than a single line, as this `aux` object is added. This isn't really an issue and i can workaround it, but was wondering if it would be changed to be in line with docker API. **Steps to reproduce the issue:** 1. Use docker py to build an image with docker and podman socket 2. See that the final response from the api is different **Describe the results you received:** {""aux"":{""ID"":""sha256:2d49cc4bc02bb457a9459736c461c52944e68e2a40cf2bb1207494b2b72c5f82""}}\n{""stream"":""Successfully built 2d49cc4bc02b\\n""}\n **Describe the results you expected:** {""stream"":""Successfully built 2d49cc4bc02b\\n""}\n **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Client: Podman Engine Version: 4.1.1 API Version: 4.1.1 Go Version: go1.17.12 Built: Wed Oct 26 15:12:06 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  host: arch: amd64 buildahVersion: 1.26.2 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.3-1.module+el8.6.0+20857+bf01bdf2.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.3, commit: 0e2e290c54cc97af44a8b96f004ff81e8abd8956' cpuUtilization: idlePercent: 98.48 systemPercent: 0.21 userPercent: 1.31 cpus: 104 distribution: distribution: '""ol""' variant: server version: ""8.6"" eventLogger: file hostname: cicd-bm-standard-ol8 idMappings: gidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 165536 size: 65536 uidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 165536 size: 65536 kernel: 5.4.17-2136.310.7.1.el8uek.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 630402412544 memTotal: 809608286208 networkBackend: cni ociRuntime: name: runc package: runc-1.1.3-2.module+el8.6.0+20857+bf01bdf2.x86_64 path: /usr/bin/runc version: |- runc version 1.1.3 spec: 1.0.2-dev go: go1.17.12 libseccomp: 2.5.2 os: linux remoteSocket: path: /run/user/1001/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /bin/slirp4netns package: slirp4netns-1.2.0-2.module+el8.6.0+20857+bf01bdf2.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 8539598848 swapTotal: 8539598848 uptime: 4h 35m 58.6s (Approximately 0.17 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - container-registry.oracle.com - docker.io store: configFile: /home/oracle/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/oracle/.local/share/containers/storage graphRootAllocated: 2187084447744 graphRootUsed: 178915131392 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /run/user/1001/containers volumePath: /home/oracle/.local/share/containers/storage/volumes version: APIVersion: 4.1.1 Built: 1666797126 BuiltTime: Wed Oct 26 15:12:06 2022 GitCommit: """" GoVersion: go1.17.12 Os: linux OsArch: linux/amd64 Version: 4.1.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman` or `brew info podman`):**  podman-4.1.1-7.module+el8.6.0+20857+bf01bdf2.x86_64  Have tested with OL9 which is 4.2.x podman **Additional environment details (AWS, VirtualBox, physical, etc.):** Cloud based vm",source-file,"Podman socket: Build API produces different results then Docker's Build API <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> As i am migrating from OL7 -> OL8 (RHEL based), i am wanting to keep my current code working - for that i am relying on podman socket to keep my docker-compose / docker py usage functional. Docker-compose seems fine but docker-py is not, there is an issue where the return values from the build API is different: https://github.com/containers/podman/blob/40e8bcb8482f2a1f60b93524ceda05770d20739e/pkg/api/handlers/compat/images_build.go#L803 The final response includes 2 lines rather than a single line, as this `aux` object is added. This isn't really an issue and i can workaround it, but was wondering if it would be changed to be in line with docker API. **Steps to reproduce the issue:** 1. Use docker py to build an image with docker and podman socket 2. See that the final response from the api is different **Describe the results you received:** {""aux"":{""ID"":""sha256:2d49cc4bc02bb457a9459736c461c52944e68e2a40cf2bb1207494b2b72c5f82""}}\n{""stream"":""Successfully built 2d49cc4bc02b\\n""}\n **Describe the results you expected:** {""stream"":""Successfully built 2d49cc4bc02b\\n""}\n **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Client: Podman Engine Version: 4.1.1 API Version: 4.1.1 Go Version: go1.17.12 Built: Wed Oct 26 15:12:06 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  host: arch: amd64 buildahVersion: 1.26.2 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.3-1.module+el8.6.0+20857+bf01bdf2.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.3, commit: 0e2e290c54cc97af44a8b96f004ff81e8abd8956' cpuUtilization: idlePercent: 98.48 systemPercent: 0.21 userPercent: 1.31 cpus: 104 distribution: distribution: '""ol""' variant: server version: ""8.6"" eventLogger: file hostname: cicd-bm-standard-ol8 idMappings: gidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 165536 size: 65536 uidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 165536 size: 65536 kernel: 5.4.17-2136.310.7.1.el8uek.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 630402412544 memTotal: 809608286208 networkBackend: cni ociRuntime: name: runc package: runc-1.1.3-2.module+el8.6.0+20857+bf01bdf2.x86_64 path: /usr/bin/runc version: |- runc version 1.1.3 spec: 1.0.2-dev go: go1.17.12 libseccomp: 2.5.2 os: linux remoteSocket: path: /run/user/1001/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /bin/slirp4netns package: slirp4netns-1.2.0-2.module+el8.6.0+20857+bf01bdf2.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 8539598848 swapTotal: 8539598848 uptime: 4h 35m 58.6s (Approximately 0.17 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - container-registry.oracle.com - docker.io store: configFile: /home/oracle/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/oracle/.local/share/containers/storage graphRootAllocated: 2187084447744 graphRootUsed: 178915131392 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /run/user/1001/containers volumePath: /home/oracle/.local/share/containers/storage/volumes version: APIVersion: 4.1.1 Built: 1666797126 BuiltTime: Wed Oct 26 15:12:06 2022 GitCommit: """" GoVersion: go1.17.12 Os: linux OsArch: linux/amd64 Version: 4.1.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman` or `brew info podman`):**  podman-4.1.1-7.module+el8.6.0+20857+bf01bdf2.x86_64  Have tested with OL9 which is 4.2.x podman **Additional environment details (AWS, VirtualBox, physical, etc.):** Cloud based vm source-file",bug,0.9
21754,podman,https://github.com/containers/podman/issues/21754,docker 25 CLI is unable to run a container in quay.io/podman/upstream: default networking argument issue," Issue Description Hi, When using a Docker 25 CLI targetting a podman socket from `quay.io/podman/upstream`, it fails with:  docker: Error response from daemon: container create: invalid config provided: networks and static ip/mac address can only be used with Bridge mode networking.  The docker CLI 24 (and older) does work fine. Investigating a bit with socat the difference between the API calls, I end up with this request from docker 24 CLI (for the `POST /v1.41/containers/create` API call):  {""Hostname"":"""",""Domainname"":"""",""User"":"""",""AttachStdin"":true,""AttachStdout"":true,""AttachStderr"":true,""Tty"":true,""OpenStdin"":true,""StdinOnce"":true,""Env"":null,""Cmd"":null,""Image"":""quay.io/fedora/fedora"",""Volumes"":{},""WorkingDir"":"""",""Entrypoint"":null,""OnBuild"":null,""Labels"":{},""HostConfig"":{""Binds"":null,""ContainerIDFile"":"""",""LogConfig"":{""Type"":"""",""Config"":{}},""NetworkMode"":""default"",""PortBindings"":{},""RestartPolicy"":{""Name"":""no"",""MaximumRetryCount"":0},""AutoRemove"":true,""VolumeDriver"":"""",""VolumesFrom"":null,""ConsoleSize"":[26,227],""CapAdd"":null,""CapDrop"":null,""CgroupnsMode"":"""",""Dns"":[],""DnsOptions"":[],""DnsSearch"":[],""ExtraHosts"":null,""GroupAdd"":null,""IpcMode"":"""",""Cgroup"":"""",""Links"":null,""OomScoreAdj"":0,""PidMode"":"""",""Privileged"":false,""PublishAllPorts"":false,""ReadonlyRootfs"":false,""SecurityOpt"":null,""UTSMode"":"""",""UsernsMode"":"""",""ShmSize"":0,""Isolation"":"""",""CpuShares"":0,""Memory"":0,""NanoCpus"":0,""CgroupParent"":"""",""BlkioWeight"":0,""BlkioWeightDevice"":[],""BlkioDeviceReadBps"":[],""BlkioDeviceWriteBps"":[],""BlkioDeviceReadIOps"":[],""BlkioDeviceWriteIOps"":[],""CpuPeriod"":0,""CpuQuota"":0,""CpuRealtimePeriod"":0,""CpuRealtimeRuntime"":0,""CpusetCpus"":"""",""CpusetMems"":"""",""Devices"":[],""DeviceCgroupRules"":null,""DeviceRequests"":null,""MemoryReservation"":0,""MemorySwap"":0,""MemorySwappiness"":-1,""OomKillDisable"":false,""PidsLimit"":0,""Ulimits"":null,""CpuCount"":0,""CpuPercent"":0,""IOMaximumIOps"":0,""IOMaximumBandwidth"":0,""MaskedPaths"":null,""ReadonlyPaths"":null},""NetworkingConfig"":{""EndpointsConfig"":{  and this request from docker 25 CLI:  {""Hostname"":"""",""Domainname"":"""",""User"":"""",""AttachStdin"":true,""AttachStdout"":true,""AttachStderr"":true,""Tty"":true,""OpenStdin"":true,""StdinOnce"":true,""Env"":null,""Cmd"":null,""Image"":""quay.io/fedora/fedora"",""Volumes"":{},""WorkingDir"":"""",""Entrypoint"":null,""OnBuild"":null,""Labels"":{},""HostConfig"":{""Binds"":null,""ContainerIDFile"":"""",""LogConfig"":{""Type"":"""",""Config"":{}},""NetworkMode"":""default"",""PortBindings"":{},""RestartPolicy"":{""Name"":""no"",""MaximumRetryCount"":0},""AutoRemove"":true,""VolumeDriver"":"""",""VolumesFrom"":null,""ConsoleSize"":[26,227],""CapAdd"":null,""CapDrop"":null,""CgroupnsMode"":"""",""Dns"":[],""DnsOptions"":[],""DnsSearch"":[],""ExtraHosts"":null,""GroupAdd"":null,""IpcMode"":"""",""Cgroup"":"""",""Links"":null,""OomScoreAdj"":0,""PidMode"":"""",""Privileged"":false,""PublishAllPorts"":false,""ReadonlyRootfs"":false,""SecurityOpt"":null,""UTSMode"":"""",""UsernsMode"":"""",""ShmSize"":0,""Isolation"":"""",""CpuShares"":0,""Memory"":0,""NanoCpus"":0,""CgroupParent"":"""",""BlkioWeight"":0,""BlkioWeightDevice"":[],""BlkioDeviceReadBps"":[],""BlkioDeviceWriteBps"":[],""BlkioDeviceReadIOps"":[],""BlkioDeviceWriteIOps"":[],""CpuPeriod"":0,""CpuQuota"":0,""CpuRealtimePeriod"":0,""CpuRealtimeRuntime"":0,""CpusetCpus"":"""",""CpusetMems"":"""",""Devices"":[],""DeviceCgroupRules"":null,""DeviceRequests"":null,""MemoryReservation"":0,""MemorySwap"":0,""MemorySwappiness"":-1,""OomKillDisable"":false,""PidsLimit"":0,""Ulimits"":[],""CpuCount"":0,""CpuPercent"":0,""IOMaximumIOps"":0,""IOMaximumBandwidth"":0,""MaskedPaths"":null,""ReadonlyPaths"":null},""NetworkingConfig"":{""EndpointsConfig"":{""default"":{""IPAMConfig"":null,""Links"":null,""Aliases"":null,""MacAddress"":"""",""NetworkID"":"""",""EndpointID"":"""",""Gateway"":"""",""IPAddress"":"""",""IPPrefixLen"":0,""IPv6Gateway"":"""",""GlobalIPv6Address"":"""",""GlobalIPv6PrefixLen"":0,""DriverOpts"":null,""DNSNames"":null  Using ""jq"" to pretty print it and show the differences, I ended up with this:  rgeissler@ncerndobedev6097:/tmp> diff -u <(cat docker-24|jq) <(cat docker-25|jq) 7:11PM  /proc/self/fd/12 2024-02-19 19:11:54.051981352 +0000  /proc/self/fd/13 2024-02-19 19:11:54.055981487 +0000 @@ -81,7 +81,7 @@ ""MemorySwappiness"": -1, ""OomKillDisable"": false, ""PidsLimit"": 0, - ""Ulimits"": null, + ""Ulimits"": [], ""CpuCount"": 0, ""CpuPercent"": 0, ""IOMaximumIOps"": 0, @@ -90,6 +90,23 @@ ""ReadonlyPaths"": null }, ""NetworkingConfig"": { - ""EndpointsConfig"": {} + ""EndpointsConfig"": { + ""default"": { + ""IPAMConfig"": null, + ""Links"": null, + ""Aliases"": null, + ""MacAddress"": """", + ""NetworkID"": """", + ""EndpointID"": """", + ""Gateway"": """", + ""IPAddress"": """", + ""IPPrefixLen"": 0, + ""IPv6Gateway"": """", + ""GlobalIPv6Address"": """", + ""GlobalIPv6PrefixLen"": 0, + ""DriverOpts"": null, + ""DNSNames"": null + } + } } }  So the ""root cause"" is this new default ""EndpointsConfig"" struct, which seems all default initialized and which podman somehow interprets differently from an empty `{}` json config. I didn't check the code yet to see if there is a quick fix for this issue. Cheers, Romain  Steps to reproduce the issue Steps to reproduce the issue 1. Start a `quay.io/podman/upstream` container start a podman server:  > podman run -t -i --rm --privileged --name=podman-server -v /shared-volume --pull=always quay.io/podman/upstream podman system service -t 0 unix:shared-volume/podman.sock Trying to pull quay.io/podman/upstream:latest Getting image source signatures Copying blob sha256:b2013b443c422f98f009bfb9f930cc424428f8ccb694c84a26ebeb98891687f9 Copying blob sha256:cf73a40571609daa501dab6a62c1d08ca1665278b2355f5a36424804281c62fe Copying blob sha256:439ec636831d395b84ce0dfd6390420c60a8aa2a128a88b7dc55236a5e54c7a1 Copying blob sha256:2026b963063adee0348d722192a5761e34b449d666f6f79a687818f40a96a67f Copying blob sha256:718a00fe32127ad01ddab9fc4b7c968ab2679c92c6385ac6865ae6e2523275e4 Copying blob sha256:d2ea58b809bcf20a5fa73f3bce6c4ac3f371fbd010700dc0979f495f53b7fc76 Copying blob sha256:e16106bb651e790ca91447e2fc47c5a2f86e4824cd3bada6d63419f9801e4f93 Copying blob sha256:7323cd5de043ae967491a338060d6d1c51eed17c19bda023d76f72b326d7b5bf Copying blob sha256:39cd8764a88b128c21ab1816efa5c0179a803ed18ada3aa9c5f09d23b231787f Copying config sha256:8739df1320065867713c219a7efdb999e478796c0059e25143f10f54272fe833 Writing manifest to image destination  2. Try to run a container inside the first one, using a docker 25 CLI. I used explicitly `--oom-score-adj=1000` to workaround the issue fixed by #21487:  > podman run -t -i --rm --pull=always --volumes-from=podman-server -e DOCKER_HOST=unix:shared-volume/podman.sock docker:25-cli docker run -t -i --rm --oom-score-adj=1000 quay.io/fedora/fedora Trying to pull docker.io/library/docker:25-cli Getting image source signatures Copying blob sha256:f23a00be1976186eef218bb9b79ab99203e0c5b235c1b49e77f3ac3264793d78 Copying blob sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 Copying blob sha256:1bd9561ee09aa7423609ac837068a53ad4a8fd35b8b6d51528d78cb126eb3b07 Copying blob sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 Copying blob sha256:a48a116699ba1fe77921e9ab9fef2dc65016ac2e483b07f03921526b8a0fcf79 Copying blob sha256:41dc1292823582d19d9bc523915a590652c16d3bedc6099b345ec38efd77e6a0 Copying blob sha256:e56d49cdf5732fc93fbea227838e14f9772a369a669592554ca37cf4695be03d Copying blob sha256:b6735844810a03df9d76e5aa58d3df2f1c56f6c208093eba5f88e5806f1cda04 Copying blob sha256:49704df22e8621ca86fd75cb61b8f23ee280754df0048a7bf46cd8b96ec2551f Copying blob sha256:b0f5f50d82ecbeeeea34901bdac4b25948f5f7146f2842d8a7aeb9a730459567 Copying config sha256:e95f54c1fcc216bc7d10270705e5ce8f98504f69d2afba666d6ce12e940da2f6 Writing manifest to image destination Unable to find image 'quay.io/fedora/fedora:latest' locally 718a00fe3212: Download complete 368a084ba17d: Download complete docker: Error response from daemon: container create: invalid config provided: networks and static ip/mac address can only be used with Bridge mode networking. See 'docker run --help'.  3. Doing exactly the same thing, with `docker:24-cli` instead works just fine:  > podman run -t -i --rm --pull=always --volumes-from=podman-server -e DOCKER_HOST=unix:shared-volume/podman.sock docker:24-cli docker run -t -i --rm --oom-score-adj=1000 quay.io/fedora/fedora Trying to pull docker.io/library/docker:24-cli Getting image source signatures Copying blob sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 Copying blob sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 Copying blob sha256:1db5a4f146e2df1f17a2c0db7ffd672b18d1750d31c7e58e352a6536d4b7ad52 Copying blob sha256:248ec8ed73b325a9cff9ec3c5bbd2c249065ee96053dfc3beafb911ef652a195 Copying blob sha256:5aed6b72066db35b8929c11c552f9e827431de2b2de62b4384a1eaed88221787 Copying blob sha256:9908927dc97522b6d63aaaf9953c4095be9b24a1d080edb1ada9124d56bf41ad Copying blob sha256:d280e8e81156b63b7306ac0701738e22b333c7a26bdd96ddafb3ec8f607d2a32 Copying blob sha256:97f6ee5ccb7fa02811eb89d903666095e18e38c42a635369912f9ba0fd11e6eb Copying blob sha256:d4462dfff57f9ac45d562ae18d1ca53fef4918ae18e003d21ebf960b3ade6f94 Copying blob sha256:a474f84a4abb535fa3a05ee5a59bff31a53d0857d4d53bab82a9f2f3684c4c7c Copying config sha256:b4e4d47cb84703dc6042823b7e29e3111074beb09b50848a31cdb9cd9565ed9a Writing manifest to image destination [root@56d0c302cd66 /]# ^C [root@56d0c302cd66 /]# exit exit   Describe the results you received Podman cannot be used with docker 25 CLI (in container mode) while it was working with docker 24 CLI.  Describe the results you expected Podman can be used with docker 25 CLI (in container mode).  podman info output yaml Tried on `quay.io/podman/upstream` on an up to date x86_64 RHEL 9.   Podman in a container Yes  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",source-file | test-file | source-file | test-file | source-file | test-file,"docker 25 CLI is unable to run a container in quay.io/podman/upstream: default networking argument issue  Issue Description Hi, When using a Docker 25 CLI targetting a podman socket from `quay.io/podman/upstream`, it fails with:  docker: Error response from daemon: container create: invalid config provided: networks and static ip/mac address can only be used with Bridge mode networking.  The docker CLI 24 (and older) does work fine. Investigating a bit with socat the difference between the API calls, I end up with this request from docker 24 CLI (for the `POST /v1.41/containers/create` API call):  {""Hostname"":"""",""Domainname"":"""",""User"":"""",""AttachStdin"":true,""AttachStdout"":true,""AttachStderr"":true,""Tty"":true,""OpenStdin"":true,""StdinOnce"":true,""Env"":null,""Cmd"":null,""Image"":""quay.io/fedora/fedora"",""Volumes"":{},""WorkingDir"":"""",""Entrypoint"":null,""OnBuild"":null,""Labels"":{},""HostConfig"":{""Binds"":null,""ContainerIDFile"":"""",""LogConfig"":{""Type"":"""",""Config"":{}},""NetworkMode"":""default"",""PortBindings"":{},""RestartPolicy"":{""Name"":""no"",""MaximumRetryCount"":0},""AutoRemove"":true,""VolumeDriver"":"""",""VolumesFrom"":null,""ConsoleSize"":[26,227],""CapAdd"":null,""CapDrop"":null,""CgroupnsMode"":"""",""Dns"":[],""DnsOptions"":[],""DnsSearch"":[],""ExtraHosts"":null,""GroupAdd"":null,""IpcMode"":"""",""Cgroup"":"""",""Links"":null,""OomScoreAdj"":0,""PidMode"":"""",""Privileged"":false,""PublishAllPorts"":false,""ReadonlyRootfs"":false,""SecurityOpt"":null,""UTSMode"":"""",""UsernsMode"":"""",""ShmSize"":0,""Isolation"":"""",""CpuShares"":0,""Memory"":0,""NanoCpus"":0,""CgroupParent"":"""",""BlkioWeight"":0,""BlkioWeightDevice"":[],""BlkioDeviceReadBps"":[],""BlkioDeviceWriteBps"":[],""BlkioDeviceReadIOps"":[],""BlkioDeviceWriteIOps"":[],""CpuPeriod"":0,""CpuQuota"":0,""CpuRealtimePeriod"":0,""CpuRealtimeRuntime"":0,""CpusetCpus"":"""",""CpusetMems"":"""",""Devices"":[],""DeviceCgroupRules"":null,""DeviceRequests"":null,""MemoryReservation"":0,""MemorySwap"":0,""MemorySwappiness"":-1,""OomKillDisable"":false,""PidsLimit"":0,""Ulimits"":null,""CpuCount"":0,""CpuPercent"":0,""IOMaximumIOps"":0,""IOMaximumBandwidth"":0,""MaskedPaths"":null,""ReadonlyPaths"":null},""NetworkingConfig"":{""EndpointsConfig"":{  and this request from docker 25 CLI:  {""Hostname"":"""",""Domainname"":"""",""User"":"""",""AttachStdin"":true,""AttachStdout"":true,""AttachStderr"":true,""Tty"":true,""OpenStdin"":true,""StdinOnce"":true,""Env"":null,""Cmd"":null,""Image"":""quay.io/fedora/fedora"",""Volumes"":{},""WorkingDir"":"""",""Entrypoint"":null,""OnBuild"":null,""Labels"":{},""HostConfig"":{""Binds"":null,""ContainerIDFile"":"""",""LogConfig"":{""Type"":"""",""Config"":{}},""NetworkMode"":""default"",""PortBindings"":{},""RestartPolicy"":{""Name"":""no"",""MaximumRetryCount"":0},""AutoRemove"":true,""VolumeDriver"":"""",""VolumesFrom"":null,""ConsoleSize"":[26,227],""CapAdd"":null,""CapDrop"":null,""CgroupnsMode"":"""",""Dns"":[],""DnsOptions"":[],""DnsSearch"":[],""ExtraHosts"":null,""GroupAdd"":null,""IpcMode"":"""",""Cgroup"":"""",""Links"":null,""OomScoreAdj"":0,""PidMode"":"""",""Privileged"":false,""PublishAllPorts"":false,""ReadonlyRootfs"":false,""SecurityOpt"":null,""UTSMode"":"""",""UsernsMode"":"""",""ShmSize"":0,""Isolation"":"""",""CpuShares"":0,""Memory"":0,""NanoCpus"":0,""CgroupParent"":"""",""BlkioWeight"":0,""BlkioWeightDevice"":[],""BlkioDeviceReadBps"":[],""BlkioDeviceWriteBps"":[],""BlkioDeviceReadIOps"":[],""BlkioDeviceWriteIOps"":[],""CpuPeriod"":0,""CpuQuota"":0,""CpuRealtimePeriod"":0,""CpuRealtimeRuntime"":0,""CpusetCpus"":"""",""CpusetMems"":"""",""Devices"":[],""DeviceCgroupRules"":null,""DeviceRequests"":null,""MemoryReservation"":0,""MemorySwap"":0,""MemorySwappiness"":-1,""OomKillDisable"":false,""PidsLimit"":0,""Ulimits"":[],""CpuCount"":0,""CpuPercent"":0,""IOMaximumIOps"":0,""IOMaximumBandwidth"":0,""MaskedPaths"":null,""ReadonlyPaths"":null},""NetworkingConfig"":{""EndpointsConfig"":{""default"":{""IPAMConfig"":null,""Links"":null,""Aliases"":null,""MacAddress"":"""",""NetworkID"":"""",""EndpointID"":"""",""Gateway"":"""",""IPAddress"":"""",""IPPrefixLen"":0,""IPv6Gateway"":"""",""GlobalIPv6Address"":"""",""GlobalIPv6PrefixLen"":0,""DriverOpts"":null,""DNSNames"":null  Using ""jq"" to pretty print it and show the differences, I ended up with this:  rgeissler@ncerndobedev6097:/tmp> diff -u <(cat docker-24|jq) <(cat docker-25|jq) 7:11PM  /proc/self/fd/12 2024-02-19 19:11:54.051981352 +0000  /proc/self/fd/13 2024-02-19 19:11:54.055981487 +0000 @@ -81,7 +81,7 @@ ""MemorySwappiness"": -1, ""OomKillDisable"": false, ""PidsLimit"": 0, - ""Ulimits"": null, + ""Ulimits"": [], ""CpuCount"": 0, ""CpuPercent"": 0, ""IOMaximumIOps"": 0, @@ -90,6 +90,23 @@ ""ReadonlyPaths"": null }, ""NetworkingConfig"": { - ""EndpointsConfig"": {} + ""EndpointsConfig"": { + ""default"": { + ""IPAMConfig"": null, + ""Links"": null, + ""Aliases"": null, + ""MacAddress"": """", + ""NetworkID"": """", + ""EndpointID"": """", + ""Gateway"": """", + ""IPAddress"": """", + ""IPPrefixLen"": 0, + ""IPv6Gateway"": """", + ""GlobalIPv6Address"": """", + ""GlobalIPv6PrefixLen"": 0, + ""DriverOpts"": null, + ""DNSNames"": null + } + } } }  So the ""root cause"" is this new default ""EndpointsConfig"" struct, which seems all default initialized and which podman somehow interprets differently from an empty `{}` json config. I didn't check the code yet to see if there is a quick fix for this issue. Cheers, Romain  Steps to reproduce the issue Steps to reproduce the issue 1. Start a `quay.io/podman/upstream` container start a podman server:  > podman run -t -i --rm --privileged --name=podman-server -v /shared-volume --pull=always quay.io/podman/upstream podman system service -t 0 unix:shared-volume/podman.sock Trying to pull quay.io/podman/upstream:latest Getting image source signatures Copying blob sha256:b2013b443c422f98f009bfb9f930cc424428f8ccb694c84a26ebeb98891687f9 Copying blob sha256:cf73a40571609daa501dab6a62c1d08ca1665278b2355f5a36424804281c62fe Copying blob sha256:439ec636831d395b84ce0dfd6390420c60a8aa2a128a88b7dc55236a5e54c7a1 Copying blob sha256:2026b963063adee0348d722192a5761e34b449d666f6f79a687818f40a96a67f Copying blob sha256:718a00fe32127ad01ddab9fc4b7c968ab2679c92c6385ac6865ae6e2523275e4 Copying blob sha256:d2ea58b809bcf20a5fa73f3bce6c4ac3f371fbd010700dc0979f495f53b7fc76 Copying blob sha256:e16106bb651e790ca91447e2fc47c5a2f86e4824cd3bada6d63419f9801e4f93 Copying blob sha256:7323cd5de043ae967491a338060d6d1c51eed17c19bda023d76f72b326d7b5bf Copying blob sha256:39cd8764a88b128c21ab1816efa5c0179a803ed18ada3aa9c5f09d23b231787f Copying config sha256:8739df1320065867713c219a7efdb999e478796c0059e25143f10f54272fe833 Writing manifest to image destination  2. Try to run a container inside the first one, using a docker 25 CLI. I used explicitly `--oom-score-adj=1000` to workaround the issue fixed by #21487:  > podman run -t -i --rm --pull=always --volumes-from=podman-server -e DOCKER_HOST=unix:shared-volume/podman.sock docker:25-cli docker run -t -i --rm --oom-score-adj=1000 quay.io/fedora/fedora Trying to pull docker.io/library/docker:25-cli Getting image source signatures Copying blob sha256:f23a00be1976186eef218bb9b79ab99203e0c5b235c1b49e77f3ac3264793d78 Copying blob sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 Copying blob sha256:1bd9561ee09aa7423609ac837068a53ad4a8fd35b8b6d51528d78cb126eb3b07 Copying blob sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 Copying blob sha256:a48a116699ba1fe77921e9ab9fef2dc65016ac2e483b07f03921526b8a0fcf79 Copying blob sha256:41dc1292823582d19d9bc523915a590652c16d3bedc6099b345ec38efd77e6a0 Copying blob sha256:e56d49cdf5732fc93fbea227838e14f9772a369a669592554ca37cf4695be03d Copying blob sha256:b6735844810a03df9d76e5aa58d3df2f1c56f6c208093eba5f88e5806f1cda04 Copying blob sha256:49704df22e8621ca86fd75cb61b8f23ee280754df0048a7bf46cd8b96ec2551f Copying blob sha256:b0f5f50d82ecbeeeea34901bdac4b25948f5f7146f2842d8a7aeb9a730459567 Copying config sha256:e95f54c1fcc216bc7d10270705e5ce8f98504f69d2afba666d6ce12e940da2f6 Writing manifest to image destination Unable to find image 'quay.io/fedora/fedora:latest' locally 718a00fe3212: Download complete 368a084ba17d: Download complete docker: Error response from daemon: container create: invalid config provided: networks and static ip/mac address can only be used with Bridge mode networking. See 'docker run --help'.  3. Doing exactly the same thing, with `docker:24-cli` instead works just fine:  > podman run -t -i --rm --pull=always --volumes-from=podman-server -e DOCKER_HOST=unix:shared-volume/podman.sock docker:24-cli docker run -t -i --rm --oom-score-adj=1000 quay.io/fedora/fedora Trying to pull docker.io/library/docker:24-cli Getting image source signatures Copying blob sha256:4abcf20661432fb2d719aaf90656f55c287f8ca915dc1c92ec14ff61e67fbaf8 Copying blob sha256:4f4fb700ef54461cfa02571ae0db9a0dc1e0cdb5577484a6d75e68dc38e8acc1 Copying blob sha256:1db5a4f146e2df1f17a2c0db7ffd672b18d1750d31c7e58e352a6536d4b7ad52 Copying blob sha256:248ec8ed73b325a9cff9ec3c5bbd2c249065ee96053dfc3beafb911ef652a195 Copying blob sha256:5aed6b72066db35b8929c11c552f9e827431de2b2de62b4384a1eaed88221787 Copying blob sha256:9908927dc97522b6d63aaaf9953c4095be9b24a1d080edb1ada9124d56bf41ad Copying blob sha256:d280e8e81156b63b7306ac0701738e22b333c7a26bdd96ddafb3ec8f607d2a32 Copying blob sha256:97f6ee5ccb7fa02811eb89d903666095e18e38c42a635369912f9ba0fd11e6eb Copying blob sha256:d4462dfff57f9ac45d562ae18d1ca53fef4918ae18e003d21ebf960b3ade6f94 Copying blob sha256:a474f84a4abb535fa3a05ee5a59bff31a53d0857d4d53bab82a9f2f3684c4c7c Copying config sha256:b4e4d47cb84703dc6042823b7e29e3111074beb09b50848a31cdb9cd9565ed9a Writing manifest to image destination [root@56d0c302cd66 /]# ^C [root@56d0c302cd66 /]# exit exit   Describe the results you received Podman cannot be used with docker 25 CLI (in container mode) while it was working with docker 24 CLI.  Describe the results you expected Podman can be used with docker 25 CLI (in container mode).  podman info output yaml Tried on `quay.io/podman/upstream` on an up to date x86_64 RHEL 9.   Podman in a container Yes  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting source-file test-file source-file test-file source-file test-file",bug,0.95
22657,podman,https://github.com/containers/podman/issues/22657,when host has podman from podman-next and podman-remote is run inside a toolbox : unmarshalling error," Issue Description I was trying to reproduce the issue https://github.com/containers/podman/issues/21974 which is valid for podman < 5 (did not bisect, take this with a grain of salt). I realized that having podman installed from podman next leads to an error.  Steps to reproduce the issue Steps to reproduce the issue 1. Install podman from the podman-next copr on the host 2. Follow the reproducer of https://github.com/containers/podman/issues/21974 until `podman-remote run` 3. Run the step `podman-remote run --cidfile /var/tmp/cidfile fedora:latest true` in the toolbox  Describe the results you received  Error: unmarshalling into &define.InspectContainerData{ID:""221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816"", Created:time.Date(2024, time.May, 9, 15, 37, 24, 363282572, time .Local), Path:""true"", Args:[]string{""true""}, State:(*define.InspectContainerState)(0x4000532000), Image:""a3b2e2a9704e6b54b78f4924f5b1318dbd7aa2553736944dd966cef32da5e37a"", ImageDigest:""sha25 6:a4a66434bd361d9c80cd6fd5b0ee3112a0347aed794141b372ce0c4f09afb791"", ImageName:""registry.fedoraproject.org/fedora:latest"", Rootfs:"""", Pod:"""", ResolvConfPath:"""", HostnamePath:"""", HostsPath:"""" , StaticDir:""/home/nsella/.local/share/containers/storage/overlay-containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816/userdata"", OCIConfigPath:"""", OCIRuntime:""crun"", ConmonPidFile:""/run/user/1000/containers/overlay-containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816/userdata/conmon.pid"", PidFile:""/run/user/1000/containers/overlay- containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816/userdata/pidfile"", Name:""nifty_clarke"", RestartCount:0, Driver:""overlay"", MountLabel:""system_u:object_r:container_ file_t:s0:c139,c898"", ProcessLabel:""system_u:system_r:container_t:s0:c139,c898"", AppArmorProfile:"""", EffectiveCaps:[]string{""CAP_CHOWN"", ""CAP_DAC_OVERRIDE"", ""CAP_FOWNER"", ""CAP_FSETID"", ""CAP_ KILL"", ""CAP_NET_BIND_SERVICE"", ""CAP_SETFCAP"", ""CAP_SETGID"", ""CAP_SETPCAP"", ""CAP_SETUID"", ""CAP_SYS_CHROOT""}, BoundingCaps:[]string{""CAP_CHOWN"", ""CAP_DAC_OVERRIDE"", ""CAP_FOWNER"", ""CAP_FSETID"", ""CAP_KILL"", ""CAP_NET_BIND_SERVICE"", ""CAP_SETFCAP"", ""CAP_SETGID"", ""CAP_SETPCAP"", ""CAP_SETUID"", ""CAP_SYS_CHROOT""}, ExecIDs:[]string{}, GraphDriver:(*define.DriverData)(0x40006b8978), SizeRw:( *int64)(nil), SizeRootFs:0, Mounts:[]define.InspectMount{}, Dependencies:[]string{}, NetworkSettings:(*define.InspectNetworkSettings)(0x40002bf680), Namespace:"""", IsInfra:false, IsService:fa lse, KubeExitCodePropagation:""invalid"", LockNumber:0x1, Config:(*define.InspectContainerConfig)(0x4000636600), HostConfig:(*define.InspectContainerHostConfig)(0x40006c0400)}, data ""{\""Id\"":\ ""221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816\"",\""Created\"":\""2024-05-09T15:37:24.363282572+02:00\"",\""Path\"":\""true\"",\""Args\"":[\""true\""],\""State\"":{\""OciVersion\"":\""1.2. 0\"",\""Status\"":\""created\"",\""Running\"":false,\""Paused\"":false,\""Restarting\"":false,\""OOMKilled\"":false,\""Dead\"":false,\""Pid\"":0,\""ExitCode\"":0,\""Error\"":\""\"",\""StartedAt\"":\""0001-01-01T00:00 :00Z\"",\""FinishedAt\"":\""0001-01-01T00:00:00Z\"",\""CheckpointedAt\"":\""0001-01-01T00:00:00Z\"",\""RestoredAt\"":\""0001-01-01T00:00:00Z\""},\""Image\"":\""a3b2e2a9704e6b54b78f4924f5b1318dbd7aa255373694 4dd966cef32da5e37a\"",\""ImageDigest\"":\""sha256:a4a66434bd361d9c80cd6fd5b0ee3112a0347aed794141b372ce0c4f09afb791\"",\""ImageName\"":\""registry.fedoraproject.org/fedora:latest\"",\""Rootfs\"":\""\"",\"" Pod\"":\""\"",\""ResolvConfPath\"":\""\"",\""HostnamePath\"":\""\"",\""HostsPath\"":\""\"",\""StaticDir\"":\""/home/nsella/.local/share/containers/storage/overlay-containers/221aadce464435e1eed931dffe7326d852 4e40139614e4c1ce0877463f064816/userdata\"",\""OCIRuntime\"":\""crun\"",\""ConmonPidFile\"":\""/run/user/1000/containers/overlay-containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f0 64816/userdata/conmon.pid\"",\""PidFile\"":\""/run/user/1000/containers/overlay-containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816/userdata/pidfile\"",\""Name\"":\""nifty_cl arke\"",\""RestartCount\"":0,\""Driver\"":\""overlay\"",\""MountLabel\"":\""system_u:object_r:container_file_t:s0:c139,c898\"",\""ProcessLabel\"":\""system_u:system_r:container_t:s0:c139,c898\"",\""AppArmor Profile\"":\""\"",\""EffectiveCaps\"":[\""CAP_CHOWN\"",\""CAP_DAC_OVERRIDE\"",\""CAP_FOWNER\"",\""CAP_FSETID\"",\""CAP_KILL\"",\""CAP_NET_BIND_SERVICE\"",\""CAP_SETFCAP\"",\""CAP_SETGID\"",\""CAP_SETPCAP\"",\""CAP_ SETUID\"",\""CAP_SYS_CHROOT\""],\""BoundingCaps\"":[\""CAP_CHOWN\"",\""CAP_DAC_OVERRIDE\"",\""CAP_FOWNER\"",\""CAP_FSETID\"",\""CAP_KILL\"",\""CAP_NET_BIND_SERVICE\"",\""CAP_SETFCAP\"",\""CAP_SETGID\"",\""CAP_SET PCAP\"",\""CAP_SETUID\"",\""CAP_SYS_CHROOT\""],\""ExecIDs\"":[],\""GraphDriver\"":{\""Name\"":\""overlay\"",\""Data\"":{\""LowerDir\"":\""/home/nsella/.local/share/containers/storage/overlay/e46c7a886cbfe1e67 41a81b494cf8f025358b9e74f625b1a930960d1974e7387/diff\"",\""UpperDir\"":\""/home/nsella/.local/share/containers/storage/overlay/45eb3c184e2e64464f1cdc979ca8c5f4b9beca07d7cad9eb911cd889bfab54c6/di ff\"",\""WorkDir\"":\""/home/nsella/.local/share/containers/storage/overlay/45eb3c184e2e64464f1cdc979ca8c5f4b9beca07d7cad9eb911cd889bfab54c6/work\""}},\""Mounts\"":[],\""Dependencies\"":[],\""NetworkS ettings\"":{\""EndpointID\"":\""\"",\""Gateway\"":\""\"",\""IPAddress\"":\""\"",\""IPPrefixLen\"":0,\""IPv6Gateway\"":\""\"",\""GlobalIPv6Address\"":\""\"",\""GlobalIPv6PrefixLen\"":0,\""MacAddress\"":\""\"",\""Bridge\"": \""\"",\""SandboxID\"":\""\"",\""HairpinMode\"":false,\""LinkLocalIPv6Address\"":\""\"",\""LinkLocalIPv6PrefixLen\"":0,\""Ports\"":{},\""SandboxKey\"":\""\"",\""Networks\"":{\""pasta\"":{\""EndpointID\"":\""\"",\""Gatew ay\"":\""\"",\""IPAddress\"":\""\"",\""IPPrefixLen\"":0,\""IPv6Gateway\"":\""\"",\""GlobalIPv6Address\"":\""\"",\""GlobalIPv6PrefixLen\"":0,\""MacAddress\"":\""\"",\""NetworkID\"":\""pasta\"",\""DriverOpts\"":null,\""IPA MConfig\"":null,\""Links\"":null,\""Namespace\"":\""\"",\""IsInfra\"":false,\""IsService\"":false,\""KubeExitCodePropagation\"":\""invalid\"",\""lockNumber\"":1,\""Config\"":{\""Hostname\"":\""221aadce4644\"",\ ""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""FGC=f39\"",\""PATH=/usr/lo cal/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"",\""container=oci\"",\""DISTTAG=f39container\""],\""Cmd\"":[\""true\""],\""Image\"":\""registry.fedoraproject.org/fedora:latest\"",\""Volumes\"":null ,\""WorkingDir\"":\""/\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":{\""license\"":\""MIT\"",\""name\"":\""fedora\"",\""vendor\"":\""Fedora Project\"",\""version\"":\""39\""},\""Annotations\"":{\""io.podman. annotations.cid-file\"":\""/var/tmp/cidfile\""},\""StopSignal\"":\""SIGTERM\"",\""HealthcheckOnFailureAction\"":\""none\"",\""CreateCommand\"":[\""podman-remote\"",\""run\"",\""--cidfile\"",\""/var/tmp/cidfile\ "",\""fedora:latest\"",\""true\""],\""Umask\"":\""0022\"",\""Timeout\"":0,\""StopTimeout\"":10,\""Passwd\"":true,\""sdNotifyMode\"":\""container\""},\""HostConfig\"":{\""Binds\"":[],\""CgroupManager\"":\""systemd\"",\ ""CgroupMode\"":\""private\"",\""ContainerIDFile\"":\""/var/tmp/cidfile\"",\""LogConfig\"":{\""Type\"":\""journald\"",\""Config\"":null,\""Path\"":\""\"",\""Tag\"":\""\"",\""Size\"":\""0B\""},\""NetworkMode\"":\""pasta\"", \""PortBindings\"":{},\""RestartPolicy\"":{\""Name\"":\""no\"",\""MaximumRetryCount\"":0},\""AutoRemove\"":false,\""Annotations\"":{\""io.podman.annotations.cid-file\"":\""/var/tmp/cidfile\""},\""VolumeDriver\ "":\""\"",\""VolumesFrom\"":null,\""CapAdd\"":[],\""CapDrop\"":[],\""Dns\"":[],\""DnsOptions\"":[],\""DnsSearch\"":[],\""ExtraHosts\"":[],\""GroupAdd\"":[],\""IpcMode\"":\""shareable\"",\""Cgroup\"":\""\"",\""Cgroups\"" :\""default\"",\""Links\"":null,\""OomScoreAdj\"":0,\""PidMode\"":\""private\"",\""Privileged\"":false,\""PublishAllPorts\"":false,\""ReadonlyRootfs\"":false,\""SecurityOpt\"":[],\""Tmpfs\"":{},\""UTSMode\"":\""pr ivate\"",\""UsernsMode\"":\""\"",\""ShmSize\"":65536000,\""Runtime\"":\""oci\"",\""ConsoleSize\"":[0,0],\""Isolation\"":\""\"",\""CpuShares\"":0,\""Memory\"":0,\""NanoCpus\"":0,\""CgroupParent\"":\""user.slice\"",\""Bl kioWeight\"":0,\""BlkioWeightDevice\"":null,\""BlkioDeviceReadBps\"":null,\""BlkioDeviceWriteBps\"":null,\""BlkioDeviceReadIOps\"":null,\""BlkioDeviceWriteIOps\"":null,\""CpuPeriod\"":0,\""CpuQuota\"":0,\"" CpuRealtimePeriod\"":0,\""CpuRealtimeRuntime\"":0,\""CpusetCpus\"":\""\"",\""CpusetMems\"":\""\"",\""Devices\"":[],\""DiskQuota\"":0,\""KernelMemory\"":0,\""MemoryReservation\"":0,\""MemorySwap\"":0,\""MemorySwap piness\"":0,\""OomKillDisable\"":false,\""PidsLimit\"":2048,\""Ulimits\"":[],\""CpuCount\"":0,\""CpuPercent\"":0,\""IOMaximumIOps\"":0,\""IOMaximumBandwidth\"":0,\""CgroupConf\"":null}}\n"": json: cannot unma rshal string into Go struct field InspectContainerConfig.Config.StopSignal of type uint   Describe the results you expected Command should work  podman info output yaml $ podman -v podman version 5.1.0-dev-63ab9275b   $ cat /etc/fedora-release Fedora Asahi Remix release 39 (Thirty Nine)   $ uname -a Linux applem1 6.8.8-400.asahi.fc39.aarch64+16k #1 SMP PREEMPT_DYNAMIC Tue Apr 30 02:30:34 UTC 2024 aarch64 GNU/Linux    Podman in a container Yes  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information I am aware that I am running this on `aarch64` did not try to run it on `x86_64` or other architectures yet.",source-file | source-file | test-file | test-file | source-file | source-file | test-file | test-file,"when host has podman from podman-next and podman-remote is run inside a toolbox : unmarshalling error  Issue Description I was trying to reproduce the issue https://github.com/containers/podman/issues/21974 which is valid for podman < 5 (did not bisect, take this with a grain of salt). I realized that having podman installed from podman next leads to an error.  Steps to reproduce the issue Steps to reproduce the issue 1. Install podman from the podman-next copr on the host 2. Follow the reproducer of https://github.com/containers/podman/issues/21974 until `podman-remote run` 3. Run the step `podman-remote run --cidfile /var/tmp/cidfile fedora:latest true` in the toolbox  Describe the results you received  Error: unmarshalling into &define.InspectContainerData{ID:""221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816"", Created:time.Date(2024, time.May, 9, 15, 37, 24, 363282572, time .Local), Path:""true"", Args:[]string{""true""}, State:(*define.InspectContainerState)(0x4000532000), Image:""a3b2e2a9704e6b54b78f4924f5b1318dbd7aa2553736944dd966cef32da5e37a"", ImageDigest:""sha25 6:a4a66434bd361d9c80cd6fd5b0ee3112a0347aed794141b372ce0c4f09afb791"", ImageName:""registry.fedoraproject.org/fedora:latest"", Rootfs:"""", Pod:"""", ResolvConfPath:"""", HostnamePath:"""", HostsPath:"""" , StaticDir:""/home/nsella/.local/share/containers/storage/overlay-containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816/userdata"", OCIConfigPath:"""", OCIRuntime:""crun"", ConmonPidFile:""/run/user/1000/containers/overlay-containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816/userdata/conmon.pid"", PidFile:""/run/user/1000/containers/overlay- containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816/userdata/pidfile"", Name:""nifty_clarke"", RestartCount:0, Driver:""overlay"", MountLabel:""system_u:object_r:container_ file_t:s0:c139,c898"", ProcessLabel:""system_u:system_r:container_t:s0:c139,c898"", AppArmorProfile:"""", EffectiveCaps:[]string{""CAP_CHOWN"", ""CAP_DAC_OVERRIDE"", ""CAP_FOWNER"", ""CAP_FSETID"", ""CAP_ KILL"", ""CAP_NET_BIND_SERVICE"", ""CAP_SETFCAP"", ""CAP_SETGID"", ""CAP_SETPCAP"", ""CAP_SETUID"", ""CAP_SYS_CHROOT""}, BoundingCaps:[]string{""CAP_CHOWN"", ""CAP_DAC_OVERRIDE"", ""CAP_FOWNER"", ""CAP_FSETID"", ""CAP_KILL"", ""CAP_NET_BIND_SERVICE"", ""CAP_SETFCAP"", ""CAP_SETGID"", ""CAP_SETPCAP"", ""CAP_SETUID"", ""CAP_SYS_CHROOT""}, ExecIDs:[]string{}, GraphDriver:(*define.DriverData)(0x40006b8978), SizeRw:( *int64)(nil), SizeRootFs:0, Mounts:[]define.InspectMount{}, Dependencies:[]string{}, NetworkSettings:(*define.InspectNetworkSettings)(0x40002bf680), Namespace:"""", IsInfra:false, IsService:fa lse, KubeExitCodePropagation:""invalid"", LockNumber:0x1, Config:(*define.InspectContainerConfig)(0x4000636600), HostConfig:(*define.InspectContainerHostConfig)(0x40006c0400)}, data ""{\""Id\"":\ ""221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816\"",\""Created\"":\""2024-05-09T15:37:24.363282572+02:00\"",\""Path\"":\""true\"",\""Args\"":[\""true\""],\""State\"":{\""OciVersion\"":\""1.2. 0\"",\""Status\"":\""created\"",\""Running\"":false,\""Paused\"":false,\""Restarting\"":false,\""OOMKilled\"":false,\""Dead\"":false,\""Pid\"":0,\""ExitCode\"":0,\""Error\"":\""\"",\""StartedAt\"":\""0001-01-01T00:00 :00Z\"",\""FinishedAt\"":\""0001-01-01T00:00:00Z\"",\""CheckpointedAt\"":\""0001-01-01T00:00:00Z\"",\""RestoredAt\"":\""0001-01-01T00:00:00Z\""},\""Image\"":\""a3b2e2a9704e6b54b78f4924f5b1318dbd7aa255373694 4dd966cef32da5e37a\"",\""ImageDigest\"":\""sha256:a4a66434bd361d9c80cd6fd5b0ee3112a0347aed794141b372ce0c4f09afb791\"",\""ImageName\"":\""registry.fedoraproject.org/fedora:latest\"",\""Rootfs\"":\""\"",\"" Pod\"":\""\"",\""ResolvConfPath\"":\""\"",\""HostnamePath\"":\""\"",\""HostsPath\"":\""\"",\""StaticDir\"":\""/home/nsella/.local/share/containers/storage/overlay-containers/221aadce464435e1eed931dffe7326d852 4e40139614e4c1ce0877463f064816/userdata\"",\""OCIRuntime\"":\""crun\"",\""ConmonPidFile\"":\""/run/user/1000/containers/overlay-containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f0 64816/userdata/conmon.pid\"",\""PidFile\"":\""/run/user/1000/containers/overlay-containers/221aadce464435e1eed931dffe7326d8524e40139614e4c1ce0877463f064816/userdata/pidfile\"",\""Name\"":\""nifty_cl arke\"",\""RestartCount\"":0,\""Driver\"":\""overlay\"",\""MountLabel\"":\""system_u:object_r:container_file_t:s0:c139,c898\"",\""ProcessLabel\"":\""system_u:system_r:container_t:s0:c139,c898\"",\""AppArmor Profile\"":\""\"",\""EffectiveCaps\"":[\""CAP_CHOWN\"",\""CAP_DAC_OVERRIDE\"",\""CAP_FOWNER\"",\""CAP_FSETID\"",\""CAP_KILL\"",\""CAP_NET_BIND_SERVICE\"",\""CAP_SETFCAP\"",\""CAP_SETGID\"",\""CAP_SETPCAP\"",\""CAP_ SETUID\"",\""CAP_SYS_CHROOT\""],\""BoundingCaps\"":[\""CAP_CHOWN\"",\""CAP_DAC_OVERRIDE\"",\""CAP_FOWNER\"",\""CAP_FSETID\"",\""CAP_KILL\"",\""CAP_NET_BIND_SERVICE\"",\""CAP_SETFCAP\"",\""CAP_SETGID\"",\""CAP_SET PCAP\"",\""CAP_SETUID\"",\""CAP_SYS_CHROOT\""],\""ExecIDs\"":[],\""GraphDriver\"":{\""Name\"":\""overlay\"",\""Data\"":{\""LowerDir\"":\""/home/nsella/.local/share/containers/storage/overlay/e46c7a886cbfe1e67 41a81b494cf8f025358b9e74f625b1a930960d1974e7387/diff\"",\""UpperDir\"":\""/home/nsella/.local/share/containers/storage/overlay/45eb3c184e2e64464f1cdc979ca8c5f4b9beca07d7cad9eb911cd889bfab54c6/di ff\"",\""WorkDir\"":\""/home/nsella/.local/share/containers/storage/overlay/45eb3c184e2e64464f1cdc979ca8c5f4b9beca07d7cad9eb911cd889bfab54c6/work\""}},\""Mounts\"":[],\""Dependencies\"":[],\""NetworkS ettings\"":{\""EndpointID\"":\""\"",\""Gateway\"":\""\"",\""IPAddress\"":\""\"",\""IPPrefixLen\"":0,\""IPv6Gateway\"":\""\"",\""GlobalIPv6Address\"":\""\"",\""GlobalIPv6PrefixLen\"":0,\""MacAddress\"":\""\"",\""Bridge\"": \""\"",\""SandboxID\"":\""\"",\""HairpinMode\"":false,\""LinkLocalIPv6Address\"":\""\"",\""LinkLocalIPv6PrefixLen\"":0,\""Ports\"":{},\""SandboxKey\"":\""\"",\""Networks\"":{\""pasta\"":{\""EndpointID\"":\""\"",\""Gatew ay\"":\""\"",\""IPAddress\"":\""\"",\""IPPrefixLen\"":0,\""IPv6Gateway\"":\""\"",\""GlobalIPv6Address\"":\""\"",\""GlobalIPv6PrefixLen\"":0,\""MacAddress\"":\""\"",\""NetworkID\"":\""pasta\"",\""DriverOpts\"":null,\""IPA MConfig\"":null,\""Links\"":null,\""Namespace\"":\""\"",\""IsInfra\"":false,\""IsService\"":false,\""KubeExitCodePropagation\"":\""invalid\"",\""lockNumber\"":1,\""Config\"":{\""Hostname\"":\""221aadce4644\"",\ ""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""FGC=f39\"",\""PATH=/usr/lo cal/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"",\""container=oci\"",\""DISTTAG=f39container\""],\""Cmd\"":[\""true\""],\""Image\"":\""registry.fedoraproject.org/fedora:latest\"",\""Volumes\"":null ,\""WorkingDir\"":\""/\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":{\""license\"":\""MIT\"",\""name\"":\""fedora\"",\""vendor\"":\""Fedora Project\"",\""version\"":\""39\""},\""Annotations\"":{\""io.podman. annotations.cid-file\"":\""/var/tmp/cidfile\""},\""StopSignal\"":\""SIGTERM\"",\""HealthcheckOnFailureAction\"":\""none\"",\""CreateCommand\"":[\""podman-remote\"",\""run\"",\""--cidfile\"",\""/var/tmp/cidfile\ "",\""fedora:latest\"",\""true\""],\""Umask\"":\""0022\"",\""Timeout\"":0,\""StopTimeout\"":10,\""Passwd\"":true,\""sdNotifyMode\"":\""container\""},\""HostConfig\"":{\""Binds\"":[],\""CgroupManager\"":\""systemd\"",\ ""CgroupMode\"":\""private\"",\""ContainerIDFile\"":\""/var/tmp/cidfile\"",\""LogConfig\"":{\""Type\"":\""journald\"",\""Config\"":null,\""Path\"":\""\"",\""Tag\"":\""\"",\""Size\"":\""0B\""},\""NetworkMode\"":\""pasta\"", \""PortBindings\"":{},\""RestartPolicy\"":{\""Name\"":\""no\"",\""MaximumRetryCount\"":0},\""AutoRemove\"":false,\""Annotations\"":{\""io.podman.annotations.cid-file\"":\""/var/tmp/cidfile\""},\""VolumeDriver\ "":\""\"",\""VolumesFrom\"":null,\""CapAdd\"":[],\""CapDrop\"":[],\""Dns\"":[],\""DnsOptions\"":[],\""DnsSearch\"":[],\""ExtraHosts\"":[],\""GroupAdd\"":[],\""IpcMode\"":\""shareable\"",\""Cgroup\"":\""\"",\""Cgroups\"" :\""default\"",\""Links\"":null,\""OomScoreAdj\"":0,\""PidMode\"":\""private\"",\""Privileged\"":false,\""PublishAllPorts\"":false,\""ReadonlyRootfs\"":false,\""SecurityOpt\"":[],\""Tmpfs\"":{},\""UTSMode\"":\""pr ivate\"",\""UsernsMode\"":\""\"",\""ShmSize\"":65536000,\""Runtime\"":\""oci\"",\""ConsoleSize\"":[0,0],\""Isolation\"":\""\"",\""CpuShares\"":0,\""Memory\"":0,\""NanoCpus\"":0,\""CgroupParent\"":\""user.slice\"",\""Bl kioWeight\"":0,\""BlkioWeightDevice\"":null,\""BlkioDeviceReadBps\"":null,\""BlkioDeviceWriteBps\"":null,\""BlkioDeviceReadIOps\"":null,\""BlkioDeviceWriteIOps\"":null,\""CpuPeriod\"":0,\""CpuQuota\"":0,\"" CpuRealtimePeriod\"":0,\""CpuRealtimeRuntime\"":0,\""CpusetCpus\"":\""\"",\""CpusetMems\"":\""\"",\""Devices\"":[],\""DiskQuota\"":0,\""KernelMemory\"":0,\""MemoryReservation\"":0,\""MemorySwap\"":0,\""MemorySwap piness\"":0,\""OomKillDisable\"":false,\""PidsLimit\"":2048,\""Ulimits\"":[],\""CpuCount\"":0,\""CpuPercent\"":0,\""IOMaximumIOps\"":0,\""IOMaximumBandwidth\"":0,\""CgroupConf\"":null}}\n"": json: cannot unma rshal string into Go struct field InspectContainerConfig.Config.StopSignal of type uint   Describe the results you expected Command should work  podman info output yaml $ podman -v podman version 5.1.0-dev-63ab9275b   $ cat /etc/fedora-release Fedora Asahi Remix release 39 (Thirty Nine)   $ uname -a Linux applem1 6.8.8-400.asahi.fc39.aarch64+16k #1 SMP PREEMPT_DYNAMIC Tue Apr 30 02:30:34 UTC 2024 aarch64 GNU/Linux    Podman in a container Yes  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details _No response_  Additional information I am aware that I am running this on `aarch64` did not try to run it on `x86_64` or other architectures yet. source-file source-file test-file test-file source-file source-file test-file test-file",bug,0.9
13986,podman,https://github.com/containers/podman/issues/13986,Cannot filter containers for `removing` status in API,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** Cannot filter for `removing` status in API. It looks like `restarting` is missing too, haven't tried other statuses. `is-task` seems to be a wrongfully invalid filter as well **Steps to reproduce the issue:** curl -g --unix-socket /run/podman/podman.sock 'http://d/v4.0.0/libpod/containers/json?filters={""status"":[""removing""]}' **Describe the results you received:** `{""cause"":""removing is not a valid status"",""message"":""removing is not a valid status"",""response"":500}` **Describe the results you expected:** According to the docs this is a valid status: https://docs.podman.io/en/v4.0.0/_static/api.html#operation/ContainerListLibpod **Output of `podman version`:**  Client: Podman Engine Version: 4.0.3 API Version: 4.0.3 Go Version: go1.18 Git Commit: 62534053086fdeba7b93117e7c4dc6e797835a3e Built: Mon Apr 4 05:54:02 2022 OS/Arch: linux/amd64 ",source-file | test-file,"Cannot filter containers for `removing` status in API <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** Cannot filter for `removing` status in API. It looks like `restarting` is missing too, haven't tried other statuses. `is-task` seems to be a wrongfully invalid filter as well **Steps to reproduce the issue:** curl -g --unix-socket /run/podman/podman.sock 'http://d/v4.0.0/libpod/containers/json?filters={""status"":[""removing""]}' **Describe the results you received:** `{""cause"":""removing is not a valid status"",""message"":""removing is not a valid status"",""response"":500}` **Describe the results you expected:** According to the docs this is a valid status: https://docs.podman.io/en/v4.0.0/_static/api.html#operation/ContainerListLibpod **Output of `podman version`:**  Client: Podman Engine Version: 4.0.3 API Version: 4.0.3 Go Version: go1.18 Git Commit: 62534053086fdeba7b93117e7c4dc6e797835a3e Built: Mon Apr 4 05:54:02 2022 OS/Arch: linux/amd64  source-file test-file",bug,0.95
14769,podman,https://github.com/containers/podman/issues/14769,"Podman system df --format ""{{json .}}"" doesn't output ""Size"" and ""Reclaimable"" columns","<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. podman system df 2. podman system df --format ""{{json ."" **Describe the results you received:** bash $ podman system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 89 5 59.2GB 57.58GB (97%) Containers 0 0 0B 0B (0%) Local Volumes 4 0 124MB 247.9MB (200%) $ podman system df --format ""{{json ."" {""Type"":""Images"",""Total"":89,""Active"":5}} {""Type"":""Containers"",""Total"":0,""Active"":0}} {""Type"":""Local Volumes"",""Total"":4,""Active"":0}}  **Describe the results you expected:** The `podman system df --format ""{{json .""` command should also output the `Size` and `Reclaimable` fields. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Client: Podman Engine Version: 4.1.1 API Version: 4.1.1 Go Version: go1.18.3 Built: Wed Jun 15 16:31:58 2022 OS/Arch: linux/amd64  **Output of `podman info --debug`:**  host: arch: amd64 buildahVersion: 1.26.1 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-2.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: ' cpuUtilization: idlePercent: 63.17 systemPercent: 5.89 userPercent: 30.95 cpus: 4 distribution: distribution: fedora variant: workstation version: ""36"" eventLogger: journald hostname: fedora idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.18.6-200.fc36.x86_64 linkmode: dynamic logDriver: journald memFree: 7157645312 memTotal: 16457158656 networkBackend: cni ociRuntime: name: crun package: crun-1.4.5-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.4.5 commit: c381048530aa750495cf502ddb7181f2ded5b400 spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 8589930496 swapTotal: 8589930496 uptime: 24h 39m 39.32s (Approximately 1.00 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - registry.centos.org - docker.io store: configFile: /home/joachim/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/joachim/.local/share/containers/storage graphRootAllocated: 748592037888 graphRootUsed: 119622201344 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 89 runRoot: /run/user/1000/containers volumePath: /home/joachim/.local/share/containers/storage/volumes version: APIVersion: 4.1.1 Built: 1655303518 BuiltTime: Wed Jun 15 16:31:58 2022 GitCommit: """" GoVersion: go1.18.3 Os: linux OsArch: linux/amd64 Version: 4.1.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman-4.1.1-1.fc36.x86_64  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** Physical",source-file | test-file,"Podman system df --format ""{{json .}}"" doesn't output ""Size"" and ""Reclaimable"" columns <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. podman system df 2. podman system df --format ""{{json ."" **Describe the results you received:** bash $ podman system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 89 5 59.2GB 57.58GB (97%) Containers 0 0 0B 0B (0%) Local Volumes 4 0 124MB 247.9MB (200%) $ podman system df --format ""{{json ."" {""Type"":""Images"",""Total"":89,""Active"":5}} {""Type"":""Containers"",""Total"":0,""Active"":0}} {""Type"":""Local Volumes"",""Total"":4,""Active"":0}}  **Describe the results you expected:** The `podman system df --format ""{{json .""` command should also output the `Size` and `Reclaimable` fields. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Client: Podman Engine Version: 4.1.1 API Version: 4.1.1 Go Version: go1.18.3 Built: Wed Jun 15 16:31:58 2022 OS/Arch: linux/amd64  **Output of `podman info --debug`:**  host: arch: amd64 buildahVersion: 1.26.1 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.0-2.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.0, commit: ' cpuUtilization: idlePercent: 63.17 systemPercent: 5.89 userPercent: 30.95 cpus: 4 distribution: distribution: fedora variant: workstation version: ""36"" eventLogger: journald hostname: fedora idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.18.6-200.fc36.x86_64 linkmode: dynamic logDriver: journald memFree: 7157645312 memTotal: 16457158656 networkBackend: cni ociRuntime: name: crun package: crun-1.4.5-1.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.4.5 commit: c381048530aa750495cf502ddb7181f2ded5b400 spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-0.2.beta.0.fc36.x86_64 version: |- slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 8589930496 swapTotal: 8589930496 uptime: 24h 39m 39.32s (Approximately 1.00 days) plugins: log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - registry.centos.org - docker.io store: configFile: /home/joachim/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/joachim/.local/share/containers/storage graphRootAllocated: 748592037888 graphRootUsed: 119622201344 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 89 runRoot: /run/user/1000/containers volumePath: /home/joachim/.local/share/containers/storage/volumes version: APIVersion: 4.1.1 Built: 1655303518 BuiltTime: Wed Jun 15 16:31:58 2022 GitCommit: """" GoVersion: go1.18.3 Os: linux OsArch: linux/amd64 Version: 4.1.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  podman-4.1.1-1.fc36.x86_64  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** Physical source-file test-file",bug,0.9
17762,podman,https://github.com/containers/podman/issues/17762,"REST API: missing string ""sha256"" in returned `Id` attribute ( `/v1.24/images/${img}/history` )"," Issue Description docker returns a string that starts with `sha256:`  $ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Id ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239""  podman does not  $ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Id ""d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239""   Steps to reproduce the issue 1. `sudo -i` 2. `useradd test1` 3. `usermod -aG docker test1` 4. `machinectl shell test1@` 5. `podman pull alpine:3.17.2` 6. `docker pull alpine:3.17.2` 7. run `podman images` and detect the image id. Record the result in a shell variable `img=d74e625d9115` 9. `systemctl --user start podman.socket` 10. `curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id` 11. `curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id`  Describe the results you received  [test1@localhost ~]$ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"" [test1@localhost ~]$ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id ""d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"" [test1@localhost ~]$   Describe the results you expected  [test1@localhost ~]$ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"" [test1@localhost ~]$ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"" [test1@localhost ~]$   podman info output yaml host: arch: arm64 buildahVersion: 1.29.0 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.6-3.fc37.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.6, commit: ' cpuUtilization: idlePercent: 99.65 systemPercent: 0.12 userPercent: 0.24 cpus: 1 distribution: distribution: fedora variant: coreos version: ""37"" eventLogger: journald hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 589824 size: 65536 uidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 589824 size: 65536 kernel: 6.1.14-200.fc37.aarch64 linkmode: dynamic logDriver: journald memFree: 925364224 memTotal: 2050260992 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.1-1.fc37.aarch64 path: /usr/bin/crun version: |- crun version 1.8.1 commit: f8a096be060b22ccd3d5f3ebe44108517fbf6c30 rundir: /run/user/1001/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1001/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.aarch64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 31h 18m 41.00s (Approximately 1.29 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /var/home/test1/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/test1/.local/share/containers/storage graphRootAllocated: 10132369408 graphRootUsed: 1956737024 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/user/1001/containers transientStore: false volumePath: /var/home/test1/.local/share/containers/storage/volumes version: APIVersion: 4.4.1 Built: 1676629538 BuiltTime: Fri Feb 17 10:25:38 2023 GitCommit: """" GoVersion: go1.19.5 Os: linux OsArch: linux/arm64 Version: 4.4.1   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details The commands above were run with __podman 4.4.1__ on Fedora CoreOS by using qemu on a macOS laptop (operating system: Ventura 13.2.1).  [test1@localhost ~]$ docker version Client: Version: 20.10.23 API version: 1.41 Go version: go1.19.5 Git commit: %{shortcommit_cli} Built: Sun Jan 29 17:38:04 2023 OS/Arch: linux/arm64 Context: default Experimental: true Server: Engine: Version: 20.10.23 API version: 1.41 (minimum version 1.12) Go version: go1.19.5 Git commit: %{shortcommit_moby} Built: Sun Jan 29 17:38:04 2023 OS/Arch: linux/arm64 Experimental: false containerd: Version: 1.6.15 GitCommit: runc: Version: 1.1.4 GitCommit: docker-init: Version: 0.19.0 GitCommit:   [test1@localhost ~]$ podman version Client: Podman Engine Version: 4.4.1 API Version: 4.4.1 Go Version: go1.19.5 Built: Fri Feb 17 10:25:38 2023 OS/Arch: linux/arm64   [test1@localhost ~]$ rpm -q podman podman-4.4.1-3.fc37.aarch64   Additional information I have another computer (arch: amd64) running Fedora 37. I re-run the Podman commands there with __podman 4.4.2__. The container image ID changed and the container image size changed but nothing else.",source-file | test-file | test-file,"REST API: missing string ""sha256"" in returned `Id` attribute ( `/v1.24/images/${img}/history` )  Issue Description docker returns a string that starts with `sha256:`  $ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Id ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239""  podman does not  $ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Id ""d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239""   Steps to reproduce the issue 1. `sudo -i` 2. `useradd test1` 3. `usermod -aG docker test1` 4. `machinectl shell test1@` 5. `podman pull alpine:3.17.2` 6. `docker pull alpine:3.17.2` 7. run `podman images` and detect the image id. Record the result in a shell variable `img=d74e625d9115` 9. `systemctl --user start podman.socket` 10. `curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id` 11. `curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id`  Describe the results you received  [test1@localhost ~]$ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"" [test1@localhost ~]$ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id ""d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"" [test1@localhost ~]$   Describe the results you expected  [test1@localhost ~]$ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"" [test1@localhost ~]$ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Id ""sha256:d74e625d91152966d38fe8a62c60daadb96d4b94c1a366de01fab5f334806239"" [test1@localhost ~]$   podman info output yaml host: arch: arm64 buildahVersion: 1.29.0 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.6-3.fc37.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.6, commit: ' cpuUtilization: idlePercent: 99.65 systemPercent: 0.12 userPercent: 0.24 cpus: 1 distribution: distribution: fedora variant: coreos version: ""37"" eventLogger: journald hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 589824 size: 65536 uidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 589824 size: 65536 kernel: 6.1.14-200.fc37.aarch64 linkmode: dynamic logDriver: journald memFree: 925364224 memTotal: 2050260992 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.1-1.fc37.aarch64 path: /usr/bin/crun version: |- crun version 1.8.1 commit: f8a096be060b22ccd3d5f3ebe44108517fbf6c30 rundir: /run/user/1001/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1001/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.aarch64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 31h 18m 41.00s (Approximately 1.29 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /var/home/test1/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/test1/.local/share/containers/storage graphRootAllocated: 10132369408 graphRootUsed: 1956737024 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/user/1001/containers transientStore: false volumePath: /var/home/test1/.local/share/containers/storage/volumes version: APIVersion: 4.4.1 Built: 1676629538 BuiltTime: Fri Feb 17 10:25:38 2023 GitCommit: """" GoVersion: go1.19.5 Os: linux OsArch: linux/arm64 Version: 4.4.1   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details The commands above were run with __podman 4.4.1__ on Fedora CoreOS by using qemu on a macOS laptop (operating system: Ventura 13.2.1).  [test1@localhost ~]$ docker version Client: Version: 20.10.23 API version: 1.41 Go version: go1.19.5 Git commit: %{shortcommit_cli} Built: Sun Jan 29 17:38:04 2023 OS/Arch: linux/arm64 Context: default Experimental: true Server: Engine: Version: 20.10.23 API version: 1.41 (minimum version 1.12) Go version: go1.19.5 Git commit: %{shortcommit_moby} Built: Sun Jan 29 17:38:04 2023 OS/Arch: linux/arm64 Experimental: false containerd: Version: 1.6.15 GitCommit: runc: Version: 1.1.4 GitCommit: docker-init: Version: 0.19.0 GitCommit:   [test1@localhost ~]$ podman version Client: Podman Engine Version: 4.4.1 API Version: 4.4.1 Go Version: go1.19.5 Built: Fri Feb 17 10:25:38 2023 OS/Arch: linux/arm64   [test1@localhost ~]$ rpm -q podman podman-4.4.1-3.fc37.aarch64   Additional information I have another computer (arch: amd64) running Fedora 37. I re-run the Podman commands there with __podman 4.4.2__. The container image ID changed and the container image size changed but nothing else. source-file test-file test-file",bug,0.95
14208,podman,https://github.com/containers/podman/issues/14208,Image conflict should return HTTP 409,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind feature **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. Attempt to delete image which is still being used by a container using `DELETE /images/sha256:xxxx` **Describe the results you received:** API returns HTTP 500. **Describe the results you expected:** Given the Docker API returns the more appropriate HTTP 409 (CONFLICT), I expected Podman to return 409 as well. **Additional information you deem important (e.g. issue happens only occasionally):** I think it would not only be more appropriate to use a HTTP response with the matching description, but I too think it would help compatibility with Docker API clients. In my case I am using the Docker Java API, which would return a appropriate conflict exception on Docker's 409 response, whereas Podman's 500 results in a very generic server error.",source-file | test-file | source-file | test-file | source-file | test-file,"Image conflict should return HTTP 409 <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind feature **Description** <!-- Briefly describe the problem you are having in a few paragraphs. --> **Steps to reproduce the issue:** 1. Attempt to delete image which is still being used by a container using `DELETE /images/sha256:xxxx` **Describe the results you received:** API returns HTTP 500. **Describe the results you expected:** Given the Docker API returns the more appropriate HTTP 409 (CONFLICT), I expected Podman to return 409 as well. **Additional information you deem important (e.g. issue happens only occasionally):** I think it would not only be more appropriate to use a HTTP response with the matching description, but I too think it would help compatibility with Docker API clients. In my case I am using the Docker Java API, which would return a appropriate conflict exception on Docker's 409 response, whereas Podman's 500 results in a very generic server error. source-file test-file source-file test-file source-file test-file",bug,0.95
23981,podman,https://github.com/containers/podman/issues/23981,Processes top api service incompatibility, Issue Description Describe your issue The compat endpoints do not behave the same - Api docs of docker <https://docs.docker.com/reference/api/engine/version/v1.39/#tag/Container/operation/ContainerTop> - Api docs of podman <https://docs.podman.io/en/latest/_static/api.html#tag/containers-(compat)/operation/ContainerTop>  Steps to reproduce the issue Steps to reproduce the issue 1. Get processes list using libpod compat endpoint 2. Get processes list using docker api engine endpoint 3. Compare  Describe the results you received Describe the results you received Received a list of strings  Describe the results you expected Describe the results you expected Receive a list of lists of strings  podman info output yaml Podman is working fine on 5.2.2   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details It happens on any environment  Additional information _No response_,source-file | test-file,Processes top api service incompatibility  Issue Description Describe your issue The compat endpoints do not behave the same - Api docs of docker <https://docs.docker.com/reference/api/engine/version/v1.39/#tag/Container/operation/ContainerTop> - Api docs of podman <https://docs.podman.io/en/latest/_static/api.html#tag/containers-(compat)/operation/ContainerTop>  Steps to reproduce the issue Steps to reproduce the issue 1. Get processes list using libpod compat endpoint 2. Get processes list using docker api engine endpoint 3. Compare  Describe the results you received Describe the results you received Received a list of strings  Describe the results you expected Describe the results you expected Receive a list of lists of strings  podman info output yaml Podman is working fine on 5.2.2   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details It happens on any environment  Additional information _No response_ source-file test-file,bug,0.9
13831,podman,https://github.com/containers/podman/issues/13831,API build: remote parameter does not work,"I'm trying to build a docker file from my [repository](https://github.com/matheusfenolio/poc), but I'm receiving an error when I call the endpoint `curl -X POST http://localhost:8080/v2.0.0/libpod/build?remote=https%3A%2F%2Fgithub.com%2Fmatheusfenolio%2Fpoc` json { ""cause"": ""stat /var/tmp/libpod_builder4161196413/build/Containerfile: no such file or directory"", ""message"": ""failed to parse query parameter 'dockerfile': \""Dockerfile\"": stat /var/tmp/libpod_builder4161196413/build/Containerfile: no such file or directory"", ""response"": 400 }{ ""errorDetail"": { ""message"": ""stat /var/tmp/libpod_builder4161196413/build/Dockerfile: no such file or directory\n"" }, ""error"": ""stat /var/tmp/libpod_builder4161196413/build/Dockerfile: no such file or directory\n"" }  API reference: https://docs.podman.io/en/v3.2.3/_static/api.html#operation/ImageBuildLibpod",source-file | test-file | source-file | test-file,"API build: remote parameter does not work I'm trying to build a docker file from my [repository](https://github.com/matheusfenolio/poc), but I'm receiving an error when I call the endpoint `curl -X POST http://localhost:8080/v2.0.0/libpod/build?remote=https%3A%2F%2Fgithub.com%2Fmatheusfenolio%2Fpoc` json { ""cause"": ""stat /var/tmp/libpod_builder4161196413/build/Containerfile: no such file or directory"", ""message"": ""failed to parse query parameter 'dockerfile': \""Dockerfile\"": stat /var/tmp/libpod_builder4161196413/build/Containerfile: no such file or directory"", ""response"": 400 }{ ""errorDetail"": { ""message"": ""stat /var/tmp/libpod_builder4161196413/build/Dockerfile: no such file or directory\n"" }, ""error"": ""stat /var/tmp/libpod_builder4161196413/build/Dockerfile: no such file or directory\n"" }  API reference: https://docs.podman.io/en/v3.2.3/_static/api.html#operation/ImageBuildLibpod source-file test-file source-file test-file",bug,0.9
14647,podman,https://github.com/containers/podman/issues/14647,containers list api route response header is text/plain instead of application/json,"/kind bug **Description** Response content-type header is not `application/json` for containers list endpoint, although negotiation requests it. Some client libraries do attempt deserializing automatically to their data structures. Although it can be mitigated as an exception for this route, it should respect the negotiation. (v3 and v4 have the same behavior) **Steps to reproduce the issue:** 1. base url `v3.0.0` - `curl -v -X GET --unix-socket ""/run/user/1000/podman/podman.sock"" ""http://d/v3.0.0/libpod/containers/json"" -H ""Accept: application/json"" -H ""Content-Type: application/json""` 2. base url `v4.0.0` - `curl -v -X GET --unix-socket ""/run/user/1000/podman/podman.sock"" ""http://d/v4.0.0/libpod/containers/json"" -H ""Accept: application/json"" -H ""Content-Type: application/json""` **Describe the results you received:**  > GET /v4.0.0/libpod/containers/json HTTP/1.1  < Content-Type: text/plain; charset=us-ascii < Libpod-Api-Version: 4.1.0   **Describe the results you expected:**  > GET /v4.0.0/libpod/containers/json HTTP/1.1  < Content-Type: application/json; charset=us-ascii < Libpod-Api-Version: 4.1.0   If any use, issued the same for docker api and it does have `application/json` as response content-type header  docker --version Docker version 20.10.16, build aa7e414fdc   curl -v -X GET --unix-socket ""/var/run/docker.sock"" ""http://localhost/containers/json"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" Note: Unnecessary use of -X or --request, GET is already inferred. * Trying /var/run/docker.sock:0 * Connected to localhost (/run/docker.sock) port 80 (#0) > GET /containers/json HTTP/1.1 > Host: localhost > User-Agent: curl/7.83.1 > Accept: application/json > Content-Type: application/json > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Docker-Experimental: false < Ostype: linux < Server: Docker/20.10.16 (linux) < Date: Fri, 17 Jun 2022 15:57:22 GMT < Transfer-Encoding: chunked  > IMPORTANT EDIT - It only happens when the list of containers is empty, as soon as at least one is present it respects!",source-file | test-file,"containers list api route response header is text/plain instead of application/json /kind bug **Description** Response content-type header is not `application/json` for containers list endpoint, although negotiation requests it. Some client libraries do attempt deserializing automatically to their data structures. Although it can be mitigated as an exception for this route, it should respect the negotiation. (v3 and v4 have the same behavior) **Steps to reproduce the issue:** 1. base url `v3.0.0` - `curl -v -X GET --unix-socket ""/run/user/1000/podman/podman.sock"" ""http://d/v3.0.0/libpod/containers/json"" -H ""Accept: application/json"" -H ""Content-Type: application/json""` 2. base url `v4.0.0` - `curl -v -X GET --unix-socket ""/run/user/1000/podman/podman.sock"" ""http://d/v4.0.0/libpod/containers/json"" -H ""Accept: application/json"" -H ""Content-Type: application/json""` **Describe the results you received:**  > GET /v4.0.0/libpod/containers/json HTTP/1.1  < Content-Type: text/plain; charset=us-ascii < Libpod-Api-Version: 4.1.0   **Describe the results you expected:**  > GET /v4.0.0/libpod/containers/json HTTP/1.1  < Content-Type: application/json; charset=us-ascii < Libpod-Api-Version: 4.1.0   If any use, issued the same for docker api and it does have `application/json` as response content-type header  docker --version Docker version 20.10.16, build aa7e414fdc   curl -v -X GET --unix-socket ""/var/run/docker.sock"" ""http://localhost/containers/json"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" Note: Unnecessary use of -X or --request, GET is already inferred. * Trying /var/run/docker.sock:0 * Connected to localhost (/run/docker.sock) port 80 (#0) > GET /containers/json HTTP/1.1 > Host: localhost > User-Agent: curl/7.83.1 > Accept: application/json > Content-Type: application/json > * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < Api-Version: 1.41 < Content-Type: application/json < Docker-Experimental: false < Ostype: linux < Server: Docker/20.10.16 (linux) < Date: Fri, 17 Jun 2022 15:57:22 GMT < Transfer-Encoding: chunked  > IMPORTANT EDIT - It only happens when the list of containers is empty, as soon as at least one is present it respects! source-file test-file",bug,0.95
17524,podman,https://github.com/containers/podman/issues/17524,podman socket API top returns Titles as single string instead of array of strings," Issue Description I'm using RHEL 9.1 and podman 4.2.0 with crun 1.5. I was trying to get a container functional with is based on [newrelic infrastructure agent](https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/). The agent was tested with docker initially, but I wanted to test it using podman. As the container requires the docker socket to be mounted into the container I started the podman.socket. However I noticed that there were errors regarding querying container processes using the `docker top` from within the container. That is the container queries the REST API of the podman.socket for processes. Curious on what why this is failing I investigated the podman REST endpoint and found that the GET request documented here: https://docs.podman.io/en/latest/_static/api.html#tag/containers-(compat)/operation/ContainerList behaves different for the ""Titles"" field in the response. This should be an array of strings, however using: curl --silent -XGET --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock -H 'Content-Type: application/json' http://localhost/containers/321be6b5b7264ee5dfe651fe78355c3ce450c3167f245136bcf5cb66ae9107fe/top delivers ONE string for ""Titles"": { ,""Titles"":[**""PID USER TIME COMMAND""**]} However this is ONE string and not an array as stated in the documentation.  Steps to reproduce the issue Steps to reproduce the issue 1. systemctl --user start podman.socket 2. start at least one container and note the id of that container 3. curl --silent -XGET --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock -H 'Content-Type: application/json' http://localhost/containers/<CONTAINER_ID_REPLACE_ME>/top  Describe the results you received The ""Titles"" from the response is: { ,""Titles"":[**""PID USER TIME COMMAND""**]}  Describe the results you expected The ""Titles"" field should be: ""Titles"":[**""PID"", ""USER"", ""TIME"", ""COMMAND""**]}  podman info output yaml host: arch: amd64 buildahVersion: 1.27.3 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.4-1.el9.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.4, commit: 56561007b6a59ea175ee9a67384639721499e160' cpuUtilization: idlePercent: 99.66 systemPercent: 0.08 userPercent: 0.26 cpus: 8 distribution: distribution: '""rhel""' version: ""9.1"" eventLogger: journald hostname: noname idMappings: gidmap: - container_id: 0 host_id: 2019 size: 1 - container_id: 1 host_id: 1476256 size: 65536 uidmap: - container_id: 0 host_id: 2019 size: 1 - container_id: 1 host_id: 1476256 size: 65536 kernel: 5.14.0-162.12.1.el9_1.x86_64 linkmode: dynamic logDriver: journald memFree: 132173471744 memTotal: 134531371008 networkBackend: netavark ociRuntime: name: crun package: crun-1.5-1.el9.x86_64 path: /usr/bin/crun version: |- crun version 1.5 commit: 54ebb8ca8bf7e6ddae2eb919f5b82d1d96863dea spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/user/2019/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-2.el9_0.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 4294963200 swapTotal: 4294963200 uptime: 73h 7m 7.00s (Approximately 3.04 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.access.redhat.com - registry.redhat.io - docker.io store: configFile: /home/noname/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/noname/.local/share/containers/storage graphRootAllocated: 33246150656 graphRootUsed: 2582650880 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 4 runRoot: /run/user/2019/containers volumePath: /home/noname/.local/share/containers/storage/volumes version: APIVersion: 4.2.0 Built: 1670843566 BuiltTime: Mon Dec 12 12:12:46 2022 GitCommit: """" GoVersion: go1.18.4 Os: linux OsArch: linux/amd64 Version: 4.2.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release No  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",source-file | source-file | source-file | test-file | source-file | source-file | source-file | test-file,"podman socket API top returns Titles as single string instead of array of strings  Issue Description I'm using RHEL 9.1 and podman 4.2.0 with crun 1.5. I was trying to get a container functional with is based on [newrelic infrastructure agent](https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/). The agent was tested with docker initially, but I wanted to test it using podman. As the container requires the docker socket to be mounted into the container I started the podman.socket. However I noticed that there were errors regarding querying container processes using the `docker top` from within the container. That is the container queries the REST API of the podman.socket for processes. Curious on what why this is failing I investigated the podman REST endpoint and found that the GET request documented here: https://docs.podman.io/en/latest/_static/api.html#tag/containers-(compat)/operation/ContainerList behaves different for the ""Titles"" field in the response. This should be an array of strings, however using: curl --silent -XGET --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock -H 'Content-Type: application/json' http://localhost/containers/321be6b5b7264ee5dfe651fe78355c3ce450c3167f245136bcf5cb66ae9107fe/top delivers ONE string for ""Titles"": { ,""Titles"":[**""PID USER TIME COMMAND""**]} However this is ONE string and not an array as stated in the documentation.  Steps to reproduce the issue Steps to reproduce the issue 1. systemctl --user start podman.socket 2. start at least one container and note the id of that container 3. curl --silent -XGET --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock -H 'Content-Type: application/json' http://localhost/containers/<CONTAINER_ID_REPLACE_ME>/top  Describe the results you received The ""Titles"" from the response is: { ,""Titles"":[**""PID USER TIME COMMAND""**]}  Describe the results you expected The ""Titles"" field should be: ""Titles"":[**""PID"", ""USER"", ""TIME"", ""COMMAND""**]}  podman info output yaml host: arch: amd64 buildahVersion: 1.27.3 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.4-1.el9.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.4, commit: 56561007b6a59ea175ee9a67384639721499e160' cpuUtilization: idlePercent: 99.66 systemPercent: 0.08 userPercent: 0.26 cpus: 8 distribution: distribution: '""rhel""' version: ""9.1"" eventLogger: journald hostname: noname idMappings: gidmap: - container_id: 0 host_id: 2019 size: 1 - container_id: 1 host_id: 1476256 size: 65536 uidmap: - container_id: 0 host_id: 2019 size: 1 - container_id: 1 host_id: 1476256 size: 65536 kernel: 5.14.0-162.12.1.el9_1.x86_64 linkmode: dynamic logDriver: journald memFree: 132173471744 memTotal: 134531371008 networkBackend: netavark ociRuntime: name: crun package: crun-1.5-1.el9.x86_64 path: /usr/bin/crun version: |- crun version 1.5 commit: 54ebb8ca8bf7e6ddae2eb919f5b82d1d96863dea spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux remoteSocket: exists: true path: /run/user/2019/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-2.el9_0.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 4294963200 swapTotal: 4294963200 uptime: 73h 7m 7.00s (Approximately 3.04 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.access.redhat.com - registry.redhat.io - docker.io store: configFile: /home/noname/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/noname/.local/share/containers/storage graphRootAllocated: 33246150656 graphRootUsed: 2582650880 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 4 runRoot: /run/user/2019/containers volumePath: /home/noname/.local/share/containers/storage/volumes version: APIVersion: 4.2.0 Built: 1670843566 BuiltTime: Mon Dec 12 12:12:46 2022 GitCommit: """" GoVersion: go1.18.4 Os: linux OsArch: linux/amd64 Version: 4.2.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release No  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting source-file source-file source-file test-file source-file source-file source-file test-file",bug,0.95
23163,podman,https://github.com/containers/podman/issues/23163,Podman manifest inspect does not display remote annotations," Issue Description Related to [discussions in thread](https://github.com/containers/podman/discussions/23140) If you run `podman manifest inspect quay.io/giuseppe/zstd-chunked:fedora-manifest` on any remote manifest, you are unable to see the annotations which were added to the zstd:chunked manifest. Compare the output of the following commands:  $ skopeo inspect --raw docker://quay.io/giuseppe/zstd-chunked:fedora-manifest | jq .   $ podman manifest inspect quay.io/giuseppe/zstd-chunked:fedora-manifest   Steps to reproduce the issue See above. 1. Push a manifest containing an image with annotations to any remote registry 2. Run `podman manifest inspect` on the local manifest (and see annotations) 3. Run `podman manifest inspect` on the remote manifest (and see no annotations)  Describe the results you received As above  Describe the results you expected I would expect `podman manifest inspect` to show all annotations on the manifests, since an annotation is a requirement for zstd images. When running `skopeo inspect --raw`, we can see the annotations, so I expected Podman to return the same.  podman info output yaml host: arch: amd64 buildahVersion: 1.36.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.10-1.fc40.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.10, commit: ' cpuUtilization: idlePercent: 96.86 systemPercent: 1.56 userPercent: 1.58 cpus: 24 databaseBackend: sqlite distribution: distribution: fedora variant: silverblue version: ""40"" eventLogger: journald freeLocks: 2045 hostname: fedora idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 kernel: 6.8.11-300.fc40.x86_64 linkmode: dynamic logDriver: journald memFree: 13290414080 memTotal: 33364271104 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.11.0-1.fc40.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.11.0 package: netavark-1.11.0-1.fc40.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.11.0 ociRuntime: name: crun package: crun-1.15-1.fc40.x86_64 path: /usr/bin/crun version: |- crun version 1.15 commit: e6eacaf4034e84185fd8780ac9262bbf57082278 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20240624.g1ee2eca-1.fc40.x86_64 version: | pasta 0^20240624.g1ee2eca-1.fc40.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: false path: /run/user/1000/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-2.fc40.x86_64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.5 swapFree: 8589930496 swapTotal: 8589930496 uptime: 2h 16m 4.00s (Approximately 0.08 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: ghcr.io/rsturla: Blocked: false Insecure: false Location: ghcr.io/rsturla MirrorByDigestOnly: false Mirrors: - Insecure: true Location: localhost:5000/rsturla PullFromMirror: """" Prefix: ghcr.io/rsturla PullFromMirror: """" localhost:5000: Blocked: false Insecure: true Location: localhost:5000 MirrorByDigestOnly: false Mirrors: null Prefix: localhost:5000 PullFromMirror: """" search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /var/home/admin/.config/containers/storage.conf containerStore: number: 1 paused: 0 running: 0 stopped: 1 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/admin/.local/share/containers/storage graphRootAllocated: 1998678130688 graphRootUsed: 285504368640 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 9 runRoot: /run/user/1000/containers transientStore: false volumePath: /var/home/admin/.local/share/containers/storage/volumes version: APIVersion: 5.1.1 Built: 1717459200 BuiltTime: Tue Jun 4 01:00:00 2024 GitCommit: """" GoVersion: go1.22.3 Os: linux OsArch: linux/amd64 Version: 5.1.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details N/A  Additional information N/A",source-file | source-file | source-file | source-file | source-file | test-file,"Podman manifest inspect does not display remote annotations  Issue Description Related to [discussions in thread](https://github.com/containers/podman/discussions/23140) If you run `podman manifest inspect quay.io/giuseppe/zstd-chunked:fedora-manifest` on any remote manifest, you are unable to see the annotations which were added to the zstd:chunked manifest. Compare the output of the following commands:  $ skopeo inspect --raw docker://quay.io/giuseppe/zstd-chunked:fedora-manifest | jq .   $ podman manifest inspect quay.io/giuseppe/zstd-chunked:fedora-manifest   Steps to reproduce the issue See above. 1. Push a manifest containing an image with annotations to any remote registry 2. Run `podman manifest inspect` on the local manifest (and see annotations) 3. Run `podman manifest inspect` on the remote manifest (and see no annotations)  Describe the results you received As above  Describe the results you expected I would expect `podman manifest inspect` to show all annotations on the manifests, since an annotation is a requirement for zstd images. When running `skopeo inspect --raw`, we can see the annotations, so I expected Podman to return the same.  podman info output yaml host: arch: amd64 buildahVersion: 1.36.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.10-1.fc40.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.10, commit: ' cpuUtilization: idlePercent: 96.86 systemPercent: 1.56 userPercent: 1.58 cpus: 24 databaseBackend: sqlite distribution: distribution: fedora variant: silverblue version: ""40"" eventLogger: journald freeLocks: 2045 hostname: fedora idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 524288 size: 65536 kernel: 6.8.11-300.fc40.x86_64 linkmode: dynamic logDriver: journald memFree: 13290414080 memTotal: 33364271104 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.11.0-1.fc40.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.11.0 package: netavark-1.11.0-1.fc40.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.11.0 ociRuntime: name: crun package: crun-1.15-1.fc40.x86_64 path: /usr/bin/crun version: |- crun version 1.15 commit: e6eacaf4034e84185fd8780ac9262bbf57082278 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20240624.g1ee2eca-1.fc40.x86_64 version: | pasta 0^20240624.g1ee2eca-1.fc40.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: false path: /run/user/1000/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-2.fc40.x86_64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.5 swapFree: 8589930496 swapTotal: 8589930496 uptime: 2h 16m 4.00s (Approximately 0.08 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: ghcr.io/rsturla: Blocked: false Insecure: false Location: ghcr.io/rsturla MirrorByDigestOnly: false Mirrors: - Insecure: true Location: localhost:5000/rsturla PullFromMirror: """" Prefix: ghcr.io/rsturla PullFromMirror: """" localhost:5000: Blocked: false Insecure: true Location: localhost:5000 MirrorByDigestOnly: false Mirrors: null Prefix: localhost:5000 PullFromMirror: """" search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io store: configFile: /var/home/admin/.config/containers/storage.conf containerStore: number: 1 paused: 0 running: 0 stopped: 1 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/admin/.local/share/containers/storage graphRootAllocated: 1998678130688 graphRootUsed: 285504368640 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 9 runRoot: /run/user/1000/containers transientStore: false volumePath: /var/home/admin/.local/share/containers/storage/volumes version: APIVersion: 5.1.1 Built: 1717459200 BuiltTime: Tue Jun 4 01:00:00 2024 GitCommit: """" GoVersion: go1.22.3 Os: linux OsArch: linux/amd64 Version: 5.1.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details N/A  Additional information N/A source-file source-file source-file source-file source-file test-file",bug,0.9
20469,podman,https://github.com/containers/podman/issues/20469,"Error when use ""image prune --all"" via forwarding API and docker cli"," Issue Description If an image exists in Podman that is no longer in use and the command ""docker image prune -a"" is executed via the Forward API and Docker CLI, the error message ""Error response from daemon: specifying ""dangling"" filter more than once with different values is not supported"" appears. If the command is executed via ""podman image prune -a"", the cleanup works correctly.  Steps to reproduce the issue Steps to reproduce the issue 1. Download Image `docker pull nginx` 2. Clean Images `docker image prune -a`  Describe the results you received Error Message: Error response from daemon: specifying ""dangling"" filter more than once with different values is not supported  Describe the results you expected The image should be removed.  podman info output yaml host: arch: amd64 buildahVersion: 1.32.0 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma - misc cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.7-2.fc38.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 99.64 systemPercent: 0.2 userPercent: 0.15 cpus: 4 databaseBackend: boltdb distribution: distribution: fedora variant: container version: ""38"" eventLogger: journald freeLocks: 2048 hostname: VGENERAL96 idMappings: gidmap: null uidmap: null kernel: 5.15.133.1-microsoft-standard-WSL2 linkmode: dynamic logDriver: journald memFree: 7665016832 memTotal: 10413207552 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.8.0-1.fc38.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.8.0 package: netavark-1.8.0-2.fc38.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.8.0 ociRuntime: name: crun package: crun-1.10-1.fc38.x86_64 path: /usr/bin/crun version: |- crun version 1.10 commit: c053c83c57551bca13ead8600237341818975974 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20231004.gf851084-1.fc38.x86_64 version: | pasta 0^20231004.gf851084-1.fc38.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-1.fc38.x86_64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 3221225472 swapTotal: 3221225472 uptime: 2h 34m 17.00s (Approximately 0.08 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 1081101176832 graphRootUsed: 2787717120 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 47 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.7.0 Built: 1695839078 BuiltTime: Wed Sep 27 20:24:38 2023 GitCommit: """" GoVersion: go1.20.8 Os: linux OsArch: linux/amd64 Version: 4.7.0   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details OS: Windows 2022 WSL version: 2.0.6.0 Kernel version: 5.15.133.1-1 WSLg version: 1.0.59 MSRDC version: 1.2.4677 Direct3D version: 1.611.1-81528511 DXCore version: 10.0.25880.1000-230602-1350.main Windows version: 10.0.20348.1787 Podman Version: 4.7.1 Docker CLI Version 24.0.6, build ed223bc",source-file | test-file,"Error when use ""image prune --all"" via forwarding API and docker cli  Issue Description If an image exists in Podman that is no longer in use and the command ""docker image prune -a"" is executed via the Forward API and Docker CLI, the error message ""Error response from daemon: specifying ""dangling"" filter more than once with different values is not supported"" appears. If the command is executed via ""podman image prune -a"", the cleanup works correctly.  Steps to reproduce the issue Steps to reproduce the issue 1. Download Image `docker pull nginx` 2. Clean Images `docker image prune -a`  Describe the results you received Error Message: Error response from daemon: specifying ""dangling"" filter more than once with different values is not supported  Describe the results you expected The image should be removed.  podman info output yaml host: arch: amd64 buildahVersion: 1.32.0 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma - misc cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.7-2.fc38.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 99.64 systemPercent: 0.2 userPercent: 0.15 cpus: 4 databaseBackend: boltdb distribution: distribution: fedora variant: container version: ""38"" eventLogger: journald freeLocks: 2048 hostname: VGENERAL96 idMappings: gidmap: null uidmap: null kernel: 5.15.133.1-microsoft-standard-WSL2 linkmode: dynamic logDriver: journald memFree: 7665016832 memTotal: 10413207552 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.8.0-1.fc38.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.8.0 package: netavark-1.8.0-2.fc38.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.8.0 ociRuntime: name: crun package: crun-1.10-1.fc38.x86_64 path: /usr/bin/crun version: |- crun version 1.10 commit: c053c83c57551bca13ead8600237341818975974 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20231004.gf851084-1.fc38.x86_64 version: | pasta 0^20231004.gf851084-1.fc38.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-1.fc38.x86_64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 3221225472 swapTotal: 3221225472 uptime: 2h 34m 17.00s (Approximately 0.08 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 1081101176832 graphRootUsed: 2787717120 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 47 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.7.0 Built: 1695839078 BuiltTime: Wed Sep 27 20:24:38 2023 GitCommit: """" GoVersion: go1.20.8 Os: linux OsArch: linux/amd64 Version: 4.7.0   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details OS: Windows 2022 WSL version: 2.0.6.0 Kernel version: 5.15.133.1-1 WSLg version: 1.0.59 MSRDC version: 1.2.4677 Direct3D version: 1.611.1-81528511 DXCore version: 10.0.25880.1000-230602-1350.main Windows version: 10.0.20348.1787 Podman Version: 4.7.1 Docker CLI Version 24.0.6, build ed223bc source-file test-file",bug,0.9
24886,podman,https://github.com/containers/podman/issues/24886,"Podman REST API /libpod/containers/create ""r_limits"" is type integer <uint64>"," Issue Description https://docs.podman.io/en/latest/_static/api.html#tag/containers/operation/ContainerCreateLibpod r_limits hard integer <uint64> Hard is the hard limit for the specified type soft integer <uint64> Soft is the soft limit for the specified type There is no direct reference to Ulimits. https://github.com/containers/podman/pull/19879 In PR 19879 Podman added support for passing Ulimits as -1 to mean min / max  Steps to reproduce the issue /podman-py containers_create https://github.com/containers/podman-py/blob/main/podman/domain/containers_create.py  for item in args.pop(""ulimits"", []): params[""r_limits""].append( { ""type"": item[""Name""], ""hard"": item[""Hard""], ""soft"": item[""Soft""], } )  Code Example `client.containers.create(image=img, command=['/bin/bash'], ulimits=[{""Name"": ""memlock"", ""Soft"": -1, ""Hard"": -1}])`  Describe the results you received `podman.errors.exceptions.APIError: 500 Server Error: Internal Server Error (decode(): json: cannot unmarshal number -1 into Go struct field POSIXRlimit.r_limits.hard of type uint64)`  Describe the results you expected Expected successful creation of container with memlock min/max set to maximum values.  podman info output yaml [root@omitted]# podman info host: arch: amd64 buildahVersion: 1.33.11 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma cgroupManager: systemd cgroupVersion: v1 conmon: package: conmon-2.1.10-1.module+el8.10.0+90449+0b7c8529.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.10, commit: 753128cb76d643886a978dba99fab8017289372d' cpuUtilization: idlePercent: 99.97 systemPercent: 0.01 userPercent: 0.02 cpus: 56 databaseBackend: sqlite distribution: distribution: ol variant: server version: ""8.3"" eventLogger: file freeLocks: 2047 hostname: omitted idMappings: gidmap: null uidmap: null kernel: 5.4.17-2011.7.4.el8uek.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 142581444608 memTotal: 200959377408 networkBackend: cni networkBackendInfo: backend: cni dns: package: podman-plugins-4.9.4-18.0.1.module+el8.10.0+90449+0b7c8529.x86_64 path: /usr/libexec/cni/dnsname version: |- CNI dnsname plugin version: 1.4.0-dev commit: unknown CNI protocol versions supported: 0.1.0, 0.2.0, 0.3.0, 0.3.1, 0.4.0, 1.0.0 package: containernetworking-plugins-1.4.0-5.module+el8.10.0+90449+0b7c8529.x86_64 path: /usr/libexec/cni ociRuntime: name: runc package: runc-1.1.12-5.module+el8.10.0+90449+0b7c8529.x86_64 path: /usr/bin/runc version: |- runc version 1.1.12 spec: 1.0.2-dev go: go1.22.7 (Red Hat 1.22.7-1.module+el8.10.0+90426+810ab996) libseccomp: 2.5.2 os: linux pasta: executable: """" package: """" version: """" remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /bin/slirp4netns package: slirp4netns-1.2.0-2.module+el8.8.0+21045+adcb6a64.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 4294963200 swapTotal: 4294963200 uptime: 1341h 26m 16.00s (Approximately 55.88 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - container-registry.oracle.com - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 1 paused: 0 running: 1 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 75125227520 graphRootUsed: 63892619264 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""false"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 33 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.9.4-rhel Built: 1732729681 BuiltTime: Wed Nov 27 17:48:01 2024 GitCommit: """" GoVersion: go1.22.7 (Red Hat 1.22.7-1.module+el8.10.0+90426+810ab996) Os: linux OsArch: linux/amd64 Version: 4.9.4-rhel   Podman in a container Yes  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",source-file | test-file | test-file | source-file | source-file | test-file | test-file | source-file | source-file | test-file | test-file | source-file | source-file | test-file | test-file | source-file | source-file | test-file | test-file | source-file | source-file | test-file | test-file | source-file | source-file | test-file | test-file | source-file | test-file | test-file | source-file | test-file | test-file,"Podman REST API /libpod/containers/create ""r_limits"" is type integer <uint64>  Issue Description https://docs.podman.io/en/latest/_static/api.html#tag/containers/operation/ContainerCreateLibpod r_limits hard integer <uint64> Hard is the hard limit for the specified type soft integer <uint64> Soft is the soft limit for the specified type There is no direct reference to Ulimits. https://github.com/containers/podman/pull/19879 In PR 19879 Podman added support for passing Ulimits as -1 to mean min / max  Steps to reproduce the issue /podman-py containers_create https://github.com/containers/podman-py/blob/main/podman/domain/containers_create.py  for item in args.pop(""ulimits"", []): params[""r_limits""].append( { ""type"": item[""Name""], ""hard"": item[""Hard""], ""soft"": item[""Soft""], } )  Code Example `client.containers.create(image=img, command=['/bin/bash'], ulimits=[{""Name"": ""memlock"", ""Soft"": -1, ""Hard"": -1}])`  Describe the results you received `podman.errors.exceptions.APIError: 500 Server Error: Internal Server Error (decode(): json: cannot unmarshal number -1 into Go struct field POSIXRlimit.r_limits.hard of type uint64)`  Describe the results you expected Expected successful creation of container with memlock min/max set to maximum values.  podman info output yaml [root@omitted]# podman info host: arch: amd64 buildahVersion: 1.33.11 cgroupControllers: - cpuset - cpu - cpuacct - blkio - memory - devices - freezer - net_cls - perf_event - net_prio - hugetlb - pids - rdma cgroupManager: systemd cgroupVersion: v1 conmon: package: conmon-2.1.10-1.module+el8.10.0+90449+0b7c8529.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.10, commit: 753128cb76d643886a978dba99fab8017289372d' cpuUtilization: idlePercent: 99.97 systemPercent: 0.01 userPercent: 0.02 cpus: 56 databaseBackend: sqlite distribution: distribution: ol variant: server version: ""8.3"" eventLogger: file freeLocks: 2047 hostname: omitted idMappings: gidmap: null uidmap: null kernel: 5.4.17-2011.7.4.el8uek.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 142581444608 memTotal: 200959377408 networkBackend: cni networkBackendInfo: backend: cni dns: package: podman-plugins-4.9.4-18.0.1.module+el8.10.0+90449+0b7c8529.x86_64 path: /usr/libexec/cni/dnsname version: |- CNI dnsname plugin version: 1.4.0-dev commit: unknown CNI protocol versions supported: 0.1.0, 0.2.0, 0.3.0, 0.3.1, 0.4.0, 1.0.0 package: containernetworking-plugins-1.4.0-5.module+el8.10.0+90449+0b7c8529.x86_64 path: /usr/libexec/cni ociRuntime: name: runc package: runc-1.1.12-5.module+el8.10.0+90449+0b7c8529.x86_64 path: /usr/bin/runc version: |- runc version 1.1.12 spec: 1.0.2-dev go: go1.22.7 (Red Hat 1.22.7-1.module+el8.10.0+90426+810ab996) libseccomp: 2.5.2 os: linux pasta: executable: """" package: """" version: """" remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_NET_RAW,CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /bin/slirp4netns package: slirp4netns-1.2.0-2.module+el8.8.0+21045+adcb6a64.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 4294963200 swapTotal: 4294963200 uptime: 1341h 26m 16.00s (Approximately 55.88 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - container-registry.oracle.com - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 1 paused: 0 running: 1 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 75125227520 graphRootUsed: 63892619264 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""false"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 33 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.9.4-rhel Built: 1732729681 BuiltTime: Wed Nov 27 17:48:01 2024 GitCommit: """" GoVersion: go1.22.7 (Red Hat 1.22.7-1.module+el8.10.0+90426+810ab996) Os: linux OsArch: linux/amd64 Version: 4.9.4-rhel   Podman in a container Yes  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting source-file test-file test-file source-file source-file test-file test-file source-file source-file test-file test-file source-file source-file test-file test-file source-file source-file test-file test-file source-file source-file test-file test-file source-file source-file test-file test-file source-file test-file test-file source-file test-file test-file",bug,0.95
21311,podman,https://github.com/containers/podman/issues/21311,Network connect/disconnect events don't include network ID/name," Issue Description Podman network events as emitted by `podman events` (connect, disconnect) don't include the network name or ID, making it impossible to use events to watch for changes to a specific network. Looking at the source code, the network name is included in the low level event data, but seems to be lost when converting between libpod Events and Docker compatible events via [ConvertToEntitiesEvent](https://github.com/containers/podman/blob/815ae77ab26cea3a5430116db682d9df46fc8845/pkg/domain/entities/events.go#L62) and [ConvertToLibpodEvent](https://github.com/containers/podman/blob/815ae77ab26cea3a5430116db682d9df46fc8845/pkg/domain/entities/events.go#L21).  Steps to reproduce the issue Steps to reproduce the issue 1. Run `podman events --format json` 2. In another terminal window, connect or disconnect a container to any networks `podman network connect <network> <container>` or `podman network disconenct <network> <container>` 3. Check the contents of the `Connect` and `Disconnect` events emitted  Describe the results you received I see events like this emitted: json {""ID"":""3d9d442dab9029e5e1b5d82307972ce3ff9b9f49d0473b4b762f256bdee1f96f"",""Status"":""disconnect"",""Time"":""2024-01-19T13:48:43.873758119-08:00"",""Type"":""network"",""Attributes"":{""podId"":""""}} {""ID"":""3d9d442dab9029e5e1b5d82307972ce3ff9b9f49d0473b4b762f256bdee1f96f"",""Status"":""connect"",""Time"":""2024-01-19T14:10:31.823991946-08:00"",""Type"":""network"",""Attributes"":{""podId"":""""}}  Each network event includes the ID of the relevant container, but not the ID or name of the network the container was connected to (or disconnected from).  Describe the results you expected I expect to see events like: json {""ID"":""3d9d442dab9029e5e1b5d82307972ce3ff9b9f49d0473b4b762f256bdee1f96f"",""Status"":""disconnect"",""Time"":""2024-01-19T13:48:43.873758119-08:00"",""Type"":""network"",""Network"":""test"",""Attributes"":{""podId"":""""}} {""ID"":""3d9d442dab9029e5e1b5d82307972ce3ff9b9f49d0473b4b762f256bdee1f96f"",""Status"":""connect"",""Time"":""2024-01-19T14:10:31.823991946-08:00"",""Type"":""network"",""Network"":""test"",""Attributes"":{""podId"":""""}}   podman info output yaml host: arch: arm64 buildahVersion: 1.33.2 cgroupControllers: - cpuset - cpu - io - memory - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.8-2.fc39.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.8, commit: ' cpuUtilization: idlePercent: 99.54 systemPercent: 0.22 userPercent: 0.24 cpus: 2 databaseBackend: sqlite distribution: distribution: fedora variant: coreos version: ""39"" eventLogger: journald freeLocks: 2045 hostname: localhost.localdomain idMappings: gidmap: null uidmap: null kernel: 6.6.8-200.fc39.aarch64 linkmode: dynamic logDriver: journald memFree: 616837120 memTotal: 3795222528 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.9.0-1.fc39.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.9.0 package: netavark-1.9.0-1.fc39.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.9.0 ociRuntime: name: crun package: crun-1.12-1.fc39.aarch64 path: /usr/bin/crun version: |- crun version 1.12 commit: ce429cb2e277d001c2179df1ac66a470f00802ae rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20231204.gb86afe3-1.fc39.aarch64 version: | pasta 0^20231204.gb86afe3-1.fc39.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-1.fc39.aarch64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 36h 19m 18.00s (Approximately 1.50 days) variant: v8 plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 1 paused: 0 running: 1 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 99252940800 graphRootUsed: 4171116544 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 6 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.8.2 Built: 1702300963 BuiltTime: Mon Dec 11 05:22:43 2023 GitCommit: """" GoVersion: go1.21.4 Os: linux OsArch: linux/arm64 Version: 4.8.2   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details Podman Desktop (v1.6.4) on ARM64 MacOS  Additional information _No response_",source-file | test-file,"Network connect/disconnect events don't include network ID/name  Issue Description Podman network events as emitted by `podman events` (connect, disconnect) don't include the network name or ID, making it impossible to use events to watch for changes to a specific network. Looking at the source code, the network name is included in the low level event data, but seems to be lost when converting between libpod Events and Docker compatible events via [ConvertToEntitiesEvent](https://github.com/containers/podman/blob/815ae77ab26cea3a5430116db682d9df46fc8845/pkg/domain/entities/events.go#L62) and [ConvertToLibpodEvent](https://github.com/containers/podman/blob/815ae77ab26cea3a5430116db682d9df46fc8845/pkg/domain/entities/events.go#L21).  Steps to reproduce the issue Steps to reproduce the issue 1. Run `podman events --format json` 2. In another terminal window, connect or disconnect a container to any networks `podman network connect <network> <container>` or `podman network disconenct <network> <container>` 3. Check the contents of the `Connect` and `Disconnect` events emitted  Describe the results you received I see events like this emitted: json {""ID"":""3d9d442dab9029e5e1b5d82307972ce3ff9b9f49d0473b4b762f256bdee1f96f"",""Status"":""disconnect"",""Time"":""2024-01-19T13:48:43.873758119-08:00"",""Type"":""network"",""Attributes"":{""podId"":""""}} {""ID"":""3d9d442dab9029e5e1b5d82307972ce3ff9b9f49d0473b4b762f256bdee1f96f"",""Status"":""connect"",""Time"":""2024-01-19T14:10:31.823991946-08:00"",""Type"":""network"",""Attributes"":{""podId"":""""}}  Each network event includes the ID of the relevant container, but not the ID or name of the network the container was connected to (or disconnected from).  Describe the results you expected I expect to see events like: json {""ID"":""3d9d442dab9029e5e1b5d82307972ce3ff9b9f49d0473b4b762f256bdee1f96f"",""Status"":""disconnect"",""Time"":""2024-01-19T13:48:43.873758119-08:00"",""Type"":""network"",""Network"":""test"",""Attributes"":{""podId"":""""}} {""ID"":""3d9d442dab9029e5e1b5d82307972ce3ff9b9f49d0473b4b762f256bdee1f96f"",""Status"":""connect"",""Time"":""2024-01-19T14:10:31.823991946-08:00"",""Type"":""network"",""Network"":""test"",""Attributes"":{""podId"":""""}}   podman info output yaml host: arch: arm64 buildahVersion: 1.33.2 cgroupControllers: - cpuset - cpu - io - memory - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.8-2.fc39.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.8, commit: ' cpuUtilization: idlePercent: 99.54 systemPercent: 0.22 userPercent: 0.24 cpus: 2 databaseBackend: sqlite distribution: distribution: fedora variant: coreos version: ""39"" eventLogger: journald freeLocks: 2045 hostname: localhost.localdomain idMappings: gidmap: null uidmap: null kernel: 6.6.8-200.fc39.aarch64 linkmode: dynamic logDriver: journald memFree: 616837120 memTotal: 3795222528 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.9.0-1.fc39.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.9.0 package: netavark-1.9.0-1.fc39.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.9.0 ociRuntime: name: crun package: crun-1.12-1.fc39.aarch64 path: /usr/bin/crun version: |- crun version 1.12 commit: ce429cb2e277d001c2179df1ac66a470f00802ae rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20231204.gb86afe3-1.fc39.aarch64 version: | pasta 0^20231204.gb86afe3-1.fc39.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-1.fc39.aarch64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 36h 19m 18.00s (Approximately 1.50 days) variant: v8 plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 1 paused: 0 running: 1 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 99252940800 graphRootUsed: 4171116544 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 6 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 4.8.2 Built: 1702300963 BuiltTime: Mon Dec 11 05:22:43 2023 GitCommit: """" GoVersion: go1.21.4 Os: linux OsArch: linux/arm64 Version: 4.8.2   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release Yes  Additional environment details Podman Desktop (v1.6.4) on ARM64 MacOS  Additional information _No response_ source-file test-file",bug,0.95
19368,podman,https://github.com/containers/podman/issues/19368,Container kill does not return 409 error code for stopped containers," Issue Description podman kill ( compat / libpod ) does not return 409 error code ( https://docs.docker.com/engine/api/v1.43/#tag/Container/operation/ContainerKill ) for stopped containers  Steps to reproduce the issue Steps to reproduce the issue 1. podman -r run -d --name nginx nginx 2. podman -r stop nginx 3. podman -r kill nginx  Describe the results you received  {""cause"":""container state improper"",""message"":""can only kill running containers. 6830d229c01064f76b9de62b76b80492fe4d08136c7b459bc0de926b1581093f is in state exited: container state improper"",""response"":500}   Describe the results you expected 409 is returned  podman info output yaml 4.6.0   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information _No response_",source-file | test-file | source-file | test-file,"Container kill does not return 409 error code for stopped containers  Issue Description podman kill ( compat / libpod ) does not return 409 error code ( https://docs.docker.com/engine/api/v1.43/#tag/Container/operation/ContainerKill ) for stopped containers  Steps to reproduce the issue Steps to reproduce the issue 1. podman -r run -d --name nginx nginx 2. podman -r stop nginx 3. podman -r kill nginx  Describe the results you received  {""cause"":""container state improper"",""message"":""can only kill running containers. 6830d229c01064f76b9de62b76b80492fe4d08136c7b459bc0de926b1581093f is in state exited: container state improper"",""response"":500}   Describe the results you expected 409 is returned  podman info output yaml 4.6.0   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information _No response_ source-file test-file source-file test-file",bug,0.95
24152,podman,https://github.com/containers/podman/issues/24152,"podman info fails: Error: parse ""[::]:2376"": first path segment in URL cannot contain colon"," Issue Description Running `podman info` remote via TCP fails with: Error: parse ""[::]:2376"": first path segment in URL cannot contain colon The HTTP request is:  GET /v5.2.3/libpod/info HTTP/1.1 Host: d User-Agent: Go-http-client/1.1  The response is:  HTTP/1.1 500 Internal Server Error Api-Version: 1.41 Content-Type: application/json Libpod-Api-Version: 5.2.3 Server: Libpod/5.2.3 (linux) X-Reference-Id: 0xc000630010 Date: Thu, 03 Oct 2024 18:01:27 GMT Content-Length: 154 {""cause"":""first path segment in URL cannot contain colon"",""message"":""parse \""[::]:2376\"": first path segment in URL cannot contain colon"",""response"":500}   Steps to reproduce the issue Steps to reproduce the issue 1. Make podman listen on TCP via systemd socket activation. Aka append the `podman.socket` unit with  [Socket] ListenStream= ListenStream=2376  2. Execute HTTP request with `echo ""GET /v5.2.3/libpod/info HTTP/1.1\nHost: d\n\n"" | nc -C localhost 2376`  Describe the results you received HTTP 500 and error `parse ""[::]:2376"": first path segment in URL cannot contain colon`  Describe the results you expected A proper info result.  podman info output yaml The request `podman info` does itself fail. * Debian 13 * Podman 5.2.3   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting",source-file | source-file | test-file,"podman info fails: Error: parse ""[::]:2376"": first path segment in URL cannot contain colon  Issue Description Running `podman info` remote via TCP fails with: Error: parse ""[::]:2376"": first path segment in URL cannot contain colon The HTTP request is:  GET /v5.2.3/libpod/info HTTP/1.1 Host: d User-Agent: Go-http-client/1.1  The response is:  HTTP/1.1 500 Internal Server Error Api-Version: 1.41 Content-Type: application/json Libpod-Api-Version: 5.2.3 Server: Libpod/5.2.3 (linux) X-Reference-Id: 0xc000630010 Date: Thu, 03 Oct 2024 18:01:27 GMT Content-Length: 154 {""cause"":""first path segment in URL cannot contain colon"",""message"":""parse \""[::]:2376\"": first path segment in URL cannot contain colon"",""response"":500}   Steps to reproduce the issue Steps to reproduce the issue 1. Make podman listen on TCP via systemd socket activation. Aka append the `podman.socket` unit with  [Socket] ListenStream= ListenStream=2376  2. Execute HTTP request with `echo ""GET /v5.2.3/libpod/info HTTP/1.1\nHost: d\n\n"" | nc -C localhost 2376`  Describe the results you received HTTP 500 and error `parse ""[::]:2376"": first path segment in URL cannot contain colon`  Describe the results you expected A proper info result.  podman info output yaml The request `podman info` does itself fail. * Debian 13 * Podman 5.2.3   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details Additional environment details  Additional information Additional information like issue happens only occasionally or issue happens with a particular architecture or on a particular setting source-file source-file test-file",bug,0.95
16150,podman,https://github.com/containers/podman/issues/16150,Podman push image to redhat quay with sigstore was failed caused by send malformed manifest to quay,"Hi Guys, When use podman 4.2.1 to push image to Redhat Quay 3.8.0, hit 500 error code, based on the log error message , seems like podman send malformed json to quay, pls review this issue and give suggestions.  [root@ip-10-0-1-76 fedora]# podman push quayregistry-quay-quay-enterprise-13240.apps.quaytest-13240.qe.azure.devcluster.openshift.com/quay/demo --tls-verify=false --sign-by-sigstore-private-key=./cosign.key Key Passphrase: Getting image source signatures Copying blob 288cf3a46e32 done Copying blob 75ba02937496 done Copying blob 0c7daf9a72c8 done Copying blob 955c9335e041 done Copying blob 8e079fee2186 done Copying blob 186da837555d done Copying blob d172a9e6f9e6 done Copying blob cf399be408ea done Copying blob 793b971ccb99 done Copying config da84e66c3a done Writing manifest to image destination Signing manifest using a sigstore signature Storing signatures Error: writing signatures: uploading manifest sha256-2353c13421e07e3d3dd1bb181cf0b7ad5e6dce3e1bb363c33f48d12e0a0ada49.sig to quayregistry-quay-quay-enterprise-13240.apps.quaytest-13240.qe.azure.devcluster.openshift.com/quay/demo: received unexpected HTTP status: 500 Internal Server Error   gunicorn-registry stdout | 2022-10-11 03:58:52,461 [214] [ERROR] [gunicorn.error] Error handling request /v2/quay/demo/manifests/sha256-2353c13421e07e3d3dd1bb181cf0b7ad5e6dce3e1bb363c33f48d12e0a0ada49.sig gunicorn-registry stdout | Traceback (most recent call last): gunicorn-registry stdout | File ""/quay-registry/image/oci/config.py"", line 209, in __init__ gunicorn-registry stdout | validate_schema(self._parsed, OCIConfig.METASCHEMA) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/jsonschema/validators.py"", line 934, in validate gunicorn-registry stdout | raise error gunicorn-registry stdout | jsonschema.exceptions.ValidationError: '' is not one of ['layers'] gunicorn-registry stdout | Failed validating 'enum' in schema['properties']['rootfs']['properties']['type']: gunicorn-registry stdout | {'description': 'MUST be set to layers.', gunicorn-registry stdout | 'enum': ['layers'], gunicorn-registry stdout | 'type': 'string'} gunicorn-registry stdout | On instance['rootfs']['type']: gunicorn-registry stdout | '' gunicorn-registry stdout | During handling of the above exception, another exception occurred: gunicorn-registry stdout | Traceback (most recent call last): gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/gunicorn/workers/base_async.py"", line 55, in handle gunicorn-registry stdout | self.handle_request(listener_name, req, client, addr) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/gunicorn/workers/ggevent.py"", line 127, in handle_request gunicorn-registry stdout | super().handle_request(listener_name, req, sock, addr) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/gunicorn/workers/base_async.py"", line 108, in handle_request gunicorn-registry stdout | respiter = self.wsgi(environ, resp.start_response) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2463, in __call__ gunicorn-registry stdout | return self.wsgi_app(environ, start_response) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/werkzeug/middleware/proxy_fix.py"", line 169, in __call__ gunicorn-registry stdout | return self.app(environ, start_response) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2449, in wsgi_app gunicorn-registry stdout | response = self.handle_exception(e) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1866, in handle_exception gunicorn-registry stdout | reraise(exc_type, exc_value, tb) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/_compat.py"", line 39, in reraise gunicorn-registry stdout | raise value gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2446, in wsgi_app gunicorn-registry stdout | response = self.full_dispatch_request() gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1951, in full_dispatch_request gunicorn-registry stdout | rv = self.handle_user_exception(e) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1820, in handle_user_exceptiongunicorn-registry stdout | 'x-ms-copy-source': 'REDACTED' ",other-file | other-file | config-file | other-file | documentation-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file,"Podman push image to redhat quay with sigstore was failed caused by send malformed manifest to quay Hi Guys, When use podman 4.2.1 to push image to Redhat Quay 3.8.0, hit 500 error code, based on the log error message , seems like podman send malformed json to quay, pls review this issue and give suggestions.  [root@ip-10-0-1-76 fedora]# podman push quayregistry-quay-quay-enterprise-13240.apps.quaytest-13240.qe.azure.devcluster.openshift.com/quay/demo --tls-verify=false --sign-by-sigstore-private-key=./cosign.key Key Passphrase: Getting image source signatures Copying blob 288cf3a46e32 done Copying blob 75ba02937496 done Copying blob 0c7daf9a72c8 done Copying blob 955c9335e041 done Copying blob 8e079fee2186 done Copying blob 186da837555d done Copying blob d172a9e6f9e6 done Copying blob cf399be408ea done Copying blob 793b971ccb99 done Copying config da84e66c3a done Writing manifest to image destination Signing manifest using a sigstore signature Storing signatures Error: writing signatures: uploading manifest sha256-2353c13421e07e3d3dd1bb181cf0b7ad5e6dce3e1bb363c33f48d12e0a0ada49.sig to quayregistry-quay-quay-enterprise-13240.apps.quaytest-13240.qe.azure.devcluster.openshift.com/quay/demo: received unexpected HTTP status: 500 Internal Server Error   gunicorn-registry stdout | 2022-10-11 03:58:52,461 [214] [ERROR] [gunicorn.error] Error handling request /v2/quay/demo/manifests/sha256-2353c13421e07e3d3dd1bb181cf0b7ad5e6dce3e1bb363c33f48d12e0a0ada49.sig gunicorn-registry stdout | Traceback (most recent call last): gunicorn-registry stdout | File ""/quay-registry/image/oci/config.py"", line 209, in __init__ gunicorn-registry stdout | validate_schema(self._parsed, OCIConfig.METASCHEMA) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/jsonschema/validators.py"", line 934, in validate gunicorn-registry stdout | raise error gunicorn-registry stdout | jsonschema.exceptions.ValidationError: '' is not one of ['layers'] gunicorn-registry stdout | Failed validating 'enum' in schema['properties']['rootfs']['properties']['type']: gunicorn-registry stdout | {'description': 'MUST be set to layers.', gunicorn-registry stdout | 'enum': ['layers'], gunicorn-registry stdout | 'type': 'string'} gunicorn-registry stdout | On instance['rootfs']['type']: gunicorn-registry stdout | '' gunicorn-registry stdout | During handling of the above exception, another exception occurred: gunicorn-registry stdout | Traceback (most recent call last): gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/gunicorn/workers/base_async.py"", line 55, in handle gunicorn-registry stdout | self.handle_request(listener_name, req, client, addr) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/gunicorn/workers/ggevent.py"", line 127, in handle_request gunicorn-registry stdout | super().handle_request(listener_name, req, sock, addr) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/gunicorn/workers/base_async.py"", line 108, in handle_request gunicorn-registry stdout | respiter = self.wsgi(environ, resp.start_response) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2463, in __call__ gunicorn-registry stdout | return self.wsgi_app(environ, start_response) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/werkzeug/middleware/proxy_fix.py"", line 169, in __call__ gunicorn-registry stdout | return self.app(environ, start_response) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2449, in wsgi_app gunicorn-registry stdout | response = self.handle_exception(e) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1866, in handle_exception gunicorn-registry stdout | reraise(exc_type, exc_value, tb) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/_compat.py"", line 39, in reraise gunicorn-registry stdout | raise value gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2446, in wsgi_app gunicorn-registry stdout | response = self.full_dispatch_request() gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1951, in full_dispatch_request gunicorn-registry stdout | rv = self.handle_user_exception(e) gunicorn-registry stdout | File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1820, in handle_user_exceptiongunicorn-registry stdout | 'x-ms-copy-source': 'REDACTED'  other-file other-file config-file other-file documentation-file source-file documentation-file source-file source-file source-file source-file source-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file",bug,0.95
17341,podman,https://github.com/containers/podman/issues/17341,[Bug]: Listing network from Docker fails during container removal," Issue Description If you start Podman API server and try to inspect a network from Docker (or Docker-compatible library, e.g. one used by Gitlab Runner) you get also a list of containers in that network (for backward compatibility with Docker, Podman doesn't show that data). But if container is currently being removed -or added, not sure here - this request fails with:  Error response from daemon: container <container id> does not exist in database: no such container  The interesting part is that you get the same error even if you run `docker network ls` instead of `docker network inspect <network>`. I believe it may be the cause of [this Gitlab Runner issue](https://gitlab.com/gitlab-org/gitlab-runner/-/issues/28971) (or it's at least one of the causes) and one similar error that I believe has not been reported to Gitlab yet that I've only observed with Podman 4.4.  Steps to reproduce the issue Steps to reproduce the issue 1. Start Podman API server (`podman system service`). 2. Create a network (`podman network create test`). 3. Configure **native docker** to use Podman's socket (`export DOCKER_HOST=unix://<path to socket>`). 4. Loop container creation (check below the list for an example code). 5. Watch either output of `docker network list` or `docker network inspect test` (check below the list for an example code). Example creation loop: bash while true do podman run -d --rm -ti --network test fedora sleep 1; done  Example watch (you need to open the file to find those lines, terminal control keys used to clear screan are stored in it so simple `cat` won't work): bash watch -tn 0.1 --exec docker network ls | tee -a test.log   Describe the results you received Podman server sometimes fails with container not found error. Log entry:  INFO[0052] Request Failed(Internal Server Error): container cf3b535ee60be15c9b5ed36240caa923119be4f06f9ff80bd98620d3c7e3ef3e does not exist in database: no such container @ - - [02/Feb/2023:17:15:09 +0000] ""GET /v1.41/networks HTTP/1.1"" 500 178 """" ""Docker-Client/20.10.23 (linux)""   Describe the results you expected No errors for either request. Ff Podman finds it cannot retrieve container details because it does no longer exist it should just remove it from `network inspect` output. In case of `network list` I'm not even sure if there is a reason to create this containers list in the first place - does JSON response contain that field? I don't see an option in docker's cli to show containers in this view.  podman info output shell host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - cpuset - cpu - memory - pids cgroupManager: cgroupfs cgroupVersion: v2 conmon: package: Unknown path: /usr/local/libexec/podman/conmon version: 'conmon version 2.1.5, commit: 4cb1e4d73699ce0cef2c3d89b652b3d15be429b3' cpuUtilization: idlePercent: 98.16 systemPercent: 0.57 userPercent: 1.27 cpus: 4 distribution: distribution: fedora variant: cloud version: ""37"" eventLogger: file hostname: <cut> idMappings: gidmap: - container_id: 0 host_id: 993 size: 1 - container_id: 1 host_id: 200000 size: 65536 uidmap: - container_id: 0 host_id: 993 size: 1 - container_id: 1 host_id: 200000 size: 65536 kernel: 6.1.8-200.fc37.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 1381150720 memTotal: 8329515008 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-3.fc37.x86_64 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /run/user/993/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/993/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 8328572928 swapTotal: 8328835072 uptime: 27h 58m 5.00s (Approximately 1.12 days) plugins: authorization: null log: - k8s-file - none - passthrough network: - bridge - macvlan volume: - local registries: docker.io: Blocked: false Insecure: false Location: docker.io MirrorByDigestOnly: false Mirrors: <cut> Prefix: docker.io PullFromMirror: """" : <cut> search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /home/gitlab-runner/.config/containers/storage.conf containerStore: number: 10 paused: 0 running: 1 stopped: 9 graphDriverName: overlay graphOptions: {} graphRoot: /home/gitlab-runner/.local/share/containers/storage graphRootAllocated: 52527345664 graphRootUsed: 5485887488 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 8 runRoot: /run/user/993/containers transientStore: false volumePath: /home/gitlab-runner/.local/share/containers/storage/volumes version: APIVersion: 4.4.0 Built: 1675343283 BuiltTime: Thu Feb 2 13:08:03 2023 GitCommit: """" GoVersion: go1.19.5 Os: linux OsArch: linux/amd64 Version: 4.4.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details Running on Fedora 37 in a VM with self-compiled Podman and conmon. SELinux enabled. Same error observed earlier using latest Fedora 37 packages (Podman 4.3.1, conmon 2.1.5). Same Gitlab Runner issue observed even earlier (on Podman 4.3.0, older conmon, runc, netavark, aardvark, etc.) although I don't have exact versions nor a way to confirm 100% that it means it's caused by the same problem. If this is the case then oldest report (author of that Gitlab issue) comes from Podman 3.4.2.  Additional information _No response_",source-file | source-file,"[Bug]: Listing network from Docker fails during container removal  Issue Description If you start Podman API server and try to inspect a network from Docker (or Docker-compatible library, e.g. one used by Gitlab Runner) you get also a list of containers in that network (for backward compatibility with Docker, Podman doesn't show that data). But if container is currently being removed -or added, not sure here - this request fails with:  Error response from daemon: container <container id> does not exist in database: no such container  The interesting part is that you get the same error even if you run `docker network ls` instead of `docker network inspect <network>`. I believe it may be the cause of [this Gitlab Runner issue](https://gitlab.com/gitlab-org/gitlab-runner/-/issues/28971) (or it's at least one of the causes) and one similar error that I believe has not been reported to Gitlab yet that I've only observed with Podman 4.4.  Steps to reproduce the issue Steps to reproduce the issue 1. Start Podman API server (`podman system service`). 2. Create a network (`podman network create test`). 3. Configure **native docker** to use Podman's socket (`export DOCKER_HOST=unix://<path to socket>`). 4. Loop container creation (check below the list for an example code). 5. Watch either output of `docker network list` or `docker network inspect test` (check below the list for an example code). Example creation loop: bash while true do podman run -d --rm -ti --network test fedora sleep 1; done  Example watch (you need to open the file to find those lines, terminal control keys used to clear screan are stored in it so simple `cat` won't work): bash watch -tn 0.1 --exec docker network ls | tee -a test.log   Describe the results you received Podman server sometimes fails with container not found error. Log entry:  INFO[0052] Request Failed(Internal Server Error): container cf3b535ee60be15c9b5ed36240caa923119be4f06f9ff80bd98620d3c7e3ef3e does not exist in database: no such container @ - - [02/Feb/2023:17:15:09 +0000] ""GET /v1.41/networks HTTP/1.1"" 500 178 """" ""Docker-Client/20.10.23 (linux)""   Describe the results you expected No errors for either request. Ff Podman finds it cannot retrieve container details because it does no longer exist it should just remove it from `network inspect` output. In case of `network list` I'm not even sure if there is a reason to create this containers list in the first place - does JSON response contain that field? I don't see an option in docker's cli to show containers in this view.  podman info output shell host: arch: amd64 buildahVersion: 1.29.0 cgroupControllers: - cpuset - cpu - memory - pids cgroupManager: cgroupfs cgroupVersion: v2 conmon: package: Unknown path: /usr/local/libexec/podman/conmon version: 'conmon version 2.1.5, commit: 4cb1e4d73699ce0cef2c3d89b652b3d15be429b3' cpuUtilization: idlePercent: 98.16 systemPercent: 0.57 userPercent: 1.27 cpus: 4 distribution: distribution: fedora variant: cloud version: ""37"" eventLogger: file hostname: <cut> idMappings: gidmap: - container_id: 0 host_id: 993 size: 1 - container_id: 1 host_id: 200000 size: 65536 uidmap: - container_id: 0 host_id: 993 size: 1 - container_id: 1 host_id: 200000 size: 65536 kernel: 6.1.8-200.fc37.x86_64 linkmode: dynamic logDriver: k8s-file memFree: 1381150720 memTotal: 8329515008 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-3.fc37.x86_64 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /run/user/993/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/993/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.x86_64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 8328572928 swapTotal: 8328835072 uptime: 27h 58m 5.00s (Approximately 1.12 days) plugins: authorization: null log: - k8s-file - none - passthrough network: - bridge - macvlan volume: - local registries: docker.io: Blocked: false Insecure: false Location: docker.io MirrorByDigestOnly: false Mirrors: <cut> Prefix: docker.io PullFromMirror: """" : <cut> search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /home/gitlab-runner/.config/containers/storage.conf containerStore: number: 10 paused: 0 running: 1 stopped: 9 graphDriverName: overlay graphOptions: {} graphRoot: /home/gitlab-runner/.local/share/containers/storage graphRootAllocated: 52527345664 graphRootUsed: 5485887488 graphStatus: Backing Filesystem: btrfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 8 runRoot: /run/user/993/containers transientStore: false volumePath: /home/gitlab-runner/.local/share/containers/storage/volumes version: APIVersion: 4.4.0 Built: 1675343283 BuiltTime: Thu Feb 2 13:08:03 2023 GitCommit: """" GoVersion: go1.19.5 Os: linux OsArch: linux/amd64 Version: 4.4.0   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details Running on Fedora 37 in a VM with self-compiled Podman and conmon. SELinux enabled. Same error observed earlier using latest Fedora 37 packages (Podman 4.3.1, conmon 2.1.5). Same Gitlab Runner issue observed even earlier (on Podman 4.3.0, older conmon, runc, netavark, aardvark, etc.) although I don't have exact versions nor a way to confirm 100% that it means it's caused by the same problem. If this is the case then oldest report (author of that Gitlab issue) comes from Podman 3.4.2.  Additional information _No response_ source-file source-file",bug,0.95
20821,podman,https://github.com/containers/podman/issues/20821,Unmarshalling error when using `podman exec`," Issue Description When I use `podman exec ` for any container, I get the following error:  Error: unmarshalling error into &errorhandling.ErrorModel{Because:"""", Message:"""", ResponseCode:0}, data ""Not Found\n"": invalid character 'N' looking for beginning of value   Steps to reproduce the issue Steps to reproduce the issue 1. Run a detached container: `podman run --name httpd -p 8080:80 -d -i -t docker.io/library/httpd` 2. Try to run podman exec: `podman exec httpd pwd` 3. Error message shows up  Describe the results you received An unexpected error message.  Describe the results you expected No error message, if the command is run correctly.  podman info output yaml host: arch: arm64 buildahVersion: 1.32.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.8-2.fc39.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.8, commit: ' cpuUtilization: idlePercent: 99.33 systemPercent: 0.34 userPercent: 0.33 cpus: 4 databaseBackend: boltdb distribution: distribution: fedora variant: coreos version: ""39"" eventLogger: journald freeLocks: 2039 hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 1000000 uidmap: - container_id: 0 host_id: 501 size: 1 - container_id: 1 host_id: 100000 size: 1000000 kernel: 6.5.11-300.fc39.aarch64 linkmode: dynamic logDriver: journald memFree: 3151986688 memTotal: 6189867008 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.8.0-1.fc39.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.8.0 package: netavark-1.8.0-2.fc39.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.8.0 ociRuntime: name: crun package: crun-1.11.1-1.fc39.aarch64 path: /usr/bin/crun version: |- crun version 1.11.1 commit: 1084f9527c143699b593b44c23555fb3cc4ff2f3 rundir: /run/user/501/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20231004.gf851084-1.fc39.aarch64 version: | pasta 0^20231004.gf851084-1.fc39.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/user/501/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-1.fc39.aarch64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 12h 26m 50.00s (Approximately 0.50 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /var/home/core/.config/containers/storage.conf containerStore: number: 3 paused: 0 running: 3 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/core/.local/share/containers/storage graphRootAllocated: 106769133568 graphRootUsed: 6775140352 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 33 runRoot: /run/user/501/containers transientStore: false volumePath: /var/home/core/.local/share/containers/storage/volumes version: APIVersion: 4.7.2 Built: 1698762633 BuiltTime: Tue Oct 31 15:30:33 2023 GitCommit: """" GoVersion: go1.21.1 Os: linux OsArch: linux/arm64 Version: 4.7.2   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details ZSH on MacOS  Additional information When downgrading to v4.7.2 the issue disappeared.",source-file | test-file | source-file | test-file,"Unmarshalling error when using `podman exec`  Issue Description When I use `podman exec ` for any container, I get the following error:  Error: unmarshalling error into &errorhandling.ErrorModel{Because:"""", Message:"""", ResponseCode:0}, data ""Not Found\n"": invalid character 'N' looking for beginning of value   Steps to reproduce the issue Steps to reproduce the issue 1. Run a detached container: `podman run --name httpd -p 8080:80 -d -i -t docker.io/library/httpd` 2. Try to run podman exec: `podman exec httpd pwd` 3. Error message shows up  Describe the results you received An unexpected error message.  Describe the results you expected No error message, if the command is run correctly.  podman info output yaml host: arch: arm64 buildahVersion: 1.32.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.8-2.fc39.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.8, commit: ' cpuUtilization: idlePercent: 99.33 systemPercent: 0.34 userPercent: 0.33 cpus: 4 databaseBackend: boltdb distribution: distribution: fedora variant: coreos version: ""39"" eventLogger: journald freeLocks: 2039 hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 1000000 uidmap: - container_id: 0 host_id: 501 size: 1 - container_id: 1 host_id: 100000 size: 1000000 kernel: 6.5.11-300.fc39.aarch64 linkmode: dynamic logDriver: journald memFree: 3151986688 memTotal: 6189867008 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.8.0-1.fc39.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.8.0 package: netavark-1.8.0-2.fc39.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.8.0 ociRuntime: name: crun package: crun-1.11.1-1.fc39.aarch64 path: /usr/bin/crun version: |- crun version 1.11.1 commit: 1084f9527c143699b593b44c23555fb3cc4ff2f3 rundir: /run/user/501/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20231004.gf851084-1.fc39.aarch64 version: | pasta 0^20231004.gf851084-1.fc39.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/user/501/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-1.fc39.aarch64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 12h 26m 50.00s (Approximately 0.50 days) variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /var/home/core/.config/containers/storage.conf containerStore: number: 3 paused: 0 running: 3 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/core/.local/share/containers/storage graphRootAllocated: 106769133568 graphRootUsed: 6775140352 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 33 runRoot: /run/user/501/containers transientStore: false volumePath: /var/home/core/.local/share/containers/storage/volumes version: APIVersion: 4.7.2 Built: 1698762633 BuiltTime: Tue Oct 31 15:30:33 2023 GitCommit: """" GoVersion: go1.21.1 Os: linux OsArch: linux/arm64 Version: 4.7.2   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details ZSH on MacOS  Additional information When downgrading to v4.7.2 the issue disappeared. source-file test-file source-file test-file",bug,0.9
18792,podman,https://github.com/containers/podman/issues/18792,"Running containers without defined or disabled health checks should not return ""Health"": {}"," Issue Description When running a container with podman that does not define any health check, the following state is returned.  Steps to reproduce the issue Steps to reproduce the issue 1. Run podman with disabled or no health checks in place  podman run --no-healthcheck -e POSTGRES_PASSWORD=password postgres:9.5 or since this image seems not to define any check podman run -e POSTGRES_PASSWORD=password postgres:9.5  2. Check the state  podman inspect $cid ""Health"": { ""Status"": """", ""FailingStreak"": 0, ""Log"": null },   Describe the results you received  podman inspect $cid ""Health"": { ""Status"": """", ""FailingStreak"": 0, ""Log"": null },   Describe the results you expected As with docker, I would expect that there is no ""Health"": section in the first place to indicate that this container is not running any health checks  podman info output yaml podman info host: arch: arm64 buildahVersion: 1.30.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.7-2.fc38.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 91.22 systemPercent: 2.77 userPercent: 6.01 cpus: 1 databaseBackend: boltdb distribution: distribution: fedora variant: coreos version: ""38"" eventLogger: journald hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 1000000 uidmap: - container_id: 0 host_id: 501 size: 1 - container_id: 1 host_id: 100000 size: 1000000 kernel: 6.2.15-300.fc38.aarch64 linkmode: dynamic logDriver: journald memFree: 363335680 memTotal: 2049069056 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.5-1.fc38.aarch64 path: /usr/bin/crun version: |- crun version 1.8.5 commit: b6f80f766c9a89eb7b1440c0a70ab287434b17ed rundir: /run/user/501/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/501/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-12.fc38.aarch64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 0h 53m 21.00s plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /var/home/core/.config/containers/storage.conf containerStore: number: 8 paused: 0 running: 1 stopped: 7 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/core/.local/share/containers/storage graphRootAllocated: 106769133568 graphRootUsed: 6792806400 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 69 runRoot: /run/user/501/containers transientStore: false volumePath: /var/home/core/.local/share/containers/storage/volumes version: APIVersion: 4.5.0 Built: 1681486872 BuiltTime: Fri Apr 14 17:41:12 2023 GitCommit: """" GoVersion: go1.20.2 Os: linux OsArch: linux/arm64 Version: 4.5.0   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information similar looking * https://github.com/containers/podman/issues/13578",source-file | test-file | test-file,"Running containers without defined or disabled health checks should not return ""Health"": {}  Issue Description When running a container with podman that does not define any health check, the following state is returned.  Steps to reproduce the issue Steps to reproduce the issue 1. Run podman with disabled or no health checks in place  podman run --no-healthcheck -e POSTGRES_PASSWORD=password postgres:9.5 or since this image seems not to define any check podman run -e POSTGRES_PASSWORD=password postgres:9.5  2. Check the state  podman inspect $cid ""Health"": { ""Status"": """", ""FailingStreak"": 0, ""Log"": null },   Describe the results you received  podman inspect $cid ""Health"": { ""Status"": """", ""FailingStreak"": 0, ""Log"": null },   Describe the results you expected As with docker, I would expect that there is no ""Health"": section in the first place to indicate that this container is not running any health checks  podman info output yaml podman info host: arch: arm64 buildahVersion: 1.30.0 cgroupControllers: - cpu - io - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.7-2.fc38.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.7, commit: ' cpuUtilization: idlePercent: 91.22 systemPercent: 2.77 userPercent: 6.01 cpus: 1 databaseBackend: boltdb distribution: distribution: fedora variant: coreos version: ""38"" eventLogger: journald hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 1000000 uidmap: - container_id: 0 host_id: 501 size: 1 - container_id: 1 host_id: 100000 size: 1000000 kernel: 6.2.15-300.fc38.aarch64 linkmode: dynamic logDriver: journald memFree: 363335680 memTotal: 2049069056 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.5-1.fc38.aarch64 path: /usr/bin/crun version: |- crun version 1.8.5 commit: b6f80f766c9a89eb7b1440c0a70ab287434b17ed rundir: /run/user/501/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/501/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-12.fc38.aarch64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 0h 53m 21.00s plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /var/home/core/.config/containers/storage.conf containerStore: number: 8 paused: 0 running: 1 stopped: 7 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/core/.local/share/containers/storage graphRootAllocated: 106769133568 graphRootUsed: 6792806400 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 69 runRoot: /run/user/501/containers transientStore: false volumePath: /var/home/core/.local/share/containers/storage/volumes version: APIVersion: 4.5.0 Built: 1681486872 BuiltTime: Fri Apr 14 17:41:12 2023 GitCommit: """" GoVersion: go1.20.2 Os: linux OsArch: linux/arm64 Version: 4.5.0   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details Additional environment details  Additional information similar looking * https://github.com/containers/podman/issues/13578 source-file test-file test-file",bug,0.9
16857,podman,https://github.com/containers/podman/issues/16857,events API docker compatibility issue,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** Podman expects the podman-style ""died"" name of the event rather than docker-style ""die"" event in an event filter expression. This is a compatibility issue with docker-compatible clients. **Steps to reproduce the issue:** 1. Start a container sleeping for a short period of time 2. Subscribe to events using Docker compatibility API, indicating the following filter `{""event"":[""die""]}` and observe the absence of events. To compare with a working case, repeat steps 1 and 2 substituting the filter for `{""event"": [""died""]}` and observe the event called `die` when the container in step 1 finishes. In form of script, steps 1 and 2 (using docker-style event name):  sudo podman run -d --rm --name test alpine sleep 2 sudo curl --unix /run/podman/podman.sock 'http://localhost/events?filters=%7B""container"":%5B""test""%5D,""event "":%5B""die""%5D%7D' # no event here  Using podman-style event name:  sudo podman run -d --rm --name test alpine sleep 2 sudo curl --unix /run/podman/podman.sock 'http://localhost/events?filters=%7B""container"":%5B""test""%5D,""event "":%5B""died""%5D%7D' # the event is printed  **Describe the results you received:** No event arrives in step 2 of the reproduction example. **Describe the results you expected:** I expected to see the ""die"" event from step 2 in the reproduction example:  {""status"":""die"",""id"":""b04f2ec34e7972ee8cd405474e73805db0aa082b3fb12e20958e9c3a781d5bc0"",""from"":""docker.io/library/alpine:latest"",""Type"":""container"",""Action"":""die"",""Actor"":{""ID"":""b04f2ec34e7972ee8cd405474e73805db0aa082b3fb12e20958e9c3a781d5bc0"",""Attributes"":{""containerExitCode"":""0"",""exitCode"":""0"",""image"":""docker.io/library/alpine:latest"",""name"":""test"",""podId"":""""}},""scope"":""local"",""time"":1671122524,""timeNano"":1671122524154484836}  **Additional information you deem important (e.g. issue happens only occasionally):** I think both `die` and `died` should be supported in a filter expression, for backwards compatibility with previous Podman versions. **Output of `podman version`:**  $ podman version Client: Podman Engine Version: 4.3.1 API Version: 4.3.1 Go Version: go1.19.4 Built: Tue Dec 13 02:00:33 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  $ podman info host: arch: amd64 buildahVersion: 1.28.0 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.5-r0 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: unknown' cpuUtilization: idlePercent: 99.73 systemPercent: 0.11 userPercent: 0.16 cpus: 1 distribution: distribution: alpine version: 3.17.0 eventLogger: file hostname: h01 idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 uidmap: - container_id: 0 host_id: 1000 size: 1 kernel: 5.15.82-0-virt linkmode: dynamic logDriver: k8s-file memFree: 1841414144 memTotal: 2087096320 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-r0 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /tmp/podman-run-1000/crun spec: 1.0.0 +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: path: /tmp/podman-run-1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-r0 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 0 swapTotal: 0 uptime: 1h 19m 14.00s (Approximately 0.04 days) plugins: authorization: null log: - k8s-file - none - passthrough network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /home/docker/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/docker/.local/share/containers/storage graphRootAllocated: 4226809856 graphRootUsed: 286445568 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /tmp/podman-run-1000/containers volumePath: /home/docker/.local/share/containers/storage/volumes version: APIVersion: 4.3.1 Built: 1670896833 BuiltTime: Tue Dec 13 02:00:33 2022 GitCommit: """" GoVersion: go1.19.4 Os: linux OsArch: linux/amd64 Version: 4.3.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman` or `brew info podman`):**  $ apk info podman podman-4.3.1-r1 description: Simple management tool for pods, containers and images podman-4.3.1-r1 webpage: https://podman.io/ podman-4.3.1-r1 installed size: 39 MiB  **Have you tested with the latest version of Podman and have you checked [the Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md)?** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):**",documentation-file | source-file | test-file,"events API docker compatibility issue <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** Podman expects the podman-style ""died"" name of the event rather than docker-style ""die"" event in an event filter expression. This is a compatibility issue with docker-compatible clients. **Steps to reproduce the issue:** 1. Start a container sleeping for a short period of time 2. Subscribe to events using Docker compatibility API, indicating the following filter `{""event"":[""die""]}` and observe the absence of events. To compare with a working case, repeat steps 1 and 2 substituting the filter for `{""event"": [""died""]}` and observe the event called `die` when the container in step 1 finishes. In form of script, steps 1 and 2 (using docker-style event name):  sudo podman run -d --rm --name test alpine sleep 2 sudo curl --unix /run/podman/podman.sock 'http://localhost/events?filters=%7B""container"":%5B""test""%5D,""event "":%5B""die""%5D%7D' # no event here  Using podman-style event name:  sudo podman run -d --rm --name test alpine sleep 2 sudo curl --unix /run/podman/podman.sock 'http://localhost/events?filters=%7B""container"":%5B""test""%5D,""event "":%5B""died""%5D%7D' # the event is printed  **Describe the results you received:** No event arrives in step 2 of the reproduction example. **Describe the results you expected:** I expected to see the ""die"" event from step 2 in the reproduction example:  {""status"":""die"",""id"":""b04f2ec34e7972ee8cd405474e73805db0aa082b3fb12e20958e9c3a781d5bc0"",""from"":""docker.io/library/alpine:latest"",""Type"":""container"",""Action"":""die"",""Actor"":{""ID"":""b04f2ec34e7972ee8cd405474e73805db0aa082b3fb12e20958e9c3a781d5bc0"",""Attributes"":{""containerExitCode"":""0"",""exitCode"":""0"",""image"":""docker.io/library/alpine:latest"",""name"":""test"",""podId"":""""}},""scope"":""local"",""time"":1671122524,""timeNano"":1671122524154484836}  **Additional information you deem important (e.g. issue happens only occasionally):** I think both `die` and `died` should be supported in a filter expression, for backwards compatibility with previous Podman versions. **Output of `podman version`:**  $ podman version Client: Podman Engine Version: 4.3.1 API Version: 4.3.1 Go Version: go1.19.4 Built: Tue Dec 13 02:00:33 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  $ podman info host: arch: amd64 buildahVersion: 1.28.0 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.5-r0 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: unknown' cpuUtilization: idlePercent: 99.73 systemPercent: 0.11 userPercent: 0.16 cpus: 1 distribution: distribution: alpine version: 3.17.0 eventLogger: file hostname: h01 idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 uidmap: - container_id: 0 host_id: 1000 size: 1 kernel: 5.15.82-0-virt linkmode: dynamic logDriver: k8s-file memFree: 1841414144 memTotal: 2087096320 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-r0 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /tmp/podman-run-1000/crun spec: 1.0.0 +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +YAJL os: linux remoteSocket: path: /tmp/podman-run-1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /etc/containers/seccomp.json selinuxEnabled: false serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-r0 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.4 swapFree: 0 swapTotal: 0 uptime: 1h 19m 14.00s (Approximately 0.04 days) plugins: authorization: null log: - k8s-file - none - passthrough network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /home/docker/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /home/docker/.local/share/containers/storage graphRootAllocated: 4226809856 graphRootUsed: 286445568 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 0 runRoot: /tmp/podman-run-1000/containers volumePath: /home/docker/.local/share/containers/storage/volumes version: APIVersion: 4.3.1 Built: 1670896833 BuiltTime: Tue Dec 13 02:00:33 2022 GitCommit: """" GoVersion: go1.19.4 Os: linux OsArch: linux/amd64 Version: 4.3.1  **Package info (e.g. output of `rpm -q podman` or `apt list podman` or `brew info podman`):**  $ apk info podman podman-4.3.1-r1 description: Simple management tool for pods, containers and images podman-4.3.1-r1 webpage: https://podman.io/ podman-4.3.1-r1 installed size: 39 MiB  **Have you tested with the latest version of Podman and have you checked [the Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md)?** Yes **Additional environment details (AWS, VirtualBox, physical, etc.):** documentation-file source-file test-file",bug,0.9
25871,podman,https://github.com/containers/podman/issues/25871,Docker compat api image removal with force stops running containers," Issue Description Using the docker compatibility api if an image is in use by a running container and is removed with the force option specified the container is stopped and removed. Whereas in docker, images with running containers results in an error. In docker, force only results in the removal of stopped containers. See: https://docs.docker.com/reference/api/engine/version/v1.48/#tag/Image/operation/ImageDelete Tested against the mac desktop 1.17.2 and podman 5.4.2 built on ubuntu  Steps to reproduce the issue Steps to reproduce the issue 1. Pull an image and get it's ID 2. Start a container from that image e.g. `docker run -d [the id] sleep 300` 3. Run `docker rmi --force [the image ID]` 4. See the container is gone The same can be achieved using the docker go client e.g.  cli, _ := client.NewClientWithOpts(client.FromEnv) _, err = cli.ImageRemove(ctx, ""the image id"", image.RemoveOptions{ Force: true, PruneChildren: true, })   Describe the results you received The running container is stopped and removed  Describe the results you expected An error to occur e.g. `Error response from daemon: conflict: unable to delete aded1e1a5b37 (cannot be forced) - image is being used by running container fe300816e647`  podman info output yaml Client: APIVersion: 5.4.2 BuildOrigin: brew Built: 1743601389 BuiltTime: Wed Apr 2 14:43:09 2025 GitCommit: """" GoVersion: go1.24.2 Os: darwin OsArch: darwin/arm64 Version: 5.4.2 host: arch: arm64 buildahVersion: 1.38.0 cgroupControllers: - cpuset - cpu - io - memory - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.12-2.fc40.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.12, commit: ' cpuUtilization: idlePercent: 98.28 systemPercent: 0.97 userPercent: 0.75 cpus: 6 databaseBackend: sqlite distribution: distribution: fedora variant: coreos version: ""40"" eventLogger: journald freeLocks: 2006 hostname: localhost.localdomain idMappings: gidmap: null uidmap: null kernel: 6.11.3-200.fc40.aarch64 linkmode: dynamic logDriver: journald memFree: 3115659264 memTotal: 3792564224 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.12.2-2.fc40.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.12.2 package: netavark-1.12.2-1.fc40.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.12.2 ociRuntime: name: crun package: crun-1.17-1.fc40.aarch64 path: /usr/bin/crun version: |- crun version 1.17 commit: 000fa0d4eeed8938301f3bcf8206405315bc1017 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20240906.g6b38f07-1.fc40.aarch64 version: | pasta 0^20240906.g6b38f07-1.fc40.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: unix:run/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-2.fc40.aarch64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.5 swapFree: 0 swapTotal: 0 uptime: 0h 1m 44.00s variant: v8 plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 19 paused: 0 running: 0 stopped: 19 graphDriverName: overlay graphOptions: overlay.imagestore: /usr/lib/containers/storage overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 99252940800 graphRootUsed: 54138920960 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 70 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.3.1 Built: 1732147200 BuiltTime: Thu Nov 21 00:00:00 2024 GitCommit: """" GoVersion: go1.22.7 Os: linux OsArch: linux/arm64 Version: 5.3.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details N/A  Additional information It is important that the image ID is used for the removal of the image, and not its tag",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file,"Docker compat api image removal with force stops running containers  Issue Description Using the docker compatibility api if an image is in use by a running container and is removed with the force option specified the container is stopped and removed. Whereas in docker, images with running containers results in an error. In docker, force only results in the removal of stopped containers. See: https://docs.docker.com/reference/api/engine/version/v1.48/#tag/Image/operation/ImageDelete Tested against the mac desktop 1.17.2 and podman 5.4.2 built on ubuntu  Steps to reproduce the issue Steps to reproduce the issue 1. Pull an image and get it's ID 2. Start a container from that image e.g. `docker run -d [the id] sleep 300` 3. Run `docker rmi --force [the image ID]` 4. See the container is gone The same can be achieved using the docker go client e.g.  cli, _ := client.NewClientWithOpts(client.FromEnv) _, err = cli.ImageRemove(ctx, ""the image id"", image.RemoveOptions{ Force: true, PruneChildren: true, })   Describe the results you received The running container is stopped and removed  Describe the results you expected An error to occur e.g. `Error response from daemon: conflict: unable to delete aded1e1a5b37 (cannot be forced) - image is being used by running container fe300816e647`  podman info output yaml Client: APIVersion: 5.4.2 BuildOrigin: brew Built: 1743601389 BuiltTime: Wed Apr 2 14:43:09 2025 GitCommit: """" GoVersion: go1.24.2 Os: darwin OsArch: darwin/arm64 Version: 5.4.2 host: arch: arm64 buildahVersion: 1.38.0 cgroupControllers: - cpuset - cpu - io - memory - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.12-2.fc40.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.12, commit: ' cpuUtilization: idlePercent: 98.28 systemPercent: 0.97 userPercent: 0.75 cpus: 6 databaseBackend: sqlite distribution: distribution: fedora variant: coreos version: ""40"" eventLogger: journald freeLocks: 2006 hostname: localhost.localdomain idMappings: gidmap: null uidmap: null kernel: 6.11.3-200.fc40.aarch64 linkmode: dynamic logDriver: journald memFree: 3115659264 memTotal: 3792564224 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.12.2-2.fc40.aarch64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.12.2 package: netavark-1.12.2-1.fc40.aarch64 path: /usr/libexec/podman/netavark version: netavark 1.12.2 ociRuntime: name: crun package: crun-1.17-1.fc40.aarch64 path: /usr/bin/crun version: |- crun version 1.17 commit: 000fa0d4eeed8938301f3bcf8206405315bc1017 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux pasta: executable: /usr/bin/pasta package: passt-0^20240906.g6b38f07-1.fc40.aarch64 version: | pasta 0^20240906.g6b38f07-1.fc40.aarch64-pasta Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: unix:run/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.2-2.fc40.aarch64 version: |- slirp4netns version 1.2.2 commit: 0ee2d87523e906518d34a6b423271e4826f71faf libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.5 swapFree: 0 swapTotal: 0 uptime: 0h 1m 44.00s variant: v8 plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - docker.io store: configFile: /usr/share/containers/storage.conf containerStore: number: 19 paused: 0 running: 0 stopped: 19 graphDriverName: overlay graphOptions: overlay.imagestore: /usr/lib/containers/storage overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 99252940800 graphRootUsed: 54138920960 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""true"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 70 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.3.1 Built: 1732147200 BuiltTime: Thu Nov 21 00:00:00 2024 GitCommit: """" GoVersion: go1.22.7 Os: linux OsArch: linux/arm64 Version: 5.3.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details N/A  Additional information It is important that the image ID is used for the removal of the image, and not its tag source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file source-file source-file source-file source-file source-file source-file test-file source-file source-file source-file source-file source-file source-file test-file source-file source-file source-file source-file source-file source-file test-file test-file source-file source-file source-file source-file source-file source-file test-file test-file source-file source-file source-file source-file source-file source-file test-file test-file",bug,0.95
17763,podman,https://github.com/containers/podman/issues/17763,REST API: missing tag from `/v1.24/images/${img}/history`," Issue Description docker returns a tag  $ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Tags [ ""alpine:3.17.2"" ]  podman does not  $ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Tags null   Steps to reproduce the issue 1. `sudo -i` 2. `useradd test1` 3. `usermod -aG docker test1` 4. `machinectl shell test1@` 5. `podman pull alpine:3.17.2` 6. `docker pull alpine:3.17.2` 7. run `podman images` and detect the image id. Record the result in a shell variable `img=d74e625d9115` 9. `systemctl --user start podman.socket` 10. `curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags` 11. `curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Tags`  Describe the results you received  [test1@localhost ~]$ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags [ ""alpine:3.17.2"" ] [test1@localhost ~]$ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags null [test1@localhost ~]$   Describe the results you expected  [test1@localhost ~]$ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags [ ""alpine:3.17.2"" ] [test1@localhost ~]$ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags [ ""alpine:3.17.2"" ] [test1@localhost ~]$   podman info output yaml host: arch: arm64 buildahVersion: 1.29.0 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.6-3.fc37.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.6, commit: ' cpuUtilization: idlePercent: 99.65 systemPercent: 0.12 userPercent: 0.24 cpus: 1 distribution: distribution: fedora variant: coreos version: ""37"" eventLogger: journald hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 589824 size: 65536 uidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 589824 size: 65536 kernel: 6.1.14-200.fc37.aarch64 linkmode: dynamic logDriver: journald memFree: 925364224 memTotal: 2050260992 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.1-1.fc37.aarch64 path: /usr/bin/crun version: |- crun version 1.8.1 commit: f8a096be060b22ccd3d5f3ebe44108517fbf6c30 rundir: /run/user/1001/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1001/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.aarch64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 31h 18m 41.00s (Approximately 1.29 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /var/home/test1/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/test1/.local/share/containers/storage graphRootAllocated: 10132369408 graphRootUsed: 1956737024 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/user/1001/containers transientStore: false volumePath: /var/home/test1/.local/share/containers/storage/volumes version: APIVersion: 4.4.1 Built: 1676629538 BuiltTime: Fri Feb 17 10:25:38 2023 GitCommit: """" GoVersion: go1.19.5 Os: linux OsArch: linux/arm64 Version: 4.4.1   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details The commands above were run with __podman 4.4.1__ on Fedora CoreOS by using qemu on a macOS laptop (operating system: Ventura 13.2.1).  [test1@localhost ~]$ docker version Client: Version: 20.10.23 API version: 1.41 Go version: go1.19.5 Git commit: %{shortcommit_cli} Built: Sun Jan 29 17:38:04 2023 OS/Arch: linux/arm64 Context: default Experimental: true Server: Engine: Version: 20.10.23 API version: 1.41 (minimum version 1.12) Go version: go1.19.5 Git commit: %{shortcommit_moby} Built: Sun Jan 29 17:38:04 2023 OS/Arch: linux/arm64 Experimental: false containerd: Version: 1.6.15 GitCommit: runc: Version: 1.1.4 GitCommit: docker-init: Version: 0.19.0 GitCommit:   [test1@localhost ~]$ podman version Client: Podman Engine Version: 4.4.1 API Version: 4.4.1 Go Version: go1.19.5 Built: Fri Feb 17 10:25:38 2023 OS/Arch: linux/arm64   [test1@localhost ~]$ rpm -q podman podman-4.4.1-3.fc37.aarch64   Additional information I have another computer (arch: amd64) running Fedora 37. I re-run the Podman commands there with __podman 4.4.2__. The container image ID changed and the container image size changed but nothing else.",source-file | other-file | other-file | source-file | test-file | test-file | source-file | source-file | source-file | other-file,"REST API: missing tag from `/v1.24/images/${img}/history`  Issue Description docker returns a tag  $ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Tags [ ""alpine:3.17.2"" ]  podman does not  $ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Tags null   Steps to reproduce the issue 1. `sudo -i` 2. `useradd test1` 3. `usermod -aG docker test1` 4. `machinectl shell test1@` 5. `podman pull alpine:3.17.2` 6. `docker pull alpine:3.17.2` 7. run `podman images` and detect the image id. Record the result in a shell variable `img=d74e625d9115` 9. `systemctl --user start podman.socket` 10. `curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags` 11. `curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/d74e625d9115/history | jq .[0].Tags`  Describe the results you received  [test1@localhost ~]$ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags [ ""alpine:3.17.2"" ] [test1@localhost ~]$ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags null [test1@localhost ~]$   Describe the results you expected  [test1@localhost ~]$ curl -s --unix-socket /var/run/docker.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags [ ""alpine:3.17.2"" ] [test1@localhost ~]$ curl -s --unix-socket $XDG_RUNTIME_DIR/podman/podman.sock http://localhost/v1.24/images/${img}/history | jq .[0].Tags [ ""alpine:3.17.2"" ] [test1@localhost ~]$   podman info output yaml host: arch: arm64 buildahVersion: 1.29.0 cgroupControllers: - memory - pids cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.6-3.fc37.aarch64 path: /usr/bin/conmon version: 'conmon version 2.1.6, commit: ' cpuUtilization: idlePercent: 99.65 systemPercent: 0.12 userPercent: 0.24 cpus: 1 distribution: distribution: fedora variant: coreos version: ""37"" eventLogger: journald hostname: localhost.localdomain idMappings: gidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 589824 size: 65536 uidmap: - container_id: 0 host_id: 1001 size: 1 - container_id: 1 host_id: 589824 size: 65536 kernel: 6.1.14-200.fc37.aarch64 linkmode: dynamic logDriver: journald memFree: 925364224 memTotal: 2050260992 networkBackend: netavark ociRuntime: name: crun package: crun-1.8.1-1.fc37.aarch64 path: /usr/bin/crun version: |- crun version 1.8.1 commit: f8a096be060b22ccd3d5f3ebe44108517fbf6c30 rundir: /run/user/1001/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +LIBKRUN +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1001/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /usr/bin/slirp4netns package: slirp4netns-1.2.0-8.fc37.aarch64 version: |- slirp4netns version 1.2.0 commit: 656041d45cfca7a4176f6b7eed9e4fe6c11e8383 libslirp: 4.7.0 SLIRP_CONFIG_VERSION_MAX: 4 libseccomp: 2.5.3 swapFree: 0 swapTotal: 0 uptime: 31h 18m 41.00s (Approximately 1.29 days) plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - registry.fedoraproject.org - registry.access.redhat.com - docker.io - quay.io store: configFile: /var/home/test1/.config/containers/storage.conf containerStore: number: 0 paused: 0 running: 0 stopped: 0 graphDriverName: overlay graphOptions: {} graphRoot: /var/home/test1/.local/share/containers/storage graphRootAllocated: 10132369408 graphRootUsed: 1956737024 graphStatus: Backing Filesystem: xfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 1 runRoot: /run/user/1001/containers transientStore: false volumePath: /var/home/test1/.local/share/containers/storage/volumes version: APIVersion: 4.4.1 Built: 1676629538 BuiltTime: Fri Feb 17 10:25:38 2023 GitCommit: """" GoVersion: go1.19.5 Os: linux OsArch: linux/arm64 Version: 4.4.1   Podman in a container No  Privileged Or Rootless Rootless  Upstream Latest Release Yes  Additional environment details The commands above were run with __podman 4.4.1__ on Fedora CoreOS by using qemu on a macOS laptop (operating system: Ventura 13.2.1).  [test1@localhost ~]$ docker version Client: Version: 20.10.23 API version: 1.41 Go version: go1.19.5 Git commit: %{shortcommit_cli} Built: Sun Jan 29 17:38:04 2023 OS/Arch: linux/arm64 Context: default Experimental: true Server: Engine: Version: 20.10.23 API version: 1.41 (minimum version 1.12) Go version: go1.19.5 Git commit: %{shortcommit_moby} Built: Sun Jan 29 17:38:04 2023 OS/Arch: linux/arm64 Experimental: false containerd: Version: 1.6.15 GitCommit: runc: Version: 1.1.4 GitCommit: docker-init: Version: 0.19.0 GitCommit:   [test1@localhost ~]$ podman version Client: Podman Engine Version: 4.4.1 API Version: 4.4.1 Go Version: go1.19.5 Built: Fri Feb 17 10:25:38 2023 OS/Arch: linux/arm64   [test1@localhost ~]$ rpm -q podman podman-4.4.1-3.fc37.aarch64   Additional information I have another computer (arch: amd64) running Fedora 37. I re-run the Podman commands there with __podman 4.4.2__. The container image ID changed and the container image size changed but nothing else. source-file other-file other-file source-file test-file test-file source-file source-file source-file other-file",bug,0.95
25851,podman,https://github.com/containers/podman/issues/25851,Podman socket doesn't output EXPOSED ports," Issue Description The ports exposed of the containers are not returned by the podman socket (only the ones bind to the host) I'm using rootfull containers, I'm not sure if it's the same for rootless. I haven't been able to find another issue regarding this, so I'm not sure if this behavior is by design  Steps to reproduce the issue Steps to reproduce the issue:  $ sudo podman --version podman version 5.2.2 $ sudo podman run -d --name port-test -p 80:80 nginx:alpine $ sudo curl -s --unix-socket /run/podman/podman.sock http://localhost/containers/json?all=true | jq '.[] | select(.Names[] == ""/port-test"") | .Ports' [ { ""PrivatePort"": 80, ""PublicPort"": 80, ""Type"": ""tcp"" } ] $ sudo podman run -d --name expose-test --expose 80 nginx:alpine $ sudo curl -s --unix-socket /run/podman/podman.sock http://localhost/containers/json?all=true | jq '.[] | select(.Names[] == ""/expose-test"") | .Ports' []   Describe the results you received Exposed ports are not returned by the Podman socket []  Describe the results you expected Docker equivalent  $ sudo docker --version Docker version 27.3.1, build ce12230 $ sudo docker run -d --name port-test -p 80:80 nginx:alpine $ sudo curl -s --unix-socket /run/docker.sock http://localhost/containers/json?all=true | jq '.[] | select(.Names[] == ""/port-test"") | .Ports' [ { ""IP"": ""0.0.0.0"", ""PrivatePort"": 80, ""PublicPort"": 80, ""Type"": ""tcp"" }, ] $ sudo docker run -d --name expose-test --expose 80 nginx:alpine $ sudo curl -s --unix-socket /run/docker.sock http://localhost/containers/json?all=true | jq '.[] | select(.Names[] == ""/expose-test"") | .Ports' [ { ""PrivatePort"": 80, ""Type"": ""tcp"" }, ]   podman info output yaml host: arch: amd64 buildahVersion: 1.37.6 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.12-1.el9.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.12, commit: eb379dceb7efebd9a9d6b3349a57424d83483065' cpuUtilization: idlePercent: 99.61 systemPercent: 0.2 userPercent: 0.19 cpus: 4 databaseBackend: sqlite distribution: distribution: almalinux version: ""9.5"" eventLogger: journald freeLocks: 2046 hostname: almalinux9 idMappings: gidmap: null uidmap: null kernel: 5.14.0-503.35.1.el9_5.x86_64 linkmode: dynamic logDriver: journald memFree: 7268474880 memTotal: 8037085184 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.12.2-1.el9_5.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.12.2 package: netavark-1.12.2-1.el9.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.12.2 ociRuntime: name: crun package: crun-1.16.1-1.el9.x86_64 path: /usr/bin/crun version: |- crun version 1.16.1 commit: afa829ca0122bd5e1d67f1f38e6cc348027e3c32 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux pasta: executable: /bin/pasta package: passt-0^20240806.gee36266-7.el9_5.x86_64 version: | pasta 0^20240806.gee36266-7.el9_5.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /bin/slirp4netns package: slirp4netns-1.3.1-1.el9.x86_64 version: |- slirp4netns version 1.3.1 commit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 0 swapTotal: 0 uptime: 0h 54m 8.00s variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.access.redhat.com - registry.redhat.io - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 2 paused: 0 running: 2 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 40352677888 graphRootUsed: 1982976000 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 2 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.2.2 Built: 1743162783 BuiltTime: Fri Mar 28 11:53:03 2025 GitCommit: """" GoVersion: go1.22.9 (Red Hat 1.22.9-2.el9_5) Os: linux OsArch: linux/amd64 Version: 5.2.2   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details  $ cat /etc/*-release AlmaLinux release 9.5 (Teal Serval) NAME=""AlmaLinux"" VERSION=""9.5 (Teal Serval)"" ID=""almalinux"" ID_LIKE=""rhel centos fedora"" VERSION_ID=""9.5"" PLATFORM_ID=""platform:el9"" PRETTY_NAME=""AlmaLinux 9.5 (Teal Serval)"" ANSI_COLOR=""0;34"" LOGO=""fedora-logo-icon"" CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"" HOME_URL=""https://almalinux.org/"" DOCUMENTATION_URL=""https://wiki.almalinux.org/"" BUG_REPORT_URL=""https://bugs.almalinux.org/"" ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"" ALMALINUX_MANTISBT_PROJECT_VERSION=""9.5"" REDHAT_SUPPORT_PRODUCT=""AlmaLinux"" REDHAT_SUPPORT_PRODUCT_VERSION=""9.5"" SUPPORT_END=2032-06-01 AlmaLinux release 9.5 (Teal Serval) AlmaLinux release 9.5 (Teal Serval)   Additional information I'm using prometheus [docker_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#docker_sd_config) I want expose the containers metrics port, not bind it",source-file | source-file | test-file | source-file | test-file,"Podman socket doesn't output EXPOSED ports  Issue Description The ports exposed of the containers are not returned by the podman socket (only the ones bind to the host) I'm using rootfull containers, I'm not sure if it's the same for rootless. I haven't been able to find another issue regarding this, so I'm not sure if this behavior is by design  Steps to reproduce the issue Steps to reproduce the issue:  $ sudo podman --version podman version 5.2.2 $ sudo podman run -d --name port-test -p 80:80 nginx:alpine $ sudo curl -s --unix-socket /run/podman/podman.sock http://localhost/containers/json?all=true | jq '.[] | select(.Names[] == ""/port-test"") | .Ports' [ { ""PrivatePort"": 80, ""PublicPort"": 80, ""Type"": ""tcp"" } ] $ sudo podman run -d --name expose-test --expose 80 nginx:alpine $ sudo curl -s --unix-socket /run/podman/podman.sock http://localhost/containers/json?all=true | jq '.[] | select(.Names[] == ""/expose-test"") | .Ports' []   Describe the results you received Exposed ports are not returned by the Podman socket []  Describe the results you expected Docker equivalent  $ sudo docker --version Docker version 27.3.1, build ce12230 $ sudo docker run -d --name port-test -p 80:80 nginx:alpine $ sudo curl -s --unix-socket /run/docker.sock http://localhost/containers/json?all=true | jq '.[] | select(.Names[] == ""/port-test"") | .Ports' [ { ""IP"": ""0.0.0.0"", ""PrivatePort"": 80, ""PublicPort"": 80, ""Type"": ""tcp"" }, ] $ sudo docker run -d --name expose-test --expose 80 nginx:alpine $ sudo curl -s --unix-socket /run/docker.sock http://localhost/containers/json?all=true | jq '.[] | select(.Names[] == ""/expose-test"") | .Ports' [ { ""PrivatePort"": 80, ""Type"": ""tcp"" }, ]   podman info output yaml host: arch: amd64 buildahVersion: 1.37.6 cgroupControllers: - cpuset - cpu - io - memory - hugetlb - pids - rdma - misc cgroupManager: systemd cgroupVersion: v2 conmon: package: conmon-2.1.12-1.el9.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.12, commit: eb379dceb7efebd9a9d6b3349a57424d83483065' cpuUtilization: idlePercent: 99.61 systemPercent: 0.2 userPercent: 0.19 cpus: 4 databaseBackend: sqlite distribution: distribution: almalinux version: ""9.5"" eventLogger: journald freeLocks: 2046 hostname: almalinux9 idMappings: gidmap: null uidmap: null kernel: 5.14.0-503.35.1.el9_5.x86_64 linkmode: dynamic logDriver: journald memFree: 7268474880 memTotal: 8037085184 networkBackend: netavark networkBackendInfo: backend: netavark dns: package: aardvark-dns-1.12.2-1.el9_5.x86_64 path: /usr/libexec/podman/aardvark-dns version: aardvark-dns 1.12.2 package: netavark-1.12.2-1.el9.x86_64 path: /usr/libexec/podman/netavark version: netavark 1.12.2 ociRuntime: name: crun package: crun-1.16.1-1.el9.x86_64 path: /usr/bin/crun version: |- crun version 1.16.1 commit: afa829ca0122bd5e1d67f1f38e6cc348027e3c32 rundir: /run/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +YAJL os: linux pasta: executable: /bin/pasta package: passt-0^20240806.gee36266-7.el9_5.x86_64 version: | pasta 0^20240806.gee36266-7.el9_5.x86_64 Copyright Red Hat GNU General Public License, version 2 or later <https://www.gnu.org/licenses/old-licenses/gpl-2.0.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. remoteSocket: exists: true path: /run/podman/podman.sock rootlessNetworkCmd: pasta security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: false seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: true serviceIsRemote: false slirp4netns: executable: /bin/slirp4netns package: slirp4netns-1.3.1-1.el9.x86_64 version: |- slirp4netns version 1.3.1 commit: e5e368c4f5db6ae75c2fce786e31eef9da6bf236 libslirp: 4.4.0 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.2 swapFree: 0 swapTotal: 0 uptime: 0h 54m 8.00s variant: """" plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan - ipvlan volume: - local registries: search: - registry.access.redhat.com - registry.redhat.io - docker.io store: configFile: /etc/containers/storage.conf containerStore: number: 2 paused: 0 running: 2 stopped: 0 graphDriverName: overlay graphOptions: overlay.mountopt: nodev,metacopy=on graphRoot: /var/lib/containers/storage graphRootAllocated: 40352677888 graphRootUsed: 1982976000 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""false"" Supports d_type: ""true"" Supports shifting: ""false"" Supports volatile: ""true"" Using metacopy: ""true"" imageCopyTmpDir: /var/tmp imageStore: number: 2 runRoot: /run/containers/storage transientStore: false volumePath: /var/lib/containers/storage/volumes version: APIVersion: 5.2.2 Built: 1743162783 BuiltTime: Fri Mar 28 11:53:03 2025 GitCommit: """" GoVersion: go1.22.9 (Red Hat 1.22.9-2.el9_5) Os: linux OsArch: linux/amd64 Version: 5.2.2   Podman in a container No  Privileged Or Rootless Privileged  Upstream Latest Release No  Additional environment details  $ cat /etc/*-release AlmaLinux release 9.5 (Teal Serval) NAME=""AlmaLinux"" VERSION=""9.5 (Teal Serval)"" ID=""almalinux"" ID_LIKE=""rhel centos fedora"" VERSION_ID=""9.5"" PLATFORM_ID=""platform:el9"" PRETTY_NAME=""AlmaLinux 9.5 (Teal Serval)"" ANSI_COLOR=""0;34"" LOGO=""fedora-logo-icon"" CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos"" HOME_URL=""https://almalinux.org/"" DOCUMENTATION_URL=""https://wiki.almalinux.org/"" BUG_REPORT_URL=""https://bugs.almalinux.org/"" ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9"" ALMALINUX_MANTISBT_PROJECT_VERSION=""9.5"" REDHAT_SUPPORT_PRODUCT=""AlmaLinux"" REDHAT_SUPPORT_PRODUCT_VERSION=""9.5"" SUPPORT_END=2032-06-01 AlmaLinux release 9.5 (Teal Serval) AlmaLinux release 9.5 (Teal Serval)   Additional information I'm using prometheus [docker_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#docker_sd_config) I want expose the containers metrics port, not bind it source-file source-file test-file source-file test-file",bug,0.95
15485,podman,https://github.com/containers/podman/issues/15485,invalid event on image removal + missing untag event from REST API,"<!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** Removing an image should send a `delete` event but I'm receiving a `remove` event https://docs.docker.com/engine/api/v1.41/#tag/System/operation/SystemEvents states: Images report these events: `delete`, `import`, `load`, `pull`, `push`, `save`, `tag`, `untag`, and `prune` **Steps to reproduce the issue:** 1. pull an image with docker bash $ docker pull httpd  track events in a separate terminal  $ curl --unix-socket /var/run/docker.sock http:/v1.41/events  remove the image  $ docker rmi a981c8992512  look at the events  {""status"":""untag"",""id"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Type"":""image"",""Action"":""untag"",""Actor"":{""ID"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Attributes"":{""name"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825""}},""scope"":""local"",""time"":1661444877,""timeNano"":1661444877274518985} {""status"":""untag"",""id"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Type"":""image"",""Action"":""untag"",""Actor"":{""ID"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Attributes"":{""name"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825""}},""scope"":""local"",""time"":1661444877,""timeNano"":1661444877278753809} {""status"":""delete"",""id"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Type"":""image"",""Action"":""delete"",""Actor"":{""ID"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Attributes"":{""name"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825""}},""scope"":""local"",""time"":1661444877,""timeNano"":1661444877436313927}  2. Now, try with podman pull httpd image  $ podman pull httpd Resolving ""httpd"" using unqualified-search registries (/etc/containers/registries.conf.d/999-podman-machine.conf) Trying to pull docker.io/library/httpd:latest Getting image source signatures Copying blob sha256:152876b0d24a5561415915c652dceeb3bcd5080dc24c3994e77343a81f64b9f1 Copying blob sha256:7a6db449b51b92eac5c81cdbd82917785343f1664b2be57b22337b0a40c5b29d Copying blob sha256:b4effd428409f1da4dc8c896afb9818ef1ba24be5e7e5e3d86dea650ac3ed8bc Copying blob sha256:6b29c2b62286e254216da297b4066a4bb79b918bc02811ba954bd7b8fd51360b Copying blob sha256:c2123effa3fcb96c62bf4d891147aef09029d429321bfb4c6a535fa3ca7e13d6 Copying config sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825 Writing manifest to image destination Storing signatures a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825  Track events in a separate terminal  $ curl --unix-socket /Users/benoitf/.local/share/containers/podman/machine/podman-machine-default/podman.sock http:/v1.41/events  remove the image  $ podman rmi a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825  Untagged: docker.io/library/httpd:latest Deleted: a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825 look at events  {""status"":""remove"",""id"":""a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Type"":""image"",""Action"":""remove"",""Actor"":{""ID"":""a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Attributes"":{""containerExitCode"":""0"",""image"":"""",""name"":""a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825""}},""scope"":""local"",""time"":1661444997,""timeNano"":1661444997046721781,""HealthStatus"":""""}  we are missing the `untag` event and we receive a `remove` event instead of `delete` **Describe the results you received:** receive `remove` **Describe the results you expected:** expect `untag` + `delete` events **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Client: Podman Engine Version: 4.2.0 API Version: 4.2.0 Go Version: go1.18.5 Built: Wed Aug 10 22:46:05 2022 OS/Arch: darwin/amd64 Server: Podman Engine Version: 4.2.0 API Version: 4.2.0 Go Version: go1.18.4 Built: Thu Aug 11 16:42:17 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  (paste your output here)  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  (paste your output here)  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes/No **Additional environment details (AWS, VirtualBox, physical, etc.):**",source-file | test-file | other-file | other-file | test-file | source-file | other-file | source-file | test-file,"invalid event on image removal + missing untag event from REST API <!--  BUG REPORT INFORMATION  Use the commands below to provide key information from your environment: You do NOT have to include this information if this is a FEATURE REQUEST **NOTE** A large number of issues reported against Podman are often found to already be fixed in more current versions of the project. Before reporting an issue, please verify the version you are running with `podman version` and compare it to the latest release documented on the top of Podman's [README.md](../README.md). If they differ, please update your version of Podman to the latest possible and retry your command before creating an issue. Also, there is a running list of known issues in the [Podman Troubleshooting Guide](https://github.com/containers/podman/blob/main/troubleshooting.md), please reference that page before opening a new issue. If you are filing a bug against `podman build`, please instead file a bug against Buildah (https://github.com/containers/buildah/issues). Podman build executes Buildah to perform container builds, and as such the Buildah maintainers are best equipped to handle these bugs. --> **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** Removing an image should send a `delete` event but I'm receiving a `remove` event https://docs.docker.com/engine/api/v1.41/#tag/System/operation/SystemEvents states: Images report these events: `delete`, `import`, `load`, `pull`, `push`, `save`, `tag`, `untag`, and `prune` **Steps to reproduce the issue:** 1. pull an image with docker bash $ docker pull httpd  track events in a separate terminal  $ curl --unix-socket /var/run/docker.sock http:/v1.41/events  remove the image  $ docker rmi a981c8992512  look at the events  {""status"":""untag"",""id"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Type"":""image"",""Action"":""untag"",""Actor"":{""ID"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Attributes"":{""name"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825""}},""scope"":""local"",""time"":1661444877,""timeNano"":1661444877274518985} {""status"":""untag"",""id"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Type"":""image"",""Action"":""untag"",""Actor"":{""ID"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Attributes"":{""name"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825""}},""scope"":""local"",""time"":1661444877,""timeNano"":1661444877278753809} {""status"":""delete"",""id"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Type"":""image"",""Action"":""delete"",""Actor"":{""ID"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Attributes"":{""name"":""sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825""}},""scope"":""local"",""time"":1661444877,""timeNano"":1661444877436313927}  2. Now, try with podman pull httpd image  $ podman pull httpd Resolving ""httpd"" using unqualified-search registries (/etc/containers/registries.conf.d/999-podman-machine.conf) Trying to pull docker.io/library/httpd:latest Getting image source signatures Copying blob sha256:152876b0d24a5561415915c652dceeb3bcd5080dc24c3994e77343a81f64b9f1 Copying blob sha256:7a6db449b51b92eac5c81cdbd82917785343f1664b2be57b22337b0a40c5b29d Copying blob sha256:b4effd428409f1da4dc8c896afb9818ef1ba24be5e7e5e3d86dea650ac3ed8bc Copying blob sha256:6b29c2b62286e254216da297b4066a4bb79b918bc02811ba954bd7b8fd51360b Copying blob sha256:c2123effa3fcb96c62bf4d891147aef09029d429321bfb4c6a535fa3ca7e13d6 Copying config sha256:a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825 Writing manifest to image destination Storing signatures a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825  Track events in a separate terminal  $ curl --unix-socket /Users/benoitf/.local/share/containers/podman/machine/podman-machine-default/podman.sock http:/v1.41/events  remove the image  $ podman rmi a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825  Untagged: docker.io/library/httpd:latest Deleted: a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825 look at events  {""status"":""remove"",""id"":""a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Type"":""image"",""Action"":""remove"",""Actor"":{""ID"":""a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825"",""Attributes"":{""containerExitCode"":""0"",""image"":"""",""name"":""a981c8992512d65c9b450a9ecabb1cb9d35bb6b03f3640f86471032d5800d825""}},""scope"":""local"",""time"":1661444997,""timeNano"":1661444997046721781,""HealthStatus"":""""}  we are missing the `untag` event and we receive a `remove` event instead of `delete` **Describe the results you received:** receive `remove` **Describe the results you expected:** expect `untag` + `delete` events **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  Client: Podman Engine Version: 4.2.0 API Version: 4.2.0 Go Version: go1.18.5 Built: Wed Aug 10 22:46:05 2022 OS/Arch: darwin/amd64 Server: Podman Engine Version: 4.2.0 API Version: 4.2.0 Go Version: go1.18.4 Built: Thu Aug 11 16:42:17 2022 OS/Arch: linux/amd64  **Output of `podman info`:**  (paste your output here)  **Package info (e.g. output of `rpm -q podman` or `apt list podman`):**  (paste your output here)  **Have you tested with the latest version of Podman and have you checked the Podman Troubleshooting Guide? (https://github.com/containers/podman/blob/main/troubleshooting.md)** Yes/No **Additional environment details (AWS, VirtualBox, physical, etc.):** source-file test-file other-file other-file test-file source-file other-file source-file test-file",bug,0.95
17585,podman,https://github.com/containers/podman/issues/17585,Creating an Existing Network Via REST API Returns Different Status Code vs. Docker," Issue Description When trying to run [Supabase](https://supabase.com) locally with their CLI you will always get a `network already exists` error. Copied [comment](https://github.com/containers/podman/issues/15499#issuecomment-1331508660) from #15499 by @jgraettinger: > I ran into this while attempting to use the supabase CLI tool with podman instead of docker. > > The supabase CLI spins up a bunch of containers, and at several points [attempts to create a docker > network if it doesn't already exist](https://github.com/supabase/cli/blob/main/internal/utils/docker.go?rgh-link-date=2022-11-30T00%3A57%3A59Z#L51-L68), as determined by [docker library error helpers which are explicitly looking for HTTP 409 status](https://github.com/moby/moby/blob/master/errdefs/http_helpers.go?rgh-link-date=2022-11-30T00%3A57%3A59Z#L20), and don't properly interpret podman's 500. > > I'm unsure if this belongs as a separate issue, but it's an example where the HTTP 500 rather than 409 behavior diverges from the Docker REST API.  Steps to reproduce the issue Steps to reproduce the issue 1. Set up and run a Supabase Project locally - [Guide](https://supabase.com/docs/guides/cli/local-development#prerequisites) 2. See error  Describe the results you received Podman says that the network already exists, but when you run `podman network ls` it does not. None of the containers have started.  Describe the results you expected The containers to all be running and accessible.  podman info output yaml  podman info host: arch: amd64 buildahVersion: 1.28.0 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.5-1.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: ' cpuUtilization: idlePercent: 99.68 systemPercent: 0.18 userPercent: 0.14 cpus: 32 distribution: distribution: fedora variant: container version: ""36"" eventLogger: journald hostname: UMBRA idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.15.57.1-microsoft-standard-WSL2+ linkmode: dynamic logDriver: journald memFree: 15241453568 memTotal: 16722722816 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-3.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 4294967296 swapTotal: 4294967296 uptime: 0h 15m 0.00s plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /home/user/.config/containers/storage.conf containerStore: number: 1 paused: 0 running: 0 stopped: 1 graphDriverName: overlay graphOptions: {} graphRoot: /home/user/.local/share/containers/storage graphRootAllocated: 1081101176832 graphRootUsed: 10558271488 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 20 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/user/.local/share/containers/storage/volumes version: APIVersion: 4.3.1 Built: 1668180253 BuiltTime: Fri Nov 11 16:24:13 2022 GitCommit: """" GoVersion: go1.18.7 Os: linux OsArch: linux/amd64 Version: 4.3.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_",source-file | test-file | source-file | test-file,"Creating an Existing Network Via REST API Returns Different Status Code vs. Docker  Issue Description When trying to run [Supabase](https://supabase.com) locally with their CLI you will always get a `network already exists` error. Copied [comment](https://github.com/containers/podman/issues/15499#issuecomment-1331508660) from #15499 by @jgraettinger: > I ran into this while attempting to use the supabase CLI tool with podman instead of docker. > > The supabase CLI spins up a bunch of containers, and at several points [attempts to create a docker > network if it doesn't already exist](https://github.com/supabase/cli/blob/main/internal/utils/docker.go?rgh-link-date=2022-11-30T00%3A57%3A59Z#L51-L68), as determined by [docker library error helpers which are explicitly looking for HTTP 409 status](https://github.com/moby/moby/blob/master/errdefs/http_helpers.go?rgh-link-date=2022-11-30T00%3A57%3A59Z#L20), and don't properly interpret podman's 500. > > I'm unsure if this belongs as a separate issue, but it's an example where the HTTP 500 rather than 409 behavior diverges from the Docker REST API.  Steps to reproduce the issue Steps to reproduce the issue 1. Set up and run a Supabase Project locally - [Guide](https://supabase.com/docs/guides/cli/local-development#prerequisites) 2. See error  Describe the results you received Podman says that the network already exists, but when you run `podman network ls` it does not. None of the containers have started.  Describe the results you expected The containers to all be running and accessible.  podman info output yaml  podman info host: arch: amd64 buildahVersion: 1.28.0 cgroupControllers: [] cgroupManager: cgroupfs cgroupVersion: v1 conmon: package: conmon-2.1.5-1.fc36.x86_64 path: /usr/bin/conmon version: 'conmon version 2.1.5, commit: ' cpuUtilization: idlePercent: 99.68 systemPercent: 0.18 userPercent: 0.14 cpus: 32 distribution: distribution: fedora variant: container version: ""36"" eventLogger: journald hostname: UMBRA idMappings: gidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 uidmap: - container_id: 0 host_id: 1000 size: 1 - container_id: 1 host_id: 100000 size: 65536 kernel: 5.15.57.1-microsoft-standard-WSL2+ linkmode: dynamic logDriver: journald memFree: 15241453568 memTotal: 16722722816 networkBackend: netavark ociRuntime: name: crun package: crun-1.7.2-3.fc36.x86_64 path: /usr/bin/crun version: |- crun version 1.7.2 commit: 0356bf4aff9a133d655dc13b1d9ac9424706cac4 rundir: /run/user/1000/crun spec: 1.0.0 +SYSTEMD +SELINUX +APPARMOR +CAP +SECCOMP +EBPF +CRIU +WASM:wasmedge +YAJL os: linux remoteSocket: exists: true path: /run/user/1000/podman/podman.sock security: apparmorEnabled: false capabilities: CAP_CHOWN,CAP_DAC_OVERRIDE,CAP_FOWNER,CAP_FSETID,CAP_KILL,CAP_NET_BIND_SERVICE,CAP_SETFCAP,CAP_SETGID,CAP_SETPCAP,CAP_SETUID,CAP_SYS_CHROOT rootless: true seccompEnabled: true seccompProfilePath: /usr/share/containers/seccomp.json selinuxEnabled: false serviceIsRemote: true slirp4netns: executable: /usr/bin/slirp4netns slirp4netns version 1.2.0-beta.0 commit: 477db14a24ff1a3de3a705e51ca2c4c1fe3dda64 libslirp: 4.6.1 SLIRP_CONFIG_VERSION_MAX: 3 libseccomp: 2.5.3 swapFree: 4294967296 swapTotal: 4294967296 uptime: 0h 15m 0.00s plugins: authorization: null log: - k8s-file - none - passthrough - journald network: - bridge - macvlan volume: - local registries: search: - docker.io store: configFile: /home/user/.config/containers/storage.conf containerStore: number: 1 paused: 0 running: 0 stopped: 1 graphDriverName: overlay graphOptions: {} graphRoot: /home/user/.local/share/containers/storage graphRootAllocated: 1081101176832 graphRootUsed: 10558271488 graphStatus: Backing Filesystem: extfs Native Overlay Diff: ""true"" Supports d_type: ""true"" Using metacopy: ""false"" imageCopyTmpDir: /var/tmp imageStore: number: 20 runRoot: /run/user/1000/containers transientStore: false volumePath: /home/user/.local/share/containers/storage/volumes version: APIVersion: 4.3.1 Built: 1668180253 BuiltTime: Fri Nov 11 16:24:13 2022 GitCommit: """" GoVersion: go1.18.7 Os: linux OsArch: linux/amd64 Version: 4.3.1   Podman in a container No  Privileged Or Rootless None  Upstream Latest Release Yes  Additional environment details _No response_  Additional information _No response_ source-file test-file source-file test-file",bug,0.95
16915,podman,https://github.com/containers/podman/issues/16915,--network=default doesn't mean the same thing as --network=,"**Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** While debugging some unexpected interactions between a docker client and podman server on my side, I found out that the flag `--network=default` doesn't mean the same thing as `--network=` (or the absence of `--network=XXX` flag). Note that docker docker client will explicitly pass NetworkMode=default when calling the docker daemon HTTP api, if no `--network` flag was provided to the docker CLI. **Steps to reproduce the issue:** All this was done using the very latest `quay.io/podman/upstream` image started in privileged mode. 1. Create couple of containers, with different `--network` flags:  [root@a3a34f67a163 /]# podman create --cidfile=no-explicit-flag fedora Resolved ""fedora"" as an alias (/etc/containers/registries.conf.d/000-shortnames.conf) Trying to pull registry.fedoraproject.org/fedora:latest Getting image source signatures Copying blob 1842e4e4b562 done Copying config 19c0ae4dd2 done Writing manifest to image destination Storing signatures b987b9bc53e056f024da0ed0cca4fc9e598edd512df7ffad8dc007d740c3b790 [root@a3a34f67a163 /]# podman create --network= --cidfile=explicitly-empty-flag fedora 92023ad1e615bcea04be21569a1139446a1c0aa2b69d3298d926ef4c5de1d045 [root@a3a34f67a163 /]# podman create --network=default --cidfile=explicitly-default-flag fedora b7287211a64864d4bad9872c973be2abbde56d67e0f43caa44348aa7717a1944  2. Now inspect a bit the NetworkMode for each (note: I use the `quay.io/podman/upstream` which has a containers.conf file which hardcodes the network mode to `host`:  [root@a3a34f67a163 /]# podman inspect $(cat no-explicit-flag)|grep NetworkMode ""NetworkMode"": ""host"", [root@a3a34f67a163 /]# podman inspect $(cat explicitly-empty-flag)|grep NetworkMode ""NetworkMode"": ""host"", [root@a3a34f67a163 /]# podman inspect $(cat explicitly-default-flag)|grep NetworkMode ""NetworkMode"": ""bridge"",  3. Now install the Docker CLI, start a podman socket with `system service` (I won't show how to do it), and try to create a container with no explicit `--network` flag with the docker client, it will use the bridge mode by default:  [root@a3a34f67a163 /]# DOCKER_HOST=unix:var/run/podman/podman.sock docker create --cidfile=no-explicit-flag-from-docker-cli fedora 1ebc2db1ef56c75a5588e3c0599c554835a7a352b700856b3fe53fbe1e53b48b [root@a3a34f67a163 /]# podman --remote inspect $(cat no-explicit-flag-from-docker-cli)|grep NetworkMode ""NetworkMode"": ""bridge"",  **Describe the results you received:** The explicit `--network=default` flags resolves to the `bridge` network mode, while I would have expected the `host` one (in my conditions, as this is one overriden in the `containers.conf` file). **Describe the results you expected:** I would expect that in the same conditions, a container started explicitly with `--network=default` (or a container started with the docker CLI without providing any `--network` flag) uses the `host` network mode (as this is one overriden in the `containers.conf` file), just like when we run podman with no explicit `--network` flag. Basically no matter which client podman vs docker is used, the end result should be the same. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  [root@a3a34f67a163 /]# podman version Client: Podman Engine Version: 4.4.0-dev API Version: 4.4.0-dev Go Version: go1.19.3 Built: Thu Jan 1 00:00:00 1970 OS/Arch: linux/amd64",source-file | test-file | test-file,"--network=default doesn't mean the same thing as --network= **Is this a BUG REPORT or FEATURE REQUEST? (leave only one on its own line)** /kind bug **Description** While debugging some unexpected interactions between a docker client and podman server on my side, I found out that the flag `--network=default` doesn't mean the same thing as `--network=` (or the absence of `--network=XXX` flag). Note that docker docker client will explicitly pass NetworkMode=default when calling the docker daemon HTTP api, if no `--network` flag was provided to the docker CLI. **Steps to reproduce the issue:** All this was done using the very latest `quay.io/podman/upstream` image started in privileged mode. 1. Create couple of containers, with different `--network` flags:  [root@a3a34f67a163 /]# podman create --cidfile=no-explicit-flag fedora Resolved ""fedora"" as an alias (/etc/containers/registries.conf.d/000-shortnames.conf) Trying to pull registry.fedoraproject.org/fedora:latest Getting image source signatures Copying blob 1842e4e4b562 done Copying config 19c0ae4dd2 done Writing manifest to image destination Storing signatures b987b9bc53e056f024da0ed0cca4fc9e598edd512df7ffad8dc007d740c3b790 [root@a3a34f67a163 /]# podman create --network= --cidfile=explicitly-empty-flag fedora 92023ad1e615bcea04be21569a1139446a1c0aa2b69d3298d926ef4c5de1d045 [root@a3a34f67a163 /]# podman create --network=default --cidfile=explicitly-default-flag fedora b7287211a64864d4bad9872c973be2abbde56d67e0f43caa44348aa7717a1944  2. Now inspect a bit the NetworkMode for each (note: I use the `quay.io/podman/upstream` which has a containers.conf file which hardcodes the network mode to `host`:  [root@a3a34f67a163 /]# podman inspect $(cat no-explicit-flag)|grep NetworkMode ""NetworkMode"": ""host"", [root@a3a34f67a163 /]# podman inspect $(cat explicitly-empty-flag)|grep NetworkMode ""NetworkMode"": ""host"", [root@a3a34f67a163 /]# podman inspect $(cat explicitly-default-flag)|grep NetworkMode ""NetworkMode"": ""bridge"",  3. Now install the Docker CLI, start a podman socket with `system service` (I won't show how to do it), and try to create a container with no explicit `--network` flag with the docker client, it will use the bridge mode by default:  [root@a3a34f67a163 /]# DOCKER_HOST=unix:var/run/podman/podman.sock docker create --cidfile=no-explicit-flag-from-docker-cli fedora 1ebc2db1ef56c75a5588e3c0599c554835a7a352b700856b3fe53fbe1e53b48b [root@a3a34f67a163 /]# podman --remote inspect $(cat no-explicit-flag-from-docker-cli)|grep NetworkMode ""NetworkMode"": ""bridge"",  **Describe the results you received:** The explicit `--network=default` flags resolves to the `bridge` network mode, while I would have expected the `host` one (in my conditions, as this is one overriden in the `containers.conf` file). **Describe the results you expected:** I would expect that in the same conditions, a container started explicitly with `--network=default` (or a container started with the docker CLI without providing any `--network` flag) uses the `host` network mode (as this is one overriden in the `containers.conf` file), just like when we run podman with no explicit `--network` flag. Basically no matter which client podman vs docker is used, the end result should be the same. **Additional information you deem important (e.g. issue happens only occasionally):** **Output of `podman version`:**  [root@a3a34f67a163 /]# podman version Client: Podman Engine Version: 4.4.0-dev API Version: 4.4.0-dev Go Version: go1.19.3 Built: Thu Jan 1 00:00:00 1970 OS/Arch: linux/amd64 source-file test-file test-file",bug,0.9
