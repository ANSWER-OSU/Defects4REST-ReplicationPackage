issue_no,repo,issue_url,title,description,patched_file_types,text_for_topic_modeling,prediction,confidence
5157,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5157,Freeze in setting>monitor history," I have found these related issues/pull requests none   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description After going to the settings and the monitor history, it is then impossible to switch to another category. https://github.com/user-attachments/assets/3e94e45f-d8b9-41a6-b1b5-f1e2cf07ce4d  Reproduction steps go to settings > monitor history and switch catgorie  Expected behavior We should be able to switch from the monitor history to Docker Hosts or Tags ect  Actual Behavior We remain stuck on the monitor history  Uptime-Kuma Version 2.0.0-dev  Operating System and Arch windows  Browser Google Chrome   Deployment Environment - Runtime: - Database: sqlite - Filesystem used to store the database on: - number of monitors:  Relevant log output _No response_",source-file | source-file,"Freeze in setting>monitor history  I have found these related issues/pull requests none   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description After going to the settings and the monitor history, it is then impossible to switch to another category. https://github.com/user-attachments/assets/3e94e45f-d8b9-41a6-b1b5-f1e2cf07ce4d  Reproduction steps go to settings > monitor history and switch catgorie  Expected behavior We should be able to switch from the monitor history to Docker Hosts or Tags ect  Actual Behavior We remain stuck on the monitor history  Uptime-Kuma Version 2.0.0-dev  Operating System and Arch windows  Browser Google Chrome   Deployment Environment - Runtime: - Database: sqlite - Filesystem used to store the database on: - number of monitors:  Relevant log output _No response_ source-file source-file",no-bug,0.8
3301,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3301,Total Time for outage,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification, UI Feature  Feature description Hi, is it possible to add into the notification and UI how long a monitor was down for.   Solution It would be great if within the notification and UI when a monitor goes down and comes back up it shows how long the outage was for. i.e. ([Monitor Name] [ Up] 200 - OK Time (UTC): 2023-06-22 02:42:48.611 **(Down for 2 mins 10 seconds)** and in the UI something similar.   Alternatives _No response_  Additional Context _No response_",,"Total Time for outage   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification, UI Feature  Feature description Hi, is it possible to add into the notification and UI how long a monitor was down for.   Solution It would be great if within the notification and UI when a monitor goes down and comes back up it shows how long the outage was for. i.e. ([Monitor Name] [ Up] 200 - OK Time (UTC): 2023-06-22 02:42:48.611 **(Down for 2 mins 10 seconds)** and in the UI something similar.   Alternatives _No response_  Additional Context _No response_",no-bug,0.95
2958,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2958,Copy button doesn't work,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Copy button doesn't work when creating API keys.  Reproduction steps Create key, press copy button  Expected behavior copy key  Actual Behavior no copy  Uptime-Kuma Version 1.21.1  Operating System and Arch DSM 7.1 (Docker)  Browser Version 111.0.5563.65 (Official Build) (64-bit)  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_",other-file,"Copy button doesn't work   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Copy button doesn't work when creating API keys.  Reproduction steps Create key, press copy button  Expected behavior copy key  Actual Behavior no copy  Uptime-Kuma Version 1.21.1  Operating System and Arch DSM 7.1 (Docker)  Browser Version 111.0.5563.65 (Official Build) (64-bit)  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_ other-file",no-bug,0.9
560,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/560,"Label cant't fit in the ""pill"" margins","**Is it a duplicate question?** Can't find another one, maybe because it is language specific. **Info** Uptime Kuma Version: 1.7.3 Using Docker?: Yes Docker Version: 20.10.8 OS: Ubuntu 20.04, Windows 10 Browser: Firefox, Chrome **The Problem** On language where the word for ""Down"" is a bit longer, the text can't fit in the ""red pill"" margins on the Dashboard. **Visual representation** ![uptime-kuma-issue-01](https://user-images.githubusercontent.com/66828538/135970507-72eb0473-057e-44f4-81dd-2f9e613a5c80.jpg) I don't think it's a bug, that's why i put the ""help"" label. **Questions** - Do I need to find a shorter word and change it in the `bg-BG.js` language file? Or, - Is it possible for someone with skills to make the ""red pill"" bigger, or some kind of ""auto fit"" trick, if it is not very time consuming?",other-file,"Label cant't fit in the ""pill"" margins **Is it a duplicate question?** Can't find another one, maybe because it is language specific. **Info** Uptime Kuma Version: 1.7.3 Using Docker?: Yes Docker Version: 20.10.8 OS: Ubuntu 20.04, Windows 10 Browser: Firefox, Chrome **The Problem** On language where the word for ""Down"" is a bit longer, the text can't fit in the ""red pill"" margins on the Dashboard. **Visual representation** ![uptime-kuma-issue-01](https://user-images.githubusercontent.com/66828538/135970507-72eb0473-057e-44f4-81dd-2f9e613a5c80.jpg) I don't think it's a bug, that's why i put the ""help"" label. **Questions** - Do I need to find a shorter word and change it in the `bg-BG.js` language file? Or, - Is it possible for someone with skills to make the ""red pill"" bigger, or some kind of ""auto fit"" trick, if it is not very time consuming? other-file",no-bug,0.95
859,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/859,[Feature]: Show monitor name in DingDing notification,"  Feature Request Type New Notification  Feature description https://github.com/louislam/uptime-kuma/blob/master/server/notification-providers/dingding.js In DingDing notification, the receiver cannot easily see the monitor name msgtype: ""markdown"", markdown: { title: monitorJSON[""name""], text: `## [${this.statusToString(heartbeatJSON[""status""])}] \n > ${heartbeatJSON[""msg""]} \n > Time(UTC):${heartbeatJSON[""time""]}`, } In this section, the title is only displayed in the preview, it is not displayed in the notification details The details will only appear like this ![image](https://user-images.githubusercontent.com/37480123/139703488-f1aa76c7-ff0f-4e36-a34f-115995cbc315.png) I think it should add the monitor name after heartbeatJSON[""status""]   Solution I think it should add the monitor name after heartbeatJSON[""status""] in /server/notification-providers/dingding.js   Alternatives None  Additional Context None   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request",source-file | test-file,"[Feature]: Show monitor name in DingDing notification   Feature Request Type New Notification  Feature description https://github.com/louislam/uptime-kuma/blob/master/server/notification-providers/dingding.js In DingDing notification, the receiver cannot easily see the monitor name msgtype: ""markdown"", markdown: { title: monitorJSON[""name""], text: `## [${this.statusToString(heartbeatJSON[""status""])}] \n > ${heartbeatJSON[""msg""]} \n > Time(UTC):${heartbeatJSON[""time""]}`, } In this section, the title is only displayed in the preview, it is not displayed in the notification details The details will only appear like this ![image](https://user-images.githubusercontent.com/37480123/139703488-f1aa76c7-ff0f-4e36-a34f-115995cbc315.png) I think it should add the monitor name after heartbeatJSON[""status""]   Solution I think it should add the monitor name after heartbeatJSON[""status""] in /server/notification-providers/dingding.js   Alternatives None  Additional Context None   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request source-file test-file",no-bug,0.95
1510,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1510,uptime-kuma reset password err,"  Please verify that this bug has NOT been raised before. - [x] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem There was an error resetting my password  docker exec -it uptime-kuma npm run reset-password > uptime-kuma@1.14.0 reset-password > node extra/reset-password.js == Uptime Kuma Reset Password Tool == Connecting the database Data Dir: ./data/ Found user: admin New Password: 123456 Confirm New Password: 123456 Error: user.resetPassword is not a function Closing the database SQLite closed Finished.   Uptime-Kuma Version 1.14.0  Operating System and Arch Debian 10  Browser Google Chrome  Docker Version Docker version 20.10.12, build e91ed57  NodeJS Version v16.14.2",other-file | source-file | documentation-file | test-file | other-file | other-file | documentation-file | documentation-file | documentation-file | source-file | source-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file,"uptime-kuma reset password err   Please verify that this bug has NOT been raised before. - [x] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem There was an error resetting my password  docker exec -it uptime-kuma npm run reset-password > uptime-kuma@1.14.0 reset-password > node extra/reset-password.js == Uptime Kuma Reset Password Tool == Connecting the database Data Dir: ./data/ Found user: admin New Password: 123456 Confirm New Password: 123456 Error: user.resetPassword is not a function Closing the database SQLite closed Finished.   Uptime-Kuma Version 1.14.0  Operating System and Arch Debian 10  Browser Google Chrome  Docker Version Docker version 20.10.12, build e91ed57  NodeJS Version v16.14.2 other-file source-file documentation-file test-file other-file other-file documentation-file documentation-file documentation-file source-file source-file database-file database-file database-file database-file container-file container-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file",no-bug,0.9
1622,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1622,Add Notification support for Ntfy,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification  Feature description Ntfy as a notification agent, it is a rapidly growing very popular sub-pub notification service and I know lots of people who want this to be included in uptime kuma as they both would go extremely well together.   Solution https://ntfy.sh/docs Allow publishing via http or curl with options to change tags,topic,message,title, and priority, examples below: HTTP:  POST / HTTP/1.1 Host: example..com { ""topic"": ""uptimekuma"", ""message"": ""Site is down"", ""title"": ""Site Down Alert"", ""tags"": [""warning"",""loudspeaker""], ""priority"": 4 }  Curl:  curl ntfy.sh \ -d '{ ""topic"": ""uptimekuma"", ""message"": ""Site is down"", ""title"": ""Low disk space alert"", ""tags"": [""warning"",""loudspeaker""], ""priority"": 4 }'    Alternatives _No response_  Additional Context _No response_",source-file | source-file | other-file | source-file | source-file | source-file | other-file | source-file,"Add Notification support for Ntfy   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification  Feature description Ntfy as a notification agent, it is a rapidly growing very popular sub-pub notification service and I know lots of people who want this to be included in uptime kuma as they both would go extremely well together.   Solution https://ntfy.sh/docs Allow publishing via http or curl with options to change tags,topic,message,title, and priority, examples below: HTTP:  POST / HTTP/1.1 Host: example..com { ""topic"": ""uptimekuma"", ""message"": ""Site is down"", ""title"": ""Site Down Alert"", ""tags"": [""warning"",""loudspeaker""], ""priority"": 4 }  Curl:  curl ntfy.sh \ -d '{ ""topic"": ""uptimekuma"", ""message"": ""Site is down"", ""title"": ""Low disk space alert"", ""tags"": [""warning"",""loudspeaker""], ""priority"": 4 }'    Alternatives _No response_  Additional Context _No response_ source-file source-file other-file source-file source-file source-file other-file source-file",no-bug,0.95
897,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/897,uptime-kuma container image and logs,"Hi there I run uptime-kuma as a container behind a nginx reverse proxy. Everything is working fine except except seeing the following entries filled up the docker log. If I do not do anything, it would fill up the 20GB of the host storage in a few days. It may well be my set up though. Here are the logs: {""log"":""Timeout duration was set to 1.\n"",""stream"":""stderr"",""time"":""2021-11-09T14:18:37.638861043Z""} {""log"":""(node:7) TimeoutOverflowWarning: 2678400000 does not fit into a 32-bit signed integer.\n"",""stream"":""stderr"",""time"":""2021-11-09T14:18:37.640455606Z""} {""log"":""Timeout duration was set to 1.\n"",""stream"":""stderr"",""time"":""2021-11-09T14:18:37.640466647Z""} {""log"":""(node:7) TimeoutOverflowWarning: 2678400000 does not fit into a 32-bit signed integer.\n"",""stream"":""stderr"",""time"":""2021-11-09T14:18:37.642007007Z""} Here is a subset of the docker-compose.yaml file related to uptime-kuma: uptime-kuma: image: louislam/uptime-kuma container_name: uptime-kuma volumes: - ./data/uptime-kuma:/app/data logging: options: max-size: ""10m"" max-file: ""3"" TIA.",source-file | other-file | source-file | source-file,"uptime-kuma container image and logs Hi there I run uptime-kuma as a container behind a nginx reverse proxy. Everything is working fine except except seeing the following entries filled up the docker log. If I do not do anything, it would fill up the 20GB of the host storage in a few days. It may well be my set up though. Here are the logs: {""log"":""Timeout duration was set to 1.\n"",""stream"":""stderr"",""time"":""2021-11-09T14:18:37.638861043Z""} {""log"":""(node:7) TimeoutOverflowWarning: 2678400000 does not fit into a 32-bit signed integer.\n"",""stream"":""stderr"",""time"":""2021-11-09T14:18:37.640455606Z""} {""log"":""Timeout duration was set to 1.\n"",""stream"":""stderr"",""time"":""2021-11-09T14:18:37.640466647Z""} {""log"":""(node:7) TimeoutOverflowWarning: 2678400000 does not fit into a 32-bit signed integer.\n"",""stream"":""stderr"",""time"":""2021-11-09T14:18:37.642007007Z""} Here is a subset of the docker-compose.yaml file related to uptime-kuma: uptime-kuma: image: louislam/uptime-kuma container_name: uptime-kuma volumes: - ./data/uptime-kuma:/app/data logging: options: max-size: ""10m"" max-file: ""3"" TIA. source-file other-file source-file source-file",no-bug,0.9
2126,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2126,Hardcoded ping executable path,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The ping binary path is hardcoded in this file : https://github.com/louislam/uptime-kuma/blob/443235b20b9db8fab84f2bf580bf50fbaa3a53c6/server/ping-lite.js#L35 Is there a reason hardcoded path are used instead of relying on the environment ? This assumes compliance with the linux Filesystem Hierarchy Standard which is not always the case. For example NixOS stores all the executables in /nix/store/. If there is no specific reason, I could try to adapt this file and submit it as a PR.  Reproduction steps /  Expected behavior Utime-kuma ping monitor is working correctly.  Actual Behavior Uptime-kuma complains the the ping binary is not installed.  Uptime-Kuma Version 1.18  Operating System and Arch NixOS 22.05  Browser Firefox  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_",,"Hardcoded ping executable path   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The ping binary path is hardcoded in this file : https://github.com/louislam/uptime-kuma/blob/443235b20b9db8fab84f2bf580bf50fbaa3a53c6/server/ping-lite.js#L35 Is there a reason hardcoded path are used instead of relying on the environment ? This assumes compliance with the linux Filesystem Hierarchy Standard which is not always the case. For example NixOS stores all the executables in /nix/store/. If there is no specific reason, I could try to adapt this file and submit it as a PR.  Reproduction steps /  Expected behavior Utime-kuma ping monitor is working correctly.  Actual Behavior Uptime-kuma complains the the ping binary is not installed.  Uptime-Kuma Version 1.18  Operating System and Arch NixOS 22.05  Browser Firefox  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_",no-bug,0.9
1890,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1890,Certificate Expiry Notification doesn't work for me any ideas?,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem Pushbullet notifications. UP/DOWN alerts arrive fine. Test notification is OK too. Certificate Expiry Notification is set for all configured monitors. Settings -- Notifications -- TLS Expiry default to 7 & 14 & 21 days On the monitor's dashboard Cert Exp. determined correctly. But no mention of expiry in the log either. What do I do wrong here? Appreciate any help. Cheers  Uptime-Kuma Version 1.17.1  Operating System and Arch Ubuntu 20.04.04 LTS x64  Browser Chrome 103.0.5060.114  Docker Version _No response_  NodeJS Version 14.19.3,source-file,Certificate Expiry Notification doesn't work for me any ideas?   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem Pushbullet notifications. UP/DOWN alerts arrive fine. Test notification is OK too. Certificate Expiry Notification is set for all configured monitors. Settings -- Notifications -- TLS Expiry default to 7 & 14 & 21 days On the monitor's dashboard Cert Exp. determined correctly. But no mention of expiry in the log either. What do I do wrong here? Appreciate any help. Cheers  Uptime-Kuma Version 1.17.1  Operating System and Arch Ubuntu 20.04.04 LTS x64  Browser Chrome 103.0.5060.114  Docker Version _No response_  NodeJS Version 14.19.3 source-file,no-bug,0.9
1067,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1067,Uptime kuma can't monitor the up status of Airdc Webclient,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem So I added a HTTPS for this self hosted app: https://github.com/airdcpp-web/airdcpp-webclient This is the link of the public demo: https://webdemo.airdcpp.net/login And adding it into a monitor shows down even though I can visit the site. Any ideas?  Uptime-Kuma Version 1.10.0  Operating System and Arch Ubuntu  Browser Firefox  Docker Version Latest Portainer  NodeJS Version _No response_,source-file,Uptime kuma can't monitor the up status of Airdc Webclient   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem So I added a HTTPS for this self hosted app: https://github.com/airdcpp-web/airdcpp-webclient This is the link of the public demo: https://webdemo.airdcpp.net/login And adding it into a monitor shows down even though I can visit the site. Any ideas?  Uptime-Kuma Version 1.10.0  Operating System and Arch Ubuntu  Browser Firefox  Docker Version Latest Portainer  NodeJS Version _No response_ source-file,no-bug,0.7
1468,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1468,No Mattermost notification after update to 1.13.1,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Hi, i am using Uptime Kuma on a Raspberry device in docker. Used docker container: - uptime-kuma:1-alpine After update from Version 1.11.4 to 1.13.1 the configured Mattermost notification seems not to work anymore for downtimes etc. - The Test notification from configuration menu works fine  Reproduction steps - Setup a Monitoring for a website - Configure Mattermost notification (+ Test) - Wait or cause a downtime for the website - Downtime is tracked by Kuma Monitoring - But no notification is send in Mattermost The Mattermost notification is set as default.  Expected behavior - A notification in Mattermost when the monitored website has a downtime  Actual Behavior - But no notification is send in Mattermost  Uptime-Kuma Version 1.13.1  Operating System and Arch Debian 9.13 (ARM)  Browser Firefox (does not really metter for this issue)  Docker Version 19.03.15, build 99e3ed8919 (ARM)  NodeJS Version n/a (Docker container)  Relevant log output _No response_",source-file | other-file | source-file | documentation-file | test-file | other-file | other-file | documentation-file | documentation-file | documentation-file | source-file | source-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file,"No Mattermost notification after update to 1.13.1   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Hi, i am using Uptime Kuma on a Raspberry device in docker. Used docker container: - uptime-kuma:1-alpine After update from Version 1.11.4 to 1.13.1 the configured Mattermost notification seems not to work anymore for downtimes etc. - The Test notification from configuration menu works fine  Reproduction steps - Setup a Monitoring for a website - Configure Mattermost notification (+ Test) - Wait or cause a downtime for the website - Downtime is tracked by Kuma Monitoring - But no notification is send in Mattermost The Mattermost notification is set as default.  Expected behavior - A notification in Mattermost when the monitored website has a downtime  Actual Behavior - But no notification is send in Mattermost  Uptime-Kuma Version 1.13.1  Operating System and Arch Debian 9.13 (ARM)  Browser Firefox (does not really metter for this issue)  Docker Version 19.03.15, build 99e3ed8919 (ARM)  NodeJS Version n/a (Docker container)  Relevant log output _No response_ source-file other-file source-file documentation-file test-file other-file other-file documentation-file documentation-file documentation-file source-file source-file database-file database-file database-file database-file container-file container-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file",no-bug,0.9
2516,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2516,Maintenance generates multiple alerts on status pages,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I have noticed that during the active period of a maintenance, the maintenance alert is displayed for each heartbeat. Thus causing an infinity of repeated alerts. **Maintenance configuration** ![uptime-kuma-2](https://user-images.githubusercontent.com/37023161/210208807-95042421-26a6-4739-a870-7de5091908c0.png) **Error** ![uptime-kuma](https://user-images.githubusercontent.com/37023161/210208803-663292e6-e504-43e3-86ab-e2609c4dd3f2.png)  Reproduction steps 1. Create a maintenance 2. Be in the maintenance interval 3. Check the status page and there you will see a notification for each maintenance heartbeat  Expected behavior That only one alert was to appear during active maintenance  Actual Behavior An alert appears for each maintenance heartbeat  Uptime-Kuma Version 1.19.2  Operating System and Arch Ubuntu 20.04.5 LTS  Browser Google Chrome 108.0.5359.125  Docker Version Docker version 20.10.22, build 3a2c30b  NodeJS Version v16.13.2  Relevant log output _No response_",source-file,"Maintenance generates multiple alerts on status pages   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I have noticed that during the active period of a maintenance, the maintenance alert is displayed for each heartbeat. Thus causing an infinity of repeated alerts. **Maintenance configuration** ![uptime-kuma-2](https://user-images.githubusercontent.com/37023161/210208807-95042421-26a6-4739-a870-7de5091908c0.png) **Error** ![uptime-kuma](https://user-images.githubusercontent.com/37023161/210208803-663292e6-e504-43e3-86ab-e2609c4dd3f2.png)  Reproduction steps 1. Create a maintenance 2. Be in the maintenance interval 3. Check the status page and there you will see a notification for each maintenance heartbeat  Expected behavior That only one alert was to appear during active maintenance  Actual Behavior An alert appears for each maintenance heartbeat  Uptime-Kuma Version 1.19.2  Operating System and Arch Ubuntu 20.04.5 LTS  Browser Google Chrome 108.0.5359.125  Docker Version Docker version 20.10.22, build 3a2c30b  NodeJS Version v16.13.2  Relevant log output _No response_ source-file",no-bug,0.9
491,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/491,Created Incident message incorrect Timezone bug,**Is it a duplicate question?** NO **Describe the bug** Created incident message on the Status Page does not show the right time for my Timezone even though the Timezone is set up correctly in my admin dashboard. **Info** Uptime Kuma Version: 1.7.0 Using Docker?: Yes Docker Version: 20.10.3 Browser: Chrome Latest **Screenshots** ![htimezone correct](https://user-images.githubusercontent.com/37554361/134824464-c8b8552f-d537-4b86-9c6d-b425e2e1d19b.png) ![aerror1](https://user-images.githubusercontent.com/37554361/134824465-decf23fc-6e3b-4484-859e-86b6eb61e9e3.png),source-file | other-file | source-file | other-file,Created Incident message incorrect Timezone bug **Is it a duplicate question?** NO **Describe the bug** Created incident message on the Status Page does not show the right time for my Timezone even though the Timezone is set up correctly in my admin dashboard. **Info** Uptime Kuma Version: 1.7.0 Using Docker?: Yes Docker Version: 20.10.3 Browser: Chrome Latest **Screenshots** ![htimezone correct](https://user-images.githubusercontent.com/37554361/134824464-c8b8552f-d537-4b86-9c6d-b425e2e1d19b.png) ![aerror1](https://user-images.githubusercontent.com/37554361/134824465-decf23fc-6e3b-4484-859e-86b6eb61e9e3.png) source-file other-file source-file other-file,bug,0.85
2447,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2447,Discord notification failing with Docker monitor,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Using docker monitor in conjunction with discord notifications results in error:  2022-12-21T09:55:44.132Z [MONITOR] ERROR: Cannot send notification to BCS INC Error: Error: AxiosError: Request failed with status code 400 {""embeds"":[""0""]} at Discord.throwGeneralAxiosError (/app/server/notification-providers/notification-provider.js:38:15) at Discord.send (/app/server/notification-providers/discord.js:116:18) at runMicrotasks (<anonymous>) at processTicksAndRejections (node:internal/process/task_queues:96:5) at async Function.sendNotification (/app/server/model/monitor.js:939:21) at async beat (/app/server/model/monitor.js:615:25) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:658:17)  Most likely cause is `undefined` value for `Failing` variable (which I assume is passed on to the Discord)  2022-12-21T09:55:23.928Z [MONITOR] WARN: Monitor #16 'BOT | Docker': Failing: undefined | Interval: 20 seconds | Type: docker | Down Count: 4 | Resend Interval: 5  Notification when container goes back `UP` works, so this seems to be strictly something with `DOWN` notifications.  Reproduction steps 1) Monitor Docker container with Discord notifications enabled 2) Shut down container 3) Check UptimeKuma logs  Expected behavior A Discord message to pop-up once notify gets triggered  Actual Behavior Errors mentioned above in logs  Uptime-Kuma Version 1.18.5  Operating System and Arch Debian 10  Browser Google Chrome 108.0.5359.124  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_",source-file,"Discord notification failing with Docker monitor   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Using docker monitor in conjunction with discord notifications results in error:  2022-12-21T09:55:44.132Z [MONITOR] ERROR: Cannot send notification to BCS INC Error: Error: AxiosError: Request failed with status code 400 {""embeds"":[""0""]} at Discord.throwGeneralAxiosError (/app/server/notification-providers/notification-provider.js:38:15) at Discord.send (/app/server/notification-providers/discord.js:116:18) at runMicrotasks (<anonymous>) at processTicksAndRejections (node:internal/process/task_queues:96:5) at async Function.sendNotification (/app/server/model/monitor.js:939:21) at async beat (/app/server/model/monitor.js:615:25) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:658:17)  Most likely cause is `undefined` value for `Failing` variable (which I assume is passed on to the Discord)  2022-12-21T09:55:23.928Z [MONITOR] WARN: Monitor #16 'BOT | Docker': Failing: undefined | Interval: 20 seconds | Type: docker | Down Count: 4 | Resend Interval: 5  Notification when container goes back `UP` works, so this seems to be strictly something with `DOWN` notifications.  Reproduction steps 1) Monitor Docker container with Discord notifications enabled 2) Shut down container 3) Check UptimeKuma logs  Expected behavior A Discord message to pop-up once notify gets triggered  Actual Behavior Errors mentioned above in logs  Uptime-Kuma Version 1.18.5  Operating System and Arch Debian 10  Browser Google Chrome 108.0.5359.124  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_ source-file",bug,0.9
3712,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3712,"""No certificate"" badge is shown for http monitors","  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description _No response_  Reproduction steps 1. Create a monitor with http url 2. Add the monitor to a status page 3. Enable ""Show Certificate Expiry"" for that status page  Expected behavior No badge is shown for http monitors.  Actual Behavior A red badge with the text ""Cert Exp.: No/Bad Certificate"" is shown for a monitor with an http address. This looks like there's an error, but it's expected to have no certificate when it's http only.  Uptime-Kuma Version 1.23.1  Operating System and Arch Docker  Browser Firefox  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_",source-file | other-file,"""No certificate"" badge is shown for http monitors   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description _No response_  Reproduction steps 1. Create a monitor with http url 2. Add the monitor to a status page 3. Enable ""Show Certificate Expiry"" for that status page  Expected behavior No badge is shown for http monitors.  Actual Behavior A red badge with the text ""Cert Exp.: No/Bad Certificate"" is shown for a monitor with an http address. This looks like there's an error, but it's expected to have no certificate when it's http only.  Uptime-Kuma Version 1.23.1  Operating System and Arch Docker  Browser Firefox  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_ source-file other-file",bug,0.85
25,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/25,[Enhancement] Add an example docker-compose.yml file to the docs,as title :D,documentation-file,[Enhancement] Add an example docker-compose.yml file to the docs as title :D documentation-file,no-bug,0.95
1024,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1024,"error in logs: ""TypeError: Cannot read property 'daysRemaining' of null""","  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I can see this error in my logs from uptime-kuma every few minutes: TypeError: Cannot read property 'daysRemaining' of null at Prometheus.update (/app/server/prometheus.js:63:91) at beat (/app/server/model/monitor.js:396:24) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:419:17) FYI I'm not using Prometheus, just the base docker image with just a few http monitors configured, no special settings or anything  Reproduction steps deploy uptime kuma, add a monitor, wait and check the container logs  Expected behavior we should not have this error every few minutes in the logs  Actual Behavior I can see this error in my logs from uptime-kuma every few minutes: TypeError: Cannot read property 'daysRemaining' of null at Prometheus.update (/app/server/prometheus.js:63:91) at beat (/app/server/model/monitor.js:396:24) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:419:17)  Uptime-Kuma Version 1.11.1  Operating System and Arch Ubuntu 20.04 - x64  Browser Chrome  Docker Version _No response_  NodeJS Version _No response_  Relevant log output shell TypeError: Cannot read property 'daysRemaining' of null at Prometheus.update (/app/server/prometheus.js:63:91) at beat (/app/server/model/monitor.js:396:24) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:419:17) ",source-file | source-file,"error in logs: ""TypeError: Cannot read property 'daysRemaining' of null""   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I can see this error in my logs from uptime-kuma every few minutes: TypeError: Cannot read property 'daysRemaining' of null at Prometheus.update (/app/server/prometheus.js:63:91) at beat (/app/server/model/monitor.js:396:24) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:419:17) FYI I'm not using Prometheus, just the base docker image with just a few http monitors configured, no special settings or anything  Reproduction steps deploy uptime kuma, add a monitor, wait and check the container logs  Expected behavior we should not have this error every few minutes in the logs  Actual Behavior I can see this error in my logs from uptime-kuma every few minutes: TypeError: Cannot read property 'daysRemaining' of null at Prometheus.update (/app/server/prometheus.js:63:91) at beat (/app/server/model/monitor.js:396:24) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:419:17)  Uptime-Kuma Version 1.11.1  Operating System and Arch Ubuntu 20.04 - x64  Browser Chrome  Docker Version _No response_  NodeJS Version _No response_  Relevant log output shell TypeError: Cannot read property 'daysRemaining' of null at Prometheus.update (/app/server/prometheus.js:63:91) at beat (/app/server/model/monitor.js:396:24) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:419:17)  source-file source-file",no-bug,0.9
1209,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1209,Logout button in navbar,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description The addition of a logout button to the navigation bar next to the settings button   Solution This would provide an easy way for the user to log out of the interface without having to go into settings first. It is confusing for first time users of the interface that the logout button is located in the settings. With most other web apps, the logout button is located either in the nav bar it's self or in a profile dropdown accessible from the navbar (think GitHub). By placing it in the settings it is harder for a user to find as many probably wouldn't expect the logout button to be with the security settings. By placing it in the taskbar it allows the user to easily and quickly log out of the interface from any of the dashboard pages.   Alternatives Whilst it would be possible to just leave the logout button in the security section of the settings, this is hard to find for first time users and isn't very convinient as it takes 3 clicks to logout compared to only one if the button is also placed in the navbar.  Additional Context Perhaps something like this: ![image](https://user-images.githubusercontent.com/67638596/150621579-0b08b12d-1242-44d6-b7e8-cc3f4857cc3c.png) I would be willing to have a go at implementing this feature.",other-file | source-file | other-file | config-file | documentation-file | documentation-file | database-file | container-file | container-file | source-file | source-file | other-file | source-file | source-file | source-file | config-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | other-file | other-file | source-file | test-file | other-file | other-file | documentation-file | documentation-file | source-file | source-file | database-file | database-file | database-file | database-file | container-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | source-file | other-file | source-file | other-file | source-file | other-file | other-file | source-file | documentation-file | test-file | other-file | other-file | documentation-file | documentation-file | documentation-file | source-file | source-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file,"Logout button in navbar   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description The addition of a logout button to the navigation bar next to the settings button   Solution This would provide an easy way for the user to log out of the interface without having to go into settings first. It is confusing for first time users of the interface that the logout button is located in the settings. With most other web apps, the logout button is located either in the nav bar it's self or in a profile dropdown accessible from the navbar (think GitHub). By placing it in the settings it is harder for a user to find as many probably wouldn't expect the logout button to be with the security settings. By placing it in the taskbar it allows the user to easily and quickly log out of the interface from any of the dashboard pages.   Alternatives Whilst it would be possible to just leave the logout button in the security section of the settings, this is hard to find for first time users and isn't very convinient as it takes 3 clicks to logout compared to only one if the button is also placed in the navbar.  Additional Context Perhaps something like this: ![image](https://user-images.githubusercontent.com/67638596/150621579-0b08b12d-1242-44d6-b7e8-cc3f4857cc3c.png) I would be willing to have a go at implementing this feature. other-file source-file other-file config-file documentation-file documentation-file database-file container-file container-file source-file source-file other-file source-file source-file source-file config-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file other-file other-file source-file test-file other-file other-file documentation-file documentation-file source-file source-file database-file database-file database-file database-file container-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file source-file other-file source-file other-file source-file other-file other-file source-file documentation-file test-file other-file other-file documentation-file documentation-file documentation-file source-file source-file database-file database-file database-file database-file container-file container-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file",no-bug,0.95
2588,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2588,Notifications via a proxy (squid),"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Telegram (axios.get) and teams (axios.post) use neither the system-wide proxy (HTTP_PROXY, HTTPS_PROXY, NO_PROXY) nor a separately defined one such as in the .npmrc (proxy, https-proxy). The extension of the respective axios calls in teams.js or telegram.js with the proxy address (https://www.npmjs.com/package/axios > `axios#post(url[, data[, config]])`) also fails because the request doesn't arrive at the Squid with `request_method=CONNECT`. The webhooks for Telegram and teams via the squid work as desired via curl.   Solution The general use of the system variables (`HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`) for the installation (`download-dist.js`), the actual monitoring (already works via the `http-proxy-agent` module) and the notifications (notification-providers) would make sense.   Alternatives _No response_  Additional Context I was able to get the download to run as part of the installation using the 'https-proxy-agent' module in download-dist.js. I couldn't get Axios to work with the Squid.",source-file,"Notifications via a proxy (squid)   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Telegram (axios.get) and teams (axios.post) use neither the system-wide proxy (HTTP_PROXY, HTTPS_PROXY, NO_PROXY) nor a separately defined one such as in the .npmrc (proxy, https-proxy). The extension of the respective axios calls in teams.js or telegram.js with the proxy address (https://www.npmjs.com/package/axios > `axios#post(url[, data[, config]])`) also fails because the request doesn't arrive at the Squid with `request_method=CONNECT`. The webhooks for Telegram and teams via the squid work as desired via curl.   Solution The general use of the system variables (`HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`) for the installation (`download-dist.js`), the actual monitoring (already works via the `http-proxy-agent` module) and the notifications (notification-providers) would make sense.   Alternatives _No response_  Additional Context I was able to get the download to run as part of the installation using the 'https-proxy-agent' module in download-dist.js. I couldn't get Axios to work with the Squid. source-file",no-bug,0.9
1802,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1802,New http monitors add port as empty string,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Hello- I am attempting to write my own RESTful API for Uptime Kuma and have been poking around the database. I noticed that if I add a new https monitor through the Uptime Kuma gui, the `port` column in the database is added as an empty string. Well in my RESTful API I am going to enforce strict type checking at the API level (both incoming and outgoing data using [pydantic](https://pydantic-docs.helpmanual.io/usage/types/)), so I want to enforce this to be an `INTEGER` only or `NULL`. This is because a port **cannot** be anything other than an `INTEGER`. I noticed that in previous Uptime Kuma builds (months ago), https monitors had the value of `port` as `NULL`, then it was changed somewhere along the way to be empty strings. Now I see [SQLite embraces dynamic type checking](https://www.sqlite.org/flextypegood.html), but am wondering if this was a conscious design decision to make new `port` entries an empty string as opposed to `NULL`. I can make my API handle empty strings as well, but I would prefer making it a strict `NULL` or `INTEGER`. I wanted to check if it was a bug first though. I would argue that if the front end only allows an `INTEGER` to be selected (which is the case), then it is a bug. :) -Joe  Reproduction steps 1. Make a new https monitor. 2. Look in the database and see `port` is an empty string, not `NULL`.  Expected behavior I would expect a monitor without a used `port` to be added as `NULL`.  Actual Behavior It is added as an empty string.  Uptime-Kuma Version 1.16.1  Operating System and Arch MacOS 11.6.6  Browser Chrome Version 102.0.5005.115  Docker Version Docker version 20.10.12, build e91ed5707e  NodeJS Version _No response_  Relevant log output shell Here is the output from my pydantic logs: pydantic.error_wrappers.ValidationError: 1 validation error for Monitor response -> 0 -> port value is not a valid integer (type=type_error.integer) <class 'str'>  ",source-file | other-file,"New http monitors add port as empty string   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Hello- I am attempting to write my own RESTful API for Uptime Kuma and have been poking around the database. I noticed that if I add a new https monitor through the Uptime Kuma gui, the `port` column in the database is added as an empty string. Well in my RESTful API I am going to enforce strict type checking at the API level (both incoming and outgoing data using [pydantic](https://pydantic-docs.helpmanual.io/usage/types/)), so I want to enforce this to be an `INTEGER` only or `NULL`. This is because a port **cannot** be anything other than an `INTEGER`. I noticed that in previous Uptime Kuma builds (months ago), https monitors had the value of `port` as `NULL`, then it was changed somewhere along the way to be empty strings. Now I see [SQLite embraces dynamic type checking](https://www.sqlite.org/flextypegood.html), but am wondering if this was a conscious design decision to make new `port` entries an empty string as opposed to `NULL`. I can make my API handle empty strings as well, but I would prefer making it a strict `NULL` or `INTEGER`. I wanted to check if it was a bug first though. I would argue that if the front end only allows an `INTEGER` to be selected (which is the case), then it is a bug. :) -Joe  Reproduction steps 1. Make a new https monitor. 2. Look in the database and see `port` is an empty string, not `NULL`.  Expected behavior I would expect a monitor without a used `port` to be added as `NULL`.  Actual Behavior It is added as an empty string.  Uptime-Kuma Version 1.16.1  Operating System and Arch MacOS 11.6.6  Browser Chrome Version 102.0.5005.115  Docker Version Docker version 20.10.12, build e91ed5707e  NodeJS Version _No response_  Relevant log output shell Here is the output from my pydantic logs: pydantic.error_wrappers.ValidationError: 1 validation error for Monitor response -> 0 -> port value is not a valid integer (type=type_error.integer) <class 'str'>   source-file other-file",bug,0.9
1059,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1059,[Feature] Allow to specify Resolver Port for DNS Monitor,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature, Other  Feature description Allow to specify Port for the Resolver in a DNS Monitor.   Solution Simplest would be to allow `:` to specify a port in the Resolver field and pass it down as entered. If I'm not completely wrong, the underlying code would already support it: https://github.com/louislam/uptime-kuma/blob/6d6cb2ad494aad7fed29f01cdea715f1c63c2bc4/server/util-server.js#L93 Regarding the Node docs, it would accept a colon already.   Alternatives Separate field for port would be an alternate solution.  Additional Context Currently it's only possible to define an IP. The syntax check if the Resolver field does not allow to specify a port. In my use-case, I have an ""internal"" DNS server which is responsible for any internal name lookup's and I have AdGuardHome in front of it to block requests. Because both sit on the same OPNsense instance, the ""internal"" DNS runs on another port and AdGuardHome is forwarding internal queries to it. I would like to monitor them independently to easier find the root cause in case of a problem.",source-file | source-file | source-file | other-file | other-file | source-file | test-file | config-file | other-file | other-file | documentation-file | documentation-file | database-file | database-file | database-file | container-file | container-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | config-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | source-file | documentation-file | test-file | documentation-file | documentation-file | source-file | source-file | database-file | database-file | container-file | container-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | other-file | other-file | other-file | source-file | source-file | source-file | test-file | documentation-file | documentation-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | other-file | test-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | other-file,"[Feature] Allow to specify Resolver Port for DNS Monitor   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature, Other  Feature description Allow to specify Port for the Resolver in a DNS Monitor.   Solution Simplest would be to allow `:` to specify a port in the Resolver field and pass it down as entered. If I'm not completely wrong, the underlying code would already support it: https://github.com/louislam/uptime-kuma/blob/6d6cb2ad494aad7fed29f01cdea715f1c63c2bc4/server/util-server.js#L93 Regarding the Node docs, it would accept a colon already.   Alternatives Separate field for port would be an alternate solution.  Additional Context Currently it's only possible to define an IP. The syntax check if the Resolver field does not allow to specify a port. In my use-case, I have an ""internal"" DNS server which is responsible for any internal name lookup's and I have AdGuardHome in front of it to block requests. Because both sit on the same OPNsense instance, the ""internal"" DNS runs on another port and AdGuardHome is forwarding internal queries to it. I would like to monitor them independently to easier find the root cause in case of a problem. source-file source-file source-file other-file other-file source-file test-file config-file other-file other-file documentation-file documentation-file database-file database-file database-file container-file container-file config-file source-file source-file source-file source-file source-file source-file other-file source-file source-file source-file source-file source-file source-file source-file config-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file source-file documentation-file test-file documentation-file documentation-file source-file source-file database-file database-file container-file container-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file other-file other-file other-file source-file source-file source-file test-file documentation-file documentation-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file source-file source-file other-file test-file source-file source-file source-file other-file source-file source-file source-file other-file",no-bug,0.95
1694,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1694,"""Ignore TLS error"" do not recover from backup",  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description i created a monitor with tag then created a status page then when check the status page not showing the tag ticked all the options of status page ![5A6D5F76-FD11-4048-9C25-DCE1BD86322F](https://user-images.githubusercontent.com/81495140/170565080-a955d8f1-2237-4157-bae5-0767babd9b96.png) **ignore TLS error in monitor from backup** When loading a backup from the other website version 1.16.0.beta.0 i ticked ignore TLS Error but when i load it shows i ticked but is showing but when the website is down is showing **Client network socket disconnected before secure TLS connection was established** ![EDA6BBCE-DEA5-4A77-B042-628660942ED6](https://user-images.githubusercontent.com/81495140/170564210-42483c6a-a0ea-4848-af5c-1f2b2ae0527e.jpeg) ![F7998263-3F0B-42B5-8DDA-CA71A7F24FC5](https://user-images.githubusercontent.com/81495140/170564226-28c09f7a-d5cf-48a2-b522-1186b28ea938.jpeg)  Reproduction steps **Tag Problem in status** 1. I created a monitor 2. added tag 3. created a status page 4. tick everything then save **ignore TLS error in monitor from backup** 1. i created a backup from 1.16.0-beta.0 2. uploading to 1.16.0  Expected behavior **Tag Problem in status** to show tag in status page **ignore TLS error in monitor from backup** ignore TLS error from my website  Actual Behavior **Tag Problem in status** doesnt show tag in status page **ignore TLS error in monitor from backup** not Ignoring TLS error from the website  Uptime-Kuma Version 1.16.0  Operating System and Arch Linix  Browser Chrome Latest  Docker Version _No response_  NodeJS Version Latest  Relevant log output _No response_,other-file,"""Ignore TLS error"" do not recover from backup   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description i created a monitor with tag then created a status page then when check the status page not showing the tag ticked all the options of status page ![5A6D5F76-FD11-4048-9C25-DCE1BD86322F](https://user-images.githubusercontent.com/81495140/170565080-a955d8f1-2237-4157-bae5-0767babd9b96.png) **ignore TLS error in monitor from backup** When loading a backup from the other website version 1.16.0.beta.0 i ticked ignore TLS Error but when i load it shows i ticked but is showing but when the website is down is showing **Client network socket disconnected before secure TLS connection was established** ![EDA6BBCE-DEA5-4A77-B042-628660942ED6](https://user-images.githubusercontent.com/81495140/170564210-42483c6a-a0ea-4848-af5c-1f2b2ae0527e.jpeg) ![F7998263-3F0B-42B5-8DDA-CA71A7F24FC5](https://user-images.githubusercontent.com/81495140/170564226-28c09f7a-d5cf-48a2-b522-1186b28ea938.jpeg)  Reproduction steps **Tag Problem in status** 1. I created a monitor 2. added tag 3. created a status page 4. tick everything then save **ignore TLS error in monitor from backup** 1. i created a backup from 1.16.0-beta.0 2. uploading to 1.16.0  Expected behavior **Tag Problem in status** to show tag in status page **ignore TLS error in monitor from backup** ignore TLS error from my website  Actual Behavior **Tag Problem in status** doesnt show tag in status page **ignore TLS error in monitor from backup** not Ignoring TLS error from the website  Uptime-Kuma Version 1.16.0  Operating System and Arch Linix  Browser Chrome Latest  Docker Version _No response_  NodeJS Version Latest  Relevant log output _No response_ other-file",no-bug,0.7
833,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/833,[Bug?]: User Data Leaked to Client Without Two Step Auth Token Verified," Reproduction steps Input the right username and password but do not input auth-token.  Expected behavior Nothing send back.  Actual Behavior Sending monitor list back, with url in it. <img width=""634"" alt=""2021-10-29 11 48 23"" src=""https://user-images.githubusercontent.com/25259084/139465445-850d0372-5dc7-434f-a04e-9d743cb84a04.png"">  Uptime-Kuma version 1.9.1 ? current upstream master d4c9431  Have you spent some time to check if this issue has been raised before? - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)",source-file,"[Bug?]: User Data Leaked to Client Without Two Step Auth Token Verified  Reproduction steps Input the right username and password but do not input auth-token.  Expected behavior Nothing send back.  Actual Behavior Sending monitor list back, with url in it. <img width=""634"" alt=""2021-10-29 11 48 23"" src=""https://user-images.githubusercontent.com/25259084/139465445-850d0372-5dc7-434f-a04e-9d743cb84a04.png"">  Uptime-Kuma version 1.9.1 ? current upstream master d4c9431  Have you spent some time to check if this issue has been raised before? - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy) source-file",bug,0.95
3420,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3420,Unable to run using America/Ciudad_Juarez TZ,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The error occurs because a new time zone was recently created in this time zone due to the elimination of daylight saving time in Mexico. As a result, new time zones were created for the cities on the border with the United States. I'm not sure if the issue is with Dayjs or the Docker image being used. https://unicode-org.atlassian.net/browse/CLDR-16181 ![image](https://github.com/louislam/uptime-kuma/assets/30887270/b20a86f2-c18d-4247-98f8-6c45b286389c)  Reproduction steps 1 .I created the container with the following command: ""docker run -d --restart=always -p 3001:3001 -v uptime-kuma:/app/data --name uptime-kuma louislam/uptime-kuma:1"" 2. After checking the logs, that error appears.  Expected behavior Dashboard should be displayed when accessing IP:3001  Actual Behavior Container crash, application not running  Uptime-Kuma Version > 1.18.5  Operating System and Arch Ubuntu 22.04.2  Browser Google Chrome 114.0.5735.199  Docker Version 20.10.18  NodeJS Version _No response_  Relevant log output shell Trace: RangeError: Invalid time zone specified: America/Ciudad_Juarez at new DateTimeFormat (<anonymous>) at /app/server/modules/dayjs/plugin/timezone.js:31:34 at a (/app/server/modules/dayjs/plugin/timezone.js:42:14) at u (/app/server/modules/dayjs/plugin/timezone.js:46:21) at Function.o.tz (/app/server/modules/dayjs/plugin/timezone.js:92:21) at Logger.log (/app/src/util.js:111:25) at Logger.debug (/app/src/util.js:165:14) at UptimeKumaServer.initAfterDatabaseReady (/app/server/uptime-kuma-server.js:103:13) at async /app/server/server.js:178:5 at process.<anonymous> (/app/server/server.js:1825:13) at process.emit (node:events:513:28) at emit (node:internal/process/promises:140:20) at processPromiseRejections (node:internal/process/promises:274:27) at processTicksAndRejections (node:internal/process/task_queues:97:32) If you keep encountering errors, please report to https://github.com/louislam/uptime-kuma/issues /app/server/modules/dayjs/plugin/timezone.js:31 return r || (r = new Intl.DateTimeFormat(""en-US"", { ^ RangeError: Invalid time zone specified: America/Ciudad_Juarez at new DateTimeFormat (<anonymous>) at /app/server/modules/dayjs/plugin/timezone.js:31:34 at a (/app/server/modules/dayjs/plugin/timezone.js:42:14) at u (/app/server/modules/dayjs/plugin/timezone.js:46:21) at Function.o.tz (/app/server/modules/dayjs/plugin/timezone.js:92:21) at Logger.log (/app/src/util.js:111:25) at Logger.debug (/app/src/util.js:165:14) at Timeout._onTimeout (/app/server/settings.js:36:21) at listOnTimeout (node:internal/timers:559:17) at processTimers (node:internal/timers:502:7) ",source-file,"Unable to run using America/Ciudad_Juarez TZ   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The error occurs because a new time zone was recently created in this time zone due to the elimination of daylight saving time in Mexico. As a result, new time zones were created for the cities on the border with the United States. I'm not sure if the issue is with Dayjs or the Docker image being used. https://unicode-org.atlassian.net/browse/CLDR-16181 ![image](https://github.com/louislam/uptime-kuma/assets/30887270/b20a86f2-c18d-4247-98f8-6c45b286389c)  Reproduction steps 1 .I created the container with the following command: ""docker run -d --restart=always -p 3001:3001 -v uptime-kuma:/app/data --name uptime-kuma louislam/uptime-kuma:1"" 2. After checking the logs, that error appears.  Expected behavior Dashboard should be displayed when accessing IP:3001  Actual Behavior Container crash, application not running  Uptime-Kuma Version > 1.18.5  Operating System and Arch Ubuntu 22.04.2  Browser Google Chrome 114.0.5735.199  Docker Version 20.10.18  NodeJS Version _No response_  Relevant log output shell Trace: RangeError: Invalid time zone specified: America/Ciudad_Juarez at new DateTimeFormat (<anonymous>) at /app/server/modules/dayjs/plugin/timezone.js:31:34 at a (/app/server/modules/dayjs/plugin/timezone.js:42:14) at u (/app/server/modules/dayjs/plugin/timezone.js:46:21) at Function.o.tz (/app/server/modules/dayjs/plugin/timezone.js:92:21) at Logger.log (/app/src/util.js:111:25) at Logger.debug (/app/src/util.js:165:14) at UptimeKumaServer.initAfterDatabaseReady (/app/server/uptime-kuma-server.js:103:13) at async /app/server/server.js:178:5 at process.<anonymous> (/app/server/server.js:1825:13) at process.emit (node:events:513:28) at emit (node:internal/process/promises:140:20) at processPromiseRejections (node:internal/process/promises:274:27) at processTicksAndRejections (node:internal/process/task_queues:97:32) If you keep encountering errors, please report to https://github.com/louislam/uptime-kuma/issues /app/server/modules/dayjs/plugin/timezone.js:31 return r || (r = new Intl.DateTimeFormat(""en-US"", { ^ RangeError: Invalid time zone specified: America/Ciudad_Juarez at new DateTimeFormat (<anonymous>) at /app/server/modules/dayjs/plugin/timezone.js:31:34 at a (/app/server/modules/dayjs/plugin/timezone.js:42:14) at u (/app/server/modules/dayjs/plugin/timezone.js:46:21) at Function.o.tz (/app/server/modules/dayjs/plugin/timezone.js:92:21) at Logger.log (/app/src/util.js:111:25) at Logger.debug (/app/src/util.js:165:14) at Timeout._onTimeout (/app/server/settings.js:36:21) at listOnTimeout (node:internal/timers:559:17) at processTimers (node:internal/timers:502:7)  source-file",no-bug,0.9
2820,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2820,Possible memory leaks,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Often more than not, I have to restart my Uptime Kuma because of an exponentially increasing memory usage. Searched through the logs but I haven't found anything relevant. I have about ~60 HTTP monitors. ![image](https://user-images.githubusercontent.com/73610779/220718270-c22cdbc5-7b50-4b98-9d12-5259c477741f.png)  Reproduction steps Have an uptime kuma container on Docker.  Expected behavior No memory leaks  Actual Behavior Huge memory leaks over time.  Uptime-Kuma Version 1.20.1  Operating System and Arch Ubuntu 22.04  Browser Irrelevant  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_",documentation-file | documentation-file,"Possible memory leaks   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Often more than not, I have to restart my Uptime Kuma because of an exponentially increasing memory usage. Searched through the logs but I haven't found anything relevant. I have about ~60 HTTP monitors. ![image](https://user-images.githubusercontent.com/73610779/220718270-c22cdbc5-7b50-4b98-9d12-5259c477741f.png)  Reproduction steps Have an uptime kuma container on Docker.  Expected behavior No memory leaks  Actual Behavior Huge memory leaks over time.  Uptime-Kuma Version 1.20.1  Operating System and Arch Ubuntu 22.04  Browser Irrelevant  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_ documentation-file documentation-file",no-bug,0.95
5773,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5773,[Minor UI issue] Overlapping tags when using multiple lines," I have found these related issues/pull requests Find no related issues/pull requests.   Security Policy - [x] I have read and agree to Uptime Kuma's [Security Policy](https://github.com/louislam/uptime-kuma/security/policy).  Description When multiple tags are assigned to a monitor or group and they span multiple lines, they overlap. <img width=""647"" alt=""Image"" src=""https://github.com/user-attachments/assets/32d4a669-711b-4077-838e-9fe488a6f21d"" />  Reproduction steps 1. Assign a large number of tags to a monitor or group. 2. Notice how the tags are displayed over multiple lines. 3. Observe that they appear to overlap.  Expected behavior Tags should be clearly spaced out even when they wrap onto multiple lines.  Actual Behavior Tags overlap and are too close together.  Uptime-Kuma Version 2.0.0-beta.2  Operating System and Arch Ubuntu 24.04.2 LTS (GNU/Linux 6.8.0-51-generic x86_64) with Docker 28.0.4  Browser Brave 1.77.97; Firefox 137.0.1 (aarch64)   Deployment Environment - **Runtime Environment**: - Docker: Version `28.0.4` (Build `Tue Mar 25 15:07:16 2025`) - Docker Compose: Version `v2.34.0` - MariaDB: Version `X.X.X` (LTS: Yes/No) - Node.js: Version `X.X.X` (LTS: Yes/No) - Kubernetes (K3S/K8S): Version `X.X.X` (LTS: Yes/No, via `[method/tool]`) - **Uptime Kuma Setup**: - Number of monitors: `3`  Relevant log output bash session ",other-file | other-file | other-file | other-file | other-file | other-file | other-file,"[Minor UI issue] Overlapping tags when using multiple lines  I have found these related issues/pull requests Find no related issues/pull requests.   Security Policy - [x] I have read and agree to Uptime Kuma's [Security Policy](https://github.com/louislam/uptime-kuma/security/policy).  Description When multiple tags are assigned to a monitor or group and they span multiple lines, they overlap. <img width=""647"" alt=""Image"" src=""https://github.com/user-attachments/assets/32d4a669-711b-4077-838e-9fe488a6f21d"" />  Reproduction steps 1. Assign a large number of tags to a monitor or group. 2. Notice how the tags are displayed over multiple lines. 3. Observe that they appear to overlap.  Expected behavior Tags should be clearly spaced out even when they wrap onto multiple lines.  Actual Behavior Tags overlap and are too close together.  Uptime-Kuma Version 2.0.0-beta.2  Operating System and Arch Ubuntu 24.04.2 LTS (GNU/Linux 6.8.0-51-generic x86_64) with Docker 28.0.4  Browser Brave 1.77.97; Firefox 137.0.1 (aarch64)   Deployment Environment - **Runtime Environment**: - Docker: Version `28.0.4` (Build `Tue Mar 25 15:07:16 2025`) - Docker Compose: Version `v2.34.0` - MariaDB: Version `X.X.X` (LTS: Yes/No) - Node.js: Version `X.X.X` (LTS: Yes/No) - Kubernetes (K3S/K8S): Version `X.X.X` (LTS: Yes/No, via `[method/tool]`) - **Uptime Kuma Setup**: - Number of monitors: `3`  Relevant log output bash session  other-file other-file other-file other-file other-file other-file other-file",no-bug,0.95
5721,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5721,v2 beta- Proxy port out of range error for Mariadb (port = 40000)," I have found these related issues/pull requests I did not find a related issue   Security Policy - [x] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I am using v2 beta version with MariaDB embedded, and when I try to create (insert) or update a proxy with a 40000 port value (a real value used by Oxylabs to provide residential IPs in certain countries), it is shown a red error that states ""update `proxy` set `user_id` = 1, `protocol` = 'https', `host` = 'XXXXXX', `port` = 40000, `auth` = true, `username` = 'XXXXXX', `password` = 'XXXXXX', `active` = true, `default` = false, `created_date` = '2025-03-19 11:51:59' where `id` = 9 - **Out of range value for column 'port' at row 1**"" (error shown when trying to update a proxy's port) I guess the reason is that the port value is defined as a 16 bit integer somewhere (max value = 32767) causing this error.  Reproduction steps 1. Go to settings --> Proxies 2. Edit an existing one (or create a new one) 3. Enter the values, including a port that is higher than 32767 (actually I tested that this value does not cause errors but 32768 does) 4. Click save  Expected behavior It does not trigger any error and the proxy is properly saved  Actual Behavior It returns an error like this one: ""update `proxy` set `user_id` = 1, `protocol` = 'https', `host` = 'XXXXXX', `port` = 40000, `auth` = true, `username` = 'XXXXXX', `password` = 'XXXXXX', `active` = true, `default` = false, `created_date` = '2025-03-19 11:51:59' where `id` = 9 - Out of range value for column 'port' at row 1""  Uptime-Kuma Version beta 2.0  Operating System and Arch Ubuntu 20.04  Browser Google Chrome Version 134.0.6998.89   Deployment Environment - Runtime: Docker 20.10.9 - Database: embedded mariadb - Filesystem used to store the database on: AWS EFS NFSv3 - number of monitors: 80  Relevant log output shell ",source-file,"v2 beta- Proxy port out of range error for Mariadb (port = 40000)  I have found these related issues/pull requests I did not find a related issue   Security Policy - [x] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I am using v2 beta version with MariaDB embedded, and when I try to create (insert) or update a proxy with a 40000 port value (a real value used by Oxylabs to provide residential IPs in certain countries), it is shown a red error that states ""update `proxy` set `user_id` = 1, `protocol` = 'https', `host` = 'XXXXXX', `port` = 40000, `auth` = true, `username` = 'XXXXXX', `password` = 'XXXXXX', `active` = true, `default` = false, `created_date` = '2025-03-19 11:51:59' where `id` = 9 - **Out of range value for column 'port' at row 1**"" (error shown when trying to update a proxy's port) I guess the reason is that the port value is defined as a 16 bit integer somewhere (max value = 32767) causing this error.  Reproduction steps 1. Go to settings --> Proxies 2. Edit an existing one (or create a new one) 3. Enter the values, including a port that is higher than 32767 (actually I tested that this value does not cause errors but 32768 does) 4. Click save  Expected behavior It does not trigger any error and the proxy is properly saved  Actual Behavior It returns an error like this one: ""update `proxy` set `user_id` = 1, `protocol` = 'https', `host` = 'XXXXXX', `port` = 40000, `auth` = true, `username` = 'XXXXXX', `password` = 'XXXXXX', `active` = true, `default` = false, `created_date` = '2025-03-19 11:51:59' where `id` = 9 - Out of range value for column 'port' at row 1""  Uptime-Kuma Version beta 2.0  Operating System and Arch Ubuntu 20.04  Browser Google Chrome Version 134.0.6998.89   Deployment Environment - Runtime: Docker 20.10.9 - Database: embedded mariadb - Filesystem used to store the database on: AWS EFS NFSv3 - number of monitors: 80  Relevant log output shell  source-file",no-bug,0.9
1685,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1685,Prometheus page API key / custom password,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Currently, to access the `/metrics` page when authentication is enabled, you must use your username and password. This username and password combination then has to be stored in the prometheus configuration file. Whilst this is ok for testing, when moving to production, it is better security practice to use a totally seperate, read only login to access the `/metrics` page.   Solution Addition of the ability to add a custom password / API key for the `/metrics` page.   Alternatives This could be solved by multiple user accounts and adding one user with read only permissions for prometheus to use but this seems a bit convoluted. Perhaps this could be implemented alongside the possible public API?  Additional Context _No response_",database-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | documentation-file | source-file | other-file | source-file,"Prometheus page API key / custom password   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Currently, to access the `/metrics` page when authentication is enabled, you must use your username and password. This username and password combination then has to be stored in the prometheus configuration file. Whilst this is ok for testing, when moving to production, it is better security practice to use a totally seperate, read only login to access the `/metrics` page.   Solution Addition of the ability to add a custom password / API key for the `/metrics` page.   Alternatives This could be solved by multiple user accounts and adding one user with read only permissions for prometheus to use but this seems a bit convoluted. Perhaps this could be implemented alongside the possible public API?  Additional Context _No response_ database-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file documentation-file source-file other-file source-file",no-bug,0.9
3610,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3610,Bitrix24 Notification,  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification Fixes #3620  Feature description I would like it to be added so that we receive notifications through the Bitrix24 system.   Solution A system that sends notifications can be added using the Bitrix24 webhook.   Alternatives _No response_  Additional Context _No response_,documentation-file | documentation-file | other-file | source-file | documentation-file | documentation-file | test-file | config-file | config-file | other-file | documentation-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | config-file | container-file | container-file | source-file | source-file | config-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | config-file | other-file | config-file | source-file | source-file | source-file | documentation-file | other-file | other-file | source-file | documentation-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | container-file | source-file | documentation-file | source-file | config-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | other-file | source-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | test-file | config-file | config-file | config-file | other-file | documentation-file | documentation-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | database-file | database-file | database-file | database-file | config-file | container-file | source-file | source-file | source-file | source-file | test-file | source-file | documentation-file | documentation-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file,Bitrix24 Notification   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification Fixes #3620  Feature description I would like it to be added so that we receive notifications through the Bitrix24 system.   Solution A system that sends notifications can be added using the Bitrix24 webhook.   Alternatives _No response_  Additional Context _No response_ documentation-file documentation-file other-file source-file documentation-file documentation-file test-file config-file config-file other-file documentation-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file container-file container-file config-file config-file container-file container-file source-file source-file config-file other-file other-file other-file other-file source-file other-file other-file config-file other-file config-file source-file source-file source-file documentation-file other-file other-file source-file documentation-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file other-file container-file source-file documentation-file source-file config-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file other-file source-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file other-file source-file source-file documentation-file documentation-file documentation-file documentation-file test-file config-file config-file config-file other-file documentation-file documentation-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file database-file database-file database-file database-file config-file container-file source-file source-file source-file source-file test-file source-file documentation-file documentation-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file source-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file,no-bug,0.95
2419,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2419,Clear data button has CSS alignment/block issues,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description See screenshot ![image](https://user-images.githubusercontent.com/12234510/207896845-a799a6cd-301f-4987-b82e-f0467b4357c5.png)  Reproduction steps See screenshot  Expected behavior Better alignment or link that fill the entire block in width.  Actual Behavior See screenshot  Uptime-Kuma Version 1.18.5  Operating System and Arch Ubutnu 22.0.4  Browser Brave  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_,other-file | other-file | other-file,Clear data button has CSS alignment/block issues   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description See screenshot ![image](https://user-images.githubusercontent.com/12234510/207896845-a799a6cd-301f-4987-b82e-f0467b4357c5.png)  Reproduction steps See screenshot  Expected behavior Better alignment or link that fill the entire block in width.  Actual Behavior See screenshot  Uptime-Kuma Version 1.18.5  Operating System and Arch Ubutnu 22.0.4  Browser Brave  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_ other-file other-file other-file,no-bug,0.95
471,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/471,Add `\status` as an alias of `\status-page`,I just feel it's more concise. That is all. Thank you!,other-file | source-file | source-file | other-file | source-file,Add `\status` as an alias of `\status-page` I just feel it's more concise. That is all. Thank you! other-file source-file source-file other-file source-file,no-bug,0.9
2608,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2608,Monitors in Maintenance Mode incorrectly shown in Monitor count,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I have several monitors in maintenance as per the screenshot attached but they are showing in the Unknown count instead of Maintenance ![image](https://user-images.githubusercontent.com/1370117/212436108-78dc1cd8-7b2f-469e-ba39-5a736b95ac44.png)  Reproduction steps Unknown  Expected behavior Monitors shown in Maintenance count  Actual Behavior Monitors shown in Unknown count  Uptime-Kuma Version 1.19.4  Operating System and Arch Ubuntu 22.04.1 x86_64  Browser Firefox  Docker Version N/A  NodeJS Version 19.3.0  Relevant log output _No response_,source-file,Monitors in Maintenance Mode incorrectly shown in Monitor count   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I have several monitors in maintenance as per the screenshot attached but they are showing in the Unknown count instead of Maintenance ![image](https://user-images.githubusercontent.com/1370117/212436108-78dc1cd8-7b2f-469e-ba39-5a736b95ac44.png)  Reproduction steps Unknown  Expected behavior Monitors shown in Maintenance count  Actual Behavior Monitors shown in Unknown count  Uptime-Kuma Version 1.19.4  Operating System and Arch Ubuntu 22.04.1 x86_64  Browser Firefox  Docker Version N/A  NodeJS Version 19.3.0  Relevant log output _No response_ source-file,no-bug,0.8
460,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/460,`CORSMissingAllowOrigin` in development,"For latest commit in `master` branch, I can't connect to frontend. https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS/Errors/CORSMissingAllowOrigin ![image](https://user-images.githubusercontent.com/905878/134491464-d63b67e9-241b-4f58-a73c-ae65bc993cba.png) **Info** Uptime Kuma Version: Using Docker?: No OS: Windows 10 Browser: Latest Firefox and Chrome",documentation-file | source-file | documentation-file | source-file,"`CORSMissingAllowOrigin` in development For latest commit in `master` branch, I can't connect to frontend. https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS/Errors/CORSMissingAllowOrigin ![image](https://user-images.githubusercontent.com/905878/134491464-d63b67e9-241b-4f58-a73c-ae65bc993cba.png) **Info** Uptime Kuma Version: Using Docker?: No OS: Windows 10 Browser: Latest Firefox and Chrome documentation-file source-file documentation-file source-file",no-bug,0.8
565,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/565,Possibility to copy a monitor,"**Describe the solution you'd like** Be able to open a Monitor in dashboard, and have a boton Copy in the same row as the Edit and Delete buttons. It would copy the whole configuracion, and maybe rename it adding a "" - Copy"" suffix",other-file | other-file,"Possibility to copy a monitor **Describe the solution you'd like** Be able to open a Monitor in dashboard, and have a boton Copy in the same row as the Edit and Delete buttons. It would copy the whole configuracion, and maybe rename it adding a "" - Copy"" suffix other-file other-file",no-bug,0.95
1756,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1756,Password protected Status Page,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description Require the ability to password protect a status page.   Solution When I visit a specific status page, it should prompt me to enter a username and a password and only display the status page once correct credentials has been entered.   Alternatives _No response_  Additional Context _No response_",other-file,"Password protected Status Page   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description Require the ability to password protect a status page.   Solution When I visit a specific status page, it should prompt me to enter a username and a password and only display the status page once correct credentials has been entered.   Alternatives _No response_  Additional Context _No response_ other-file",no-bug,0.95
4479,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/4479,Migrate from `omsrivastava/timezones-list` to `vvo/tzdb`," I have found these related issues/pull requests none   Feature Request Type Other  Feature description [omsrivastava/timezones-list](https://github.com/omsrivastava/timezones-list), the library currently used by our front-end, is rarely updated and has [inaccuracies](https://github.com/omsrivastava/timezones-list/issues/14). [vvo/tzdb](https://github.com/vvo/tzdb) looks much better as it is automatically kept up-to-date with authoritative sources. Some migration work is needed since the data format is different.   Solution Migrate from `omsrivastava/timezones-list` to `vvo/tzdb`   Alternatives _No response_  Additional Context _No response_",documentation-file | documentation-file | source-file,"Migrate from `omsrivastava/timezones-list` to `vvo/tzdb`  I have found these related issues/pull requests none   Feature Request Type Other  Feature description [omsrivastava/timezones-list](https://github.com/omsrivastava/timezones-list), the library currently used by our front-end, is rarely updated and has [inaccuracies](https://github.com/omsrivastava/timezones-list/issues/14). [vvo/tzdb](https://github.com/vvo/tzdb) looks much better as it is automatically kept up-to-date with authoritative sources. Some migration work is needed since the data format is different.   Solution Migrate from `omsrivastava/timezones-list` to `vvo/tzdb`   Alternatives _No response_  Additional Context _No response_ documentation-file documentation-file source-file",no-bug,0.9
378,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/378,Monitor Item Duplicate Checker,"**Is it a duplicate question?** Not in sight. **Is your feature request related to a problem? Please describe.** After adding the JSON import/export function. It becomes an issue. **Describe the solution you'd like** Add function to check if the monitor exists. Warn the user that the monitor is already created. At JSON import/export, let the user to choose to keep or discard all the existing monitors. Also, checks duplicated monitors when importing JSON config.",source-file | source-file | source-file | source-file | other-file,"Monitor Item Duplicate Checker **Is it a duplicate question?** Not in sight. **Is your feature request related to a problem? Please describe.** After adding the JSON import/export function. It becomes an issue. **Describe the solution you'd like** Add function to check if the monitor exists. Warn the user that the monitor is already created. At JSON import/export, let the user to choose to keep or discard all the existing monitors. Also, checks duplicated monitors when importing JSON config. source-file source-file source-file source-file other-file",no-bug,0.9
5314,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5314,[SendMail] Email Address Friendly Name Error," I have found these related issues/pull requests NA   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When selecting the SendGrid alert time and entering the ""From Email"" in the: name <email address> format, this gives a validation error: ![image](https://github.com/user-attachments/assets/485ef9b0-6ef6-41c2-b758-b21f764d6bea) If you click on the Test button, you get the email in the expected format, suggesting that this way of specifying the From Email is supported.  Reproduction steps On the Settings > Notifications tab, add a new notification type and select the SendGrid option. Enter the email address such as: Uptime Kuma <test@test.com> Select Test, if it's a valid email it will send. Select the Save button, the above field validation error will show.  Expected behavior I would expect this option to save without error.  Actual Behavior Validation error: A part followed by '@' should not contain the symbol ' '.  Uptime-Kuma Version 2.0.0-beta.0-nightly-20241105100611  Operating System and Arch Windows 11  Browser Chrome 130.0.6723.92 (Official Build) (64-bit)   Deployment Environment - Runtime: docker - Database: embedded MariaDB - Filesystem used to store the database on: - number of monitors:  Relevant log output _No response_",other-file | other-file,"[SendMail] Email Address Friendly Name Error  I have found these related issues/pull requests NA   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When selecting the SendGrid alert time and entering the ""From Email"" in the: name <email address> format, this gives a validation error: ![image](https://github.com/user-attachments/assets/485ef9b0-6ef6-41c2-b758-b21f764d6bea) If you click on the Test button, you get the email in the expected format, suggesting that this way of specifying the From Email is supported.  Reproduction steps On the Settings > Notifications tab, add a new notification type and select the SendGrid option. Enter the email address such as: Uptime Kuma <test@test.com> Select Test, if it's a valid email it will send. Select the Save button, the above field validation error will show.  Expected behavior I would expect this option to save without error.  Actual Behavior Validation error: A part followed by '@' should not contain the symbol ' '.  Uptime-Kuma Version 2.0.0-beta.0-nightly-20241105100611  Operating System and Arch Windows 11  Browser Chrome 130.0.6723.92 (Official Build) (64-bit)   Deployment Environment - Runtime: docker - Database: embedded MariaDB - Filesystem used to store the database on: - number of monitors:  Relevant log output _No response_ other-file other-file",no-bug,0.9
10,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/10,res.data.includes is not a function,"Getting this message with a ""HTTP(s) - Keyword"" service. Let me know what you may need to help diagnose the issue, the logs are pretty empty.",source-file | other-file,"res.data.includes is not a function Getting this message with a ""HTTP(s) - Keyword"" service. Let me know what you may need to help diagnose the issue, the logs are pretty empty. source-file other-file",no-bug,0.3
1919,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1919,Add `Authorization` headers to webhook notifications,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification  Feature description You should add the possibility to send Webhooks to Servers with an Authorization.   Solution For the Notification Provider Webhook, it would be helpfull if you can set the Value of the Authorization Header.   Alternatives A new Notification Provider for secured Webhook receivers.  Additional Context _No response_",,"Add `Authorization` headers to webhook notifications   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification  Feature description You should add the possibility to send Webhooks to Servers with an Authorization.   Solution For the Notification Provider Webhook, it would be helpfull if you can set the Value of the Authorization Header.   Alternatives A new Notification Provider for secured Webhook receivers.  Additional Context _No response_",no-bug,0.95
1765,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1765,"""Default Enabled"" notification not working","  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description On the ""Setup Notification"" screen, the ""Default enabled"" option for notifications is not working. When ""Default enabled"" is toggled on and you create a new monitor, the Notification alert is not enabled.  Reproduction steps 1. Go to http://{ip}:3001/settings/notifications 2. Click Setup notification button 3. Complete required fields 4. Toggle on ""Default enabled"" 5. Click ""Add New Monitor"" button.  Expected behavior Notification option should be toggled on automatically.  Actual Behavior Notification option is toggled off.  Uptime-Kuma Version 1.16.1  Operating System and Arch Ubuntu 22.04 LTS  Browser Firefox 101.0.1  Docker Version Docker 20.10.17  NodeJS Version _No response_  Relevant log output _No response_",source-file,"""Default Enabled"" notification not working   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description On the ""Setup Notification"" screen, the ""Default enabled"" option for notifications is not working. When ""Default enabled"" is toggled on and you create a new monitor, the Notification alert is not enabled.  Reproduction steps 1. Go to http://{ip}:3001/settings/notifications 2. Click Setup notification button 3. Complete required fields 4. Toggle on ""Default enabled"" 5. Click ""Add New Monitor"" button.  Expected behavior Notification option should be toggled on automatically.  Actual Behavior Notification option is toggled off.  Uptime-Kuma Version 1.16.1  Operating System and Arch Ubuntu 22.04 LTS  Browser Firefox 101.0.1  Docker Version Docker 20.10.17  NodeJS Version _No response_  Relevant log output _No response_ source-file",no-bug,0.9
3334,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3334,Add SMSC () provider notification,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification  Feature description Add new notification service https://smsc.kz/. I took the ClickSendSMS code as a base and added additional parameters. This service allows you to send not only SMS, but I needed to send SMS   Solution File src/components/notifications/index.js after the line `import ClickSendSMS from ""./ClickSendSMS.vue"";` added `import SMSC from ""./SMSC.vue"";` after the line `clicksendsms"": ClickSendSMS,` added `""smsc"": SMSC,` File server/notification.js after the line `const ClickSendSMS = require(""./notification-providers/clicksendsms"");` added `const SMSC = require(""./notification-providers/smsc"");` after the line `new ClickSendSMS(),` added `new SMSC(),` File src/components/NotificationDialog.vue after the line `""clicksendsms"": ""ClickSend SMS"",` added `""smsc"": ""SMSC"",` And new created files server/notification-providers/smsc.js and src/components/notifications/SMSC.vue[smsc.zip](https://github.com/louislam/uptime-kuma/files/11907037/smsc.zip) Full files in provider [fullfiles.zip](https://github.com/louislam/uptime-kuma/files/11907084/fullfiles.zip)   Alternatives _No response_  Additional Context With git fork I did not understand the instructions, so I posted them as files ![image](https://github.com/louislam/uptime-kuma/assets/8991450/a599c03b-aa8e-4604-b84d-668e5cdc265f)",source-file | source-file | other-file | other-file | source-file | source-file | source-file | other-file | other-file | source-file,"Add SMSC () provider notification   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification  Feature description Add new notification service https://smsc.kz/. I took the ClickSendSMS code as a base and added additional parameters. This service allows you to send not only SMS, but I needed to send SMS   Solution File src/components/notifications/index.js after the line `import ClickSendSMS from ""./ClickSendSMS.vue"";` added `import SMSC from ""./SMSC.vue"";` after the line `clicksendsms"": ClickSendSMS,` added `""smsc"": SMSC,` File server/notification.js after the line `const ClickSendSMS = require(""./notification-providers/clicksendsms"");` added `const SMSC = require(""./notification-providers/smsc"");` after the line `new ClickSendSMS(),` added `new SMSC(),` File src/components/NotificationDialog.vue after the line `""clicksendsms"": ""ClickSend SMS"",` added `""smsc"": ""SMSC"",` And new created files server/notification-providers/smsc.js and src/components/notifications/SMSC.vue[smsc.zip](https://github.com/louislam/uptime-kuma/files/11907037/smsc.zip) Full files in provider [fullfiles.zip](https://github.com/louislam/uptime-kuma/files/11907084/fullfiles.zip)   Alternatives _No response_  Additional Context With git fork I did not understand the instructions, so I posted them as files ![image](https://github.com/louislam/uptime-kuma/assets/8991450/a599c03b-aa8e-4604-b84d-668e5cdc265f) source-file source-file other-file other-file source-file source-file source-file other-file other-file source-file",no-bug,0.9
3868,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3868,"Postgres database monitoring - connection string ending with ""ssl=false"" crashes the application","  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description While adding various services to be monitored by Uptime Kuma, a bug has been spotted with the PostgreSQL monitoring. If you provide a connection string ending with ""ssl=false"" and then add the monitor, after a few minutes the application will crash. This is probably due to missing validation of the connection string that is parsed from the front-end. After the application crashes, the container keeps restarting. Most people might be aware that this flag is not supported in a connection string such as this one, but this is still a potential bug, which can bring the whole application down as in my case.  Reproduction steps 1. Add new monitoring service 2. Choose PostgreSQL 3. Provide connection string ending with ""ssl=false"" 4. Add the monitored service 5. Wait for a few minutes  Expected behavior Either showing an error indicating that the connection string is not correct or successful connection to the database.  Actual Behavior The application throws Type Error and crashes. The container then keeps on restarting forever.  Uptime-Kuma Version 1.23.2  Operating System and Arch Windows  Browser Google Chrome 117.0.5938.150  Docker Version _No response_  NodeJS Version 16.20.2  Relevant log output shell 2023-10-10 13:31:53.947 at TCP.onStreamRead (node:internal/stream_base_commons:190:23) 2023-10-10 13:31:53.947 at Socket.Readable.push (node:internal/streams/readable:228:10) 2023-10-10 13:31:53.947 at readableAddChunk (node:internal/streams/readable:289:9) 2023-10-10 13:31:53.947 at addChunk (node:internal/streams/readable:315:12) 2023-10-10 13:31:53.947 at Socket.emit (node:events:513:28) 2023-10-10 13:31:53.947 at Object.onceWrapper (node:events:628:26) 2023-10-10 13:31:53.947 at Socket.<anonymous> (/app/node_modules/pg/lib/connection.js:85:19) 2023-10-10 13:31:53.947 TypeError: Cannot use 'in' operator to search for 'key' in false 2023-10-10 13:31:53.947 2023-10-10 13:31:53.947 ^ 2023-10-10 13:31:53.947 if ('key' in self.ssl) { 2023-10-10 13:31:53.947 /app/node_modules/pg/lib/connection.js:85 ",documentation-file | documentation-file | source-file | source-file | other-file | documentation-file | documentation-file | other-file | source-file | documentation-file | documentation-file | test-file | config-file | config-file | config-file | config-file | config-file | other-file | documentation-file | documentation-file | documentation-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | config-file | container-file | container-file | source-file | source-file | config-file | other-file | other-file | other-file | other-file | source-file | source-file | other-file | other-file | config-file | other-file | config-file | source-file | source-file | source-file | documentation-file | other-file | other-file | source-file | documentation-file | source-file | documentation-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | container-file | source-file | documentation-file | source-file | config-file | documentation-file | documentation-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file,"Postgres database monitoring - connection string ending with ""ssl=false"" crashes the application   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description While adding various services to be monitored by Uptime Kuma, a bug has been spotted with the PostgreSQL monitoring. If you provide a connection string ending with ""ssl=false"" and then add the monitor, after a few minutes the application will crash. This is probably due to missing validation of the connection string that is parsed from the front-end. After the application crashes, the container keeps restarting. Most people might be aware that this flag is not supported in a connection string such as this one, but this is still a potential bug, which can bring the whole application down as in my case.  Reproduction steps 1. Add new monitoring service 2. Choose PostgreSQL 3. Provide connection string ending with ""ssl=false"" 4. Add the monitored service 5. Wait for a few minutes  Expected behavior Either showing an error indicating that the connection string is not correct or successful connection to the database.  Actual Behavior The application throws Type Error and crashes. The container then keeps on restarting forever.  Uptime-Kuma Version 1.23.2  Operating System and Arch Windows  Browser Google Chrome 117.0.5938.150  Docker Version _No response_  NodeJS Version 16.20.2  Relevant log output shell 2023-10-10 13:31:53.947 at TCP.onStreamRead (node:internal/stream_base_commons:190:23) 2023-10-10 13:31:53.947 at Socket.Readable.push (node:internal/streams/readable:228:10) 2023-10-10 13:31:53.947 at readableAddChunk (node:internal/streams/readable:289:9) 2023-10-10 13:31:53.947 at addChunk (node:internal/streams/readable:315:12) 2023-10-10 13:31:53.947 at Socket.emit (node:events:513:28) 2023-10-10 13:31:53.947 at Object.onceWrapper (node:events:628:26) 2023-10-10 13:31:53.947 at Socket.<anonymous> (/app/node_modules/pg/lib/connection.js:85:19) 2023-10-10 13:31:53.947 TypeError: Cannot use 'in' operator to search for 'key' in false 2023-10-10 13:31:53.947 2023-10-10 13:31:53.947 ^ 2023-10-10 13:31:53.947 if ('key' in self.ssl) { 2023-10-10 13:31:53.947 /app/node_modules/pg/lib/connection.js:85  documentation-file documentation-file source-file source-file other-file documentation-file documentation-file other-file source-file documentation-file documentation-file test-file config-file config-file config-file config-file config-file other-file documentation-file documentation-file documentation-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file container-file container-file config-file config-file container-file container-file source-file source-file config-file other-file other-file other-file other-file source-file source-file other-file other-file config-file other-file config-file source-file source-file source-file documentation-file other-file other-file source-file documentation-file source-file documentation-file source-file source-file source-file test-file source-file source-file source-file source-file source-file source-file other-file container-file source-file documentation-file source-file config-file documentation-file documentation-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file",no-bug,0.7
871,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/871,[Bug] Telegram Bot Token displayed in notification setup view,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description While setting up the Telegram notification, there's the ""updateURL"" displayed at the bottom. It should help users finding their userID - but since Telegram puts the token right in the URL, you are displaying a secret again. Related to #268. ![grafik](https://user-images.githubusercontent.com/5798157/140061776-7c2fe861-e608-47f1-8af0-ffbb155dd9b6.png) The piece of code that's responsible for this: https://github.com/louislam/uptime-kuma/blob/6944b35ea74edbd703f2c5a867bbdf1981f3b452/src/components/notifications/Telegram.vue#L23-L35 Since the bug report form required me to read the security policy: I do not think that this an actual ""security issue"" as in a vulnerability.  Reproduction steps 1. Open a monitor 2. Click on Edit 3. Setup Notification  Expected behavior The full URL is not shown (or shown masked) to the user. That way other viewers (screen share or real life meeting) can't copy the bot token.  Actual Behavior In the setup view the token is shown within the URL.  Uptime-Kuma Version 1.10.0  Operating System and Arch Debian 5.10.46-4 (2021-08-03) x86_64  Browser Firefox 93.0 (64-Bit)  Docker Version 20.10.10, build b485636  NodeJS Version N/A  Relevant log output shell N/A ",other-file,"[Bug] Telegram Bot Token displayed in notification setup view   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description While setting up the Telegram notification, there's the ""updateURL"" displayed at the bottom. It should help users finding their userID - but since Telegram puts the token right in the URL, you are displaying a secret again. Related to #268. ![grafik](https://user-images.githubusercontent.com/5798157/140061776-7c2fe861-e608-47f1-8af0-ffbb155dd9b6.png) The piece of code that's responsible for this: https://github.com/louislam/uptime-kuma/blob/6944b35ea74edbd703f2c5a867bbdf1981f3b452/src/components/notifications/Telegram.vue#L23-L35 Since the bug report form required me to read the security policy: I do not think that this an actual ""security issue"" as in a vulnerability.  Reproduction steps 1. Open a monitor 2. Click on Edit 3. Setup Notification  Expected behavior The full URL is not shown (or shown masked) to the user. That way other viewers (screen share or real life meeting) can't copy the bot token.  Actual Behavior In the setup view the token is shown within the URL.  Uptime-Kuma Version 1.10.0  Operating System and Arch Debian 5.10.46-4 (2021-08-03) x86_64  Browser Firefox 93.0 (64-Bit)  Docker Version 20.10.10, build b485636  NodeJS Version N/A  Relevant log output shell N/A  other-file",no-bug,0.9
4526,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/4526,"LINE messenger has changed their developer portal, making some helptexts outdated","  Please verify that this question has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem I'm trying to add LINE messenger alerts into Uptime kuma but everytime I try to run a test, I get 401'ed with this error: ![image](https://github.com/louislam/uptime-kuma/assets/55105920/71cf8d94-f7b3-4465-8d0c-f876fe6e311e) LINE's dev portal doesn't have have a Channel access token anymore but instead seems to have Channel secrets instead, does this change anything on how Uptime Kuma communicates with LINE's API?  Error Message(s) or Log _No response_  Uptime-Kuma Version 1.23.11  Operating System and Arch UNRAID 6.12.8 - x64 (Intel)  Browser Brave Version 1.63.162 Chromium: 122.0.6261.69 (Official Build) (64-bit)   Deployment Environment - Runtime: Docker (UNRAID 6.12.8) - Database: Unknown (whatever is default) - Filesystem used to store the database on: btrfs (in Docker) - number of monitors: 12",other-file | documentation-file,"LINE messenger has changed their developer portal, making some helptexts outdated   Please verify that this question has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem I'm trying to add LINE messenger alerts into Uptime kuma but everytime I try to run a test, I get 401'ed with this error: ![image](https://github.com/louislam/uptime-kuma/assets/55105920/71cf8d94-f7b3-4465-8d0c-f876fe6e311e) LINE's dev portal doesn't have have a Channel access token anymore but instead seems to have Channel secrets instead, does this change anything on how Uptime Kuma communicates with LINE's API?  Error Message(s) or Log _No response_  Uptime-Kuma Version 1.23.11  Operating System and Arch UNRAID 6.12.8 - x64 (Intel)  Browser Brave Version 1.63.162 Chromium: 122.0.6261.69 (Official Build) (64-bit)   Deployment Environment - Runtime: Docker (UNRAID 6.12.8) - Database: Unknown (whatever is default) - Filesystem used to store the database on: btrfs (in Docker) - number of monitors: 12 other-file documentation-file",no-bug,0.9
1364,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1364,"(Unwanted) Notifications ""Certificate will expire in xx days""","  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem It seems that for HTTP(S) monitors a notification is triggered whenever a certificate expires in 21 or 14 or 7 days and each day if expiry is in less than 4 days. I am aware of the option to ""Ignore TLS/SSL error for HTTPS websites"" but an ""expiry notice"" is not an ""error"". Is there any option to change this behaviour or toggle it on/off?  Uptime-Kuma Version 1.12.1  Operating System and Arch Ubuntu 20.04  Browser Chrome / FF / Edge  Docker Version 20.10.12  NodeJS Version _No response_",database-file | source-file | source-file | source-file | other-file | other-file | source-file | documentation-file | test-file | other-file | other-file | documentation-file | documentation-file | documentation-file | source-file | source-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file,"(Unwanted) Notifications ""Certificate will expire in xx days""   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem It seems that for HTTP(S) monitors a notification is triggered whenever a certificate expires in 21 or 14 or 7 days and each day if expiry is in less than 4 days. I am aware of the option to ""Ignore TLS/SSL error for HTTPS websites"" but an ""expiry notice"" is not an ""error"". Is there any option to change this behaviour or toggle it on/off?  Uptime-Kuma Version 1.12.1  Operating System and Arch Ubuntu 20.04  Browser Chrome / FF / Edge  Docker Version 20.10.12  NodeJS Version _No response_ database-file source-file source-file source-file other-file other-file source-file documentation-file test-file other-file other-file documentation-file documentation-file documentation-file source-file source-file database-file database-file database-file database-file container-file container-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file",no-bug,0.9
1126,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1126,Google Chat notification error,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The new Google Chat notification produces an error upon testing sending a notification  Reproduction steps 1. create a notification of type Google Chat 2. click the test button  Expected behavior a notification is sent to Google Chat  Actual Behavior an error is shown : `Error: Notification type is not supported`  Uptime-Kuma Version 1.11.2  Operating System and Arch docker  Browser Google Chrome 96.0.4664.110 (Build officiel) (32 bits)  Docker Version 20.10.12  NodeJS Version _No response_  Relevant log output shell server log kuma_1 | Error: Notification type is not supported kuma_1 | at Function.send (/app/server/notification.js:96:19) kuma_1 | at Socket.<anonymous> (/app/server/server.js:1036:46) kuma_1 | at Socket.emit (events.js:400:28) kuma_1 | at Socket.emitUntyped (/app/node_modules/socket.io/dist/typed-events.js:69:22) kuma_1 | at /app/node_modules/socket.io/dist/socket.js:428:39 kuma_1 | at processTicksAndRejections (internal/process/task_queues.js:77:11)  ,source-file,Google Chat notification error   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The new Google Chat notification produces an error upon testing sending a notification  Reproduction steps 1. create a notification of type Google Chat 2. click the test button  Expected behavior a notification is sent to Google Chat  Actual Behavior an error is shown : `Error: Notification type is not supported`  Uptime-Kuma Version 1.11.2  Operating System and Arch docker  Browser Google Chrome 96.0.4664.110 (Build officiel) (32 bits)  Docker Version 20.10.12  NodeJS Version _No response_  Relevant log output shell server log kuma_1 | Error: Notification type is not supported kuma_1 | at Function.send (/app/server/notification.js:96:19) kuma_1 | at Socket.<anonymous> (/app/server/server.js:1036:46) kuma_1 | at Socket.emit (events.js:400:28) kuma_1 | at Socket.emitUntyped (/app/node_modules/socket.io/dist/typed-events.js:69:22) kuma_1 | at /app/node_modules/socket.io/dist/socket.js:428:39 kuma_1 | at processTicksAndRejections (internal/process/task_queues.js:77:11)   source-file,no-bug,0.9
506,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/506,Heartbeat Retry Interval time (seconds) not working,"**Is it a duplicate question?** No. **Describe the bug** I noticed this issue since 1.7.0, but I was unsure it was a configuration error from my side or a bug The setting Heartbeat Retry Interval time in seconds is not working. Screenshot settings: ![uk_scr1](https://user-images.githubusercontent.com/8298741/135231959-99276ec2-bffe-4ce3-aa25-7faf7de34552.png) When a service goes down, it should retry 3 times every 60 seconds before it sends a notification the service is down. **Screencapture**: https://user-images.githubusercontent.com/8298741/135232370-e0cd2714-80dc-4c39-beb8-39badebf913e.mp4 As you can see in this screencapture, all three (3) Heartbeat Retries are send within one (1) minute and the service is marked as down. **Expected behavior** Retries: 3 Heartbeat Retry Interval: 60 Send 3 heartbeats every 60 seconds before the service is marked as down and a notification is sent. **Info** Uptime Kuma Version: 1.7.0. Using Docker?: Yes OS: Ubuntu 21.04 Browser: Chrome",source-file,"Heartbeat Retry Interval time (seconds) not working **Is it a duplicate question?** No. **Describe the bug** I noticed this issue since 1.7.0, but I was unsure it was a configuration error from my side or a bug The setting Heartbeat Retry Interval time in seconds is not working. Screenshot settings: ![uk_scr1](https://user-images.githubusercontent.com/8298741/135231959-99276ec2-bffe-4ce3-aa25-7faf7de34552.png) When a service goes down, it should retry 3 times every 60 seconds before it sends a notification the service is down. **Screencapture**: https://user-images.githubusercontent.com/8298741/135232370-e0cd2714-80dc-4c39-beb8-39badebf913e.mp4 As you can see in this screencapture, all three (3) Heartbeat Retries are send within one (1) minute and the service is marked as down. **Expected behavior** Retries: 3 Heartbeat Retry Interval: 60 Send 3 heartbeats every 60 seconds before the service is marked as down and a notification is sent. **Info** Uptime Kuma Version: 1.7.0. Using Docker?: Yes OS: Ubuntu 21.04 Browser: Chrome source-file",no-bug,0.9
2480,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2480,gRPC check throws errors,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The gRPC check fails but i get the expected output in bloomrpc.  Reproduction steps URL: `routing.ot-hosting.de:443` Keyword: `version` Enable TLS: :heavy_check_mark: Proto Service Name:  service.v1.RoutingService  Proto Method:  Info  Proto Content: proto syntax = ""proto3""; package service.v1; message Envelope { double min_longitude = 1; double max_longitude = 2; double min_latitude = 3; double max_latitude = 4; } message InfoRequest { } message InfoResponse { string version = 1; string gh_version = 2; int64 data_timestamp = 3; Envelope bounding_box = 4; map<string,string> configuration = 5; } service RoutingService { rpc Info (InfoRequest) returns (InfoResponse); }  Body:  {}   Expected behavior ![image](https://user-images.githubusercontent.com/22315436/209657257-93235079-89ad-4f75-b799-4b02078c5a1c.png)  Actual Behavior Depending on the case of the method `Info`/`info` i get the following errors:  WARN: Monitor #67 'routing': Failing: response.substring is not a function | Interval: 60 seconds | Type: grpc-keyword | Down Count: 0 | Resend Interval: 0   WARN: Monitor #67 'routing': Failing: grpcService[grpcMethod] is not a function | Interval: 60 seconds | Type: grpc-keyword | Down Count: 0 | Resend Interval: 0   Uptime-Kuma Version 1.19.2  Operating System and Arch Ubuntu 22.04  Browser Firefox  Docker Version 20.10.22  NodeJS Version _No response_  Relevant log output _No response_",source-file | source-file | source-file,"gRPC check throws errors   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The gRPC check fails but i get the expected output in bloomrpc.  Reproduction steps URL: `routing.ot-hosting.de:443` Keyword: `version` Enable TLS: :heavy_check_mark: Proto Service Name:  service.v1.RoutingService  Proto Method:  Info  Proto Content: proto syntax = ""proto3""; package service.v1; message Envelope { double min_longitude = 1; double max_longitude = 2; double min_latitude = 3; double max_latitude = 4; } message InfoRequest { } message InfoResponse { string version = 1; string gh_version = 2; int64 data_timestamp = 3; Envelope bounding_box = 4; map<string,string> configuration = 5; } service RoutingService { rpc Info (InfoRequest) returns (InfoResponse); }  Body:  {}   Expected behavior ![image](https://user-images.githubusercontent.com/22315436/209657257-93235079-89ad-4f75-b799-4b02078c5a1c.png)  Actual Behavior Depending on the case of the method `Info`/`info` i get the following errors:  WARN: Monitor #67 'routing': Failing: response.substring is not a function | Interval: 60 seconds | Type: grpc-keyword | Down Count: 0 | Resend Interval: 0   WARN: Monitor #67 'routing': Failing: grpcService[grpcMethod] is not a function | Interval: 60 seconds | Type: grpc-keyword | Down Count: 0 | Resend Interval: 0   Uptime-Kuma Version 1.19.2  Operating System and Arch Ubuntu 22.04  Browser Firefox  Docker Version 20.10.22  NodeJS Version _No response_  Relevant log output _No response_ source-file source-file source-file",no-bug,0.9
2135,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2135,Add icon support to ntfy notification provider,  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Add icon support to the ntfy notification provider (icon support has been added to ntfy with a release on 2022-09-27).   Solution There's just another parameter to have in the notification providers' settings and the http request sent.   Alternatives none  Additional Context _No response_,,Add icon support to ntfy notification provider   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Add icon support to the ntfy notification provider (icon support has been added to ntfy with a release on 2022-09-27).   Solution There's just another parameter to have in the notification providers' settings and the http request sent.   Alternatives none  Additional Context _No response_,no-bug,0.95
1817,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1817,Add support for monitoring MySQL/MariaDB databases,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Monitor  Feature description [PR1430 ](https://github.com/louislam/uptime-kuma/issues/1430) has just added the ability to monitor MS SQL Server DB's. It would be good to also be able to monitor other popular DBs too - specifically MySQL/MariaDB which many people use.   Solution Replicate the setup for MS SQL server, but with different connection methods and parameters to connect and test MySQL/MariaDB   Alternatives Currently doing a TCP Port test, but this only checks if the port is open, not whether a DB is actually available on that port and functioning.  Additional Context _No response_",documentation-file | documentation-file | source-file | source-file | other-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | documentation-file | database-file | database-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | documentation-file | documentation-file | source-file | source-file | other-file,"Add support for monitoring MySQL/MariaDB databases   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Monitor  Feature description [PR1430 ](https://github.com/louislam/uptime-kuma/issues/1430) has just added the ability to monitor MS SQL Server DB's. It would be good to also be able to monitor other popular DBs too - specifically MySQL/MariaDB which many people use.   Solution Replicate the setup for MS SQL server, but with different connection methods and parameters to connect and test MySQL/MariaDB   Alternatives Currently doing a TCP Port test, but this only checks if the port is open, not whether a DB is actually available on that port and functioning.  Additional Context _No response_ documentation-file documentation-file source-file source-file other-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file source-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file documentation-file database-file database-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file documentation-file documentation-file source-file source-file other-file",no-bug,0.95
1448,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1448,Discord notifications : the Service URL is empty for some tests,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description In the Discord notification, the Service URL is empty or only contain ""https://"" when the Monitor type is not HTTP. ![image](https://user-images.githubusercontent.com/36127788/161433699-cbe79d51-1752-49fd-93f3-1115fcd24919.png)  Reproduction steps 1. Setup a Discord Notification 2. Create a Ping Monitor with this notification enabled 3. Make it fail or succed  Expected behavior The Service URL should not be here.  Actual Behavior The Sevice URL field is empty or only contain ""https://"".  Uptime-Kuma Version 1.11.3  Operating System and Arch Debian 10  Browser Firefox 98  Docker Version _No response_  NodeJS Version _No response_  Relevant log output https://github.com/louislam/uptime-kuma/blob/dd183e2ec25d74429541ca06c412d53c92e2cb92/server/notification-providers/discord.js#L50-L53",,"Discord notifications : the Service URL is empty for some tests   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description In the Discord notification, the Service URL is empty or only contain ""https://"" when the Monitor type is not HTTP. ![image](https://user-images.githubusercontent.com/36127788/161433699-cbe79d51-1752-49fd-93f3-1115fcd24919.png)  Reproduction steps 1. Setup a Discord Notification 2. Create a Ping Monitor with this notification enabled 3. Make it fail or succed  Expected behavior The Service URL should not be here.  Actual Behavior The Sevice URL field is empty or only contain ""https://"".  Uptime-Kuma Version 1.11.3  Operating System and Arch Debian 10  Browser Firefox 98  Docker Version _No response_  NodeJS Version _No response_  Relevant log output https://github.com/louislam/uptime-kuma/blob/dd183e2ec25d74429541ca06c412d53c92e2cb92/server/notification-providers/discord.js#L50-L53",no-bug,0.9
21,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/21,Enhancements,"First of all great work! Looks really awesome. Though I have a few enhancement requests to make it even greater! - [x] Select all monitors at once to pause them: at the moment you have to pause them all manually one by one - [x] Scheduled pausing: for scheduled maintenance, server reboot etc (https://github.com/louislam/uptime-kuma/issues/191) - [x] Retries: for instance 3 retries in xx seconds before a service is being marked as down - [x] Accepted HTTP codes: I would like to add status codes to some HTTP request, 401 for instance (#173 ) - [x] Public dashboard: create a status page where monitors can be shown but not edited - [x] 2FA: would be great to have 2FA (Google auth, Authy etc) for logging in with the admin account - [x] Send notification/alert after xx time: have an interval for sending notifications, to avoid small downtimes - [x] Reset/delete up/downtime history: be able to delete the history for a monitor (after history delete it's 100% up again) - [x] Version number on webinterface (thx @rezzorix) - [x] Link to this github page (thx @rezzorix) - [x] Monitor groups: e.g. Server1, Server2 etc. (thx @LeviSnoot) - [x] Re-order monitors: create your own list priority (thx @LeviSnoot) - [x] Combined monitors : A monitor that looks at two instances but is displayed as one and/or has a dropdown to show all contained (thx @LeviSnoot) - [x] Dark mode: based on system settings (thx @LeviSnoot) - [ ] Incident reports: ties into public status page, the ability to write a message relating to the status of a monitor, with a status on each message such as "" Investigating"", "" Resolving"" and "" Resolved"". (thx @LeviSnoot) (https://github.com/louislam/uptime-kuma/pull/1253) - [x] White labeling public status page with custom logo, page title and favicon - that said there should obviously still be credit to the project, maybe in the form of a footer or something (thx @LeviSnoot) - [ ] Pagination: next page for the Dashboard / Dashboard of each monitor (thx @rezzorix) (https://github.com/louislam/uptime-kuma/issues/4106 https://github.com/louislam/uptime-kuma/issues/2993) - [x] Import/export monitors list as JSON (thx @kvpt) - [x] Add the possibility to define an announcement (thx @kvpt) - [x] Ability to use a separate docker container as the database e.g. mariadb (thx @joe-eklund) - [x] Ability to authenticate with digest or basic auth (thx @joe-eklund) - [x] Heartbeat monitoring, instead of pinging services, those services must check in (thx @joe-eklund) - [x] Able to ignore SSL issues for specific monitors (thx @joe-eklund) - [ ] User management with LDAP and oAuth. This would allow us to use our built in user management tools instead of having to create yet another user for people that need access. (thx @joe-eklund) (#128) - [x] Ability to set default notifications in settings so you don't have to select each time you create a new monitor ""Check now"" button to perform a check a single monitor right away (thx @parityhero) - [x] Tags to group monitors with filtering: by tag, by type (thx @parityhero) - [ ] More information about monitor: date created, last modified etc. (thx @parityhero) (https://github.com/louislam/uptime-kuma/issues/3481) - [x] Ability to SMTP multiple email addresses or CC/BCC (thx @parityhero) - [ ] Personnalize notifications for texts / icons to be used, depending on private/public channels for example (thx @magicgg91) (#975) - [x] Check certficate validity and expiration time (thx @magicgg91) - [x] Ability to configure how long the monitor history is retained. A global setting to automatically delete monitor entries older than ${x} days would declutter the user interface. (thx @Spiritreader) - [ ] Ability for a ""Bulk Upload"" via csv etc (thx @EclipseOfficeTech) (https://github.com/louislam/uptime-kuma/issues/4065 https://github.com/louislam/uptime-kuma/issues/2934 https://github.com/louislam/uptime-kuma/issues/1323) Keep up the good work! <details><summary>updates</summary> <p> _2021-07-15 @ 3.20pm UTC:_ Formatting changed to checkboxes per request @louislam _2021-07-15 @ 4.00pm UTC:_ Added enhancements requested by @kvpt _2021-07-17 @ 8.00am UTC:_ Added enhancements requested by @joe-eklund _2021-07-19 @ 8.02am UTC:_ Added enhancements requested by @parityhero _2021-07-21 @ 9.04am UTC:_ Added enhancements requested by @magicgg91 _2021-07-29 @ 1.40pm UTC:_ Added enhancements requested by @Spiritreader & @EclipseOfficeTech _2023-12-01 @ 17.20pm UTC:_ Updated the todo list by @CommanderStorm </p> </details>",other-file | other-file | other-file | other-file | other-file | other-file | other-file,"Enhancements First of all great work! Looks really awesome. Though I have a few enhancement requests to make it even greater! - [x] Select all monitors at once to pause them: at the moment you have to pause them all manually one by one - [x] Scheduled pausing: for scheduled maintenance, server reboot etc (https://github.com/louislam/uptime-kuma/issues/191) - [x] Retries: for instance 3 retries in xx seconds before a service is being marked as down - [x] Accepted HTTP codes: I would like to add status codes to some HTTP request, 401 for instance (#173 ) - [x] Public dashboard: create a status page where monitors can be shown but not edited - [x] 2FA: would be great to have 2FA (Google auth, Authy etc) for logging in with the admin account - [x] Send notification/alert after xx time: have an interval for sending notifications, to avoid small downtimes - [x] Reset/delete up/downtime history: be able to delete the history for a monitor (after history delete it's 100% up again) - [x] Version number on webinterface (thx @rezzorix) - [x] Link to this github page (thx @rezzorix) - [x] Monitor groups: e.g. Server1, Server2 etc. (thx @LeviSnoot) - [x] Re-order monitors: create your own list priority (thx @LeviSnoot) - [x] Combined monitors : A monitor that looks at two instances but is displayed as one and/or has a dropdown to show all contained (thx @LeviSnoot) - [x] Dark mode: based on system settings (thx @LeviSnoot) - [ ] Incident reports: ties into public status page, the ability to write a message relating to the status of a monitor, with a status on each message such as "" Investigating"", "" Resolving"" and "" Resolved"". (thx @LeviSnoot) (https://github.com/louislam/uptime-kuma/pull/1253) - [x] White labeling public status page with custom logo, page title and favicon - that said there should obviously still be credit to the project, maybe in the form of a footer or something (thx @LeviSnoot) - [ ] Pagination: next page for the Dashboard / Dashboard of each monitor (thx @rezzorix) (https://github.com/louislam/uptime-kuma/issues/4106 https://github.com/louislam/uptime-kuma/issues/2993) - [x] Import/export monitors list as JSON (thx @kvpt) - [x] Add the possibility to define an announcement (thx @kvpt) - [x] Ability to use a separate docker container as the database e.g. mariadb (thx @joe-eklund) - [x] Ability to authenticate with digest or basic auth (thx @joe-eklund) - [x] Heartbeat monitoring, instead of pinging services, those services must check in (thx @joe-eklund) - [x] Able to ignore SSL issues for specific monitors (thx @joe-eklund) - [ ] User management with LDAP and oAuth. This would allow us to use our built in user management tools instead of having to create yet another user for people that need access. (thx @joe-eklund) (#128) - [x] Ability to set default notifications in settings so you don't have to select each time you create a new monitor ""Check now"" button to perform a check a single monitor right away (thx @parityhero) - [x] Tags to group monitors with filtering: by tag, by type (thx @parityhero) - [ ] More information about monitor: date created, last modified etc. (thx @parityhero) (https://github.com/louislam/uptime-kuma/issues/3481) - [x] Ability to SMTP multiple email addresses or CC/BCC (thx @parityhero) - [ ] Personnalize notifications for texts / icons to be used, depending on private/public channels for example (thx @magicgg91) (#975) - [x] Check certficate validity and expiration time (thx @magicgg91) - [x] Ability to configure how long the monitor history is retained. A global setting to automatically delete monitor entries older than ${x} days would declutter the user interface. (thx @Spiritreader) - [ ] Ability for a ""Bulk Upload"" via csv etc (thx @EclipseOfficeTech) (https://github.com/louislam/uptime-kuma/issues/4065 https://github.com/louislam/uptime-kuma/issues/2934 https://github.com/louislam/uptime-kuma/issues/1323) Keep up the good work! <details><summary>updates</summary> <p> _2021-07-15 @ 3.20pm UTC:_ Formatting changed to checkboxes per request @louislam _2021-07-15 @ 4.00pm UTC:_ Added enhancements requested by @kvpt _2021-07-17 @ 8.00am UTC:_ Added enhancements requested by @joe-eklund _2021-07-19 @ 8.02am UTC:_ Added enhancements requested by @parityhero _2021-07-21 @ 9.04am UTC:_ Added enhancements requested by @magicgg91 _2021-07-29 @ 1.40pm UTC:_ Added enhancements requested by @Spiritreader & @EclipseOfficeTech _2023-12-01 @ 17.20pm UTC:_ Updated the todo list by @CommanderStorm </p> </details> other-file other-file other-file other-file other-file other-file other-file",no-bug,0.95
2009,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2009,DNS alert features unnecessary URL field,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The Uptime Kuma notification for a DNS monitor provides an empty URL field - this can be seen in Teams as in the screenshot below:  Reproduction steps - Setup a DNS monitor - Use a Teams webhook for alerts - Trigger the alert  Expected behavior The URL field does not need to be displayed.  Actual Behavior ![image](https://user-images.githubusercontent.com/26556541/185880409-134d8dc5-53cd-4cea-8d30-be0a19d81bb7.png)  Uptime-Kuma Version 1.17.1  Operating System and Arch Ubuntu 20.04  Browser Edge 104  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_,source-file | source-file | source-file,DNS alert features unnecessary URL field   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The Uptime Kuma notification for a DNS monitor provides an empty URL field - this can be seen in Teams as in the screenshot below:  Reproduction steps - Setup a DNS monitor - Use a Teams webhook for alerts - Trigger the alert  Expected behavior The URL field does not need to be displayed.  Actual Behavior ![image](https://user-images.githubusercontent.com/26556541/185880409-134d8dc5-53cd-4cea-8d30-be0a19d81bb7.png)  Uptime-Kuma Version 1.17.1  Operating System and Arch Ubuntu 20.04  Browser Edge 104  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_ source-file source-file source-file,no-bug,0.8
640,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/640,Harden 2FA/TOTP implementation according to rfc6238,"**Is your feature request related to a problem? Please describe.** No, this is about following the RFC's recommendations **Describe the solution you'd like** Harden security for the [TOTP solution](https://github.com/louislam/uptime-kuma/pull/363) by: 1. creating TOTP secrets using a cryptographically strong pseudorandom generator 2. making tokens actually one-time only (currently: token can be used multiple times within time windows) 3. avoiding the TOTP library's default values for window and time in `totp.verify` ([notp](https://www.npmjs.com/package/notp)) **Additional context** The TOTP standard as described in [rfc6238](https://datatracker.ietf.org/doc/html/rfc6238) defines some important recommendations for secure implementation of TOTP.  1. TOTP secret generation (see [rfc6238 section 5.1](https://datatracker.ietf.org/doc/html/rfc6238#section-5.1)) > As indicated in the algorithm requirement section, keys SHOULD be chosen at random or using a cryptographically strong pseudorandom generator properly seeded with a random value. The secret is currently generated using the standard `math.random` function (see [code](https://github.com/Ponkhy/uptime-kuma/blob/c4f78d776e166e7372ecd13c47c9d36c9b7667a7/server/util-server.js#L280)). This is explicitely **not** a cryptographically strong random number generator (see note [here](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/random)). Using [Web Crypto API](https://nodejs.org/docs/latest/api/webcrypto.html#webcrypto_crypto_getrandomvalues_typedarray) instead should respect the standard and produce less predictable secrets.  2. Invalidating used tokens (see [rfc6238 section 5.2](https://datatracker.ietf.org/doc/html/rfc6238#section-5.2)) > Note that a prover may send the same OTP inside a given time-step window multiple times to a verifier. The verifier MUST NOT accept the second attempt of the OTP after the successful validation has been issued for the first OTP, which ensures one-time only use of an OTP. Currently tokens can be used multiple times. The last used token should be stored (incl. timestamp) to check for re-use within the same window. Note that in this case the rfc standard phrases this as a **MUST** not just as a recommendation. With the current implementation a MitM could login without being noticed (as an intercepted token can be re-used).  3. Avoiding notp default values for token verification (see [rfc6238 section 5.2](https://datatracker.ietf.org/doc/html/rfc6238#section-5.2)) > The validation system should compare OTPs not only with the receiving timestamp but also the past timestamps that are within the transmission delay. A larger acceptable delay window would expose a larger window for attacks. We RECOMMEND that at most one time step is allowed as the network delay. > We RECOMMEND a default time-step size of 30 seconds. This default value of 30 seconds is selected as a balance between security and usability. Currently `totp.verifyToken` is called with 2 parameters only (token + key) leading notp to use default values for the number of lookahead windows and the window size (see [code](https://github.com/Ponkhy/uptime-kuma/blob/c4f78d776e166e7372ecd13c47c9d36c9b7667a7/server/server.js#L242)). Provide those options in the [3rd parameter](https://www.npmjs.com/package/notp#totpverifytoken-key-opt) to explicitely define settings recommended in the rfc. While the windows size is by default 30 seconds (and thus the recommended rfc value), the allowable margin is `6` resulting in tokens being valid for +/- 6*30 seconds = 3 minutes. Google Authenticator uses an allowable margin of `1` so I'd suggest using this as it's IMHO the most used TOTP implementation globally. For reference see their [libpam module's code](https://github.com/google/google-authenticator-libpam/blob/0b02aadc28ac261b6c7f5785d2f7f36b3e199d97/src/pam_google_authenticator.c#L1190). It states the number `3` but they interpret this differently than notp does. notp checks `allowable_margin` windows in the past and `allowable_margin` windows in the future while Google checks `allowable_margin` windows total (so for `3` it is: the current window + 1 window in the past + 1 window in the future). Hence Google's `3` is a `1` in notp's interpretation.",source-file | source-file,"Harden 2FA/TOTP implementation according to rfc6238 **Is your feature request related to a problem? Please describe.** No, this is about following the RFC's recommendations **Describe the solution you'd like** Harden security for the [TOTP solution](https://github.com/louislam/uptime-kuma/pull/363) by: 1. creating TOTP secrets using a cryptographically strong pseudorandom generator 2. making tokens actually one-time only (currently: token can be used multiple times within time windows) 3. avoiding the TOTP library's default values for window and time in `totp.verify` ([notp](https://www.npmjs.com/package/notp)) **Additional context** The TOTP standard as described in [rfc6238](https://datatracker.ietf.org/doc/html/rfc6238) defines some important recommendations for secure implementation of TOTP.  1. TOTP secret generation (see [rfc6238 section 5.1](https://datatracker.ietf.org/doc/html/rfc6238#section-5.1)) > As indicated in the algorithm requirement section, keys SHOULD be chosen at random or using a cryptographically strong pseudorandom generator properly seeded with a random value. The secret is currently generated using the standard `math.random` function (see [code](https://github.com/Ponkhy/uptime-kuma/blob/c4f78d776e166e7372ecd13c47c9d36c9b7667a7/server/util-server.js#L280)). This is explicitely **not** a cryptographically strong random number generator (see note [here](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/random)). Using [Web Crypto API](https://nodejs.org/docs/latest/api/webcrypto.html#webcrypto_crypto_getrandomvalues_typedarray) instead should respect the standard and produce less predictable secrets.  2. Invalidating used tokens (see [rfc6238 section 5.2](https://datatracker.ietf.org/doc/html/rfc6238#section-5.2)) > Note that a prover may send the same OTP inside a given time-step window multiple times to a verifier. The verifier MUST NOT accept the second attempt of the OTP after the successful validation has been issued for the first OTP, which ensures one-time only use of an OTP. Currently tokens can be used multiple times. The last used token should be stored (incl. timestamp) to check for re-use within the same window. Note that in this case the rfc standard phrases this as a **MUST** not just as a recommendation. With the current implementation a MitM could login without being noticed (as an intercepted token can be re-used).  3. Avoiding notp default values for token verification (see [rfc6238 section 5.2](https://datatracker.ietf.org/doc/html/rfc6238#section-5.2)) > The validation system should compare OTPs not only with the receiving timestamp but also the past timestamps that are within the transmission delay. A larger acceptable delay window would expose a larger window for attacks. We RECOMMEND that at most one time step is allowed as the network delay. > We RECOMMEND a default time-step size of 30 seconds. This default value of 30 seconds is selected as a balance between security and usability. Currently `totp.verifyToken` is called with 2 parameters only (token + key) leading notp to use default values for the number of lookahead windows and the window size (see [code](https://github.com/Ponkhy/uptime-kuma/blob/c4f78d776e166e7372ecd13c47c9d36c9b7667a7/server/server.js#L242)). Provide those options in the [3rd parameter](https://www.npmjs.com/package/notp#totpverifytoken-key-opt) to explicitely define settings recommended in the rfc. While the windows size is by default 30 seconds (and thus the recommended rfc value), the allowable margin is `6` resulting in tokens being valid for +/- 6*30 seconds = 3 minutes. Google Authenticator uses an allowable margin of `1` so I'd suggest using this as it's IMHO the most used TOTP implementation globally. For reference see their [libpam module's code](https://github.com/google/google-authenticator-libpam/blob/0b02aadc28ac261b6c7f5785d2f7f36b3e199d97/src/pam_google_authenticator.c#L1190). It states the number `3` but they interpret this differently than notp does. notp checks `allowable_margin` windows in the past and `allowable_margin` windows in the future while Google checks `allowable_margin` windows total (so for `3` it is: the current window + 1 window in the past + 1 window in the future). Hence Google's `3` is a `1` in notp's interpretation. source-file source-file",no-bug,0.9
3520,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3520,Incorrect accepted_statuscodes via API (using python uptime-kuma-api package) made kuma completely non-functional,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Passing incorrect input [200] to accepted_statuscodes via the python api package made kuma completely non-functional. The backend should do atleast some validation on fields, before saving monitors.  Reproduction steps Create a Monitor via the Python API using ""uptime-kuma-api""  api = UptimeKumaApi(f'https://kuma.example.com') api.login('root', '1234') monitor = api.add_monitor( type=MonitorType.HTTP, name=f'Customer: xy', url='https://something.example.com', interval=60 * 5, maxretries=3, accepted_statuscodes=[200], )   Expected behavior Throwing some sort of error and not saving the incorrect monitor.  Actual Behavior Server crashes directly and on every startup, can not reach Backend or use the API.  Uptime-Kuma Version 1.22.1 with docker alpine  Operating System and Arch Ubuntu 22.04.2 LTS  Browser Firefox 115  Docker Version Docker version 24.0.5  NodeJS Version 16  Relevant log output shell 2023-08-03T07:47:03+02:00 [MONITOR] INFO: Added Monitor: undefined User ID: 1 /app/server/util-server.js:585 const codeRangeSplit = codeRange.split(""-"").map(string => parseInt(string)); ^ TypeError: codeRange.split is not a function at exports.checkStatusCode (/app/server/util-server.js:585:42) at validateStatus (/app/server/model/monitor.js:356:36) at settle (/app/node_modules/axios/lib/core/settle.js:14:46) at IncomingMessage.handleStreamEnd (/app/node_modules/axios/lib/adapters/http.js:347:11) at IncomingMessage.emit (node:events:402:35) at endReadableNT (node:internal/streams/readable:1343:12) at processTicksAndRejections (node:internal/process/task_queues:83:21) ==> Performing startup jobs and maintenance tasks ==> Starting application with user 0 group 0 Welcome to Uptime Kuma Your Node.js version: 16 2023-08-03T05:47:03Z [SERVER] INFO: Welcome to Uptime Kuma 2023-08-03T05:47:03Z [SERVER] INFO: Node Env: production 2023-08-03T05:47:03Z [SERVER] INFO: Importing Node libraries 2023-08-03T05:47:03Z [SERVER] INFO: Importing 3rd-party libraries 2023-08-03T05:47:05Z [SERVER] INFO: Creating express and socket.io instance 2023-08-03T05:47:05Z [SERVER] INFO: Server Type: HTTP 2023-08-03T05:47:05Z [SERVER] INFO: Importing this project modules 2023-08-03T05:47:05Z [NOTIFICATION] INFO: Prepare Notification Providers 2023-08-03T05:47:05Z [SERVER] INFO: Version: 1.22.1 2023-08-03T05:47:05Z [DB] INFO: Data Dir: ./data/ 2023-08-03T05:47:05Z [SERVER] INFO: Connecting to the Database 2023-08-03T05:47:05Z [DB] INFO: SQLite config: [ { journal_mode: 'wal' } ] [ { cache_size: -12000 } ] 2023-08-03T05:47:05Z [DB] INFO: SQLite Version: 3.41.1 2023-08-03T05:47:05Z [SERVER] INFO: Connected 2023-08-03T05:47:05Z [DB] INFO: Your database version: 10 2023-08-03T05:47:05Z [DB] INFO: Latest database version: 10 2023-08-03T05:47:05Z [DB] INFO: Database patch not needed 2023-08-03T05:47:05Z [DB] INFO: Database Patch 2.0 Process 2023-08-03T05:47:05Z [SERVER] INFO: Load JWT secret from database. 2023-08-03T07:47:05+02:00 [SERVER] INFO: Adding route 2023-08-03T07:47:05+02:00 [SERVER] INFO: Adding socket handler 2023-08-03T07:47:05+02:00 [SERVER] INFO: Init the server 2023-08-03T07:47:05+02:00 [SERVER] INFO: Listening on 3001 /app/server/util-server.js:585 const codeRangeSplit = codeRange.split(""-"").map(string => parseInt(string)); ^ TypeError: codeRange.split is not a function at exports.checkStatusCode (/app/server/util-server.js:585:42) at validateStatus (/app/server/model/monitor.js:356:36) at settle (/app/node_modules/axios/lib/core/settle.js:14:46) at IncomingMessage.handleStreamEnd (/app/node_modules/axios/lib/adapters/http.js:347:11) at IncomingMessage.emit (node:events:402:35) at endReadableNT (node:internal/streams/readable:1343:12) at processTicksAndRejections (node:internal/process/task_queues:83:21) ",source-file | source-file | source-file | source-file | source-file,"Incorrect accepted_statuscodes via API (using python uptime-kuma-api package) made kuma completely non-functional   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Passing incorrect input [200] to accepted_statuscodes via the python api package made kuma completely non-functional. The backend should do atleast some validation on fields, before saving monitors.  Reproduction steps Create a Monitor via the Python API using ""uptime-kuma-api""  api = UptimeKumaApi(f'https://kuma.example.com') api.login('root', '1234') monitor = api.add_monitor( type=MonitorType.HTTP, name=f'Customer: xy', url='https://something.example.com', interval=60 * 5, maxretries=3, accepted_statuscodes=[200], )   Expected behavior Throwing some sort of error and not saving the incorrect monitor.  Actual Behavior Server crashes directly and on every startup, can not reach Backend or use the API.  Uptime-Kuma Version 1.22.1 with docker alpine  Operating System and Arch Ubuntu 22.04.2 LTS  Browser Firefox 115  Docker Version Docker version 24.0.5  NodeJS Version 16  Relevant log output shell 2023-08-03T07:47:03+02:00 [MONITOR] INFO: Added Monitor: undefined User ID: 1 /app/server/util-server.js:585 const codeRangeSplit = codeRange.split(""-"").map(string => parseInt(string)); ^ TypeError: codeRange.split is not a function at exports.checkStatusCode (/app/server/util-server.js:585:42) at validateStatus (/app/server/model/monitor.js:356:36) at settle (/app/node_modules/axios/lib/core/settle.js:14:46) at IncomingMessage.handleStreamEnd (/app/node_modules/axios/lib/adapters/http.js:347:11) at IncomingMessage.emit (node:events:402:35) at endReadableNT (node:internal/streams/readable:1343:12) at processTicksAndRejections (node:internal/process/task_queues:83:21) ==> Performing startup jobs and maintenance tasks ==> Starting application with user 0 group 0 Welcome to Uptime Kuma Your Node.js version: 16 2023-08-03T05:47:03Z [SERVER] INFO: Welcome to Uptime Kuma 2023-08-03T05:47:03Z [SERVER] INFO: Node Env: production 2023-08-03T05:47:03Z [SERVER] INFO: Importing Node libraries 2023-08-03T05:47:03Z [SERVER] INFO: Importing 3rd-party libraries 2023-08-03T05:47:05Z [SERVER] INFO: Creating express and socket.io instance 2023-08-03T05:47:05Z [SERVER] INFO: Server Type: HTTP 2023-08-03T05:47:05Z [SERVER] INFO: Importing this project modules 2023-08-03T05:47:05Z [NOTIFICATION] INFO: Prepare Notification Providers 2023-08-03T05:47:05Z [SERVER] INFO: Version: 1.22.1 2023-08-03T05:47:05Z [DB] INFO: Data Dir: ./data/ 2023-08-03T05:47:05Z [SERVER] INFO: Connecting to the Database 2023-08-03T05:47:05Z [DB] INFO: SQLite config: [ { journal_mode: 'wal' } ] [ { cache_size: -12000 } ] 2023-08-03T05:47:05Z [DB] INFO: SQLite Version: 3.41.1 2023-08-03T05:47:05Z [SERVER] INFO: Connected 2023-08-03T05:47:05Z [DB] INFO: Your database version: 10 2023-08-03T05:47:05Z [DB] INFO: Latest database version: 10 2023-08-03T05:47:05Z [DB] INFO: Database patch not needed 2023-08-03T05:47:05Z [DB] INFO: Database Patch 2.0 Process 2023-08-03T05:47:05Z [SERVER] INFO: Load JWT secret from database. 2023-08-03T07:47:05+02:00 [SERVER] INFO: Adding route 2023-08-03T07:47:05+02:00 [SERVER] INFO: Adding socket handler 2023-08-03T07:47:05+02:00 [SERVER] INFO: Init the server 2023-08-03T07:47:05+02:00 [SERVER] INFO: Listening on 3001 /app/server/util-server.js:585 const codeRangeSplit = codeRange.split(""-"").map(string => parseInt(string)); ^ TypeError: codeRange.split is not a function at exports.checkStatusCode (/app/server/util-server.js:585:42) at validateStatus (/app/server/model/monitor.js:356:36) at settle (/app/node_modules/axios/lib/core/settle.js:14:46) at IncomingMessage.handleStreamEnd (/app/node_modules/axios/lib/adapters/http.js:347:11) at IncomingMessage.emit (node:events:402:35) at endReadableNT (node:internal/streams/readable:1343:12) at processTicksAndRejections (node:internal/process/task_queues:83:21)  source-file source-file source-file source-file source-file",bug,0.95
39,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/39,indentation hostnames is off,"Might not a big deal, but maybe it can be easily corrected. <img width=""447"" alt=""Screenshot 2021-07-13 at 22 50 33"" src=""https://user-images.githubusercontent.com/54367657/125523092-d21f2037-8811-4cb4-85bc-7a5385622b63.png""> Edit: This issue persist on my macbook using safari/chrome/brave, but not on Windows using Chrome.",other-file | other-file,"indentation hostnames is off Might not a big deal, but maybe it can be easily corrected. <img width=""447"" alt=""Screenshot 2021-07-13 at 22 50 33"" src=""https://user-images.githubusercontent.com/54367657/125523092-d21f2037-8811-4cb4-85bc-7a5385622b63.png""> Edit: This issue persist on my macbook using safari/chrome/brave, but not on Windows using Chrome. other-file other-file",no-bug,0.9
2365,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2365,Status page: Custom Links in Footer and Description,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description I would like to link to an imprint / legal notices in the status page footer. Currently, the footer is text-only.   Solution It would be nice to be able to include an imprint using Markdown or html: markdown [Imprint](https://example.org/imprint)  html <a href=""https://example.org/imprint"">Imprint</a>    Alternatives An alternative would be to add custom fields for links with imprint and privacy policies: Mockup of the config sidebar: ![grafik](https://user-images.githubusercontent.com/7175914/205633571-b06620c9-ec93-4aec-a4a5-86f3c5dde498.png)  Additional Context _No response_",test-file | documentation-file | container-file | container-file | container-file | source-file | documentation-file | source-file | source-file | other-file | source-file | source-file | source-file | other-file | documentation-file | test-file | documentation-file | documentation-file | source-file | database-file | database-file | container-file | container-file | container-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | source-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | documentation-file | documentation-file | documentation-file | other-file | documentation-file | other-file | other-file,"Status page: Custom Links in Footer and Description   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description I would like to link to an imprint / legal notices in the status page footer. Currently, the footer is text-only.   Solution It would be nice to be able to include an imprint using Markdown or html: markdown [Imprint](https://example.org/imprint)  html <a href=""https://example.org/imprint"">Imprint</a>    Alternatives An alternative would be to add custom fields for links with imprint and privacy policies: Mockup of the config sidebar: ![grafik](https://user-images.githubusercontent.com/7175914/205633571-b06620c9-ec93-4aec-a4a5-86f3c5dde498.png)  Additional Context _No response_ test-file documentation-file container-file container-file container-file source-file documentation-file source-file source-file other-file source-file source-file source-file other-file documentation-file test-file documentation-file documentation-file source-file database-file database-file container-file container-file container-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file source-file source-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file source-file source-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file documentation-file documentation-file documentation-file other-file documentation-file other-file other-file",no-bug,0.95
2455,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2455,After 1.19.0 update cert expiry promtheus metrics are missing from http(s) monitors,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description After the update to version 0.19.0 some Prometheus metrics are missing for the the monitor type http(s) with certificate expiration checks enabled. However metrics of type http(s) - keyword are still reported via the metric `monitor_cert_days_remaining `  Reproduction steps Update to the latest version: - Check the prometheus metric `monitor_cert_days_remaining` and see that the metris are missing  Expected behavior The metrics for both http(s) and http(s) - keyword monitors should be there  Actual Behavior The metrics for http(s) monitors are missing  Uptime-Kuma Version 1.19.0  Operating System and Arch Ubuntu 20.04  Browser Safari  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_,source-file,After 1.19.0 update cert expiry promtheus metrics are missing from http(s) monitors   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description After the update to version 0.19.0 some Prometheus metrics are missing for the the monitor type http(s) with certificate expiration checks enabled. However metrics of type http(s) - keyword are still reported via the metric `monitor_cert_days_remaining `  Reproduction steps Update to the latest version: - Check the prometheus metric `monitor_cert_days_remaining` and see that the metris are missing  Expected behavior The metrics for both http(s) and http(s) - keyword monitors should be there  Actual Behavior The metrics for http(s) monitors are missing  Uptime-Kuma Version 1.19.0  Operating System and Arch Ubuntu 20.04  Browser Safari  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_ source-file,no-bug,0.8
1221,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1221,Clickable hostname to site in name on status page.,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description link to site in Status Page name.   Solution A clickable link in the name of the monitored site   Alternatives _No response_  Additional Context I would like to be able to click on a name on a monitor to go directly to the webpage, just like the way it works in the Dashboard. By that, i can use the status page as a launcher for all my sites.",source-file | other-file | database-file | source-file | source-file | source-file | source-file | other-file | source-file | database-file | source-file | source-file | source-file | source-file | other-file | source-file | database-file | source-file | source-file | source-file | source-file | other-file | source-file | documentation-file | documentation-file | source-file | database-file | database-file | container-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | other-file | source-file | config-file | source-file | documentation-file | documentation-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | database-file | source-file | source-file | source-file | source-file | other-file | source-file,"Clickable hostname to site in name on status page.   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description link to site in Status Page name.   Solution A clickable link in the name of the monitored site   Alternatives _No response_  Additional Context I would like to be able to click on a name on a monitor to go directly to the webpage, just like the way it works in the Dashboard. By that, i can use the status page as a launcher for all my sites. source-file other-file database-file source-file source-file source-file source-file other-file source-file database-file source-file source-file source-file source-file other-file source-file database-file source-file source-file source-file source-file other-file source-file documentation-file documentation-file source-file database-file database-file container-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file other-file source-file config-file source-file documentation-file documentation-file other-file source-file source-file source-file source-file source-file source-file other-file other-file source-file documentation-file source-file source-file source-file source-file source-file other-file other-file database-file source-file source-file source-file source-file other-file source-file",no-bug,0.95
486,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/486,1.7.0 - Setup 2FA - Invalid Barcode - Google Authenticator,**Is it a duplicate question?** No **Describe the bug** Invalid barcode error when setting up 2FA using the Google Authenticator app on IOS. Error message: `The barcode 'REDACTED' is not a valid authentication token barcode.` **To Reproduce** Steps to reproduce the behavior: Go to Settings > 2FA Settings > Open Google Authenticator App (IOS) > + Sign > Scan a QR code > Scan QR code from Uptime-Kuma > Error message is shown. **Expected behavior** The barcode is accepted and a time-based token is generated. **Info** Uptime Kuma Version: 1.7.0-debian Using Docker?: Yes Docker Version: 20.10.8 Node.js Version (Without Docker only): N/A OS: Raspbian GNU/Linux 10 (buster) Browser: Firefox 92.0.1 **Screenshots** N/A **Error Log** N/A Docker: `docker logs <container id>` PM2: `~/.pm2/logs/` (e.g. `/home/ubuntu/.pm2/logs`) N/A,source-file,1.7.0 - Setup 2FA - Invalid Barcode - Google Authenticator **Is it a duplicate question?** No **Describe the bug** Invalid barcode error when setting up 2FA using the Google Authenticator app on IOS. Error message: `The barcode 'REDACTED' is not a valid authentication token barcode.` **To Reproduce** Steps to reproduce the behavior: Go to Settings > 2FA Settings > Open Google Authenticator App (IOS) > + Sign > Scan a QR code > Scan QR code from Uptime-Kuma > Error message is shown. **Expected behavior** The barcode is accepted and a time-based token is generated. **Info** Uptime Kuma Version: 1.7.0-debian Using Docker?: Yes Docker Version: 20.10.8 Node.js Version (Without Docker only): N/A OS: Raspbian GNU/Linux 10 (buster) Browser: Firefox 92.0.1 **Screenshots** N/A **Error Log** N/A Docker: `docker logs <container id>` PM2: `~/.pm2/logs/` (e.g. `/home/ubuntu/.pm2/logs`) N/A source-file,no-bug,0.9
56,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/56,Implementing the retries enhancement,"I've been looking at the code responsible for sending notifications when a service goes down mentioned in #21. It looks like this would be quite reasonable to implement, as such I am writing a draft and am curious to see if ends up being any good. I've had the issue many times now since using Uptime Kuma that my services get marked as down and I get a notification, when it was just a pinging issue. I would actually try and implement this myself if that is welcome, however I am currently quite busy and don't know if I can follow up in a timely enough matter before someone else picks it up. But making a draft is potentially helpful for the developer who ends up implementing it. The steps would pretty much include (if I'm not mistaken, I only had 30 min or so to look at the codebase) - **[BACKEND]** add a field to the monitor called ""maxRetries"" which holds the maximum amount of retries allowed before a service will go ""down"" and trigger notifications - **[BACKEND]** Create a new monitor.status value (for example `2`, indicating that the service is currently ""disrupted"") - **[BACKEND]** keep track of the threshold by storing the amount of failed retries with the monitor ID in an array or KV data structure (easy) or in-memory database (more sophisticated). And also update the status to the new value (instead of it being `0` if the service has not reached `maxRetries`. I believe this should go here in the catch section of the polling try block https://github.com/louislam/uptime-kuma/blob/b3bff8d7357d75d3871aa68bc71db35dd79506a9/server/model/monitor.js#L112 - **[BACKEND]** make the modifications here and in similar places below https://github.com/louislam/uptime-kuma/blob/b3bff8d7357d75d3871aa68bc71db35dd79506a9/server/model/monitor.js#L121 to incorporate the notification triggering only when the threshold has been reached - **[FRONTEND]** Add the retries field to the monitor edit page - **[FRONTEND]** _Optional: Indicate that the service is potentially disrupted by displaying the retry count. This could add bloat to the UI experience, so not sure if that makes sense._ I also never worked with bean, so it's possible that a manual data migration would be necessary if a new field is added to the database. The documentation lists that bean [does not have migration? ](https://redbean-node.whatsticker.online/Migration) Does anyone know how @louislam is solving the migration task? I think this would describe the minimum valuable product. Using the configured monitor ping interval doesn't require too many changes and requires little modification in the codebase. What do you think? Also I apologize that this issue is such a mess, I accidentally hit enter instead of backspace when editing the title so I essentially had to write the draft while the issue already existed",other-file | other-file | database-file | database-file | source-file | source-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file,"Implementing the retries enhancement I've been looking at the code responsible for sending notifications when a service goes down mentioned in #21. It looks like this would be quite reasonable to implement, as such I am writing a draft and am curious to see if ends up being any good. I've had the issue many times now since using Uptime Kuma that my services get marked as down and I get a notification, when it was just a pinging issue. I would actually try and implement this myself if that is welcome, however I am currently quite busy and don't know if I can follow up in a timely enough matter before someone else picks it up. But making a draft is potentially helpful for the developer who ends up implementing it. The steps would pretty much include (if I'm not mistaken, I only had 30 min or so to look at the codebase) - **[BACKEND]** add a field to the monitor called ""maxRetries"" which holds the maximum amount of retries allowed before a service will go ""down"" and trigger notifications - **[BACKEND]** Create a new monitor.status value (for example `2`, indicating that the service is currently ""disrupted"") - **[BACKEND]** keep track of the threshold by storing the amount of failed retries with the monitor ID in an array or KV data structure (easy) or in-memory database (more sophisticated). And also update the status to the new value (instead of it being `0` if the service has not reached `maxRetries`. I believe this should go here in the catch section of the polling try block https://github.com/louislam/uptime-kuma/blob/b3bff8d7357d75d3871aa68bc71db35dd79506a9/server/model/monitor.js#L112 - **[BACKEND]** make the modifications here and in similar places below https://github.com/louislam/uptime-kuma/blob/b3bff8d7357d75d3871aa68bc71db35dd79506a9/server/model/monitor.js#L121 to incorporate the notification triggering only when the threshold has been reached - **[FRONTEND]** Add the retries field to the monitor edit page - **[FRONTEND]** _Optional: Indicate that the service is potentially disrupted by displaying the retry count. This could add bloat to the UI experience, so not sure if that makes sense._ I also never worked with bean, so it's possible that a manual data migration would be necessary if a new field is added to the database. The documentation lists that bean [does not have migration? ](https://redbean-node.whatsticker.online/Migration) Does anyone know how @louislam is solving the migration task? I think this would describe the minimum valuable product. Using the configured monitor ping interval doesn't require too many changes and requires little modification in the codebase. What do you think? Also I apologize that this issue is such a mess, I accidentally hit enter instead of backspace when editing the title so I essentially had to write the draft while the issue already existed other-file other-file database-file database-file source-file source-file other-file other-file other-file other-file source-file other-file other-file",no-bug,0.9
294,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/294,monitor type ping stopped working,"**Info** Uptime Kuma Version: 1.3.2 Using Docker?: Yes OS: Windows (client) Docker with uptime-kuma runs on Debian 10 Browser: Chrome I installed uptime-kuma a few days ago and setup a few monitors, most of them https and one as ping type. The ping type one stopped working yesterday evening. I know the target is up, even if I do a `docker exec -ti uptime-kuma sh ` and do a manual ping to the same target the ping works so why does it keep failing for a full 24h inside uptime-kuma? Also, uptimerobot reports no issues pinging this particular host. Open for any suggestions.",source-file,"monitor type ping stopped working **Info** Uptime Kuma Version: 1.3.2 Using Docker?: Yes OS: Windows (client) Docker with uptime-kuma runs on Debian 10 Browser: Chrome I installed uptime-kuma a few days ago and setup a few monitors, most of them https and one as ping type. The ping type one stopped working yesterday evening. I know the target is up, even if I do a `docker exec -ti uptime-kuma sh ` and do a manual ping to the same target the ping works so why does it keep failing for a full 24h inside uptime-kuma? Also, uptimerobot reports no issues pinging this particular host. Open for any suggestions. source-file",no-bug,0.8
2468,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2468,"""unexpected end of file"" error with 1.19.0",  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description After upgrading to latest version (1.19.0) I noticed several targets I was monitoring was detected as down with this error: unexpected end of file After downgrading to 1.18.5 everything works fine again!  Reproduction steps Install 1.19.0 and try to monitor this url: http://REDACTED/  Expected behavior monitoring should work  Actual Behavior This will fail on 1.19.0 and is working on previous versions  Uptime-Kuma Version 1.19.0  Operating System and Arch Ubuntu 22.04  Browser Google Chrome  Docker Version _No response_  NodeJS Version _No response_  Relevant log output shell [MONITOR] WARN: Monitor #6 'node 4': Failing: unexpected end of file | Interval: 20 seconds | Type: http | Down Count: 0 | Resend Interval: 0 ,source-file,"""unexpected end of file"" error with 1.19.0   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description After upgrading to latest version (1.19.0) I noticed several targets I was monitoring was detected as down with this error: unexpected end of file After downgrading to 1.18.5 everything works fine again!  Reproduction steps Install 1.19.0 and try to monitor this url: http://REDACTED/  Expected behavior monitoring should work  Actual Behavior This will fail on 1.19.0 and is working on previous versions  Uptime-Kuma Version 1.19.0  Operating System and Arch Ubuntu 22.04  Browser Google Chrome  Docker Version _No response_  NodeJS Version _No response_  Relevant log output shell [MONITOR] WARN: Monitor #6 'node 4': Failing: unexpected end of file | Interval: 20 seconds | Type: http | Down Count: 0 | Resend Interval: 0  source-file",bug,0.9
728,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/728,Favicon counter,It would be nice to have a favicon counter in Uptime Kuma. It displays how many monitors are up or down while browsing in other tabs. **Example:** ![image](https://user-images.githubusercontent.com/8298741/137601679-f6a9d98a-5d96-4562-9e02-505e939af6b4.png) **Uptime Robot** ![image](https://user-images.githubusercontent.com/8298741/137601754-4d8e8cbb-7ca0-4f12-894f-0d153f730fc3.png),other-file | other-file | other-file | documentation-file | source-file | other-file | other-file | other-file | documentation-file | source-file,Favicon counter It would be nice to have a favicon counter in Uptime Kuma. It displays how many monitors are up or down while browsing in other tabs. **Example:** ![image](https://user-images.githubusercontent.com/8298741/137601679-f6a9d98a-5d96-4562-9e02-505e939af6b4.png) **Uptime Robot** ![image](https://user-images.githubusercontent.com/8298741/137601754-4d8e8cbb-7ca0-4f12-894f-0d153f730fc3.png) other-file other-file other-file documentation-file source-file other-file other-file other-file documentation-file source-file,no-bug,0.95
1412,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1412,Upgrading Kuma Wipes Out Data,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem Thankfully not too critical but when upgrading to the latest version of kuma it wipes out the data collected from monitors. The upgrade basically makes it start from scratch. https://github.com/louislam/uptime-kuma/wiki/%F0%9F%86%99-How-to-Update#-docker  Uptime-Kuma Version 1.13.1  Operating System and Arch Ubuntu 20.04 LTS  Browser 99.0.4844.82  Docker Version 20.10.13  NodeJS Version none,source-file,Upgrading Kuma Wipes Out Data   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem Thankfully not too critical but when upgrading to the latest version of kuma it wipes out the data collected from monitors. The upgrade basically makes it start from scratch. https://github.com/louislam/uptime-kuma/wiki/%F0%9F%86%99-How-to-Update#-docker  Uptime-Kuma Version 1.13.1  Operating System and Arch Ubuntu 20.04 LTS  Browser 99.0.4844.82  Docker Version 20.10.13  NodeJS Version none source-file,no-bug,0.9
2182,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2182,Add support for different ports for Radius,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Monitor  Feature description Could we please have the ability to set the port for the radius monitor?   Solution Either a new field for a port, or perhaps its easier to allow hostname:port Either would be great.   Alternatives _No response_  Additional Context _No response_",source-file | source-file | other-file | source-file | source-file | other-file | source-file | source-file | other-file,"Add support for different ports for Radius   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Monitor  Feature description Could we please have the ability to set the port for the radius monitor?   Solution Either a new field for a port, or perhaps its easier to allow hostname:port Either would be great.   Alternatives _No response_  Additional Context _No response_ source-file source-file other-file source-file source-file other-file source-file source-file other-file",no-bug,0.95
3767,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3767,Podman container Pending without healthcheck,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Podman API provide State.Health with empty values for containers without healthcheck, which detected as PENDING by Kuma. Podman container **with** healthcheck defined:   ""State"":{ ""Status"":""running"", ""Running"":true, ""Paused"":false, ""Restarting"":false, ""OOMKilled"":false, ""Dead"":false, ""Pid"":165204, ""ExitCode"":0, ""Error"":"""", ""StartedAt"":""2023-09-13T12:01:30.947248168Z"", ""FinishedAt"":""0001-01-01T00:00:00Z"", ""Health"":{ ""Status"":""healthy"", ""FailingStreak"":0, ""Log"":[ SKIPPED ] } },   Podman container **without** healthcheck defined:   ""State"":{ ""Status"":""running"", ""Running"":true, ""Paused"":false, ""Restarting"":false, ""OOMKilled"":false, ""Dead"":false, ""Pid"":3089279, ""ExitCode"":0, ""Error"":"""", ""StartedAt"":""2023-09-19T09:41:10.64045291Z"", ""FinishedAt"":""0001-01-01T00:00:00Z"", ""Health"":{ ""Status"":"""", ""FailingStreak"":0, ""Log"":null } },    Reproduction steps * run container without healthcheck in podman * configure kuma to monitor it  Expected behavior Kuma detect container as UP  Actual Behavior Kuma detect container as PENDING  Uptime-Kuma Version Uptime Kuma Version: 1.23.1  Operating System and Arch Fedora 37 amd64  Browser Firefox  Docker Version podman version 4.6.2  NodeJS Version _No response_  Relevant log output shell Kuma log: 2023-09-19T17:42:08+06:00 [MONITOR] WARN: Monitor #25 'TEST': Pending: | Max retries: 1 | Retry: 0 | Retry Interval: 60 seconds | Type: docker TcpDump: GET /containers/keen_torvalds/json HTTP/1.1 Accept: */* User-Agent: Uptime-Kuma/1.23.1 Host: podmanproxy:2375 Connection: close HTTP/1.1 200 OK Api-Version: 1.41 Content-Type: application/json Libpod-Api-Version: 4.6.2 Server: Libpod/4.6.2 (linux) X-Reference-Id: 0xc000be6558 Date: Tue, 19 Sep 2023 11:41:08 GMT Connection: close Transfer-Encoding: chunked {""Id"":""496a2cb6e16a7822d7669be2ebbf1f32f6d09751d677a93d54b67c3020e66f72"",""Created"":""2023-09-19T09:41:10.589808818Z"",""Path"":""sh"",""Args"":[""-c"",""apk add tcpdump socat; tcpdump -w - -Unq tcp port not 9999 | socat -t0 -T0 - tcp:laptop.lan:9999""],""State"":{""Status"":""running"",""Running"":true,""Paused"":false,""Restarting"":false,""OOMKilled"":false,""Dead"":false,""Pid"":3089279,""ExitCode"":0,""Error"":"""",""StartedAt"":""2023-09-19T09:41:10.64045291Z"",""FinishedAt"":""0001-01-01T00:00:00Z"",""Health"":{""Status"":"""",""FailingStreak"":0,""Log"":null}},""Image"":""sha256:7e01a0d0a1dcd9e539f8e9bbd80106d59efbdf97293b3d38f5d7a34501526cdb"",""ResolvConfPath"":""/run/containers/storage/btrfs-containers/42fca70f376092b3d93fbe719df76b824c99ea689fbb8bfcab416371d17d6d18/userdata/resolv.conf"",""HostnamePath"":""/run/containers/storage/btrfs-containers/496a2cb6e16a7822d7669be2ebbf1f32f6d09751d677a93d54b67c3020e66f72/userdata/hostname"",""HostsPath"":""/run/containers/storage/btrfs-containers/42fca70f376092b3d93fbe719df76b824c99ea689fbb8bfcab416371d17d6d18/userdata/hosts"",""LogPath"":"""",""Name"":""/keen_torvalds"",""RestartCount"":0,""Driver"":""btrfs"",""Platform"":""linux"",""MountLabel"":"""",""ProcessLabel"":"""",""AppArmorProfile"":"""",""ExecIDs"":[],""HostConfig"":{""Binds"":[],""ContainerIDFile"":"""",""LogConfig"":{""Type"":""journald"",""Config"":null},""NetworkMode"":""container:42fca70f376092b3d93fbe719df76b824c99ea689fbb8bfcab416371d17d6d18"",""PortBindings"":{},""RestartPolicy"":{""Name"":"""",""MaximumRetryCount"":0},""AutoRemove"":true,""VolumeDriver"":"""",""VolumesFrom"":null,""ConsoleSize"":[0,0],""CapAdd"":[""NET_RAW""],""CapDrop"":[],""CgroupnsMode"":"""",""Dns"":[],""DnsOptions"":[],""DnsSearch"":[],""ExtraHosts"":[],""GroupAdd"":[],""IpcMode"":""shareable"",""Cgroup"":"""",""Links"":null,""OomScoreAdj"":0,""PidMode"":""private"",""Privileged"":false,""PublishAllPorts"":false,""ReadonlyRootfs"":false,""SecurityOpt"":[],""UTSMode"":""private"",""UsernsMode"":"""",""ShmSize"":65536000,""Runtime"":""oci"",""Isolation"":"""",""CpuShares"":0,""Memory"":0,""NanoCpus"":0,""CgroupParent"":"""",""BlkioWeight"":0,""BlkioWeightDevice"":null,""BlkioDeviceReadBps"":null,""BlkioDeviceWriteBps"":null,""BlkioDeviceReadIOps"":null,""BlkioDeviceWriteIOps"":null,""CpuPeriod"":0,""CpuQuota"":0,""CpuRealtimePeriod"":0,""CpuRealtimeRuntime"":0,""CpusetCpus"":"""",""CpusetMems"":"""",""Devices"":[],""DeviceCgroupRules"":null,""DeviceRequests"":null,""MemoryReservation"":0,""MemorySwap"":0,""MemorySwappiness"":0,""OomKillDisable"":false,""PidsLimit"":2048,""Ulimits"":[{""Name"":""RLIMIT_NPROC"",""Hard"":4194304,""Soft"":4194304}],""CpuCount"":0,""CpuPercent"":0,""IOMaximumIOps"":0,""IOMaximumBandwidth"":0,""MaskedPaths"":null,""ReadonlyPaths"":null},""GraphDriver"":{""Data"":null,""Name"":""btrfs""},""SizeRootFs"":0,""Mounts"":[],""Config"":{""Hostname"":""496a2cb6e16a"",""Domainname"":"""",""User"":"""",""AttachStdin"":false,""AttachStdout"":false,""AttachStderr"":false,""ExposedPorts"":{""2375/tcp"":{}},""Tty"":true,""OpenStdin"":true,""StdinOnce"":false,""Env"":[""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"",""TERM=xterm"",""container=podman"",""HOME=/root"",""HOSTNAME=496a2cb6e16a""],""Cmd"":[""sh"",""-c"",""apk add tcpdump socat; tcpdump -w - -Unq tcp port not 9999 | socat -t0 -T0 - tcp:gram.lan:9999""],""Image"":""docker.io/library/alpine:latest"",""Volumes"":null,""WorkingDir"":""/"",""Entrypoint"":[],""OnBuild"":null,""Labels"":{},""StopSignal"":""15"",""StopTimeout"":10},""NetworkSettings"":{""Bridge"":"""",""SandboxID"":"""",""HairpinMode"":false,""LinkLocalIPv6Address"":"""",""LinkLocalIPv6PrefixLen"":0,""Ports"":{""2375/tcp"":null},""SandboxKey"":""/run/netns/netns-0ceac2ae-63a2-0a69-b779-3f25d373ed50"",""SecondaryIPAddresses"":null,""SecondaryIPv6Addresses"":null,""EndpointID"":"""",""Gateway"":"""",""GlobalIPv6Address"":"""",""GlobalIPv6PrefixLen"":0,""IPAddress"":"""",""IPPrefixLen"":0,""IPv6Gateway"":"""",""MacAddress"":"""",""Networks"":{""podwg"":{""IPAMConfig"":null,""Links"":null,""Aliases"":[""42fca70f3760""],""NetworkID"":""podwg"",""EndpointID"":"""",""Gateway"":""10.89.0.1"",""IPAddress"":""10.89.0.133"",""IPPrefixLen"":24,""IPv6Gateway"":"""",""GlobalIPv6Address"":"""",""GlobalIPv6PrefixLen"":0,""MacAddress"":""5a:bb:0a:f9:56:a0"",""DriverOpts"":null ",source-file | source-file | source-file | source-file,"Podman container Pending without healthcheck   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Podman API provide State.Health with empty values for containers without healthcheck, which detected as PENDING by Kuma. Podman container **with** healthcheck defined:   ""State"":{ ""Status"":""running"", ""Running"":true, ""Paused"":false, ""Restarting"":false, ""OOMKilled"":false, ""Dead"":false, ""Pid"":165204, ""ExitCode"":0, ""Error"":"""", ""StartedAt"":""2023-09-13T12:01:30.947248168Z"", ""FinishedAt"":""0001-01-01T00:00:00Z"", ""Health"":{ ""Status"":""healthy"", ""FailingStreak"":0, ""Log"":[ SKIPPED ] } },   Podman container **without** healthcheck defined:   ""State"":{ ""Status"":""running"", ""Running"":true, ""Paused"":false, ""Restarting"":false, ""OOMKilled"":false, ""Dead"":false, ""Pid"":3089279, ""ExitCode"":0, ""Error"":"""", ""StartedAt"":""2023-09-19T09:41:10.64045291Z"", ""FinishedAt"":""0001-01-01T00:00:00Z"", ""Health"":{ ""Status"":"""", ""FailingStreak"":0, ""Log"":null } },    Reproduction steps * run container without healthcheck in podman * configure kuma to monitor it  Expected behavior Kuma detect container as UP  Actual Behavior Kuma detect container as PENDING  Uptime-Kuma Version Uptime Kuma Version: 1.23.1  Operating System and Arch Fedora 37 amd64  Browser Firefox  Docker Version podman version 4.6.2  NodeJS Version _No response_  Relevant log output shell Kuma log: 2023-09-19T17:42:08+06:00 [MONITOR] WARN: Monitor #25 'TEST': Pending: | Max retries: 1 | Retry: 0 | Retry Interval: 60 seconds | Type: docker TcpDump: GET /containers/keen_torvalds/json HTTP/1.1 Accept: */* User-Agent: Uptime-Kuma/1.23.1 Host: podmanproxy:2375 Connection: close HTTP/1.1 200 OK Api-Version: 1.41 Content-Type: application/json Libpod-Api-Version: 4.6.2 Server: Libpod/4.6.2 (linux) X-Reference-Id: 0xc000be6558 Date: Tue, 19 Sep 2023 11:41:08 GMT Connection: close Transfer-Encoding: chunked {""Id"":""496a2cb6e16a7822d7669be2ebbf1f32f6d09751d677a93d54b67c3020e66f72"",""Created"":""2023-09-19T09:41:10.589808818Z"",""Path"":""sh"",""Args"":[""-c"",""apk add tcpdump socat; tcpdump -w - -Unq tcp port not 9999 | socat -t0 -T0 - tcp:laptop.lan:9999""],""State"":{""Status"":""running"",""Running"":true,""Paused"":false,""Restarting"":false,""OOMKilled"":false,""Dead"":false,""Pid"":3089279,""ExitCode"":0,""Error"":"""",""StartedAt"":""2023-09-19T09:41:10.64045291Z"",""FinishedAt"":""0001-01-01T00:00:00Z"",""Health"":{""Status"":"""",""FailingStreak"":0,""Log"":null}},""Image"":""sha256:7e01a0d0a1dcd9e539f8e9bbd80106d59efbdf97293b3d38f5d7a34501526cdb"",""ResolvConfPath"":""/run/containers/storage/btrfs-containers/42fca70f376092b3d93fbe719df76b824c99ea689fbb8bfcab416371d17d6d18/userdata/resolv.conf"",""HostnamePath"":""/run/containers/storage/btrfs-containers/496a2cb6e16a7822d7669be2ebbf1f32f6d09751d677a93d54b67c3020e66f72/userdata/hostname"",""HostsPath"":""/run/containers/storage/btrfs-containers/42fca70f376092b3d93fbe719df76b824c99ea689fbb8bfcab416371d17d6d18/userdata/hosts"",""LogPath"":"""",""Name"":""/keen_torvalds"",""RestartCount"":0,""Driver"":""btrfs"",""Platform"":""linux"",""MountLabel"":"""",""ProcessLabel"":"""",""AppArmorProfile"":"""",""ExecIDs"":[],""HostConfig"":{""Binds"":[],""ContainerIDFile"":"""",""LogConfig"":{""Type"":""journald"",""Config"":null},""NetworkMode"":""container:42fca70f376092b3d93fbe719df76b824c99ea689fbb8bfcab416371d17d6d18"",""PortBindings"":{},""RestartPolicy"":{""Name"":"""",""MaximumRetryCount"":0},""AutoRemove"":true,""VolumeDriver"":"""",""VolumesFrom"":null,""ConsoleSize"":[0,0],""CapAdd"":[""NET_RAW""],""CapDrop"":[],""CgroupnsMode"":"""",""Dns"":[],""DnsOptions"":[],""DnsSearch"":[],""ExtraHosts"":[],""GroupAdd"":[],""IpcMode"":""shareable"",""Cgroup"":"""",""Links"":null,""OomScoreAdj"":0,""PidMode"":""private"",""Privileged"":false,""PublishAllPorts"":false,""ReadonlyRootfs"":false,""SecurityOpt"":[],""UTSMode"":""private"",""UsernsMode"":"""",""ShmSize"":65536000,""Runtime"":""oci"",""Isolation"":"""",""CpuShares"":0,""Memory"":0,""NanoCpus"":0,""CgroupParent"":"""",""BlkioWeight"":0,""BlkioWeightDevice"":null,""BlkioDeviceReadBps"":null,""BlkioDeviceWriteBps"":null,""BlkioDeviceReadIOps"":null,""BlkioDeviceWriteIOps"":null,""CpuPeriod"":0,""CpuQuota"":0,""CpuRealtimePeriod"":0,""CpuRealtimeRuntime"":0,""CpusetCpus"":"""",""CpusetMems"":"""",""Devices"":[],""DeviceCgroupRules"":null,""DeviceRequests"":null,""MemoryReservation"":0,""MemorySwap"":0,""MemorySwappiness"":0,""OomKillDisable"":false,""PidsLimit"":2048,""Ulimits"":[{""Name"":""RLIMIT_NPROC"",""Hard"":4194304,""Soft"":4194304}],""CpuCount"":0,""CpuPercent"":0,""IOMaximumIOps"":0,""IOMaximumBandwidth"":0,""MaskedPaths"":null,""ReadonlyPaths"":null},""GraphDriver"":{""Data"":null,""Name"":""btrfs""},""SizeRootFs"":0,""Mounts"":[],""Config"":{""Hostname"":""496a2cb6e16a"",""Domainname"":"""",""User"":"""",""AttachStdin"":false,""AttachStdout"":false,""AttachStderr"":false,""ExposedPorts"":{""2375/tcp"":{}},""Tty"":true,""OpenStdin"":true,""StdinOnce"":false,""Env"":[""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"",""TERM=xterm"",""container=podman"",""HOME=/root"",""HOSTNAME=496a2cb6e16a""],""Cmd"":[""sh"",""-c"",""apk add tcpdump socat; tcpdump -w - -Unq tcp port not 9999 | socat -t0 -T0 - tcp:gram.lan:9999""],""Image"":""docker.io/library/alpine:latest"",""Volumes"":null,""WorkingDir"":""/"",""Entrypoint"":[],""OnBuild"":null,""Labels"":{},""StopSignal"":""15"",""StopTimeout"":10},""NetworkSettings"":{""Bridge"":"""",""SandboxID"":"""",""HairpinMode"":false,""LinkLocalIPv6Address"":"""",""LinkLocalIPv6PrefixLen"":0,""Ports"":{""2375/tcp"":null},""SandboxKey"":""/run/netns/netns-0ceac2ae-63a2-0a69-b779-3f25d373ed50"",""SecondaryIPAddresses"":null,""SecondaryIPv6Addresses"":null,""EndpointID"":"""",""Gateway"":"""",""GlobalIPv6Address"":"""",""GlobalIPv6PrefixLen"":0,""IPAddress"":"""",""IPPrefixLen"":0,""IPv6Gateway"":"""",""MacAddress"":"""",""Networks"":{""podwg"":{""IPAMConfig"":null,""Links"":null,""Aliases"":[""42fca70f3760""],""NetworkID"":""podwg"",""EndpointID"":"""",""Gateway"":""10.89.0.1"",""IPAddress"":""10.89.0.133"",""IPPrefixLen"":24,""IPv6Gateway"":"""",""GlobalIPv6Address"":"""",""GlobalIPv6PrefixLen"":0,""MacAddress"":""5a:bb:0a:f9:56:a0"",""DriverOpts"":null  source-file source-file source-file source-file",no-bug,0.9
448,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/448,Reset Admin password not working,"As described here https://github.com/louislam/uptime-kuma/wiki/Reset-Password-via-CLI docker exec -it <container name> npm run reset-password gives me an error: > uptime-kuma@1.6.2 reset-password /app > node extra/reset-password.js == Uptime Kuma Reset Password Tool == Loading the database (node:55148) UnhandledPromiseRejectionWarning: KnexTimeoutError: Knex: Timeout acquiring a connection. The pool is probably full. Are you missing a .transacting(trx) call? at Client_SQLite3.acquireConnection (/app/node_modules/knex/lib/client.js:295:26) at async Runner.ensureConnection (/app/node_modules/knex/lib/execution/runner.js:259:28) at async Runner.run (/app/node_modules/knex/lib/execution/runner.js:30:19) at async RedBeanNode.normalizeRaw (/app/node_modules/redbean-node/dist/redbean-node.js:566:22) at async RedBeanNode.exec (/app/node_modules/redbean-node/dist/redbean-node.js:530:9) at async Function.connect (/app/server/database.js:67:9) at async /app/extra/reset-password.js:15:5 (Use `node --trace-warnings ` to show where the warning was created) (node:55148) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 599) (node:55148) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.",source-file | source-file | source-file | source-file | source-file | source-file,"Reset Admin password not working As described here https://github.com/louislam/uptime-kuma/wiki/Reset-Password-via-CLI docker exec -it <container name> npm run reset-password gives me an error: > uptime-kuma@1.6.2 reset-password /app > node extra/reset-password.js == Uptime Kuma Reset Password Tool == Loading the database (node:55148) UnhandledPromiseRejectionWarning: KnexTimeoutError: Knex: Timeout acquiring a connection. The pool is probably full. Are you missing a .transacting(trx) call? at Client_SQLite3.acquireConnection (/app/node_modules/knex/lib/client.js:295:26) at async Runner.ensureConnection (/app/node_modules/knex/lib/execution/runner.js:259:28) at async Runner.run (/app/node_modules/knex/lib/execution/runner.js:30:19) at async RedBeanNode.normalizeRaw (/app/node_modules/redbean-node/dist/redbean-node.js:566:22) at async RedBeanNode.exec (/app/node_modules/redbean-node/dist/redbean-node.js:530:9) at async Function.connect (/app/server/database.js:67:9) at async /app/extra/reset-password.js:15:5 (Use `node --trace-warnings ` to show where the warning was created) (node:55148) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 599) (node:55148) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code. source-file source-file source-file source-file source-file source-file",no-bug,0.9
1457,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1457,Feature: Make it extremly easy to add multiple monitors fast,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description While the interface is amazingly nice it's still a big task to add 5, 10, 50 monitors. Make it easy.   Solution # The simple I suggest to make an option to add multiple montors in a row. Consider this: you have a list of monitors prepared in your notepad, exported from another system or whatever. Now consider you * copy all 50 lines of hostnames, * click on add multiple, insert the 50 hostnames in a textarea and * then choose monitor type for all * click create all Now you have 50 monitors. A few needs adjustments, most dont # The one with extra ""ommph"" Expand the format to allow for specifying a few key options directly in the copy paste. Key is keeping it simple and not go nuts on how much to specify: ping:hostname http://hostname http://hostname, keyword tcp:hostname dns:hostname   Alternatives Api - but this is diffferent. It's easy for onboarding and you get ppl to see the full potential  Additional Context _No response_",,"Feature: Make it extremly easy to add multiple monitors fast   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description While the interface is amazingly nice it's still a big task to add 5, 10, 50 monitors. Make it easy.   Solution # The simple I suggest to make an option to add multiple montors in a row. Consider this: you have a list of monitors prepared in your notepad, exported from another system or whatever. Now consider you * copy all 50 lines of hostnames, * click on add multiple, insert the 50 hostnames in a textarea and * then choose monitor type for all * click create all Now you have 50 monitors. A few needs adjustments, most dont # The one with extra ""ommph"" Expand the format to allow for specifying a few key options directly in the copy paste. Key is keeping it simple and not go nuts on how much to specify: ping:hostname http://hostname http://hostname, keyword tcp:hostname dns:hostname   Alternatives Api - but this is diffferent. It's easy for onboarding and you get ppl to see the full potential  Additional Context _No response_",no-bug,0.95
3351,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3351,[Slack] Include URL in slack alert,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description URL in slack notification is missing now: <img width=""606"" alt=""image"" src=""https://github.com/louislam/uptime-kuma/assets/19979741/565ffeb8-a0f4-4da7-8066-3ab093068d7a""> Which is cumbersome to check the website that is failing - I have to go to the dashboard and search for it directly.   Solution Add URL anywhere in the message - it would be highly appreciated   Alternatives _No response_  Additional Context _No response_",source-file,"[Slack] Include URL in slack alert   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description URL in slack notification is missing now: <img width=""606"" alt=""image"" src=""https://github.com/louislam/uptime-kuma/assets/19979741/565ffeb8-a0f4-4da7-8066-3ab093068d7a""> Which is cumbersome to check the website that is failing - I have to go to the dashboard and search for it directly.   Solution Add URL anywhere in the message - it would be highly appreciated   Alternatives _No response_  Additional Context _No response_ source-file",no-bug,0.95
5745,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5745,2.0.0-beta.2 Some labels without value still show a 'null' value," I have found these related issues/pull requests unable to find any related issues   Security Policy - [x] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description ![Image](https://github.com/user-attachments/assets/d9a6ef93-9206-48b8-9316-b88de93614bb) These two monitors have same label 'DNS' without value, but one of them show 'DNS: null'.  Reproduction steps Upgrade to 2.0.0-beta.2  Expected behavior Only show label key if no value  Actual Behavior Some labels show a 'null' value if no value  Uptime-Kuma Version 2.0.0-beta.2  Operating System and Arch AlmaLinux 9.2  Browser Edge 134   Deployment Environment - Runtime: Docker 24.0.4 - Database: sqlite - Filesystem used to store the database on: xfs - number of monitors: 67  Relevant log output shell ",other-file,"2.0.0-beta.2 Some labels without value still show a 'null' value  I have found these related issues/pull requests unable to find any related issues   Security Policy - [x] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description ![Image](https://github.com/user-attachments/assets/d9a6ef93-9206-48b8-9316-b88de93614bb) These two monitors have same label 'DNS' without value, but one of them show 'DNS: null'.  Reproduction steps Upgrade to 2.0.0-beta.2  Expected behavior Only show label key if no value  Actual Behavior Some labels show a 'null' value if no value  Uptime-Kuma Version 2.0.0-beta.2  Operating System and Arch AlmaLinux 9.2  Browser Edge 134   Deployment Environment - Runtime: Docker 24.0.4 - Database: sqlite - Filesystem used to store the database on: xfs - number of monitors: 67  Relevant log output shell  other-file",no-bug,0.8
2636,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2636,Ping error: There is no deadline option on windows,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Using PING in latest version of uptime-kuma gives the following error: There is no deadline option on windows  Reproduction steps Updated uptime-kuma to latest version 1.19.5 when performing a PING to an host it's marked as DOWN with the following error: There is no deadline option on windows. I have no error using version 1.19.4.  Expected behavior Have the PING command work.  Actual Behavior error: There is no deadline option on windows  Uptime-Kuma Version 1.19.5  Operating System and Arch Windows Server 2019  Browser Google Chrome 108.0.5359.125  Docker Version _No response_  NodeJS Version 16.14.2  Relevant log output _No response_,documentation-file | documentation-file,Ping error: There is no deadline option on windows   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Using PING in latest version of uptime-kuma gives the following error: There is no deadline option on windows  Reproduction steps Updated uptime-kuma to latest version 1.19.5 when performing a PING to an host it's marked as DOWN with the following error: There is no deadline option on windows. I have no error using version 1.19.4.  Expected behavior Have the PING command work.  Actual Behavior error: There is no deadline option on windows  Uptime-Kuma Version 1.19.5  Operating System and Arch Windows Server 2019  Browser Google Chrome 108.0.5359.125  Docker Version _No response_  NodeJS Version 16.14.2  Relevant log output _No response_ documentation-file documentation-file,no-bug,0.9
3587,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3587,Handle cookies on redirection,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Accept/send cookies on redirection. Sometimes it is nice to test a service by performing an action such as a log in. In these circumstances cookies are needed when following redirects as some services will set cookies on a redirect.   Solution When making a request to a service that does a redirect and sets cookies on the redirect, the redirect should be followed with the cookies in place.   Alternatives _No response_  Additional Context _No response_",documentation-file | documentation-file | source-file | documentation-file | documentation-file | source-file | documentation-file | documentation-file | source-file | documentation-file | documentation-file | source-file | source-file | documentation-file | documentation-file | other-file | source-file | documentation-file | documentation-file | test-file | config-file | config-file | config-file | config-file | config-file | other-file | documentation-file | documentation-file | documentation-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | config-file | container-file | container-file | source-file | source-file | config-file | other-file | other-file | other-file | other-file | source-file | source-file | other-file | other-file | config-file | other-file | config-file | source-file | source-file | source-file | documentation-file | other-file | other-file | source-file | documentation-file | source-file | documentation-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | container-file | source-file | documentation-file | source-file | config-file | documentation-file | documentation-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file,"Handle cookies on redirection   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Accept/send cookies on redirection. Sometimes it is nice to test a service by performing an action such as a log in. In these circumstances cookies are needed when following redirects as some services will set cookies on a redirect.   Solution When making a request to a service that does a redirect and sets cookies on the redirect, the redirect should be followed with the cookies in place.   Alternatives _No response_  Additional Context _No response_ documentation-file documentation-file source-file documentation-file documentation-file source-file documentation-file documentation-file source-file documentation-file documentation-file source-file source-file documentation-file documentation-file other-file source-file documentation-file documentation-file test-file config-file config-file config-file config-file config-file other-file documentation-file documentation-file documentation-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file container-file container-file config-file config-file container-file container-file source-file source-file config-file other-file other-file other-file other-file source-file source-file other-file other-file config-file other-file config-file source-file source-file source-file documentation-file other-file other-file source-file documentation-file source-file documentation-file source-file source-file source-file test-file source-file source-file source-file source-file source-file source-file other-file container-file source-file documentation-file source-file config-file documentation-file documentation-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file",no-bug,0.9
2999,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2999,EMail: Ignore TLS error doesn't suppress selfsigned cert errors,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The flag `Ignore TLS error` doesn't suppress errors generated by selfsigned certificates: ![grafik](https://user-images.githubusercontent.com/22315436/228795826-99203aec-ed71-4ea7-a3cb-6a12da31d7b6.png) ![grafik](https://user-images.githubusercontent.com/22315436/228795672-67337141-3ee9-40d6-b6bf-db1c04febc41.png)  Reproduction steps 1. Create a new notification of type `EMail (SMTP)` 2. Chose an SMTP server which uses encryption with a selfsigned certificate 3. Select `TLS` security 4. Select `Ignore TLS error` 5. Hit `Test`  Expected behavior The TLS error should be ignored.  Actual Behavior `self signed certificate in certificate chain`  Uptime-Kuma Version 1.21.1  Operating System and Arch Ubuntu 22.04  Browser Firefox  Docker Version _No response_  NodeJS Version _No response_  Relevant log output shell Error: self signed certificate in certificate chain at TLSSocket.onConnectSecure (node:_tls_wrap:1539:34) at TLSSocket.emit (node:events:513:28) at TLSSocket._finishInit (node:_tls_wrap:953:8) at TLSWrap.ssl.onhandshakedone (node:_tls_wrap:734:12) { code: 'ESOCKET', command: 'CONN' }  ",source-file | source-file,"EMail: Ignore TLS error doesn't suppress selfsigned cert errors   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description The flag `Ignore TLS error` doesn't suppress errors generated by selfsigned certificates: ![grafik](https://user-images.githubusercontent.com/22315436/228795826-99203aec-ed71-4ea7-a3cb-6a12da31d7b6.png) ![grafik](https://user-images.githubusercontent.com/22315436/228795672-67337141-3ee9-40d6-b6bf-db1c04febc41.png)  Reproduction steps 1. Create a new notification of type `EMail (SMTP)` 2. Chose an SMTP server which uses encryption with a selfsigned certificate 3. Select `TLS` security 4. Select `Ignore TLS error` 5. Hit `Test`  Expected behavior The TLS error should be ignored.  Actual Behavior `self signed certificate in certificate chain`  Uptime-Kuma Version 1.21.1  Operating System and Arch Ubuntu 22.04  Browser Firefox  Docker Version _No response_  NodeJS Version _No response_  Relevant log output shell Error: self signed certificate in certificate chain at TLSSocket.onConnectSecure (node:_tls_wrap:1539:34) at TLSSocket.emit (node:events:513:28) at TLSSocket._finishInit (node:_tls_wrap:953:8) at TLSWrap.ssl.onhandshakedone (node:_tls_wrap:734:12) { code: 'ESOCKET', command: 'CONN' }   source-file source-file",no-bug,0.9
461,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/461,Multiple CC/BCC email addresses,"**Is it a duplicate question?** No **Describe the bug** ![NVD5Ux](https://user-images.githubusercontent.com/8298741/134493256-584c2ab8-c5cb-483a-923b-c8c9b210e1e1.png) It's not possible to add multiple BCC/CC (komma separated) e-mail addresses to BCC and/or CC and keep the **To Email** field empty and click **SAVE**. Before you save the form you have to enter a To Email address. **Info** Uptime Kuma Version: 1.6.3 Using Docker?: Yes Docker Version: Docker version 20.10.7, build 20.10.7-0ubuntu1~21.04.1 OS: Ubuntu 21.04 Browser: Google Chrome",documentation-file | other-file,"Multiple CC/BCC email addresses **Is it a duplicate question?** No **Describe the bug** ![NVD5Ux](https://user-images.githubusercontent.com/8298741/134493256-584c2ab8-c5cb-483a-923b-c8c9b210e1e1.png) It's not possible to add multiple BCC/CC (komma separated) e-mail addresses to BCC and/or CC and keep the **To Email** field empty and click **SAVE**. Before you save the form you have to enter a To Email address. **Info** Uptime Kuma Version: 1.6.3 Using Docker?: Yes Docker Version: Docker version 20.10.7, build 20.10.7-0ubuntu1~21.04.1 OS: Ubuntu 21.04 Browser: Google Chrome documentation-file other-file",no-bug,0.8
5111,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5111,Adding Elestio as deployment option," I have found these related issues/pull requests Related to #5110   Feature Request Type Other  Feature description Hey team, I am Kaiwalya, Developer Advocate at Elestio. Elestio has been providing options of fully deploying and managing Uptime Kuma application as shown [here](https://elest.io/open-source/uptime-kuma). I think it would be a great idea if we can add it to official readme/documentation here. In addition to this, if you are interested we provide partnership opportunities with tools we support by **revenue share** upon addition of this method in docs. If you would like to collaborate, just shoot me an email at [kaiwalya@elest.io](mailto:kaiwalya@elest.io) :)   Solution Adding the one-click deployment button to the readme.   Alternatives _No response_  Additional Context _No response_",documentation-file,"Adding Elestio as deployment option  I have found these related issues/pull requests Related to #5110   Feature Request Type Other  Feature description Hey team, I am Kaiwalya, Developer Advocate at Elestio. Elestio has been providing options of fully deploying and managing Uptime Kuma application as shown [here](https://elest.io/open-source/uptime-kuma). I think it would be a great idea if we can add it to official readme/documentation here. In addition to this, if you are interested we provide partnership opportunities with tools we support by **revenue share** upon addition of this method in docs. If you would like to collaborate, just shoot me an email at [kaiwalya@elest.io](mailto:kaiwalya@elest.io) :)   Solution Adding the one-click deployment button to the readme.   Alternatives _No response_  Additional Context _No response_ documentation-file",no-bug,0.95
4051,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/4051,Cannot read properties of undefined (reading 'SSL_OP_LEGACY_SERVER_CONNECT'),  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Monitoring a HTTPS email server service now does not work on 1.23.5 Got the error: Cannot read properties of undefined (reading 'SSL_OP_LEGACY_SERVER_CONNECT') ![image](https://github.com/louislam/uptime-kuma/assets/11341002/15773a46-1c9e-49ff-9a36-5e47ff452d69)  Reproduction steps Just upgrade to the version 1.23.5 Must have an old server with a https service to monitor.  Expected behavior Should work.  Actual Behavior It reports the service as down when actually it is UP  Uptime-Kuma Version 1.23.5  Operating System and Arch Ubuntu 22.04 LTS  Browser Google Chrome  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_,documentation-file | source-file | documentation-file | documentation-file | other-file | source-file | documentation-file | documentation-file | test-file | config-file | config-file | config-file | config-file | config-file | other-file | documentation-file | documentation-file | documentation-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | config-file | container-file | container-file | source-file | source-file | config-file | other-file | other-file | other-file | other-file | source-file | source-file | other-file | other-file | config-file | other-file | config-file | source-file | source-file | source-file | documentation-file | other-file | other-file | source-file | documentation-file | source-file | documentation-file | source-file | source-file | source-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | container-file | source-file | documentation-file | source-file | config-file | documentation-file | documentation-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file,Cannot read properties of undefined (reading 'SSL_OP_LEGACY_SERVER_CONNECT')   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Monitoring a HTTPS email server service now does not work on 1.23.5 Got the error: Cannot read properties of undefined (reading 'SSL_OP_LEGACY_SERVER_CONNECT') ![image](https://github.com/louislam/uptime-kuma/assets/11341002/15773a46-1c9e-49ff-9a36-5e47ff452d69)  Reproduction steps Just upgrade to the version 1.23.5 Must have an old server with a https service to monitor.  Expected behavior Should work.  Actual Behavior It reports the service as down when actually it is UP  Uptime-Kuma Version 1.23.5  Operating System and Arch Ubuntu 22.04 LTS  Browser Google Chrome  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_ documentation-file source-file documentation-file documentation-file other-file source-file documentation-file documentation-file test-file config-file config-file config-file config-file config-file other-file documentation-file documentation-file documentation-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file database-file container-file container-file config-file config-file container-file container-file source-file source-file config-file other-file other-file other-file other-file source-file source-file other-file other-file config-file other-file config-file source-file source-file source-file documentation-file other-file other-file source-file documentation-file source-file documentation-file source-file source-file source-file test-file source-file source-file source-file source-file source-file source-file other-file container-file source-file documentation-file source-file config-file documentation-file documentation-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file,no-bug,0.9
4614,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/4614,Extend Monitor JSON Data evaluation with additional operators," I have found these related issues/pull requests No similar issues / prs found   Feature Request Type Change to existing monitor  Feature description **Current implementation** The monitor type ""HTTP(s) - Json Query"" allows to add an expression in JSONata format and compare agains an expected value. It is currently only possible to compare the expression via equals. **Requested change** Adding functionality to also allow using other operators such as !=, <, <=, >, >= **Why** It is possible to define a JSONata expression with including the operator into the expression and turn it into a boolean expression that can be compared. However, it would be more clear to add the value to the monitoring configuration itself rather than to the expression. Also that way, the value could be added to the log of the monitor.   Solution For the monitoring type ""HTTP(s) - Json Query"" - Add a new property that holds the compare operator (==, !=, <, <=, >, >=). - For not introducing a breaking change, the default operator should be == if not available in an existing configuration. - Extend the UI to allow the user to select the operator s/he wants to use for comparison - Default ==   Alternatives _No response_  Additional Context If fine, I'd like to provide a PR for that change. Following the rules for adding PRs, I wanted to clarify if this feature request sounds interesting to the Uptime Kuma team first.",source-file | documentation-file | source-file | source-file | source-file | documentation-file | documentation-file | other-file | other-file | source-file | source-file,"Extend Monitor JSON Data evaluation with additional operators  I have found these related issues/pull requests No similar issues / prs found   Feature Request Type Change to existing monitor  Feature description **Current implementation** The monitor type ""HTTP(s) - Json Query"" allows to add an expression in JSONata format and compare agains an expected value. It is currently only possible to compare the expression via equals. **Requested change** Adding functionality to also allow using other operators such as !=, <, <=, >, >= **Why** It is possible to define a JSONata expression with including the operator into the expression and turn it into a boolean expression that can be compared. However, it would be more clear to add the value to the monitoring configuration itself rather than to the expression. Also that way, the value could be added to the log of the monitor.   Solution For the monitoring type ""HTTP(s) - Json Query"" - Add a new property that holds the compare operator (==, !=, <, <=, >, >=). - For not introducing a breaking change, the default operator should be == if not available in an existing configuration. - Extend the UI to allow the user to select the operator s/he wants to use for comparison - Default ==   Alternatives _No response_  Additional Context If fine, I'd like to provide a PR for that change. Following the rules for adding PRs, I wanted to clarify if this feature request sounds interesting to the Uptime Kuma team first. source-file documentation-file source-file source-file source-file documentation-file documentation-file other-file other-file source-file source-file",no-bug,0.9
2628,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2628,uptime value exceeds 100%,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description ![image](https://user-images.githubusercontent.com/97247944/213014602-4fa2c5a5-a30a-4da2-a085-c0dbfbc6a052.png)  Reproduction steps I don't know.  Expected behavior The max. uptime value should be 100%.  Actual Behavior The max. uptime value is over 100%.  Uptime-Kuma Version 1.19.3  Operating System and Arch Raspbian bullseye  Browser Google Chrome, Safari, Mozilla Firefox  Docker Version 2.14.1  NodeJS Version _No response_  Relevant log output _No response_",other-file,"uptime value exceeds 100%   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description ![image](https://user-images.githubusercontent.com/97247944/213014602-4fa2c5a5-a30a-4da2-a085-c0dbfbc6a052.png)  Reproduction steps I don't know.  Expected behavior The max. uptime value should be 100%.  Actual Behavior The max. uptime value is over 100%.  Uptime-Kuma Version 1.19.3  Operating System and Arch Raspbian bullseye  Browser Google Chrome, Safari, Mozilla Firefox  Docker Version 2.14.1  NodeJS Version _No response_  Relevant log output _No response_ other-file",no-bug,0.9
2727,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2727,Discord Webhook Display Empty Service URL (MYSQL/MariaDB Check),  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description _No response_  Reproduction steps 1. Create service (MYSQL/MariaDB) ![image](https://user-images.githubusercontent.com/25010528/217066104-a5c68a3f-6057-4a14-a912-58c5205b0896.png) 2. Create Discord Webhook and setup alert 3. Get down your service  Expected behavior You didn't get any service URL or should get right address (like IP:PORT)  Actual Behavior You get discord embed message with empty (`https://`) **Service URL** ![image](https://user-images.githubusercontent.com/25010528/217065653-03f0374b-c6ef-4ed4-a98d-d578c04770bc.png)  Uptime-Kuma Version 1.19.4  Operating System and Arch <Not Required>  Browser <Not Required>  Docker Version <Not Required>  NodeJS Version <Not Required>  Relevant log output _No response_,source-file,Discord Webhook Display Empty Service URL (MYSQL/MariaDB Check)   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description _No response_  Reproduction steps 1. Create service (MYSQL/MariaDB) ![image](https://user-images.githubusercontent.com/25010528/217066104-a5c68a3f-6057-4a14-a912-58c5205b0896.png) 2. Create Discord Webhook and setup alert 3. Get down your service  Expected behavior You didn't get any service URL or should get right address (like IP:PORT)  Actual Behavior You get discord embed message with empty (`https://`) **Service URL** ![image](https://user-images.githubusercontent.com/25010528/217065653-03f0374b-c6ef-4ed4-a98d-d578c04770bc.png)  Uptime-Kuma Version 1.19.4  Operating System and Arch <Not Required>  Browser <Not Required>  Docker Version <Not Required>  NodeJS Version <Not Required>  Relevant log output _No response_ source-file,bug,0.9
3798,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3798,Full monitor path in notifications,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Hi there! When using groups to nest monitors, it would be great if the alarm notification would include the path. It often happens that we have monitors with same names (e.g. ""Grafana"") but they are nested under different groups (e.g. ""Dev"", ""Stage""). In such cases based only on the name of the monitor, there's not enough information to know which part of the infrastructure is in question.   Solution Including the path here [https://github.com/louislam/uptime-kuma/blob/bef6a7911fa326d5f0b8b2459ca89d249eb83c59/server/model/monitor.js#L1257](https://github.com/louislam/uptime-kuma/blob/bef6a7911fa326d5f0b8b2459ca89d249eb83c59/server/model/monitor.js#L1257) would make it more actionable.   Alternatives _No response_  Additional Context _No response_",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file,"Full monitor path in notifications   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Hi there! When using groups to nest monitors, it would be great if the alarm notification would include the path. It often happens that we have monitors with same names (e.g. ""Grafana"") but they are nested under different groups (e.g. ""Dev"", ""Stage""). In such cases based only on the name of the monitor, there's not enough information to know which part of the infrastructure is in question.   Solution Including the path here [https://github.com/louislam/uptime-kuma/blob/bef6a7911fa326d5f0b8b2459ca89d249eb83c59/server/model/monitor.js#L1257](https://github.com/louislam/uptime-kuma/blob/bef6a7911fa326d5f0b8b2459ca89d249eb83c59/server/model/monitor.js#L1257) would make it more actionable.   Alternatives _No response_  Additional Context _No response_ source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file",no-bug,0.9
2504,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2504,MSSQL concurrent connections to multiple servers interfere with each other,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Attempting to monitor multiple SQL servers, and have been receiving odd messages, **intermittently**. Have changed the connection strings multiple times to verify that everything connects properly, and when clicking 'Save', the connection succeeds. Errors received: 'Database [Database] does not exist. Make sure that the name is entered correctly.' (after putting _USE Database;_ into the query) 'Invalid object name 'Table'.' (without _USE Database;_ in the query) The head scratching part was that multiple servers would start having this issue simultaneously, and then they would all clear. Changing the timeout period did not have an effect. Increasing the 'retries' would cause the servers to all go yellow (Pending) for a while, and then after ONE failure they would all go green and start working again. Increasing the retries would simply increase the yellow/pending period, they would still return to green after one red/failure. The giveaway was one period when there was an actual network disruption, one monitor showed a failure to connect -- to different monitor's SQL server address! Well no wonder you can't find the database and table, you're looking on the wrong server! The connection pool is sending queries out to any active connection, not necessarily the correct one. Resetting the connections, which is done when clicking 'save' on the configuration or presumably when a server errors out, gets them going again for a while until the next mix-up. Digging through I have found some documentation on Node.js mssql which refers to common connection pools and a confirmation of my thought on StackOverflow here: https://stackoverflow.com/questions/64254145/node-js-mssql-multiple-concurrent-connections-to-sql-servers-interfering-with-ea The relevant Node MSSQL documentation is here: https://www.npmjs.com/package/mssql#advanced-pool-management _Advanced Pool Management_ _For some use-cases you may want to implement your own connection pool management, rather than using the global connection pool. Reasons for doing this include:_ _Supporting connections to multiple databases_ I unfortunately don't have the skill to merge the code together right now for a fix, but the page above includes a function that was built to check the connection pool for the proper connection string, and if present use that, but if not create a new connection.  Reproduction steps Add SQL monitors that link to multiple different servers which do not share identical database and table names.  Expected behavior Each SQL server is monitored separately and returns 'up' when the server is up.  Actual Behavior Servers (multiple at one time) would simultaneously fail with 'object not found' or 'database does not exist'. The queries do not appear to be going out through their respective connections, but to random or otherwise incorrect connections to the wrong server.  Uptime-Kuma Version 1.18.5  Operating System and Arch Ubuntu 20.04 amd64  Browser Chrome 108.0.5359.124  Docker Version Docker 20.10.17  NodeJS Version _No response_  Relevant log output _No response_",source-file,"MSSQL concurrent connections to multiple servers interfere with each other   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Attempting to monitor multiple SQL servers, and have been receiving odd messages, **intermittently**. Have changed the connection strings multiple times to verify that everything connects properly, and when clicking 'Save', the connection succeeds. Errors received: 'Database [Database] does not exist. Make sure that the name is entered correctly.' (after putting _USE Database;_ into the query) 'Invalid object name 'Table'.' (without _USE Database;_ in the query) The head scratching part was that multiple servers would start having this issue simultaneously, and then they would all clear. Changing the timeout period did not have an effect. Increasing the 'retries' would cause the servers to all go yellow (Pending) for a while, and then after ONE failure they would all go green and start working again. Increasing the retries would simply increase the yellow/pending period, they would still return to green after one red/failure. The giveaway was one period when there was an actual network disruption, one monitor showed a failure to connect -- to different monitor's SQL server address! Well no wonder you can't find the database and table, you're looking on the wrong server! The connection pool is sending queries out to any active connection, not necessarily the correct one. Resetting the connections, which is done when clicking 'save' on the configuration or presumably when a server errors out, gets them going again for a while until the next mix-up. Digging through I have found some documentation on Node.js mssql which refers to common connection pools and a confirmation of my thought on StackOverflow here: https://stackoverflow.com/questions/64254145/node-js-mssql-multiple-concurrent-connections-to-sql-servers-interfering-with-ea The relevant Node MSSQL documentation is here: https://www.npmjs.com/package/mssql#advanced-pool-management _Advanced Pool Management_ _For some use-cases you may want to implement your own connection pool management, rather than using the global connection pool. Reasons for doing this include:_ _Supporting connections to multiple databases_ I unfortunately don't have the skill to merge the code together right now for a fix, but the page above includes a function that was built to check the connection pool for the proper connection string, and if present use that, but if not create a new connection.  Reproduction steps Add SQL monitors that link to multiple different servers which do not share identical database and table names.  Expected behavior Each SQL server is monitored separately and returns 'up' when the server is up.  Actual Behavior Servers (multiple at one time) would simultaneously fail with 'object not found' or 'database does not exist'. The queries do not appear to be going out through their respective connections, but to random or otherwise incorrect connections to the wrong server.  Uptime-Kuma Version 1.18.5  Operating System and Arch Ubuntu 20.04 amd64  Browser Chrome 108.0.5359.124  Docker Version Docker 20.10.17  NodeJS Version _No response_  Relevant log output _No response_ source-file",no-bug,0.9
3,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3,Example Proxy Configurations,"Given that not many people are exposing Docker containers directly to the internet, having an example of a nginx or apache config that allows proxying could be beneficial. I tried looking at it myself, but all my attempts with apache are coming up moot. Hopefully someone smarter than me can figure this out.",other-file | source-file | documentation-file | documentation-file | other-file | documentation-file | documentation-file | documentation-file | documentation-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | source-file | source-file | source-file | other-file | documentation-file | config-file | documentation-file | documentation-file | other-file | other-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | documentation-file,"Example Proxy Configurations Given that not many people are exposing Docker containers directly to the internet, having an example of a nginx or apache config that allows proxying could be beneficial. I tried looking at it myself, but all my attempts with apache are coming up moot. Hopefully someone smarter than me can figure this out. other-file source-file documentation-file documentation-file other-file documentation-file documentation-file documentation-file documentation-file database-file database-file database-file database-file database-file database-file database-file database-file container-file container-file config-file source-file source-file source-file other-file documentation-file config-file documentation-file documentation-file other-file other-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file documentation-file",no-bug,0.9
5381,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5381,"MariaDB Error: ""LIMIT & IN/ALL/ANY/SOME subquery"" Not Supported"," I have found these related issues/pull requests I have checked, but couldnt find anything similar.   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description While executing a `DELETE` query on the `heartbeat` table, the following error occurs: This version of MariaDB doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery'  Query sql DELETE FROM heartbeat WHERE monitor_id = 1 AND important = 0 AND time < DATE_ADD(NOW(), INTERVAL -24 HOUR) AND id NOT IN ( SELECT id FROM heartbeat WHERE monitor_id = 1 ORDER BY time DESC LIMIT 100 )   Error Details - **Error Code:** ER_NOT_SUPPORTED_YET - **SQL State:** 42000 - **MySQL Version:** MariaDB 11.6.2  Stack Trace log Error at Packet.asError (/app/node_modules/mysql2/lib/packets/packet.js:738:17)  The current version of MariaDB does not support combining `LIMIT` with `IN/ALL/ANY/SOME` subqueries, which causes this error.  Suggested Fix Modify the query to avoid using `LIMIT` inside the `IN` clause. Example suggestion: sql DELETE FROM heartbeat WHERE monitor_id = 1 AND important = 0 AND time < DATE_ADD(NOW(), INTERVAL -24 HOUR) AND id NOT IN ( SELECT id FROM ( SELECT id FROM heartbeat WHERE monitor_id = 1 ORDER BY time DESC LIMIT 100 ) AS limited_ids )  This structure ensures compatibility by avoiding the direct use of `LIMIT` in the outer `IN` clause.  Reproduction steps docker-compose.yml  networks: uptime-kuma-beta_net: attachable: false internal: false external: false name: uptime-kuma-beta driver: bridge ipam: driver: default config: - subnet: 172.20.3.0/24 ip_range: 172.20.3.0/24 gateway: 172.20.3.1 driver_opts: com.docker.network.bridge.default_bridge: ""false"" com.docker.network.bridge.enable_icc: ""true"" com.docker.network.bridge.enable_ip_masquerade: ""true"" com.docker.network.bridge.host_binding_ipv4: ""0.0.0.0"" com.docker.network.bridge.name: ""uptime-kuma"" com.docker.network.driver.mtu: ""1500"" labels: com.uptime-kuma-beta.network.description: ""is an isolated bridge network."" services: uptime-kuma-beta_db: restart: unless-stopped logging: driver: ""json-file"" options: max-size: ""1M"" max-file: ""2"" stop_grace_period: 1m container_name: uptime-kuma-beta_db image: mariadb:latest pull_policy: if_not_present volumes: - /docker/uptime-kuma-beta/db:/var/lib/mysql environment: PUID: ""1000"" PGID: ""1000"" TZ: Europe/Amsterdam MYSQL_ROOT_PASSWORD: ""ThisIsADemoRootPassword"" MYSQL_DATABASE: ""uptime_kuma_db"" MYSQL_USER: ""uptime-kuma"" MYSQL_PASSWORD: ""ThisIsADemoPassword"" command: [ ""--transaction-isolation=READ-COMMITTED"", ""--log-bin=binlog"", ""--binlog-format=ROW"", ] hostname: uptime-kuma-beta_db networks: uptime-kuma-beta_net: ipv4_address: 172.20.3.2 security_opt: - no-new-privileges:true labels: com.uptime-kuma-beta.db.description: ""is an MySQL database."" healthcheck: disable: true uptime-kuma-beta_app: restart: unless-stopped logging: driver: ""json-file"" options: max-size: ""1M"" max-file: ""2"" stop_grace_period: 1m container_name: uptime-kuma-beta image: louislam/uptime-kuma:2.0.0-beta.0 pull_policy: if_not_present depends_on: uptime-kuma-beta_db: condition: service_started links: - uptime-kuma-beta_db volumes: - /docker/uptime-kuma-beta/app:/app/data - /var/run/docker.sock:/var/run/docker.sock:ro - /usr/local/share/ca-certificates:/app/data/docker-tls environment: PUID: ""1000"" PGID: ""1000"" TZ: Europe/Amsterdam # This is a certificate authority for demonstration purposes. # NODE_EXTRA_CA_CERTS: /app/data/docker-tls/root_ca_demo_cert.pem MYSQL_HOST: ""uptime-kuma-beta_db"" MYSQL_PORT: 3306 MYSQL_NAME: ""uptime_kuma_db"" MYSQL_USER: ""uptime-kuma"" MYSQL_PASSWORD: ""ThisIsADemoPassword"" domainname: status.local hostname: status networks: uptime-kuma-beta_net: ipv4_address: 172.20.3.3 ports: - ""3001:3001/tcp"" # HTTP - ""3001:3001/udp"" # HTTP security_opt: - no-new-privileges:true labels: com.docker.compose.project: ""uptime-kuma-beta"" com.uptime-kuma-beta.description: ""is an self-hosted monitoring tool that allows you to monitor uptime, status, and notifications for various services and domains."" healthcheck: disable: true   Expected behavior The query should execute successfully, deleting older entries from the `heartbeat` table while keeping the most recent 100 entries.  Actual Behavior The query fails due to the unsupported subquery syntax in MariaDB.  Uptime-Kuma Version 2.0.0-beta.0  Operating System and Arch Ubuntu Server 24.04.1 LTS (GNU/Linux 6.8.0-49-generic x86_64)  Browser Brave Version 1.73.91 Chromium: 131.0.6778.85 (Official Build) (64-bit)   Deployment Environment - **Runtime:** Docker version 26.1.0, build 9714adc - **Runtime:** docker-compose version 1.29.2, build unknown - **Runtime:** Portainer Business Edition version 2.21.4 - **Runtime:** MariaDB 11.6.2 - **Database:** (external) - **Filesystem used for the database:** Linux/ext4 on an SSD - **Number of monitors:** 92  Relevant log output shell Trace: Error: DELETE FROM heartbeat WHERE monitor_id = 1 AND important = 0 AND time < DATE_ADD(NOW(), INTERVAL -24 HOUR) AND id NOT IN ( SELECT id FROM heartbeat WHERE monitor_id = 1 ORDER BY time DESC LIMIT 100 ) - This version of MariaDB doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery' at Packet.asError (/app/node_modules/mysql2/lib/packets/packet.js:738:17) at Query.execute (/app/node_modules/mysql2/lib/commands/command.js:29:26) at Connection.handlePacket (/app/node_modules/mysql2/lib/connection.js:481:34) at PacketParser.onPacket (/app/node_modules/mysql2/lib/connection.js:97:12) at PacketParser.executeStart (/app/node_modules/mysql2/lib/packet_parser.js:75:16) at Socket.<anonymous> (/app/node_modules/mysql2/lib/connection.js:104:25) at Socket.emit (node:events:519:28) at addChunk (node:internal/streams/readable:559:12) at readableAddChunkPushByteMode (node:internal/streams/readable:510:3) at Readable.push (node:internal/streams/readable:390:5) { code: 'ER_NOT_SUPPORTED_YET', errno: 1235, sqlState: '42000', sqlMessage: ""This version of MariaDB doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery'"", sql: '\n' + ' DELETE FROM heartbeat\n' + ' WHERE monitor_id = 1\n' + ' AND important = 0\n' + ' AND time < DATE_ADD(NOW(), INTERVAL -24 HOUR)\n' + ' AND id NOT IN (\n' + ' SELECT id\n' + ' FROM heartbeat\n' + ' WHERE monitor_id = 1\n' + ' ORDER BY time DESC\n' + ' LIMIT 100\n' + ' )\n' + ' ' } at process.unexpectedErrorHandler (/app/server/server.js:1872:13) at process.emit (node:events:519:28) at emitUnhandledRejection (node:internal/process/promises:250:13) at throwUnhandledRejectionsMode (node:internal/process/promises:385:19) at processPromiseRejections (node:internal/process/promises:470:17) at process.processTicksAndRejections (node:internal/process/task_queues:96:32) If you keep encountering errors, please report to https://github.com/louislam/uptime-kuma/issues ",documentation-file | source-file | documentation-file | source-file,"MariaDB Error: ""LIMIT & IN/ALL/ANY/SOME subquery"" Not Supported  I have found these related issues/pull requests I have checked, but couldnt find anything similar.   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description While executing a `DELETE` query on the `heartbeat` table, the following error occurs: This version of MariaDB doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery'  Query sql DELETE FROM heartbeat WHERE monitor_id = 1 AND important = 0 AND time < DATE_ADD(NOW(), INTERVAL -24 HOUR) AND id NOT IN ( SELECT id FROM heartbeat WHERE monitor_id = 1 ORDER BY time DESC LIMIT 100 )   Error Details - **Error Code:** ER_NOT_SUPPORTED_YET - **SQL State:** 42000 - **MySQL Version:** MariaDB 11.6.2  Stack Trace log Error at Packet.asError (/app/node_modules/mysql2/lib/packets/packet.js:738:17)  The current version of MariaDB does not support combining `LIMIT` with `IN/ALL/ANY/SOME` subqueries, which causes this error.  Suggested Fix Modify the query to avoid using `LIMIT` inside the `IN` clause. Example suggestion: sql DELETE FROM heartbeat WHERE monitor_id = 1 AND important = 0 AND time < DATE_ADD(NOW(), INTERVAL -24 HOUR) AND id NOT IN ( SELECT id FROM ( SELECT id FROM heartbeat WHERE monitor_id = 1 ORDER BY time DESC LIMIT 100 ) AS limited_ids )  This structure ensures compatibility by avoiding the direct use of `LIMIT` in the outer `IN` clause.  Reproduction steps docker-compose.yml  networks: uptime-kuma-beta_net: attachable: false internal: false external: false name: uptime-kuma-beta driver: bridge ipam: driver: default config: - subnet: 172.20.3.0/24 ip_range: 172.20.3.0/24 gateway: 172.20.3.1 driver_opts: com.docker.network.bridge.default_bridge: ""false"" com.docker.network.bridge.enable_icc: ""true"" com.docker.network.bridge.enable_ip_masquerade: ""true"" com.docker.network.bridge.host_binding_ipv4: ""0.0.0.0"" com.docker.network.bridge.name: ""uptime-kuma"" com.docker.network.driver.mtu: ""1500"" labels: com.uptime-kuma-beta.network.description: ""is an isolated bridge network."" services: uptime-kuma-beta_db: restart: unless-stopped logging: driver: ""json-file"" options: max-size: ""1M"" max-file: ""2"" stop_grace_period: 1m container_name: uptime-kuma-beta_db image: mariadb:latest pull_policy: if_not_present volumes: - /docker/uptime-kuma-beta/db:/var/lib/mysql environment: PUID: ""1000"" PGID: ""1000"" TZ: Europe/Amsterdam MYSQL_ROOT_PASSWORD: ""ThisIsADemoRootPassword"" MYSQL_DATABASE: ""uptime_kuma_db"" MYSQL_USER: ""uptime-kuma"" MYSQL_PASSWORD: ""ThisIsADemoPassword"" command: [ ""--transaction-isolation=READ-COMMITTED"", ""--log-bin=binlog"", ""--binlog-format=ROW"", ] hostname: uptime-kuma-beta_db networks: uptime-kuma-beta_net: ipv4_address: 172.20.3.2 security_opt: - no-new-privileges:true labels: com.uptime-kuma-beta.db.description: ""is an MySQL database."" healthcheck: disable: true uptime-kuma-beta_app: restart: unless-stopped logging: driver: ""json-file"" options: max-size: ""1M"" max-file: ""2"" stop_grace_period: 1m container_name: uptime-kuma-beta image: louislam/uptime-kuma:2.0.0-beta.0 pull_policy: if_not_present depends_on: uptime-kuma-beta_db: condition: service_started links: - uptime-kuma-beta_db volumes: - /docker/uptime-kuma-beta/app:/app/data - /var/run/docker.sock:/var/run/docker.sock:ro - /usr/local/share/ca-certificates:/app/data/docker-tls environment: PUID: ""1000"" PGID: ""1000"" TZ: Europe/Amsterdam # This is a certificate authority for demonstration purposes. # NODE_EXTRA_CA_CERTS: /app/data/docker-tls/root_ca_demo_cert.pem MYSQL_HOST: ""uptime-kuma-beta_db"" MYSQL_PORT: 3306 MYSQL_NAME: ""uptime_kuma_db"" MYSQL_USER: ""uptime-kuma"" MYSQL_PASSWORD: ""ThisIsADemoPassword"" domainname: status.local hostname: status networks: uptime-kuma-beta_net: ipv4_address: 172.20.3.3 ports: - ""3001:3001/tcp"" # HTTP - ""3001:3001/udp"" # HTTP security_opt: - no-new-privileges:true labels: com.docker.compose.project: ""uptime-kuma-beta"" com.uptime-kuma-beta.description: ""is an self-hosted monitoring tool that allows you to monitor uptime, status, and notifications for various services and domains."" healthcheck: disable: true   Expected behavior The query should execute successfully, deleting older entries from the `heartbeat` table while keeping the most recent 100 entries.  Actual Behavior The query fails due to the unsupported subquery syntax in MariaDB.  Uptime-Kuma Version 2.0.0-beta.0  Operating System and Arch Ubuntu Server 24.04.1 LTS (GNU/Linux 6.8.0-49-generic x86_64)  Browser Brave Version 1.73.91 Chromium: 131.0.6778.85 (Official Build) (64-bit)   Deployment Environment - **Runtime:** Docker version 26.1.0, build 9714adc - **Runtime:** docker-compose version 1.29.2, build unknown - **Runtime:** Portainer Business Edition version 2.21.4 - **Runtime:** MariaDB 11.6.2 - **Database:** (external) - **Filesystem used for the database:** Linux/ext4 on an SSD - **Number of monitors:** 92  Relevant log output shell Trace: Error: DELETE FROM heartbeat WHERE monitor_id = 1 AND important = 0 AND time < DATE_ADD(NOW(), INTERVAL -24 HOUR) AND id NOT IN ( SELECT id FROM heartbeat WHERE monitor_id = 1 ORDER BY time DESC LIMIT 100 ) - This version of MariaDB doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery' at Packet.asError (/app/node_modules/mysql2/lib/packets/packet.js:738:17) at Query.execute (/app/node_modules/mysql2/lib/commands/command.js:29:26) at Connection.handlePacket (/app/node_modules/mysql2/lib/connection.js:481:34) at PacketParser.onPacket (/app/node_modules/mysql2/lib/connection.js:97:12) at PacketParser.executeStart (/app/node_modules/mysql2/lib/packet_parser.js:75:16) at Socket.<anonymous> (/app/node_modules/mysql2/lib/connection.js:104:25) at Socket.emit (node:events:519:28) at addChunk (node:internal/streams/readable:559:12) at readableAddChunkPushByteMode (node:internal/streams/readable:510:3) at Readable.push (node:internal/streams/readable:390:5) { code: 'ER_NOT_SUPPORTED_YET', errno: 1235, sqlState: '42000', sqlMessage: ""This version of MariaDB doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery'"", sql: '\n' + ' DELETE FROM heartbeat\n' + ' WHERE monitor_id = 1\n' + ' AND important = 0\n' + ' AND time < DATE_ADD(NOW(), INTERVAL -24 HOUR)\n' + ' AND id NOT IN (\n' + ' SELECT id\n' + ' FROM heartbeat\n' + ' WHERE monitor_id = 1\n' + ' ORDER BY time DESC\n' + ' LIMIT 100\n' + ' )\n' + ' ' } at process.unexpectedErrorHandler (/app/server/server.js:1872:13) at process.emit (node:events:519:28) at emitUnhandledRejection (node:internal/process/promises:250:13) at throwUnhandledRejectionsMode (node:internal/process/promises:385:19) at processPromiseRejections (node:internal/process/promises:470:17) at process.processTicksAndRejections (node:internal/process/task_queues:96:32) If you keep encountering errors, please report to https://github.com/louislam/uptime-kuma/issues  documentation-file source-file documentation-file source-file",no-bug,0.9
574,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/574,[Discussion][Dev] node-sqlite3 does not support worker_threads,"While trying to implement scheduled jobs (for auto clear old statistics), I ran into an issue with sqlite crashing in native code. Turns out sqlite3 > 5.0 breaks when running in worker_threads (see [node-sqlite3#1381](https://github.com/mapbox/node-sqlite3/issues/1381)). So here are my options: - Drop the use of `worker_threads`. For me this isn't ideal. Although there isn't a strong need for running background jobs in another thread for now, any future CPU-intensive work (like generating reports) would benefit greatly from it. It also helps with modularization since we can ensure each background job acquire it's own db connection. But not using threading does have benefit of a more simple implementation. - Switch to `better-sqlite3`, since I tested it seems to work fine on a worker. From the READMEs it also looks like an all-round better library. Searching the repo I see that the project briefly switched to better-sqlite3 before switching back, but I don't know the reasoning behind. Is there any specific issues users encountered?",documentation-file | documentation-file,"[Discussion][Dev] node-sqlite3 does not support worker_threads While trying to implement scheduled jobs (for auto clear old statistics), I ran into an issue with sqlite crashing in native code. Turns out sqlite3 > 5.0 breaks when running in worker_threads (see [node-sqlite3#1381](https://github.com/mapbox/node-sqlite3/issues/1381)). So here are my options: - Drop the use of `worker_threads`. For me this isn't ideal. Although there isn't a strong need for running background jobs in another thread for now, any future CPU-intensive work (like generating reports) would benefit greatly from it. It also helps with modularization since we can ensure each background job acquire it's own db connection. But not using threading does have benefit of a more simple implementation. - Switch to `better-sqlite3`, since I tested it seems to work fine on a worker. From the READMEs it also looks like an all-round better library. Searching the repo I see that the project briefly switched to better-sqlite3 before switching back, but I don't know the reasoning behind. Is there any specific issues users encountered? documentation-file documentation-file",no-bug,0.9
482,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/482,Add a Description to Monitor on Status Page,**Is it a duplicate question?** _Not that I am aware of._ **Is your feature request related to a problem? Please describe.** The current status page does not have a description of each monitor for what it is/what it does. **Describe the solution you'd like** It would be great to add a small help icon next to the monitor that you can either hover over or click and it will show a description of what the service/monitor is. The description could be updated in the settings for each monitor. **Describe alternatives you've considered** None that I can think of. **Additional context** ![Capture](https://user-images.githubusercontent.com/1341459/134784662-e8700720-dfa0-4cef-8702-a4727d90c850.PNG),database-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | other-file | other-file | database-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | other-file | other-file | database-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | other-file | other-file,Add a Description to Monitor on Status Page **Is it a duplicate question?** _Not that I am aware of._ **Is your feature request related to a problem? Please describe.** The current status page does not have a description of each monitor for what it is/what it does. **Describe the solution you'd like** It would be great to add a small help icon next to the monitor that you can either hover over or click and it will show a description of what the service/monitor is. The description could be updated in the settings for each monitor. **Describe alternatives you've considered** None that I can think of. **Additional context** ![Capture](https://user-images.githubusercontent.com/1341459/134784662-e8700720-dfa0-4cef-8702-a4727d90c850.PNG) database-file source-file source-file source-file other-file source-file source-file source-file other-file other-file database-file source-file source-file source-file other-file source-file source-file source-file other-file other-file database-file source-file source-file source-file other-file source-file source-file source-file other-file other-file,no-bug,0.95
1114,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1114,Pushover notification device not working,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem I have set up a Pushover notification and I wanted to use the Device field to specify only my devices. But when the notification is fired it is delivered to every device in my Pushover account.  Uptime-Kuma Version 1.11.1  Operating System and Arch DietPi v7.9.3  Browser Chrome 96.0.4664.110  Docker Version 20.10.12  NodeJS Version _No response_,source-file,Pushover notification device not working   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem I have set up a Pushover notification and I wanted to use the Device field to specify only my devices. But when the notification is fired it is delivered to every device in my Pushover account.  Uptime-Kuma Version 1.11.1  Operating System and Arch DietPi v7.9.3  Browser Chrome 96.0.4664.110  Docker Version 20.10.12  NodeJS Version _No response_ source-file,no-bug,0.7
5564,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5564,[BUG] Slack Notifications - [no preview available]," I have found these related issues/pull requests N/A   Security Policy - [x] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Slack Notifications worked as expected in the Uptime Kuma **`1.x.x`** and displayed a notification preview correctly. Now in Uptime Kuma **`2.x.x`**, Slack Notifications do not work as expected and now display **`[no preview available]`**  Reproduction steps Any Uptime Kuma **POST** Request to a Slack webhook causes this issue  Expected behavior Slack is meant to display a brief Notification Preview  Actual Behavior Slack doesn't display a notification preview ![Image](https://github.com/user-attachments/assets/6f00032d-2ef8-4672-a496-b1414c361957)  Uptime-Kuma Version 2.0.0-beta.1  Operating System and Arch Debian 12.9 - ARM64  Browser Safari 18.3 (20620.2.4.11.4)   Deployment Environment - Runtime: Docker 27.5.0 - Database: SQLite - Filesystem used to store the database on: EXT4 on an NVMe - Number of Monitors: 42  Relevant log output shell 2025-01-22T19:41:12+11:00 [MONITOR] ERROR: Cannot send notification to Slack 2025-01-22T19:41:12+11:00 [MONITOR] ERROR: Error: Error: AxiosError: Request failed with status code 400 invalid_attachments at Slack.throwGeneralAxiosError (/app/server/notification-providers/notification-provider.js:69:15) at Slack.send (/app/server/notification-providers/slack.js:172:18) at process.processTicksAndRejections (node:internal/process/task_queues:95:5) at async Monitor.sendNotification (/app/server/model/monitor.js:1343:21) at async beat (/app/server/model/monitor.js:925:21) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:1010:17) ",source-file,"[BUG] Slack Notifications - [no preview available]  I have found these related issues/pull requests N/A   Security Policy - [x] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Slack Notifications worked as expected in the Uptime Kuma **`1.x.x`** and displayed a notification preview correctly. Now in Uptime Kuma **`2.x.x`**, Slack Notifications do not work as expected and now display **`[no preview available]`**  Reproduction steps Any Uptime Kuma **POST** Request to a Slack webhook causes this issue  Expected behavior Slack is meant to display a brief Notification Preview  Actual Behavior Slack doesn't display a notification preview ![Image](https://github.com/user-attachments/assets/6f00032d-2ef8-4672-a496-b1414c361957)  Uptime-Kuma Version 2.0.0-beta.1  Operating System and Arch Debian 12.9 - ARM64  Browser Safari 18.3 (20620.2.4.11.4)   Deployment Environment - Runtime: Docker 27.5.0 - Database: SQLite - Filesystem used to store the database on: EXT4 on an NVMe - Number of Monitors: 42  Relevant log output shell 2025-01-22T19:41:12+11:00 [MONITOR] ERROR: Cannot send notification to Slack 2025-01-22T19:41:12+11:00 [MONITOR] ERROR: Error: Error: AxiosError: Request failed with status code 400 invalid_attachments at Slack.throwGeneralAxiosError (/app/server/notification-providers/notification-provider.js:69:15) at Slack.send (/app/server/notification-providers/slack.js:172:18) at process.processTicksAndRejections (node:internal/process/task_queues:95:5) at async Monitor.sendNotification (/app/server/model/monitor.js:1343:21) at async beat (/app/server/model/monitor.js:925:21) at async Timeout.safeBeat [as _onTimeout] (/app/server/model/monitor.js:1010:17)  source-file",bug,0.9
2494,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2494,Maintenance not removed from status page if removed while active,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When adding a new manual maintenance, removing it while active does not remove it from the active status pages  Reproduction steps Go to the maintenance panel. Click ""Schedule Maintenance"" Give it a name, select at least one monitor Tick ""All Status Pages"" Use strategy: ""Active/Inactive Manually"" ""Save"" The maintenance is now active. Go on one of your status pages, the maintenance is active. Go back to the maintenance page. ""Delete"" the maintenance  Expected behavior The maintenance should disappear on affected status pages  Actual Behavior The maintenance is still here. Adding any other active maintenance removes it however.  Uptime-Kuma Version 1.19.2  Operating System and Arch Manjaro, kernel:5.10.161-1-MANJARO  Browser Firefox Developer Edition 109.0b6  Docker Version Docker 20.10.22  NodeJS Version 16.18.1  Relevant log output _No response_  EDIT: It is removed but it takes some time before updating, I must have checked a bit longer.",source-file,"Maintenance not removed from status page if removed while active   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When adding a new manual maintenance, removing it while active does not remove it from the active status pages  Reproduction steps Go to the maintenance panel. Click ""Schedule Maintenance"" Give it a name, select at least one monitor Tick ""All Status Pages"" Use strategy: ""Active/Inactive Manually"" ""Save"" The maintenance is now active. Go on one of your status pages, the maintenance is active. Go back to the maintenance page. ""Delete"" the maintenance  Expected behavior The maintenance should disappear on affected status pages  Actual Behavior The maintenance is still here. Adding any other active maintenance removes it however.  Uptime-Kuma Version 1.19.2  Operating System and Arch Manjaro, kernel:5.10.161-1-MANJARO  Browser Firefox Developer Edition 109.0b6  Docker Version Docker 20.10.22  NodeJS Version 16.18.1  Relevant log output _No response_  EDIT: It is removed but it takes some time before updating, I must have checked a bit longer. source-file",no-bug,0.9
3723,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3723,Timezone Issue Causes Incorrect Monitor Time Display,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem Environment: Uptime Kuma Version: 1.23.1 OS: Ubuntu 22.04 Database: MariaDB Issue: I've encountered an issue where the monitor time displayed in Uptime Kuma is incorrect. The time initially appears correct but later jumps forward by 2 hours. I've confirmed that the server's timezone and MariaDB's timezone are both correctly set to 'Europe/Berlin'. Logs: The following warning appears in the logs: `Ignoring invalid timezone passed to Connection: UTC. This is currently a warning, but in future versions of MySQL2, an error will be thrown if you pass an invalid configuration option t>` This log entry coincides with the time jump in the Uptime Kuma interface. Steps to Reproduce: - Install Uptime Kuma on a server with the 'Europe/Berlin' timezone. - Set up a monitor. - Observe the time displayed for the monitor. Expected Behavior: The monitor time should be consistent with the server's timezone setting. Actual Behavior: The monitor time jumps forward by 2 hours after a short period. Additional Information: I've tried modifying the MariaDB configuration to set the global timezone, but the issue persists.  Error Message(s) or Log _No response_  Uptime-Kuma Version 1.23.1  Operating System and Arch Ubuntu 22.04  Browser Chrome 116.0.5845.142  Docker Version _No response_  NodeJS Version 16.20.2",source-file,"Timezone Issue Causes Incorrect Monitor Time Display   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem Environment: Uptime Kuma Version: 1.23.1 OS: Ubuntu 22.04 Database: MariaDB Issue: I've encountered an issue where the monitor time displayed in Uptime Kuma is incorrect. The time initially appears correct but later jumps forward by 2 hours. I've confirmed that the server's timezone and MariaDB's timezone are both correctly set to 'Europe/Berlin'. Logs: The following warning appears in the logs: `Ignoring invalid timezone passed to Connection: UTC. This is currently a warning, but in future versions of MySQL2, an error will be thrown if you pass an invalid configuration option t>` This log entry coincides with the time jump in the Uptime Kuma interface. Steps to Reproduce: - Install Uptime Kuma on a server with the 'Europe/Berlin' timezone. - Set up a monitor. - Observe the time displayed for the monitor. Expected Behavior: The monitor time should be consistent with the server's timezone setting. Actual Behavior: The monitor time jumps forward by 2 hours after a short period. Additional Information: I've tried modifying the MariaDB configuration to set the global timezone, but the issue persists.  Error Message(s) or Log _No response_  Uptime-Kuma Version 1.23.1  Operating System and Arch Ubuntu 22.04  Browser Chrome 116.0.5845.142  Docker Version _No response_  NodeJS Version 16.20.2 source-file",no-bug,0.8
2778,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2778,Database migration can end up deleting db,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I upgraded my uptime-kuma docker container to 1.20. During the upgrade uptime-kuma failed to create a DB backup (I'm running on a small VPS with only 10GB space) du to insufficient HDD space. *(I think)* when trying to restore the (non-existant) backup uptime-kuma first deleted the kuma.db file, leaving me at the end with neither a backup nor the actual DB file. In the end I restored a backup from january and only lost ~ 3 weeks of data. But it was a bit scary :D  Reproduction steps update uptime-kuma on a system with not enough space for a full copy of kuma.db. If there are no prior backups you end up with no DB file and no backup file.  Expected behavior do not delete the kuma.db file in the backup-restore process. (perhaps only rename it first and then delete it at the end of the process, after the copy suceeded)  Actual Behavior kuma.db deleted an no backup.  Uptime-Kuma Version 1.19 -> 1.20  Operating System and Arch Debian GNU/Linux 11 (bullseye)  Docker Version 20.10.23  NodeJS Version _No response_  Relevant log output shell ==> Performing startup jobs and maintenance tasks changed ownership of '/app/data/sqlite3' from node:node to 0:0 ==> Starting application with user 0 group 0 Welcome to Uptime Kuma Your Node.js version: 16 2023-02-14T00:00:57Z [SERVER] INFO: Welcome to Uptime Kuma 2023-02-14T00:00:58Z [SERVER] INFO: Node Env: production 2023-02-14T00:00:58Z [SERVER] INFO: Importing Node libraries 2023-02-14T00:00:58Z [SERVER] INFO: Importing 3rd-party libraries 2023-02-14T00:01:03Z [SERVER] INFO: Creating express and socket.io instance 2023-02-14T00:01:03Z [SERVER] INFO: Server Type: HTTP 2023-02-14T00:01:03Z [SERVER] INFO: Importing this project modules 2023-02-14T00:01:04Z [NOTIFICATION] INFO: Prepare Notification Providers 2023-02-14T00:01:04Z [SERVER] INFO: Version: 1.20.0 2023-02-14T00:01:04Z [DB] INFO: Data Dir: ./data/ 2023-02-14T00:01:04Z [SERVER] INFO: Connecting to the Database 2023-02-14T00:01:05Z [DB] INFO: SQLite config: [ { journal_mode: 'wal' } ] [ { cache_size: -12000 } ] 2023-02-14T00:01:05Z [DB] INFO: SQLite Version: 3.39.4 2023-02-14T00:01:05Z [SERVER] INFO: Connected 2023-02-14T00:01:05Z [DB] INFO: Your database version: 10 2023-02-14T00:01:05Z [DB] INFO: Latest database version: 10 2023-02-14T00:01:05Z [DB] INFO: Database patch not needed 2023-02-14T00:01:05Z [DB] INFO: Database Patch 2.0 Process 2023-02-14T00:01:05Z [DB] INFO: patch-ping-packet-size.sql is not patched 2023-02-14T00:01:05Z [DB] INFO: Backing up the database 2023-02-14T00:01:23Z [DB] INFO: Closing the database 2023-02-14T00:01:25Z [DB] INFO: SQLite closed Error: ENOSPC: no space left on device, copyfile './data/kuma.db' -> './data/kuma.db.bak20230214000105' at Object.copyFileSync (node:fs:2847:3) at Function.backup (/app/server/database.js:458:16) at Function.patch2Recursion (/app/server/database.js:366:18) at Function.patch2 (/app/server/database.js:249:28) at async Function.patch (/app/server/database.js:226:9) at async initDatabase (/app/server/server.js:1656:5) at async /app/server/server.js:171:5 { errno: -28, syscall: 'copyfile', code: 'ENOSPC', path: './data/kuma.db', dest: './data/kuma.db.bak20230214000105' } 2023-02-14T00:01:25Z [DB] ERROR: Start Uptime-Kuma failed due to issue patching the database 2023-02-14T00:01:25Z [DB] ERROR: Please submit the bug report if you still encounter the problem after restart: https://github.com/louislam/uptime-kuma/issues 2023-02-14T00:01:25Z [DB] ERROR: Patching the database failed Restoring the backup Trace: Error: ENOENT: no such file or directory, copyfile './data/kuma.db.bak20230214000105' -> './data/kuma.db' at Object.copyFileSync (node:fs:2847:3) at Function.restore (/app/server/database.js:518:16) at Function.patch2 (/app/server/database.js:263:18) at async Function.patch (/app/server/database.js:226:9) at async initDatabase (/app/server/server.js:1656:5) at async /app/server/server.js:171:5 { errno: -2, syscall: 'copyfile', code: 'ENOENT', path: './data/kuma.db.bak20230214000105', dest: './data/kuma.db' } at process.<anonymous> (/app/server/server.js:1794:13) at process.emit (node:events:513:28) at emit (node:internal/process/promises:140:20) at processPromiseRejections (node:internal/process/promises:274:27) at processTicksAndRejections (node:internal/process/task_queues:97:32) If you keep encountering errors, please report to https://github.com/louislam/uptime-kuma/issues ",source-file | source-file,"Database migration can end up deleting db   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I upgraded my uptime-kuma docker container to 1.20. During the upgrade uptime-kuma failed to create a DB backup (I'm running on a small VPS with only 10GB space) du to insufficient HDD space. *(I think)* when trying to restore the (non-existant) backup uptime-kuma first deleted the kuma.db file, leaving me at the end with neither a backup nor the actual DB file. In the end I restored a backup from january and only lost ~ 3 weeks of data. But it was a bit scary :D  Reproduction steps update uptime-kuma on a system with not enough space for a full copy of kuma.db. If there are no prior backups you end up with no DB file and no backup file.  Expected behavior do not delete the kuma.db file in the backup-restore process. (perhaps only rename it first and then delete it at the end of the process, after the copy suceeded)  Actual Behavior kuma.db deleted an no backup.  Uptime-Kuma Version 1.19 -> 1.20  Operating System and Arch Debian GNU/Linux 11 (bullseye)  Docker Version 20.10.23  NodeJS Version _No response_  Relevant log output shell ==> Performing startup jobs and maintenance tasks changed ownership of '/app/data/sqlite3' from node:node to 0:0 ==> Starting application with user 0 group 0 Welcome to Uptime Kuma Your Node.js version: 16 2023-02-14T00:00:57Z [SERVER] INFO: Welcome to Uptime Kuma 2023-02-14T00:00:58Z [SERVER] INFO: Node Env: production 2023-02-14T00:00:58Z [SERVER] INFO: Importing Node libraries 2023-02-14T00:00:58Z [SERVER] INFO: Importing 3rd-party libraries 2023-02-14T00:01:03Z [SERVER] INFO: Creating express and socket.io instance 2023-02-14T00:01:03Z [SERVER] INFO: Server Type: HTTP 2023-02-14T00:01:03Z [SERVER] INFO: Importing this project modules 2023-02-14T00:01:04Z [NOTIFICATION] INFO: Prepare Notification Providers 2023-02-14T00:01:04Z [SERVER] INFO: Version: 1.20.0 2023-02-14T00:01:04Z [DB] INFO: Data Dir: ./data/ 2023-02-14T00:01:04Z [SERVER] INFO: Connecting to the Database 2023-02-14T00:01:05Z [DB] INFO: SQLite config: [ { journal_mode: 'wal' } ] [ { cache_size: -12000 } ] 2023-02-14T00:01:05Z [DB] INFO: SQLite Version: 3.39.4 2023-02-14T00:01:05Z [SERVER] INFO: Connected 2023-02-14T00:01:05Z [DB] INFO: Your database version: 10 2023-02-14T00:01:05Z [DB] INFO: Latest database version: 10 2023-02-14T00:01:05Z [DB] INFO: Database patch not needed 2023-02-14T00:01:05Z [DB] INFO: Database Patch 2.0 Process 2023-02-14T00:01:05Z [DB] INFO: patch-ping-packet-size.sql is not patched 2023-02-14T00:01:05Z [DB] INFO: Backing up the database 2023-02-14T00:01:23Z [DB] INFO: Closing the database 2023-02-14T00:01:25Z [DB] INFO: SQLite closed Error: ENOSPC: no space left on device, copyfile './data/kuma.db' -> './data/kuma.db.bak20230214000105' at Object.copyFileSync (node:fs:2847:3) at Function.backup (/app/server/database.js:458:16) at Function.patch2Recursion (/app/server/database.js:366:18) at Function.patch2 (/app/server/database.js:249:28) at async Function.patch (/app/server/database.js:226:9) at async initDatabase (/app/server/server.js:1656:5) at async /app/server/server.js:171:5 { errno: -28, syscall: 'copyfile', code: 'ENOSPC', path: './data/kuma.db', dest: './data/kuma.db.bak20230214000105' } 2023-02-14T00:01:25Z [DB] ERROR: Start Uptime-Kuma failed due to issue patching the database 2023-02-14T00:01:25Z [DB] ERROR: Please submit the bug report if you still encounter the problem after restart: https://github.com/louislam/uptime-kuma/issues 2023-02-14T00:01:25Z [DB] ERROR: Patching the database failed Restoring the backup Trace: Error: ENOENT: no such file or directory, copyfile './data/kuma.db.bak20230214000105' -> './data/kuma.db' at Object.copyFileSync (node:fs:2847:3) at Function.restore (/app/server/database.js:518:16) at Function.patch2 (/app/server/database.js:263:18) at async Function.patch (/app/server/database.js:226:9) at async initDatabase (/app/server/server.js:1656:5) at async /app/server/server.js:171:5 { errno: -2, syscall: 'copyfile', code: 'ENOENT', path: './data/kuma.db.bak20230214000105', dest: './data/kuma.db' } at process.<anonymous> (/app/server/server.js:1794:13) at process.emit (node:events:513:28) at emit (node:internal/process/promises:140:20) at processPromiseRejections (node:internal/process/promises:274:27) at processTicksAndRejections (node:internal/process/task_queues:97:32) If you keep encountering errors, please report to https://github.com/louislam/uptime-kuma/issues  source-file source-file",no-bug,0.95
1422,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1422,Race condition in push monitors,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description There is a race condition in the code for push monitors that can cause repeat notifications within the same second or shortly thereafter. (see some background in #922) ![image](https://user-images.githubusercontent.com/1147328/160219491-5d077488-49b8-4a77-99b4-7397873384f3.png) http logs show that the GET calls are generally 60s apart, but because of discrepancies in system time down to the ms level, it can appear that they happen a second apart, even if they're very close together (ie.e 17:58:05.999 to 17:59:06.000)  [25/Mar/2022:17:54:05 -0700] ""GET /api/push/xxxx?msg=OK&ping=92.177 HTTP/1.1"" ""-"" ""curl/7.74.0"" TLSv1.3 TLS_AES_256_GCM_SHA384 200 5567 [25/Mar/2022:17:55:05 -0700] ""GET /api/push/xxxx?msg=OK&ping=92.196 HTTP/1.1"" ""-"" ""curl/7.74.0"" TLSv1.3 TLS_AES_256_GCM_SHA384 200 5567  [25/Mar/2022:17:58:05 -0700] ""GET /api/push/xxx?msg=OK&ping=90.552 HTTP/1.1"" ""-"" ""curl/7.74.0"" TLSv1.3 TLS_AES_256_GCM_SHA384 200 5567 [25/Mar/2022:17:59:06 -0700] ""GET /api/push/xxx?msg=OK&ping=91.308 HTTP/1.1"" ""-"" ""curl/7.74.0"" TLSv1.3 TLS_AES_256_GCM_SHA384 200 5567   Reproduction steps Repro is tricky, but it could be possible by monitoring uptime kuma debug logs and synchronizing to them. Basically, the push URL calls have to happen just shortly after the monitor's calls to `beat()` or `safe_beat()`  Expected behavior Uptime kuma should be robust against very small clock discrepancies  Actual Behavior Get double notifications  Uptime-Kuma Version 1.12.1  Operating System and Arch Docker on Ubuntu 20.04  Browser Firefox  Docker Version Latest  NodeJS Version _No response_  Relevant log output _No response_",documentation-file | documentation-file | source-file | source-file,"Race condition in push monitors   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description There is a race condition in the code for push monitors that can cause repeat notifications within the same second or shortly thereafter. (see some background in #922) ![image](https://user-images.githubusercontent.com/1147328/160219491-5d077488-49b8-4a77-99b4-7397873384f3.png) http logs show that the GET calls are generally 60s apart, but because of discrepancies in system time down to the ms level, it can appear that they happen a second apart, even if they're very close together (ie.e 17:58:05.999 to 17:59:06.000)  [25/Mar/2022:17:54:05 -0700] ""GET /api/push/xxxx?msg=OK&ping=92.177 HTTP/1.1"" ""-"" ""curl/7.74.0"" TLSv1.3 TLS_AES_256_GCM_SHA384 200 5567 [25/Mar/2022:17:55:05 -0700] ""GET /api/push/xxxx?msg=OK&ping=92.196 HTTP/1.1"" ""-"" ""curl/7.74.0"" TLSv1.3 TLS_AES_256_GCM_SHA384 200 5567  [25/Mar/2022:17:58:05 -0700] ""GET /api/push/xxx?msg=OK&ping=90.552 HTTP/1.1"" ""-"" ""curl/7.74.0"" TLSv1.3 TLS_AES_256_GCM_SHA384 200 5567 [25/Mar/2022:17:59:06 -0700] ""GET /api/push/xxx?msg=OK&ping=91.308 HTTP/1.1"" ""-"" ""curl/7.74.0"" TLSv1.3 TLS_AES_256_GCM_SHA384 200 5567   Reproduction steps Repro is tricky, but it could be possible by monitoring uptime kuma debug logs and synchronizing to them. Basically, the push URL calls have to happen just shortly after the monitor's calls to `beat()` or `safe_beat()`  Expected behavior Uptime kuma should be robust against very small clock discrepancies  Actual Behavior Get double notifications  Uptime-Kuma Version 1.12.1  Operating System and Arch Docker on Ubuntu 20.04  Browser Firefox  Docker Version Latest  NodeJS Version _No response_  Relevant log output _No response_ documentation-file documentation-file source-file source-file",bug,0.85
168,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/168,Dashboard produces a scrollbar although there's enough space,"**Describe the bug** The dashboard produces a scrollbar although there's enough vertical space - I think this was not the case in earlier versions **To Reproduce** Just load the dashboard **Expected behavior** No scrollbar if there's enough space **Screenshots** ![2021-08-04_110621](https://user-images.githubusercontent.com/683680/128154307-e281ab0e-bd29-48f1-8a7d-40852f324295.png) **Desktop (please complete the following information):** - Uptime Kuma Version: 1.0.10 - Using Docker?: Yes - OS: Ubuntu 20.04 - Browser: Firefox, Edge and Chrome",other-file,"Dashboard produces a scrollbar although there's enough space **Describe the bug** The dashboard produces a scrollbar although there's enough vertical space - I think this was not the case in earlier versions **To Reproduce** Just load the dashboard **Expected behavior** No scrollbar if there's enough space **Screenshots** ![2021-08-04_110621](https://user-images.githubusercontent.com/683680/128154307-e281ab0e-bd29-48f1-8a7d-40852f324295.png) **Desktop (please complete the following information):** - Uptime Kuma Version: 1.0.10 - Using Docker?: Yes - OS: Ubuntu 20.04 - Browser: Firefox, Edge and Chrome other-file",no-bug,0.95
5,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5,categorized as hacking tool,"My web application firewall is categorizing the checking request as ""Node.js (Hacking Tool) from United States"" - bad bot, and triggering a CAPTCHA security check (a setting for bots that I turn on). I can see that the agent is: User Agent:axios/0.21.1 I can add this to an allowed list, but I'd prefer to have the agent listed as something unique to Kuma.",source-file | test-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file,"categorized as hacking tool My web application firewall is categorizing the checking request as ""Node.js (Hacking Tool) from United States"" - bad bot, and triggering a CAPTCHA security check (a setting for bots that I turn on). I can see that the agent is: User Agent:axios/0.21.1 I can add this to an allowed list, but I'd prefer to have the agent listed as something unique to Kuma. source-file test-file source-file source-file source-file source-file source-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file",no-bug,0.9
2785,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2785,Sort tags in dashboard list view,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description It would be really nice if the tags were sorted so that monitors that have the same tag set would appear identical. ![screenshot_wmJuEjxU](https://user-images.githubusercontent.com/1992842/219125508-2b9be221-436d-4c59-adf7-263a53b2cdf7.png)   Solution I quickly glanced at the code and it seemed like a simple `ORDER BY tag.name` would do here https://github.com/louislam/uptime-kuma/blob/fdc3b2d57a9037c4415d18fe01762e7960a7fd43/server/model/monitor.js#LL146C19-L146C19 js async getTags() { return await R.getAll(""SELECT mt.*, tag.name, tag.color FROM monitor_tag mt JOIN tag ON mt.tag_id = tag.id WHERE mt.monitor_id = ? ORDER BY tag.name"", [ this.id ]); }  I manually patched my 1.20.1 install and  it worked! ![screenshot_IemXTUCb](https://user-images.githubusercontent.com/1992842/219127540-1844b8f3-e987-40f8-be1e-223d6349a63a.png) So this could be a quick PR if accepted.   Alternatives _No response_  Additional Context _No response_",source-file,"Sort tags in dashboard list view   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description It would be really nice if the tags were sorted so that monitors that have the same tag set would appear identical. ![screenshot_wmJuEjxU](https://user-images.githubusercontent.com/1992842/219125508-2b9be221-436d-4c59-adf7-263a53b2cdf7.png)   Solution I quickly glanced at the code and it seemed like a simple `ORDER BY tag.name` would do here https://github.com/louislam/uptime-kuma/blob/fdc3b2d57a9037c4415d18fe01762e7960a7fd43/server/model/monitor.js#LL146C19-L146C19 js async getTags() { return await R.getAll(""SELECT mt.*, tag.name, tag.color FROM monitor_tag mt JOIN tag ON mt.tag_id = tag.id WHERE mt.monitor_id = ? ORDER BY tag.name"", [ this.id ]); }  I manually patched my 1.20.1 install and  it worked! ![screenshot_IemXTUCb](https://user-images.githubusercontent.com/1992842/219127540-1844b8f3-e987-40f8-be1e-223d6349a63a.png) So this could be a quick PR if accepted.   Alternatives _No response_  Additional Context _No response_ source-file",no-bug,0.95
3596,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3596,Elapsed time not responsive and invisible for non-logged in users,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description ""Elapsed time under the heartbeat bar"" setting only works if you are signed in, public facing statuspage still display elapsed time and it's not very mobile friendly as seen in attached screenshot. ![original_a8ce379c-642a-48cb-8524-688b9beddb9a_Screenshot_20230818-065002](https://github.com/louislam/uptime-kuma/assets/6471050/bf3c67b3-6016-4898-b27d-9bb9d3ae241d)  Reproduction steps Toggle ""Elapsed time under the heartbeat bar"" setting.  Expected behavior Elapsed time show if selected, not if turned off.  Actual Behavior Setting works if you are signed in, if not, statuspage still show elapsed time.  Uptime-Kuma Version 1.23.0  Operating System and Arch Ubuntu 22.04  Browser Firefox, Chrome.  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_",other-file,"Elapsed time not responsive and invisible for non-logged in users   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description ""Elapsed time under the heartbeat bar"" setting only works if you are signed in, public facing statuspage still display elapsed time and it's not very mobile friendly as seen in attached screenshot. ![original_a8ce379c-642a-48cb-8524-688b9beddb9a_Screenshot_20230818-065002](https://github.com/louislam/uptime-kuma/assets/6471050/bf3c67b3-6016-4898-b27d-9bb9d3ae241d)  Reproduction steps Toggle ""Elapsed time under the heartbeat bar"" setting.  Expected behavior Elapsed time show if selected, not if turned off.  Actual Behavior Setting works if you are signed in, if not, statuspage still show elapsed time.  Uptime-Kuma Version 1.23.0  Operating System and Arch Ubuntu 22.04  Browser Firefox, Chrome.  Docker Version _No response_  NodeJS Version _No response_  Relevant log output _No response_ other-file",no-bug,0.9
1790,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1790,TLS certificate expires,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description _No response_  Reproduction steps 1. put a new notification service - Lunasea 2. put TLS certificate expires in: 35 days 3. add a new monitor  Expected behavior Send TLS Certificate Expiry notification when TLS Certification is in 35 Days  Actual Behavior did not send TLS Certificate Expiry notification  Uptime-Kuma Version 1.17.0-beta.1  Operating System and Arch Linix  Browser Chrome  Docker Version _No response_  NodeJS Version Latest  Relevant log output shell No Log ,source-file,TLS certificate expires   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description _No response_  Reproduction steps 1. put a new notification service - Lunasea 2. put TLS certificate expires in: 35 days 3. add a new monitor  Expected behavior Send TLS Certificate Expiry notification when TLS Certification is in 35 Days  Actual Behavior did not send TLS Certificate Expiry notification  Uptime-Kuma Version 1.17.0-beta.1  Operating System and Arch Linix  Browser Chrome  Docker Version _No response_  NodeJS Version Latest  Relevant log output shell No Log  source-file,no-bug,0.9
4066,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/4066,Maintenance start/end time input incorrectly uses system timezone,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description ![image](https://github.com/louislam/uptime-kuma/assets/56870521/30ddd5ee-fbfb-44ae-b125-19a554c1799f) ![image](https://github.com/louislam/uptime-kuma/assets/56870521/5ec67895-c413-4b59-82f5-5f8b6ff6f533) I set the Chinese time, but after exiting, the time displayed was 1 hour later than the set time, but the actual execution time was correct.  Reproduction steps Select the time zone as Shanghai and set up planned maintenance  Expected behavior Time display leading to status page errors  Actual Behavior Status page error time display  Uptime-Kuma Version 1.23.6  Operating System and Arch Ubuntu 22.04.3 LTS  Browser edge  Docker Version Docker version 24.0.5, build 24.0.5-0ubuntu1~22.04.1  NodeJS Version _No response_  Relevant log output _No response_",other-file | other-file,"Maintenance start/end time input incorrectly uses system timezone   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description ![image](https://github.com/louislam/uptime-kuma/assets/56870521/30ddd5ee-fbfb-44ae-b125-19a554c1799f) ![image](https://github.com/louislam/uptime-kuma/assets/56870521/5ec67895-c413-4b59-82f5-5f8b6ff6f533) I set the Chinese time, but after exiting, the time displayed was 1 hour later than the set time, but the actual execution time was correct.  Reproduction steps Select the time zone as Shanghai and set up planned maintenance  Expected behavior Time display leading to status page errors  Actual Behavior Status page error time display  Uptime-Kuma Version 1.23.6  Operating System and Arch Ubuntu 22.04.3 LTS  Browser edge  Docker Version Docker version 24.0.5, build 24.0.5-0ubuntu1~22.04.1  NodeJS Version _No response_  Relevant log output _No response_ other-file other-file",no-bug,0.9
3274,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3274,[Ntfy] Action link should only be included if url is defined,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When using the ntfy notification, uptime-kuma create an action button to go to the uptime-kuma page on the ntfy notification but the actual link is not completed. The main URL is setup on uptime-kuma  Reproduction steps Create an ntfy notification Receive a down or up notification Click on the action button on ntfy See the result  Expected behavior the action button take you to the uptime-kuma page  Actual Behavior the action button take you to a about:blank page ![image](https://github.com/louislam/uptime-kuma/assets/51720655/ea758a08-ff64-4980-aed0-745daed2a139)  Uptime-Kuma Version 1.22.0-beta.0  Operating System and Arch Fly.io  Browser Not browser related  Docker Version Fly.io  NodeJS Version _No response_  Relevant log output _No response_",source-file | source-file | source-file,"[Ntfy] Action link should only be included if url is defined   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When using the ntfy notification, uptime-kuma create an action button to go to the uptime-kuma page on the ntfy notification but the actual link is not completed. The main URL is setup on uptime-kuma  Reproduction steps Create an ntfy notification Receive a down or up notification Click on the action button on ntfy See the result  Expected behavior the action button take you to the uptime-kuma page  Actual Behavior the action button take you to a about:blank page ![image](https://github.com/louislam/uptime-kuma/assets/51720655/ea758a08-ff64-4980-aed0-745daed2a139)  Uptime-Kuma Version 1.22.0-beta.0  Operating System and Arch Fly.io  Browser Not browser related  Docker Version Fly.io  NodeJS Version _No response_  Relevant log output _No response_ source-file source-file source-file",no-bug,0.8
2296,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2296,(Socks) Proxy: use DNS of proxy,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description After some testing it currently seems like the that if a proxy is configured, uptime-kuma is using its own local or defined DNS server even when a proxy is defined instead of using the proxy DNS.   Solution Add option that when a proxy is configured, to also use conduct DNS resolution through this proxy.   Alternatives _No response_  Additional Context _No response_",source-file | other-file,"(Socks) Proxy: use DNS of proxy   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description After some testing it currently seems like the that if a proxy is configured, uptime-kuma is using its own local or defined DNS server even when a proxy is defined instead of using the proxy DNS.   Solution Add option that when a proxy is configured, to also use conduct DNS resolution through this proxy.   Alternatives _No response_  Additional Context _No response_ source-file other-file",no-bug,0.95
685,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/685,Pill remains at 0% if PUSH monitor even once switch to Pending or Down.,"Oh, I see. Pill remains at 0% if PUSH monitor even once switch to Pending or Down. If monitor status never change from OK, then pills will have 100%. ![image](https://user-images.githubusercontent.com/35193662/137032143-e1084e07-b7a5-4781-a8a6-28b5add84fa2.png) ![image](https://user-images.githubusercontent.com/35193662/137032150-cbfd8edb-a740-446a-8f3d-2a704634a9f9.png) That could be linked to not logging UP events. _Originally posted by @NixNotCastey in https://github.com/louislam/uptime-kuma/issues/679#issuecomment-941607857_",source-file,"Pill remains at 0% if PUSH monitor even once switch to Pending or Down. Oh, I see. Pill remains at 0% if PUSH monitor even once switch to Pending or Down. If monitor status never change from OK, then pills will have 100%. ![image](https://user-images.githubusercontent.com/35193662/137032143-e1084e07-b7a5-4781-a8a6-28b5add84fa2.png) ![image](https://user-images.githubusercontent.com/35193662/137032150-cbfd8edb-a740-446a-8f3d-2a704634a9f9.png) That could be linked to not logging UP events. _Originally posted by @NixNotCastey in https://github.com/louislam/uptime-kuma/issues/679#issuecomment-941607857_ source-file",no-bug,0.8
757,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/757,SMTP problem,"Hi, I have a problem with smtp notifications and still got error result ""**self signed certificate**"". ![image](https://user-images.githubusercontent.com/92888480/138164102-c5d30f7c-ffa0-438c-8e4f-48fecf6e805f.png) or docker logs 58f720e88xxx | grep Error **Error: self signed certificate at TLSSocket.onConnectSecure (_tls_wrap.js:1515:34) at TLSSocket.emit (events.js:400:28) at TLSSocket._finishInit (_tls_wrap.js:937:8) at TLSWrap.ssl.onhandshakedone (_tls_wrap.js:709:12) { code: 'ESOCKET', command: 'CONN' }** The current SMTP configuration is: ![image](https://user-images.githubusercontent.com/92888480/138164742-9b40023e-1d03-4c35-8b40-d6b828232d1d.png) If I try to send emai via telnet everythning goes well: # telnet email.prahax.cz 25 Trying 10.xx.xx.xxx Connected to email.prahax.cz. Escape character is '^]'. 220 email.prahax.cz Microsoft ESMTP MAIL Service ready at Wed, 20 Oct 2021 22:19:51 +0200 ehlo prahax.cz 250-email.prahax.cz Hello [10.139.xx.xxx] 250-SIZE 37748736 250-PIPELINING 250-DSN 250-ENHANCEDSTATUSCODES 250-STARTTLS 250-AUTH LOGIN 250-8BITMIME 250-BINARYMIME 250 CHUNKING MAIL FROM: someaddress@linuxconfig.org 250 2.1.0 Sender OK RCPT TO: karel@xxx.xx 250 2.1.5 Recipient OK DATA Subject: Sending an email using telnet Hello, Here is my body? Do you like it? cheers354 Start mail input; end with <CRLF>.<CRLF> . quit . 250 2.6.0 <561b471c-3159-49c4-969e-22f290a387cd@xxx.xZ> [xxxx] 1858 bytes in 1:30.413, 0,020 KB/sec Queued mail for delivery quit 221 2.0.0 Service closing transmission channel Connection closed by foreign host. I tried different version of Kuma 1.8 and 1.9.1 and got same results :(. **Info** Uptime Kuma Version: 1.8 and 1.9.1 Using Docker?: Yes Docker Version: 20.10.9 Node.js Version (Without Docker only): OS: ubntu Browser: chrome",source-file,"SMTP problem Hi, I have a problem with smtp notifications and still got error result ""**self signed certificate**"". ![image](https://user-images.githubusercontent.com/92888480/138164102-c5d30f7c-ffa0-438c-8e4f-48fecf6e805f.png) or docker logs 58f720e88xxx | grep Error **Error: self signed certificate at TLSSocket.onConnectSecure (_tls_wrap.js:1515:34) at TLSSocket.emit (events.js:400:28) at TLSSocket._finishInit (_tls_wrap.js:937:8) at TLSWrap.ssl.onhandshakedone (_tls_wrap.js:709:12) { code: 'ESOCKET', command: 'CONN' }** The current SMTP configuration is: ![image](https://user-images.githubusercontent.com/92888480/138164742-9b40023e-1d03-4c35-8b40-d6b828232d1d.png) If I try to send emai via telnet everythning goes well: # telnet email.prahax.cz 25 Trying 10.xx.xx.xxx Connected to email.prahax.cz. Escape character is '^]'. 220 email.prahax.cz Microsoft ESMTP MAIL Service ready at Wed, 20 Oct 2021 22:19:51 +0200 ehlo prahax.cz 250-email.prahax.cz Hello [10.139.xx.xxx] 250-SIZE 37748736 250-PIPELINING 250-DSN 250-ENHANCEDSTATUSCODES 250-STARTTLS 250-AUTH LOGIN 250-8BITMIME 250-BINARYMIME 250 CHUNKING MAIL FROM: someaddress@linuxconfig.org 250 2.1.0 Sender OK RCPT TO: karel@xxx.xx 250 2.1.5 Recipient OK DATA Subject: Sending an email using telnet Hello, Here is my body? Do you like it? cheers354 Start mail input; end with <CRLF>.<CRLF> . quit . 250 2.6.0 <561b471c-3159-49c4-969e-22f290a387cd@xxx.xZ> [xxxx] 1858 bytes in 1:30.413, 0,020 KB/sec Queued mail for delivery quit 221 2.0.0 Service closing transmission channel Connection closed by foreign host. I tried different version of Kuma 1.8 and 1.9.1 and got same results :(. **Info** Uptime Kuma Version: 1.8 and 1.9.1 Using Docker?: Yes Docker Version: 20.10.9 Node.js Version (Without Docker only): OS: ubntu Browser: chrome source-file",no-bug,0.9
1478,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1478,Issue with import backup.json,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I export the backup file on my first server and I try to import this file to my new server. But I encounter an issue when I try that with all methods (Keep both, Skip existing, Overwrite), I've got this log: `cannot read property of undefined (reading 'length')`. I use this method a lot of time and it's the first time I've got problems. The 2 servers run with the latest version of docker on Debian 11 with Kuma 1.14.0  Reproduction steps - Export Backup - Import Backup  Expected behavior It will import the backup.json without any issues.  Actual Behavior Got an issue with this log: `cannot read property of undefined (reading 'length')`  Uptime-Kuma Version 1.14.0  Operating System and Arch Debian 11 x86  Browser Safari and Firefox  Docker Version Docker 20.10.5  NodeJS Version _No response_  Relevant log output _No response_",other-file | source-file | documentation-file | test-file | other-file | other-file | documentation-file | documentation-file | documentation-file | source-file | source-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file,"Issue with import backup.json   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I export the backup file on my first server and I try to import this file to my new server. But I encounter an issue when I try that with all methods (Keep both, Skip existing, Overwrite), I've got this log: `cannot read property of undefined (reading 'length')`. I use this method a lot of time and it's the first time I've got problems. The 2 servers run with the latest version of docker on Debian 11 with Kuma 1.14.0  Reproduction steps - Export Backup - Import Backup  Expected behavior It will import the backup.json without any issues.  Actual Behavior Got an issue with this log: `cannot read property of undefined (reading 'length')`  Uptime-Kuma Version 1.14.0  Operating System and Arch Debian 11 x86  Browser Safari and Firefox  Docker Version Docker 20.10.5  NodeJS Version _No response_  Relevant log output _No response_ other-file source-file documentation-file test-file other-file other-file documentation-file documentation-file documentation-file source-file source-file database-file database-file database-file database-file container-file container-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file",no-bug,0.9
5087,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5087,Data too long for column `info_json` when monitoring cloudfunctions.net Endpoints," I have found these related issues/pull requests -   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When monitoring Google cloudfunctions `ER_DATA_TOO_LONG` errors are thrown in the logs and no Certificate Expiry dates are shown.  Reproduction steps Any fake cloudfunction url does the trick for testing even though it returns 404 - Like: https://abc.cloudfunctions.net/  Expected behavior No errors in the logs / Certificate Expiry etc. are displayed correctly. Considering that the JSON it's trying to save has a significant amount of characters it probably makes sense to strip some infos from it..?  Actual Behavior No certificate expiry date is visible / errors are shown in logs  Uptime-Kuma Version 2.0.0-dev  Operating System and Arch Docker  Browser Google Chrome 128.0.6613.114   Deployment Environment - Runtime: Docker - Database: MariaDB 11.4.3 - Filesystem used to store the database on: HDD - number of monitors: 5  Relevant log output shell uptime-kuma | code: 'ER_DATA_TOO_LONG', uptime-kuma | errno: 1406, uptime-kuma | sqlState: '22001', uptime-kuma | sqlMessage: ""Data too long for column 'info_json' at row 1"", uptime-kuma | sql: 'insert into `monitor_tls_info` (`info_json`, `monitor_id`) values (\'{\\""valid\\"":true,\\""certInfo\\"":{\\""subject\\"":{\\""CN\\"":\\""misc.google.com\\""},\\""issuer\\"":{\\""C\\"":\\""US\\"",\\""O\\"":\\""Google Trust Services\\"",\\""CN\\"":\\""WR2\\""},\\""subjectaltname\\"":\\""DNS:misc.google.com, DNS:*.actions.google.com,  :EC:' 60832 more characters uptime-kuma | } ",source-file | source-file | source-file | source-file,"Data too long for column `info_json` when monitoring cloudfunctions.net Endpoints  I have found these related issues/pull requests -   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When monitoring Google cloudfunctions `ER_DATA_TOO_LONG` errors are thrown in the logs and no Certificate Expiry dates are shown.  Reproduction steps Any fake cloudfunction url does the trick for testing even though it returns 404 - Like: https://abc.cloudfunctions.net/  Expected behavior No errors in the logs / Certificate Expiry etc. are displayed correctly. Considering that the JSON it's trying to save has a significant amount of characters it probably makes sense to strip some infos from it..?  Actual Behavior No certificate expiry date is visible / errors are shown in logs  Uptime-Kuma Version 2.0.0-dev  Operating System and Arch Docker  Browser Google Chrome 128.0.6613.114   Deployment Environment - Runtime: Docker - Database: MariaDB 11.4.3 - Filesystem used to store the database on: HDD - number of monitors: 5  Relevant log output shell uptime-kuma | code: 'ER_DATA_TOO_LONG', uptime-kuma | errno: 1406, uptime-kuma | sqlState: '22001', uptime-kuma | sqlMessage: ""Data too long for column 'info_json' at row 1"", uptime-kuma | sql: 'insert into `monitor_tls_info` (`info_json`, `monitor_id`) values (\'{\\""valid\\"":true,\\""certInfo\\"":{\\""subject\\"":{\\""CN\\"":\\""misc.google.com\\""},\\""issuer\\"":{\\""C\\"":\\""US\\"",\\""O\\"":\\""Google Trust Services\\"",\\""CN\\"":\\""WR2\\""},\\""subjectaltname\\"":\\""DNS:misc.google.com, DNS:*.actions.google.com,  :EC:' 60832 more characters uptime-kuma | }  source-file source-file source-file source-file",no-bug,0.9
2777,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2777,Setting an empty postgresql query results in a crash,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I have decided to monitor my postgresql cluster. I have setup a monitor, but forgot to add a query. This makes uptime-kuma crash immiediately, and I can't get it back up <img width=""623"" alt=""Screenshot 2023-02-14 at 10 40 06"" src=""https://user-images.githubusercontent.com/50323052/218697394-0cc26141-bfb4-4f33-9bac-cd8002b1f51f.png"">  Reproduction steps 1. Setup a postgresql monitor 2. Add hostname, username, password 3. Do not add a query 4. Save  Expected behavior I would hope at least for an error message, or a non-working monitor  Actual Behavior Crashes  Uptime-Kuma Version uptime-kuma:1.20.0-alpine  Operating System and Arch Ubuntu 22.04  Browser Firefox 109.1  Docker Version k3s v1.26.1+k3s1  NodeJS Version _No response_  Relevant log output shell /app/node_modules/pg/lib/client.js:508 throw new TypeError('Client was passed a null or undefined query') ^ TypeError: Client was passed a null or undefined query at Client.query (/app/node_modules/pg/lib/client.js:508:13) at Client._connectionCallback (/app/server/util-server.js:295:24) at Client._handleReadyForQuery (/app/node_modules/pg/lib/client.js:279:14) at Connection.emit (node:events:390:28) at /app/node_modules/pg/lib/connection.js:114:12 at Parser.parse (/app/node_modules/pg-protocol/dist/parser.js:40:17) at Socket.<anonymous> (/app/node_modules/pg-protocol/dist/index.js:11:42) at Socket.emit (node:events:390:28) at addChunk (node:internal/streams/readable:315:12) at readableAddChunk (node:internal/streams/readable:289:9) ",source-file,"Setting an empty postgresql query results in a crash   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description I have decided to monitor my postgresql cluster. I have setup a monitor, but forgot to add a query. This makes uptime-kuma crash immiediately, and I can't get it back up <img width=""623"" alt=""Screenshot 2023-02-14 at 10 40 06"" src=""https://user-images.githubusercontent.com/50323052/218697394-0cc26141-bfb4-4f33-9bac-cd8002b1f51f.png"">  Reproduction steps 1. Setup a postgresql monitor 2. Add hostname, username, password 3. Do not add a query 4. Save  Expected behavior I would hope at least for an error message, or a non-working monitor  Actual Behavior Crashes  Uptime-Kuma Version uptime-kuma:1.20.0-alpine  Operating System and Arch Ubuntu 22.04  Browser Firefox 109.1  Docker Version k3s v1.26.1+k3s1  NodeJS Version _No response_  Relevant log output shell /app/node_modules/pg/lib/client.js:508 throw new TypeError('Client was passed a null or undefined query') ^ TypeError: Client was passed a null or undefined query at Client.query (/app/node_modules/pg/lib/client.js:508:13) at Client._connectionCallback (/app/server/util-server.js:295:24) at Client._handleReadyForQuery (/app/node_modules/pg/lib/client.js:279:14) at Connection.emit (node:events:390:28) at /app/node_modules/pg/lib/connection.js:114:12 at Parser.parse (/app/node_modules/pg-protocol/dist/parser.js:40:17) at Socket.<anonymous> (/app/node_modules/pg-protocol/dist/index.js:11:42) at Socket.emit (node:events:390:28) at addChunk (node:internal/streams/readable:315:12) at readableAddChunk (node:internal/streams/readable:289:9)  source-file",no-bug,0.8
1658,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1658,Not possible to add new monitors to editing status page,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Hi guys, I have created some monitor and then I have created a status page. I could be able to add monitors created to status page successfully. However, I have added some additional monitors. Therefore, I wanted to add those new monitors to the status page. So I click edit status page button, but I couldn't be able to add the new monitors.  Reproduction steps - Add X monitors - Create a status page - Add monitors available to status page - Save changes - After create X additional monitors. - Then, come back to status page previously created and click to edit status page button. - No monitors available. Add one (this message is showed). Despite that I created new monitors. ![image](https://user-images.githubusercontent.com/13409704/168828290-5507cf8c-5f75-4b00-b2d9-e3cf2b8c1e64.png)  Expected behavior Be able to add additional monitors to status page to editing. There is a workaround that consists to create again the status page. But is not very productive.  Actual Behavior Is not possible to add new monitors (previously created to editing status page)  Uptime-Kuma Version 1.15.1  Operating System and Arch Mac OS big Sur  Browser Chrome Versin 85.0.4183.83 (Build oficial) (64 bits)  Docker Version 20.10.6  NodeJS Version _No response_  Relevant log output _No response_",container-file | documentation-file,"Not possible to add new monitors to editing status page   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Hi guys, I have created some monitor and then I have created a status page. I could be able to add monitors created to status page successfully. However, I have added some additional monitors. Therefore, I wanted to add those new monitors to the status page. So I click edit status page button, but I couldn't be able to add the new monitors.  Reproduction steps - Add X monitors - Create a status page - Add monitors available to status page - Save changes - After create X additional monitors. - Then, come back to status page previously created and click to edit status page button. - No monitors available. Add one (this message is showed). Despite that I created new monitors. ![image](https://user-images.githubusercontent.com/13409704/168828290-5507cf8c-5f75-4b00-b2d9-e3cf2b8c1e64.png)  Expected behavior Be able to add additional monitors to status page to editing. There is a workaround that consists to create again the status page. But is not very productive.  Actual Behavior Is not possible to add new monitors (previously created to editing status page)  Uptime-Kuma Version 1.15.1  Operating System and Arch Mac OS big Sur  Browser Chrome Versin 85.0.4183.83 (Build oficial) (64 bits)  Docker Version 20.10.6  NodeJS Version _No response_  Relevant log output _No response_ container-file documentation-file",no-bug,0.9
5323,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5323,[Toast] useToast not called properly in pages/EditMonitor.vue," I have found these related issues/pull requests No similar bugs have been reported   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Saving the monitor config not working in monitor editing page. Look into dev console, TypeError: Iu.error is not a function at Proxy.isInputValid (index-qm3xr2Qi.js:579:4928) at Proxy.submit  Changing `const toast = useToast;` to `const toast = useToast();` in src/pages/EditMonitor.vue can fix it.  Reproduction steps - Set any custom request body to a HTTP monitor - Save (success) - Change HTTP to TCP Port - Save (failed, there should no be any header)  Expected behavior A toast shows to tell what is going wrong. And user can edit continuing.  Actual Behavior No toast shown and unable to edit.  Uptime-Kuma Version 2.0.0-beta.0  Operating System and Arch Ubuntu 2404 x64  Browser Microsoft Edge 124.0.2478.80 x64   Deployment Environment - Runtime: nodejs 18 - Database: default sqlite - Filesystem used to store the database on: ext4 - number of monitors: 20+  Relevant log output _No response_",other-file | other-file,"[Toast] useToast not called properly in pages/EditMonitor.vue  I have found these related issues/pull requests No similar bugs have been reported   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description Saving the monitor config not working in monitor editing page. Look into dev console, TypeError: Iu.error is not a function at Proxy.isInputValid (index-qm3xr2Qi.js:579:4928) at Proxy.submit  Changing `const toast = useToast;` to `const toast = useToast();` in src/pages/EditMonitor.vue can fix it.  Reproduction steps - Set any custom request body to a HTTP monitor - Save (success) - Change HTTP to TCP Port - Save (failed, there should no be any header)  Expected behavior A toast shows to tell what is going wrong. And user can edit continuing.  Actual Behavior No toast shown and unable to edit.  Uptime-Kuma Version 2.0.0-beta.0  Operating System and Arch Ubuntu 2404 x64  Browser Microsoft Edge 124.0.2478.80 x64   Deployment Environment - Runtime: nodejs 18 - Database: default sqlite - Filesystem used to store the database on: ext4 - number of monitors: 20+  Relevant log output _No response_ other-file other-file",no-bug,0.9
570,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/570,Encoding problem of ping output on non-English Windows,"got 2 question  1. when send message to slack , can't show the correct message  2. slack message show the UTC time , not the time of time zone setting ![image](https://user-images.githubusercontent.com/12062341/136137404-c8484b68-3f14-47d4-bab7-be11d272f5c3.png)",documentation-file | documentation-file | source-file | source-file,"Encoding problem of ping output on non-English Windows got 2 question  1. when send message to slack , can't show the correct message  2. slack message show the UTC time , not the time of time zone setting ![image](https://user-images.githubusercontent.com/12062341/136137404-c8484b68-3f14-47d4-bab7-be11d272f5c3.png) documentation-file documentation-file source-file source-file",no-bug,0.7
1669,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1669,MQTT Monitor Type - Timeout,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem I am unable to monitor the status of an MQTT server. I specified the URL, Port, Username and Password but it keeps returning 'MQTT': Failing: Timeout | Interval: 60 seconds | Type: mqtt I don't know how to diagnose this problem. I searched existing issues and found nothing.  Uptime-Kuma Version 1.15.1  Operating System and Arch Debian Bullseye on Raspberry Pi 4 ARM64  Browser Google Chrome 100  Docker Version 20.10.16  NodeJS Version _No response_",source-file,"MQTT Monitor Type - Timeout   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Describe your problem I am unable to monitor the status of an MQTT server. I specified the URL, Port, Username and Password but it keeps returning 'MQTT': Failing: Timeout | Interval: 60 seconds | Type: mqtt I don't know how to diagnose this problem. I searched existing issues and found nothing.  Uptime-Kuma Version 1.15.1  Operating System and Arch Debian Bullseye on Raspberry Pi 4 ARM64  Browser Google Chrome 100  Docker Version 20.10.16  NodeJS Version _No response_ source-file",no-bug,0.9
5787,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/5787,Use CLI to reset passwords allows for the setting of weak passwords," I have found these related issues/pull requests I am unable to find any related issues/pull requests   Security Policy - [x] I have read and agree to Uptime Kuma's [Security Policy](https://github.com/louislam/uptime-kuma/security/policy).  Description When users **log in for the first time or reset their passwords on the web interface**, the system will check the password to ensure it is strong. This is designed to prevent the use of overly simplistic passwords. However, during password **reset via the CLI(npm runeset-password)**, this crucial validation step is skipped. As a result, **users are able to set weak passwords**, which presents a significant security vulnerability. In one of my test cases, I set the password as ""1"". Surprisingly, it bypassed all checks and the password was successfully reset, clearly demonstrating the existing flaw in the CLI password reset process.  root@7e7791690b13:/app# npm run reset-password > uptime-kuma@2.0.0-beta.2-nightly-20250419062800 reset-password > node extra/reset-password.js == Uptime Kuma Reset Password Tool == Connecting the database 2025-04-20T07:44:41+00:00 [SERVER] INFO: Data Dir: ./data/ 2025-04-20T07:44:41+00:00 [DB] INFO: Database Type: sqlite Found user: happy New Password: 1 Confirm New Password: 1 Connecting to ws://localhost:3001 to disconnect all other socket clients Logged in. Password reset successfully. 2025-04-20T07:44:44+00:00 [DB] INFO: Closing the database 2025-04-20T07:44:46+00:00 [DB] INFO: Database closed Finished.  Furthermore, for enhanced security and privacy, I recommend that user - entered passwords be masked and displayed as * instead of showing the actual characters.  Reproduction steps Run `npm run reset-password` and set a weak password  Expected behavior When a weak password is entered, users should be prompted and a failure feedback should be provided.  Actual Behavior Password reset successfully.  Uptime-Kuma Version uptime-kuma@2.0.0-beta.2-nightly-20250419062800  Operating System and Arch Ubuntu 22.04  Browser Microsoft Edge135.0.3179.73   Deployment Environment - **Runtime Environment**: - Docker: `27.5.1-1`, build `9f9e4058019a37304dc6572ffcbb409d529b59d8` - Docker Compose: Version `v2.34.0` - **Database**: - SQLite: Embedded - **Database Storage**: - **Filesystem**: ext4  Relevant log output bash session root@7e7791690b13:/app# npm run reset-password > uptime-kuma@2.0.0-beta.2-nightly-20250419062800 reset-password > node extra/reset-password.js == Uptime Kuma Reset Password Tool == Connecting the database 2025-04-20T07:44:41+00:00 [SERVER] INFO: Data Dir: ./data/ 2025-04-20T07:44:41+00:00 [DB] INFO: Database Type: sqlite Found user: happy New Password: 1 Confirm New Password: 1 Connecting to ws://localhost:3001 to disconnect all other socket clients Logged in. Password reset successfully. 2025-04-20T07:44:44+00:00 [DB] INFO: Closing the database 2025-04-20T07:44:46+00:00 [DB] INFO: Database closed Finished. ",source-file | source-file | source-file,"Use CLI to reset passwords allows for the setting of weak passwords  I have found these related issues/pull requests I am unable to find any related issues/pull requests   Security Policy - [x] I have read and agree to Uptime Kuma's [Security Policy](https://github.com/louislam/uptime-kuma/security/policy).  Description When users **log in for the first time or reset their passwords on the web interface**, the system will check the password to ensure it is strong. This is designed to prevent the use of overly simplistic passwords. However, during password **reset via the CLI(npm runeset-password)**, this crucial validation step is skipped. As a result, **users are able to set weak passwords**, which presents a significant security vulnerability. In one of my test cases, I set the password as ""1"". Surprisingly, it bypassed all checks and the password was successfully reset, clearly demonstrating the existing flaw in the CLI password reset process.  root@7e7791690b13:/app# npm run reset-password > uptime-kuma@2.0.0-beta.2-nightly-20250419062800 reset-password > node extra/reset-password.js == Uptime Kuma Reset Password Tool == Connecting the database 2025-04-20T07:44:41+00:00 [SERVER] INFO: Data Dir: ./data/ 2025-04-20T07:44:41+00:00 [DB] INFO: Database Type: sqlite Found user: happy New Password: 1 Confirm New Password: 1 Connecting to ws://localhost:3001 to disconnect all other socket clients Logged in. Password reset successfully. 2025-04-20T07:44:44+00:00 [DB] INFO: Closing the database 2025-04-20T07:44:46+00:00 [DB] INFO: Database closed Finished.  Furthermore, for enhanced security and privacy, I recommend that user - entered passwords be masked and displayed as * instead of showing the actual characters.  Reproduction steps Run `npm run reset-password` and set a weak password  Expected behavior When a weak password is entered, users should be prompted and a failure feedback should be provided.  Actual Behavior Password reset successfully.  Uptime-Kuma Version uptime-kuma@2.0.0-beta.2-nightly-20250419062800  Operating System and Arch Ubuntu 22.04  Browser Microsoft Edge135.0.3179.73   Deployment Environment - **Runtime Environment**: - Docker: `27.5.1-1`, build `9f9e4058019a37304dc6572ffcbb409d529b59d8` - Docker Compose: Version `v2.34.0` - **Database**: - SQLite: Embedded - **Database Storage**: - **Filesystem**: ext4  Relevant log output bash session root@7e7791690b13:/app# npm run reset-password > uptime-kuma@2.0.0-beta.2-nightly-20250419062800 reset-password > node extra/reset-password.js == Uptime Kuma Reset Password Tool == Connecting the database 2025-04-20T07:44:41+00:00 [SERVER] INFO: Data Dir: ./data/ 2025-04-20T07:44:41+00:00 [DB] INFO: Database Type: sqlite Found user: happy New Password: 1 Confirm New Password: 1 Connecting to ws://localhost:3001 to disconnect all other socket clients Logged in. Password reset successfully. 2025-04-20T07:44:44+00:00 [DB] INFO: Closing the database 2025-04-20T07:44:46+00:00 [DB] INFO: Database closed Finished.  source-file source-file source-file",no-bug,0.9
279,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/279,Push-based monitoring,"**Is it a duplicate question?** Not that I'm aware (after searching ""push"", ""push-based"", and ""healthchecks). **Is your feature request related to a problem? Please describe.** Nope **Describe the solution you'd like** A push-based system where computers (potentially behind NATs) can ping uptime-kuma to indicate that it's up. This allows for arbitrary checks (can run any command and report its status). One project that implements this is [healthchecks.io](https://github.com/healthchecks/healthchecks). It would be awesome if we can have integration with Healthchecks.io. **Describe alternatives you've considered** [healthchecks.io](https://github.com/healthchecks/healthchecks). Though there's no readily available dashboard implementations that show historical uptime. **Additional context** N/A",database-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | other-file | source-file | source-file | source-file,"Push-based monitoring **Is it a duplicate question?** Not that I'm aware (after searching ""push"", ""push-based"", and ""healthchecks). **Is your feature request related to a problem? Please describe.** Nope **Describe the solution you'd like** A push-based system where computers (potentially behind NATs) can ping uptime-kuma to indicate that it's up. This allows for arbitrary checks (can run any command and report its status). One project that implements this is [healthchecks.io](https://github.com/healthchecks/healthchecks). It would be awesome if we can have integration with Healthchecks.io. **Describe alternatives you've considered** [healthchecks.io](https://github.com/healthchecks/healthchecks). Though there's no readily available dashboard implementations that show historical uptime. **Additional context** N/A database-file source-file source-file source-file source-file source-file other-file other-file source-file source-file other-file source-file source-file source-file",no-bug,0.95
1520,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1520,Crashed during / after cleanup,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description _No response_  Reproduction steps At 3:14am, it looks like the 'clear-old-data' job happened. Immediately after an error was thrown and the container stopped. I did not have 'restart=unless-stopped' on so it was down until 9:11pm when I noticed and restarted the container.  Expected behavior That a crash does not occur  Actual Behavior Crash occurred that appears to be related to 'clear-old-data' job.  Uptime-Kuma Version 1.14.0  Operating System and Arch Docker on Unraid  Browser Firefox  Docker Version 20.10.5  NodeJS Version _No response_  Relevant log output shell today at 3:13:46 AMMonitor #22 'unifi': Successful Response: 22 ms | Interval: 60 seconds | Type: http today at 3:14:00 AMWorker for job ""clear-old-data"" online undefined today at 3:14:00 AMData Dir: data/ today at 3:14:00 AM(node:7) Warning: Accessing non-existent property 'Proxy' of module exports inside circular dependency today at 3:14:00 AM(Use `node --trace-warnings ` to show where the warning was created) today at 3:14:00 AMWelcome to Uptime Kuma today at 3:14:00 AMYour Node.js version: 16 today at 3:14:00 AMNode Env: production today at 3:14:00 AMImporting Node libraries today at 3:14:00 AMImporting 3rd-party libraries today at 3:14:00 AMImporting this project modules today at 3:14:00 AMPrepare Notification Providers today at 3:14:00 AMVersion: 1.14.0 today at 3:14:00 AMCreating express and socket.io instance today at 3:14:00 AMServer Type: HTTP today at 3:14:00 AMData Dir: ./data/ today at 3:14:00 AMConnecting to the Database today at 3:14:00 AM./server/model/monitor.js is not a valid BeanModel, skipped today at 3:14:00 AM(node:7) Warning: Accessing non-existent property 'Proxy' of module exports inside circular dependency today at 3:14:00 AM(node:7) Warning: Accessing non-existent property 'prototype' of module exports inside circular dependency today at 3:14:00 AMSQLite config: today at 3:14:00 AMSQLite config: today at 3:14:00 AM[ { journal_mode: 'wal' } ] today at 3:14:00 AM[ { journal_mode: 'wal' } ] today at 3:14:00 AM[ { cache_size: -12000 } ] today at 3:14:00 AM[ { cache_size: -12000 } ] today at 3:14:00 AMSQLite Version: 3.36.0 today at 3:14:00 AMConnected today at 3:14:00 AMSQLite Version: 3.36.0 today at 3:14:00 AMYour database version: 10 today at 3:14:00 AMLatest database version: 10 today at 3:14:00 AMDatabase patch not needed today at 3:14:00 AMDatabase Patch 2.0 Process today at 3:14:00 AM[Background Job]: { today at 3:14:00 AM name: 'clear-old-data', today at 3:14:00 AM message: 'Clearing Data older than 30 days' today at 3:14:00 AM} today at 3:14:00 AM[Background Job]: { name: 'clear-old-data', message: 'done' } today at 3:14:00 AMFATAL ERROR: Error::ThrowAsJavaScriptException napi_throw today at 3:14:00 AM 1: 0xb09980 node::Abort() [node] today at 3:14:00 AM 2: 0xa1c235 node::FatalError(char const*, char const*) [node] today at 3:14:00 AM 3: 0xa1c23e [node] today at 3:14:00 AM 4: 0xad139b napi_fatal_error [node] today at 3:14:00 AM 5: 0x1457dde7c556 [/app/node_modules/@louislam/sqlite3/lib/binding/napi-v6-linux-x64/node_sqlite3.node] today at 3:14:00 AM 6: 0x1457dde7c9d8 Napi::Error::ThrowAsJavaScriptException() const [/app/node_modules/@louislam/sqlite3/lib/binding/napi-v6-linux-x64/node_sqlite3.node] today at 3:14:00 AM 7: 0x1457dde916ae node_sqlite3::Statement::Statement(Napi::CallbackInfo const&) [/app/node_modules/@louislam/sqlite3/lib/binding/napi-v6-linux-x64/node_sqlite3.node] today at 3:14:00 AM 8: 0x1457dde96885 Napi::ObjectWrap<node_sqlite3::Statement>::ConstructorCallbackWrapper(napi_env__*, napi_callback_info__*) [/app/node_modules/@louislam/sqlite3/lib/binding/napi-v6-linux-x64/node_sqlite3.node] today at 3:14:00 AM 9: 0xab44dd [node] today at 3:14:00 AM10: 0xd54c3e [node] today at 3:14:00 AM11: 0xd55207 v8::internal::Builtin_HandleApiCall(int, unsigned long*, v8::internal::Isolate*) [node] today at 3:14:00 AM12: 0x15f0bf9 [node] today at 9:11:35 PMContainer stopped ",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | documentation-file | test-file | other-file | other-file | documentation-file | documentation-file | documentation-file | source-file | source-file | database-file | database-file | database-file | database-file | container-file | container-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file,"Crashed during / after cleanup   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description _No response_  Reproduction steps At 3:14am, it looks like the 'clear-old-data' job happened. Immediately after an error was thrown and the container stopped. I did not have 'restart=unless-stopped' on so it was down until 9:11pm when I noticed and restarted the container.  Expected behavior That a crash does not occur  Actual Behavior Crash occurred that appears to be related to 'clear-old-data' job.  Uptime-Kuma Version 1.14.0  Operating System and Arch Docker on Unraid  Browser Firefox  Docker Version 20.10.5  NodeJS Version _No response_  Relevant log output shell today at 3:13:46 AMMonitor #22 'unifi': Successful Response: 22 ms | Interval: 60 seconds | Type: http today at 3:14:00 AMWorker for job ""clear-old-data"" online undefined today at 3:14:00 AMData Dir: data/ today at 3:14:00 AM(node:7) Warning: Accessing non-existent property 'Proxy' of module exports inside circular dependency today at 3:14:00 AM(Use `node --trace-warnings ` to show where the warning was created) today at 3:14:00 AMWelcome to Uptime Kuma today at 3:14:00 AMYour Node.js version: 16 today at 3:14:00 AMNode Env: production today at 3:14:00 AMImporting Node libraries today at 3:14:00 AMImporting 3rd-party libraries today at 3:14:00 AMImporting this project modules today at 3:14:00 AMPrepare Notification Providers today at 3:14:00 AMVersion: 1.14.0 today at 3:14:00 AMCreating express and socket.io instance today at 3:14:00 AMServer Type: HTTP today at 3:14:00 AMData Dir: ./data/ today at 3:14:00 AMConnecting to the Database today at 3:14:00 AM./server/model/monitor.js is not a valid BeanModel, skipped today at 3:14:00 AM(node:7) Warning: Accessing non-existent property 'Proxy' of module exports inside circular dependency today at 3:14:00 AM(node:7) Warning: Accessing non-existent property 'prototype' of module exports inside circular dependency today at 3:14:00 AMSQLite config: today at 3:14:00 AMSQLite config: today at 3:14:00 AM[ { journal_mode: 'wal' } ] today at 3:14:00 AM[ { journal_mode: 'wal' } ] today at 3:14:00 AM[ { cache_size: -12000 } ] today at 3:14:00 AM[ { cache_size: -12000 } ] today at 3:14:00 AMSQLite Version: 3.36.0 today at 3:14:00 AMConnected today at 3:14:00 AMSQLite Version: 3.36.0 today at 3:14:00 AMYour database version: 10 today at 3:14:00 AMLatest database version: 10 today at 3:14:00 AMDatabase patch not needed today at 3:14:00 AMDatabase Patch 2.0 Process today at 3:14:00 AM[Background Job]: { today at 3:14:00 AM name: 'clear-old-data', today at 3:14:00 AM message: 'Clearing Data older than 30 days' today at 3:14:00 AM} today at 3:14:00 AM[Background Job]: { name: 'clear-old-data', message: 'done' } today at 3:14:00 AMFATAL ERROR: Error::ThrowAsJavaScriptException napi_throw today at 3:14:00 AM 1: 0xb09980 node::Abort() [node] today at 3:14:00 AM 2: 0xa1c235 node::FatalError(char const*, char const*) [node] today at 3:14:00 AM 3: 0xa1c23e [node] today at 3:14:00 AM 4: 0xad139b napi_fatal_error [node] today at 3:14:00 AM 5: 0x1457dde7c556 [/app/node_modules/@louislam/sqlite3/lib/binding/napi-v6-linux-x64/node_sqlite3.node] today at 3:14:00 AM 6: 0x1457dde7c9d8 Napi::Error::ThrowAsJavaScriptException() const [/app/node_modules/@louislam/sqlite3/lib/binding/napi-v6-linux-x64/node_sqlite3.node] today at 3:14:00 AM 7: 0x1457dde916ae node_sqlite3::Statement::Statement(Napi::CallbackInfo const&) [/app/node_modules/@louislam/sqlite3/lib/binding/napi-v6-linux-x64/node_sqlite3.node] today at 3:14:00 AM 8: 0x1457dde96885 Napi::ObjectWrap<node_sqlite3::Statement>::ConstructorCallbackWrapper(napi_env__*, napi_callback_info__*) [/app/node_modules/@louislam/sqlite3/lib/binding/napi-v6-linux-x64/node_sqlite3.node] today at 3:14:00 AM 9: 0xab44dd [node] today at 3:14:00 AM10: 0xd54c3e [node] today at 3:14:00 AM11: 0xd55207 v8::internal::Builtin_HandleApiCall(int, unsigned long*, v8::internal::Isolate*) [node] today at 3:14:00 AM12: 0x15f0bf9 [node] today at 9:11:35 PMContainer stopped  source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file documentation-file test-file other-file other-file documentation-file documentation-file documentation-file source-file source-file database-file database-file database-file database-file container-file container-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file",no-bug,0.9
2793,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2793,Send mail via the systems mail subsystem,  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description It would be great if UK would not only send mail via smtp but optional via the mail subsystem on the system it is installed at.   Solution Make use of system MTA to send mail to receiver   Alternatives _No response_  Additional Context _No response_,other-file | other-file | other-file | documentation-file,Send mail via the systems mail subsystem   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description It would be great if UK would not only send mail via smtp but optional via the mail subsystem on the system it is installed at.   Solution Make use of system MTA to send mail to receiver   Alternatives _No response_  Additional Context _No response_ other-file other-file other-file documentation-file,no-bug,0.95
2144,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2144,Octopush Notifier not working - simple fix?,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When attempting to create a new Octopush notification, even when using correct identifiers, the testing fails with a generic notification. ![Screen Shot 2022-09-30 at 7 32 57 PM](https://user-images.githubusercontent.com/1290033/193325282-5973e0ce-4786-458d-b9a2-b04297bf5c50.png) [Looking at the server-side code](https://github.com/louislam/uptime-kuma/blob/master/server/notification-providers/octopush.js) This is not due to an incorrect configuration, but rather notification.octopushVersion being set to something other than 1 or 2. I don't know Vue well enough to figure out if [this front-end code is correct or not](https://github.com/louislam/uptime-kuma/blob/master/src/components/notifications/Octopush.vue)  Reproduction steps On a new Uptime-Kuma instance, create a new Notification using Octopush. API Version: Either API Key: Any string ""Login"" Any string SMS Type: Either Telephone: Any numbers Then click the ""Test"" button -> Get error.  Expected behavior Expected Octopush to send or give a failure of credentials, etc.  Actual Behavior Generic message about incorrect Octopush version.  Uptime-Kuma Version 1.18.0  Operating System and Arch Mac OS Monterey 12.6  Browser Brave  Docker Version _No response_  NodeJS Version 16  Relevant log output shell uptime-kuma_1 | Error: Error: Error: Unknown Octopush version! uptime-kuma_1 | at Octopush.throwGeneralAxiosError (/app/server/notification-providers/notification-provider.js:38:15) uptime-kuma_1 | at Octopush.send (/app/server/notification-providers/octopush.js:59:18) uptime-kuma_1 | at Function.send (/app/server/notification.js:116:57) uptime-kuma_1 | at Socket.<anonymous> (/app/server/server.js:1149:46) uptime-kuma_1 | at Socket.emit (node:events:527:28) uptime-kuma_1 | at Socket.emitUntyped (/app/node_modules/socket.io/dist/typed-events.js:69:22) uptime-kuma_1 | at /app/node_modules/socket.io/dist/socket.js:466:39 uptime-kuma_1 | at processTicksAndRejections (node:internal/process/task_queues:78:11) ",source-file | source-file,"Octopush Notifier not working - simple fix?   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When attempting to create a new Octopush notification, even when using correct identifiers, the testing fails with a generic notification. ![Screen Shot 2022-09-30 at 7 32 57 PM](https://user-images.githubusercontent.com/1290033/193325282-5973e0ce-4786-458d-b9a2-b04297bf5c50.png) [Looking at the server-side code](https://github.com/louislam/uptime-kuma/blob/master/server/notification-providers/octopush.js) This is not due to an incorrect configuration, but rather notification.octopushVersion being set to something other than 1 or 2. I don't know Vue well enough to figure out if [this front-end code is correct or not](https://github.com/louislam/uptime-kuma/blob/master/src/components/notifications/Octopush.vue)  Reproduction steps On a new Uptime-Kuma instance, create a new Notification using Octopush. API Version: Either API Key: Any string ""Login"" Any string SMS Type: Either Telephone: Any numbers Then click the ""Test"" button -> Get error.  Expected behavior Expected Octopush to send or give a failure of credentials, etc.  Actual Behavior Generic message about incorrect Octopush version.  Uptime-Kuma Version 1.18.0  Operating System and Arch Mac OS Monterey 12.6  Browser Brave  Docker Version _No response_  NodeJS Version 16  Relevant log output shell uptime-kuma_1 | Error: Error: Error: Unknown Octopush version! uptime-kuma_1 | at Octopush.throwGeneralAxiosError (/app/server/notification-providers/notification-provider.js:38:15) uptime-kuma_1 | at Octopush.send (/app/server/notification-providers/octopush.js:59:18) uptime-kuma_1 | at Function.send (/app/server/notification.js:116:57) uptime-kuma_1 | at Socket.<anonymous> (/app/server/server.js:1149:46) uptime-kuma_1 | at Socket.emit (node:events:527:28) uptime-kuma_1 | at Socket.emitUntyped (/app/node_modules/socket.io/dist/typed-events.js:69:22) uptime-kuma_1 | at /app/node_modules/socket.io/dist/socket.js:466:39 uptime-kuma_1 | at processTicksAndRejections (node:internal/process/task_queues:78:11)  source-file source-file",no-bug,0.8
2593,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2593,[Improvement] Support TLS Expiry alerts also for CA certs in cert chain,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Currently only the first certificate within the certificate chain is checked and alerted for expiry but not the other (signing) certificates within the chain. When monitoring the certificate expiry days, not only the first (server) certificate should be evaluated but also all certificates within the certificate chain (each issuer certificate) and then alerted separately. This raises the awareness for sys admins for required ca-chain modifications/replacements.   Solution See provided pull request.   Alternatives _No response_  Additional Context Last november we had a situation where an intermediate CA certificate expired (QuoVadis Global SSL ICA G3) despite the fact that the signed server certificate still was valid for 10 more month. This is a rather unusual situation and even more complicated to analyze and understand.",source-file | source-file,"[Improvement] Support TLS Expiry alerts also for CA certs in cert chain   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Currently only the first certificate within the certificate chain is checked and alerted for expiry but not the other (signing) certificates within the chain. When monitoring the certificate expiry days, not only the first (server) certificate should be evaluated but also all certificates within the certificate chain (each issuer certificate) and then alerted separately. This raises the awareness for sys admins for required ca-chain modifications/replacements.   Solution See provided pull request.   Alternatives _No response_  Additional Context Last november we had a situation where an intermediate CA certificate expired (QuoVadis Global SSL ICA G3) despite the fact that the signed server certificate still was valid for 10 more month. This is a rather unusual situation and even more complicated to analyze and understand. source-file source-file",no-bug,0.95
1399,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1399,1.13.0-beta.0 - status page empty,  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When you create 2 or more status pages only one is saved by Uptime Kuma.  Reproduction steps 1. Create status page-1 and fill it with a few monitors and click save. 2. Create status page-2 and fill it with a few monitors and click save. 3. Go back to your dashboard and open page-1 4. Result: page-1 = empty.  Expected behavior Both pages should have monitors.  Actual Behavior   Uptime-Kuma Version 1.13.0-beta.0  Operating System and Arch Ubuntu 20.04  Browser Chrome  Docker Version Docker version 20.10.13  NodeJS Version _No response_  Relevant log output _No response_,source-file,1.13.0-beta.0 - status page empty   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description When you create 2 or more status pages only one is saved by Uptime Kuma.  Reproduction steps 1. Create status page-1 and fill it with a few monitors and click save. 2. Create status page-2 and fill it with a few monitors and click save. 3. Go back to your dashboard and open page-1 4. Result: page-1 = empty.  Expected behavior Both pages should have monitors.  Actual Behavior   Uptime-Kuma Version 1.13.0-beta.0  Operating System and Arch Ubuntu 20.04  Browser Chrome  Docker Version Docker version 20.10.13  NodeJS Version _No response_  Relevant log output _No response_ source-file,no-bug,0.8
226,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/226,`Cannot call a namespace ('dayjs')`,**Is it a duplicate question?** No **Describe the bug** Warnings during frontend build **To Reproduce** Steps to reproduce the behavior: 1. `npm run build` **Expected behavior** No warnings during build **Info** - Uptime Kuma Version: https://github.com/louislam/uptime-kuma/commit/440c178403bb1086b4caa1c868568a6f191928c1 - Using Docker?:  - OS: Windows - Browser:  **Screenshots**  **Error Log**  vite v2.4.4 building for production transforming (589) node_modules\vue-chart-3\node_modules\lodash\_hashClear.jsCannot call a namespace ('dayjs') Cannot call a namespace ('dayjs')  606 modules transformed. ,source-file | source-file,`Cannot call a namespace ('dayjs')` **Is it a duplicate question?** No **Describe the bug** Warnings during frontend build **To Reproduce** Steps to reproduce the behavior: 1. `npm run build` **Expected behavior** No warnings during build **Info** - Uptime Kuma Version: https://github.com/louislam/uptime-kuma/commit/440c178403bb1086b4caa1c868568a6f191928c1 - Using Docker?:  - OS: Windows - Browser:  **Screenshots**  **Error Log**  vite v2.4.4 building for production transforming (589) node_modules\vue-chart-3\node_modules\lodash\_hashClear.jsCannot call a namespace ('dayjs') Cannot call a namespace ('dayjs')  606 modules transformed.  source-file source-file,no-bug,0.9
50,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/50,Enhancement: Gotify priorities,"Thanks for releasing with the Gotify PR included so soon! Makes it easier a lot - I had used Webhook to NodeRed and from there relayed to my Gotify as a workaround in the meantime. As commented on the Commit already, I'd really love to see priorities being supported. I use different notification classes in the Gotify Android App. This way I could react on critical failures fast, while not getting woken up by minor things in the night (just an example ;)) Thx!",source-file | other-file,"Enhancement: Gotify priorities Thanks for releasing with the Gotify PR included so soon! Makes it easier a lot - I had used Webhook to NodeRed and from there relayed to my Gotify as a workaround in the meantime. As commented on the Commit already, I'd really love to see priorities being supported. I use different notification classes in the Gotify Android App. This way I could react on critical failures fast, while not getting woken up by minor things in the night (just an example ;)) Thx! source-file other-file",no-bug,0.95
3553,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/3553,Please show something more than a blank page when the user has JavaScript disabled,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description Please show something other than a blank page when the user has JavaScript disabled.   Solution At least some kind of `<noscript>` message that tells the user they need to enable JavaScript. Even better would be if the page could display at least some status information rendered server-side as well.   Alternatives _No response_  Additional Context The f-droid.org website is designed to work without JavaScript, as many of our users are very privacy/security-conscious. Ideally, our status page would also work without JavaScript, at least partially, or at the very least show some kind of message other than a blank screen.",other-file | other-file | other-file,"Please show something more than a blank page when the user has JavaScript disabled   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type UI Feature  Feature description Please show something other than a blank page when the user has JavaScript disabled.   Solution At least some kind of `<noscript>` message that tells the user they need to enable JavaScript. Even better would be if the page could display at least some status information rendered server-side as well.   Alternatives _No response_  Additional Context The f-droid.org website is designed to work without JavaScript, as many of our users are very privacy/security-conscious. Ideally, our status page would also work without JavaScript, at least partially, or at the very least show some kind of message other than a blank screen. other-file other-file other-file",no-bug,0.9
928,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/928,timetable not to do checks,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Hello, you are doing a very good job but I think that a new feature that give the option to configure down times where the checks should not be done.   Solution Have a time table where we can set the time where we do not want the monitor to notify a down.   Alternatives _No response_  Additional Context _No response_",database-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file,"timetable not to do checks   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description Hello, you are doing a very good job but I think that a new feature that give the option to configure down times where the checks should not be done.   Solution Have a time table where we can set the time where we do not want the monitor to notify a down.   Alternatives _No response_  Additional Context _No response_ database-file source-file source-file source-file source-file source-file source-file other-file",no-bug,0.95
2969,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2969,Push type monitor would get the same push URL after clone.,"  Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description After cloning a Push-type monitor, I got the same Push URL as the old one.  Reproduction steps 1. Create a Push-type monitor. 2. Clone it. 3. Check the Push URL of both the new and old monitors.  Expected behavior Generate a new Push URL during cloning.  Actual Behavior Got the same Push URL as the old one.  Uptime-Kuma Version 1.21.0  Operating System and Arch Debian 11  Browser Microsoft Edge 111.0.1661.51  Docker Version Docker 23.0.1  NodeJS Version _No response_  Relevant log output _No response_",other-file,"Push type monitor would get the same push URL after clone.   Please verify that this bug has NOT been raised before. - [X] I checked and didn't find similar issue   Security Policy - [X] I agree to have read this project [Security Policy](https://github.com/louislam/uptime-kuma/security/policy)  Description After cloning a Push-type monitor, I got the same Push URL as the old one.  Reproduction steps 1. Create a Push-type monitor. 2. Clone it. 3. Check the Push URL of both the new and old monitors.  Expected behavior Generate a new Push URL during cloning.  Actual Behavior Got the same Push URL as the old one.  Uptime-Kuma Version 1.21.0  Operating System and Arch Debian 11  Browser Microsoft Edge 111.0.1661.51  Docker Version Docker 23.0.1  NodeJS Version _No response_  Relevant log output _No response_ other-file",no-bug,0.9
2618,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/2618,Configurable number of packets send for tests,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description After the change in #2223 , the number of packets sent changed from 10 to 1, which in some cases could lead to false alerts.   Solution It could be interesting to have a setting for the number of packets sent for tests, the same as retries for example. If not, I think it may be better to set it back to 10 as it was.   Alternatives _No response_  Additional Context _No response_",source-file,"Configurable number of packets send for tests   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type Other  Feature description After the change in #2223 , the number of packets sent changed from 10 to 1, which in some cases could lead to false alerts.   Solution It could be interesting to have a setting for the number of packets sent for tests, the same as retries for example. If not, I think it may be better to set it back to 10 as it was.   Alternatives _No response_  Additional Context _No response_ source-file",no-bug,0.9
1891,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1891,Ping packet size,"  Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification  Feature description If devices only repond to pings from windows machines: Change ping packet size with -s option. Some devices I had only reply with ping size of 32 not 56 which linux uses.   Solution (I did this for my installation :) } else if (util.LIN) { this._bin = ""/bin/ping""; const defaultArgs = [ ""-n"", ""-s"", ""32"", ""-w"", timeout, ""-c"", ""1"", host ]; if (net.isIPv6(host) || options.ipv6) { defaultArgs.unshift(""-6""); } this._args = (options.args) ? options.args : defaultArgs; this._regmatch = /=([0-9.]+?) ms/;   Alternatives modding my own install  Additional Context rare",database-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | database-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | documentation-file | database-file | database-file | database-file | container-file | container-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | other-file | other-file | source-file | other-file | test-file | other-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | container-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | documentation-file | documentation-file | config-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | database-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | source-file | source-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | documentation-file | documentation-file | test-file | documentation-file | documentation-file | source-file | database-file | container-file | container-file | container-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | other-file | source-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | source-file | source-file | source-file | other-file | other-file | other-file | other-file | other-file | source-file | source-file | source-file | source-file | test-file | test-file | database-file | source-file | source-file | source-file | source-file | documentation-file | other-file,"Ping packet size   Please verify that this feature request has NOT been suggested before. - [X] I checked and didn't find similar feature request   Feature Request Type New Notification  Feature description If devices only repond to pings from windows machines: Change ping packet size with -s option. Some devices I had only reply with ping size of 32 not 56 which linux uses.   Solution (I did this for my installation :) } else if (util.LIN) { this._bin = ""/bin/ping""; const defaultArgs = [ ""-n"", ""-s"", ""32"", ""-w"", timeout, ""-c"", ""1"", host ]; if (net.isIPv6(host) || options.ipv6) { defaultArgs.unshift(""-6""); } this._args = (options.args) ? options.args : defaultArgs; this._regmatch = /=([0-9.]+?) ms/;   Alternatives modding my own install  Additional Context rare database-file source-file source-file source-file source-file source-file source-file other-file database-file source-file source-file source-file source-file source-file source-file other-file documentation-file database-file database-file database-file container-file container-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file source-file other-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file other-file other-file source-file other-file test-file other-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file container-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file other-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file documentation-file documentation-file config-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file database-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file source-file source-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file documentation-file documentation-file test-file documentation-file documentation-file source-file database-file container-file container-file container-file source-file source-file source-file source-file source-file source-file source-file source-file source-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file other-file other-file other-file other-file other-file other-file other-file other-file source-file other-file other-file other-file source-file source-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file source-file source-file source-file other-file other-file other-file other-file other-file source-file source-file source-file source-file test-file test-file database-file source-file source-file source-file source-file documentation-file other-file",no-bug,0.9
1,uptime-kuma,https://github.com/louislam/uptime-kuma/issues/1,Support TLS Expiry alerts,"This is an amazing project, I had it up and running within minutes of your post on Reddit, thank you so much! It would be awesome if you could add TLS monitoring alongside the ""uptime"" monitoring. Although [LetsEncrypt](https://letsencrypt.org) does a great job of auto-renewing certs, it would be great to monitor the following metrics as well: 1. Days until certificate expires 2. Whether the certificate is valid or invalid I've been bitten by this so many times in the past!",documentation-file | documentation-file | documentation-file | database-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file | other-file | source-file | other-file | other-file | other-file | other-file | other-file | source-file | other-file | other-file | source-file | source-file | source-file | other-file | source-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | documentation-file | database-file | documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | other-file,"Support TLS Expiry alerts This is an amazing project, I had it up and running within minutes of your post on Reddit, thank you so much! It would be awesome if you could add TLS monitoring alongside the ""uptime"" monitoring. Although [LetsEncrypt](https://letsencrypt.org) does a great job of auto-renewing certs, it would be great to monitor the following metrics as well: 1. Days until certificate expires 2. Whether the certificate is valid or invalid I've been bitten by this so many times in the past! documentation-file documentation-file documentation-file database-file documentation-file documentation-file source-file source-file source-file source-file source-file source-file other-file other-file source-file other-file other-file other-file other-file other-file source-file other-file other-file source-file source-file source-file other-file source-file documentation-file documentation-file documentation-file documentation-file documentation-file documentation-file database-file documentation-file documentation-file source-file source-file source-file source-file source-file other-file",no-bug,0.95
