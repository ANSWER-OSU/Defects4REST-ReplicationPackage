issue_no,repo,issue_url,title,description,patched_file_types,text_for_topic_modeling,prediction,confidence
65,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/65,Consumer read request hangs if you try to create two consumer instances with the same ID,"If you: 1. Create consumer with an ID 2. Read from a topic 3. Create consumer with same ID (don't delete previous one) 4. Read from topic the second create call succeeds and the read hangs. The stack trace shows it stuck trying to create a zookeeper node in a method that can also handle conflicts, which we'd expect since the IDs are the same. Turns out that we ended up conflating the meaning of ""ID"", which is used both to name the consumer nicely so you have readable consumer URLs and it got passed in as the ID for the underlying old Kafka consumer. The latter should really only be happening for debugging purposes since a random UUID (generated automatically for the consumer if the ID is omitted) is way more reliable. Things get a bit more confusing when we consider moving to the new consumer, which has a separate client ID that it uses to make logging on the brokers clearer and traceable to specific applications. A fix for this has to be careful about compatibility. I think the right thing to do is add a new parameter (""name"") which will be used in the URL. If the ""id"" is specified and ""name"" isn't, ""name"" will default to ""id"". The underlying Kafka ""consumer.id"" will only be set if ""id"" was explicitly set. Then, the name is only relevant to the single instance of the REST proxy since consumer URLs are specific to a single process. We can return a 409 if we see a naming conflict during creation, which will prevent the subsequent hang during read. Finally, we'll want to deprecate and deemphasize the ""id"" field in the docs since ""name"" will now be the field they really should be using.",documentation-file | documentation-file | documentation-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file,"Consumer read request hangs if you try to create two consumer instances with the same ID If you: 1. Create consumer with an ID 2. Read from a topic 3. Create consumer with same ID (don't delete previous one) 4. Read from topic the second create call succeeds and the read hangs. The stack trace shows it stuck trying to create a zookeeper node in a method that can also handle conflicts, which we'd expect since the IDs are the same. Turns out that we ended up conflating the meaning of ""ID"", which is used both to name the consumer nicely so you have readable consumer URLs and it got passed in as the ID for the underlying old Kafka consumer. The latter should really only be happening for debugging purposes since a random UUID (generated automatically for the consumer if the ID is omitted) is way more reliable. Things get a bit more confusing when we consider moving to the new consumer, which has a separate client ID that it uses to make logging on the brokers clearer and traceable to specific applications. A fix for this has to be careful about compatibility. I think the right thing to do is add a new parameter (""name"") which will be used in the URL. If the ""id"" is specified and ""name"" isn't, ""name"" will default to ""id"". The underlying Kafka ""consumer.id"" will only be set if ""id"" was explicitly set. Then, the name is only relevant to the single instance of the REST proxy since consumer URLs are specific to a single process. We can return a 409 if we see a naming conflict during creation, which will prevent the subsequent hang during read. Finally, we'll want to deprecate and deemphasize the ""id"" field in the docs since ""name"" will now be the field they really should be using. documentation-file documentation-file documentation-file source-file source-file source-file test-file test-file test-file test-file test-file",bug,0.9
118,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/118,Not possible to send keyed Avro messages via Proxy,"We tried different ways, like curl or [kafka-node-rest](https://github.com/confluentinc/kafka-rest-node), to send a Avro message to a topic with a key. The topic is created and schema submitted to schema registry, however, sending the message fails with a HTTP error code 500. Setting like ports etc. are all defaults for the platform. Here an example what we tried with kafka-node-rest: - create the schemata for key and value  javascript var idSchema = new KafkaRest.AvroSchema('string'); var valueSchema = new KafkaRest.AvroSchema( { 'namespace': 'mynamespace', 'type': 'record', 'name': 'myschema', 'fields': [ { 'name': 'key', 'type': 'string' }, { 'name': 'date', 'type': 'string' }, { 'name': 'title', 'type': 'string' } ] });  - send it  javascript topic.produce(idSchema, valueSchema, { 'key': link, 'value': { 'key': link, 'date': date, 'title': title } }, function(err, res) {  });  what throws an error.  javascript { name: 'APIError', status: 500, message: 'Internal Server Error', data: { error_code: 500, message: 'Internal Server Error' } }  - sending without a key works  javascript topic.produce(valueSchema, { 'key': link, 'date': date, 'title': title }, function(err, res) {  });  result:  javascript { offsets: [ { partition: 0, offset: 750, error_code: null, error: null } ], key_schema_id: null, value_schema_id: 121 }  However, with the **kafka-avro-console-producer** works fine  shell ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic rss \ --property parse.key=true \ --property key.separator=, \ --property key.schema='{""type"":""string""}' \ --property value.schema=""{\""type\"": \""record\"", \""name\"": \""rss\"", \""fields\"": [{\""name\"": \""key\"", \""type\"": \""string\""}, {\""name\"": \""title\"", \""type\"": \""string\""}, {\""name\"": \""content\"", \""type\"": \""string\""}, {\""name\"": \""date\"", \""type\"": \""string\""}]}"" ""test"",{""key"": ""http://example.com"", ""title"": ""test title"", ""content"": ""test content"", ""date"": ""2015-10-18'T'20:20:20""}  Any idea what could be wrong? Maybe this issue relates to [#114](https://github.com/confluentinc/kafka-rest/issues/114). Thanks in advance.",source-file | test-file,"Not possible to send keyed Avro messages via Proxy We tried different ways, like curl or [kafka-node-rest](https://github.com/confluentinc/kafka-rest-node), to send a Avro message to a topic with a key. The topic is created and schema submitted to schema registry, however, sending the message fails with a HTTP error code 500. Setting like ports etc. are all defaults for the platform. Here an example what we tried with kafka-node-rest: - create the schemata for key and value  javascript var idSchema = new KafkaRest.AvroSchema('string'); var valueSchema = new KafkaRest.AvroSchema( { 'namespace': 'mynamespace', 'type': 'record', 'name': 'myschema', 'fields': [ { 'name': 'key', 'type': 'string' }, { 'name': 'date', 'type': 'string' }, { 'name': 'title', 'type': 'string' } ] });  - send it  javascript topic.produce(idSchema, valueSchema, { 'key': link, 'value': { 'key': link, 'date': date, 'title': title } }, function(err, res) {  });  what throws an error.  javascript { name: 'APIError', status: 500, message: 'Internal Server Error', data: { error_code: 500, message: 'Internal Server Error' } }  - sending without a key works  javascript topic.produce(valueSchema, { 'key': link, 'date': date, 'title': title }, function(err, res) {  });  result:  javascript { offsets: [ { partition: 0, offset: 750, error_code: null, error: null } ], key_schema_id: null, value_schema_id: 121 }  However, with the **kafka-avro-console-producer** works fine  shell ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic rss \ --property parse.key=true \ --property key.separator=, \ --property key.schema='{""type"":""string""}' \ --property value.schema=""{\""type\"": \""record\"", \""name\"": \""rss\"", \""fields\"": [{\""name\"": \""key\"", \""type\"": \""string\""}, {\""name\"": \""title\"", \""type\"": \""string\""}, {\""name\"": \""content\"", \""type\"": \""string\""}, {\""name\"": \""date\"", \""type\"": \""string\""}]}"" ""test"",{""key"": ""http://example.com"", ""title"": ""test title"", ""content"": ""test content"", ""date"": ""2015-10-18'T'20:20:20""}  Any idea what could be wrong? Maybe this issue relates to [#114](https://github.com/confluentinc/kafka-rest/issues/114). Thanks in advance. source-file test-file",bug,0.9
37,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/37,Catch MessageStreamsExistException from ConsumerConnector.createMessageStreams,"When ConsumersResource.readTopic is invoked, it calls createMessageStreams if there isn't already a stream. However, if the consumer has already invoked the method for a different topic, it will throw a MessageStreamsExistException. Currently this isn't being caught, so it'll be turned into a 500. We should catch it and turn it into a more useful error code. This will also require updating the docs since it'll introduce a new error code.",documentation-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | documentation-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file,"Catch MessageStreamsExistException from ConsumerConnector.createMessageStreams When ConsumersResource.readTopic is invoked, it calls createMessageStreams if there isn't already a stream. However, if the consumer has already invoked the method for a different topic, it will throw a MessageStreamsExistException. Currently this isn't being caught, so it'll be turned into a 500. We should catch it and turn it into a more useful error code. This will also require updating the docs since it'll introduce a new error code. documentation-file source-file source-file source-file source-file source-file test-file test-file test-file documentation-file source-file source-file source-file source-file source-file test-file test-file test-file",bug,0.9
475,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/475,Kafka Rest API incorrect message returned by GET request for specified offset when data is compressed,"Test on a compressed topic vs a non-compressed topic. Should fail on compressed topic. GET /topics/myTopic/partitions/0/messages?offset=10&count=1 Tested using this call on the rest api using docker image version 4.1.0 and 5.0.0 and re-created on both. When requesting an offset between 0-8 it returns offset 0, when requesting from 9-149 it returns 9, when requesting from 150-265 it returns 150 etc Performing a /topics/myTopic/partitions/0/messages?offset=0&count=1000 shows that we have no offset gaps from 0 to 1000 Compressed format: ByteBufferMessageSet( LegacyRecordBatch(offset=8, Record(magic=1, attributes=1, compression=GZIP, crc=3359863988, CreateTime=1537196426232, key=0 bytes, value=703 bytes)), LegacyRecordBatch(offset=149, Record(magic=1, attributes=1, compression=GZIP, crc=712877603, CreateTime=1537196426317, key=0 bytes, value=6199 bytes)), Regular format: ByteBufferMessageSet( LegacyRecordBatch(offset=1, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=2, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=3, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)),",source-file,"Kafka Rest API incorrect message returned by GET request for specified offset when data is compressed Test on a compressed topic vs a non-compressed topic. Should fail on compressed topic. GET /topics/myTopic/partitions/0/messages?offset=10&count=1 Tested using this call on the rest api using docker image version 4.1.0 and 5.0.0 and re-created on both. When requesting an offset between 0-8 it returns offset 0, when requesting from 9-149 it returns 9, when requesting from 150-265 it returns 150 etc Performing a /topics/myTopic/partitions/0/messages?offset=0&count=1000 shows that we have no offset gaps from 0 to 1000 Compressed format: ByteBufferMessageSet( LegacyRecordBatch(offset=8, Record(magic=1, attributes=1, compression=GZIP, crc=3359863988, CreateTime=1537196426232, key=0 bytes, value=703 bytes)), LegacyRecordBatch(offset=149, Record(magic=1, attributes=1, compression=GZIP, crc=712877603, CreateTime=1537196426317, key=0 bytes, value=6199 bytes)), Regular format: ByteBufferMessageSet( LegacyRecordBatch(offset=1, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=2, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=3, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), source-file",bug,0.95
77,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/77,Get java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable when calling the /topics/:topic endpoint,"Whenever I call the topics/:topic endpoint I get the following error  java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable(Ljava/util/Collection;)Lscala/collection/Iterable; at io.confluent.kafkarest.MetadataObserver.getTopic(MetadataObserver.java:91) at io.confluent.kafkarest.resources.TopicsResource.getTopic(TopicsResource.java:70) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) (stack trace continues)  From some preliminary research it appear this could be due to differences between the compile time version of scala and the runtime. http://stackoverflow.com/questions/4393946/nosuchmethoderror-when-attempting-to-implicitly-convert-a-java-to-scala-collecti I'm running on Centos. Here's the output of java -version  java version ""1.8.0_05"" Java(TM) SE Runtime Environment (build 1.8.0_05-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)  I installed the package `confluent-platform-2.11.5`",other-file,"Get java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable when calling the /topics/:topic endpoint Whenever I call the topics/:topic endpoint I get the following error  java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable(Ljava/util/Collection;)Lscala/collection/Iterable; at io.confluent.kafkarest.MetadataObserver.getTopic(MetadataObserver.java:91) at io.confluent.kafkarest.resources.TopicsResource.getTopic(TopicsResource.java:70) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) (stack trace continues)  From some preliminary research it appear this could be due to differences between the compile time version of scala and the runtime. http://stackoverflow.com/questions/4393946/nosuchmethoderror-when-attempting-to-implicitly-convert-a-java-to-scala-collecti I'm running on Centos. Here's the output of java -version  java version ""1.8.0_05"" Java(TM) SE Runtime Environment (build 1.8.0_05-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)  I installed the package `confluent-platform-2.11.5` other-file",bug,0.8
341,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/341,Consumer read takes long time with multiple consumers in consumer group,"A fundamental issue is seen with REST Proxy. Suppose if we create two consumer instances under the same consumer group, read call is taking longer time with calls with subsequent instance. Steps to reproduce: 1. Create my_consumer_instance1 and my_consumer_instance2 under my_json_consumer1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance1"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance2"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 2. Subscribe both the instances to topic test1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/subscription curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/subscription 3. Read the message from my_consumer_instance1 several times and we will get responses back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records 4. Read the message from my_consumer_instance2 and the call will hang. It will take around 8 minutes to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/records 5. Again read the message from my_consumer_instance1 and the call will hang. It will take some time to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records",source-file | test-file | source-file | test-file | source-file | test-file,"Consumer read takes long time with multiple consumers in consumer group A fundamental issue is seen with REST Proxy. Suppose if we create two consumer instances under the same consumer group, read call is taking longer time with calls with subsequent instance. Steps to reproduce: 1. Create my_consumer_instance1 and my_consumer_instance2 under my_json_consumer1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance1"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance2"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 2. Subscribe both the instances to topic test1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/subscription curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/subscription 3. Read the message from my_consumer_instance1 several times and we will get responses back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records 4. Read the message from my_consumer_instance2 and the call will hang. It will take around 8 minutes to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/records 5. Again read the message from my_consumer_instance1 and the call will hang. It will take some time to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records source-file test-file source-file test-file source-file test-file",bug,0.9
