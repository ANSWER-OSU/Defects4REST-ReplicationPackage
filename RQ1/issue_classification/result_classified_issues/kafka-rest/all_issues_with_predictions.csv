issue_no,repo,issue_url,title,description,patched_file_types,text_for_topic_modeling,prediction,confidence
3,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/3,Lightweight admin APIs (create/delete/list/describe topics),,source-file | source-file | source-file | config-file,Lightweight admin APIs (create/delete/list/describe topics)  source-file source-file source-file config-file,no-bug,0.8
5,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/5,Add basic metrics,"We should be reporting basic metrics about the API server via JMX -- things like request throughput, latency, etc.",other-file | other-file | source-file | source-file | test-file | test-file | test-file | test-file,"Add basic metrics We should be reporting basic metrics about the API server via JMX -- things like request throughput, latency, etc. other-file other-file source-file source-file test-file test-file test-file test-file",no-bug,0.9
2,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/2,Performance testing,,config-file | source-file,Performance testing  config-file source-file,no-bug,0.9
65,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/65,Consumer read request hangs if you try to create two consumer instances with the same ID,"If you: 1. Create consumer with an ID 2. Read from a topic 3. Create consumer with same ID (don't delete previous one) 4. Read from topic the second create call succeeds and the read hangs. The stack trace shows it stuck trying to create a zookeeper node in a method that can also handle conflicts, which we'd expect since the IDs are the same. Turns out that we ended up conflating the meaning of ""ID"", which is used both to name the consumer nicely so you have readable consumer URLs and it got passed in as the ID for the underlying old Kafka consumer. The latter should really only be happening for debugging purposes since a random UUID (generated automatically for the consumer if the ID is omitted) is way more reliable. Things get a bit more confusing when we consider moving to the new consumer, which has a separate client ID that it uses to make logging on the brokers clearer and traceable to specific applications. A fix for this has to be careful about compatibility. I think the right thing to do is add a new parameter (""name"") which will be used in the URL. If the ""id"" is specified and ""name"" isn't, ""name"" will default to ""id"". The underlying Kafka ""consumer.id"" will only be set if ""id"" was explicitly set. Then, the name is only relevant to the single instance of the REST proxy since consumer URLs are specific to a single process. We can return a 409 if we see a naming conflict during creation, which will prevent the subsequent hang during read. Finally, we'll want to deprecate and deemphasize the ""id"" field in the docs since ""name"" will now be the field they really should be using.",documentation-file | documentation-file | documentation-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file,"Consumer read request hangs if you try to create two consumer instances with the same ID If you: 1. Create consumer with an ID 2. Read from a topic 3. Create consumer with same ID (don't delete previous one) 4. Read from topic the second create call succeeds and the read hangs. The stack trace shows it stuck trying to create a zookeeper node in a method that can also handle conflicts, which we'd expect since the IDs are the same. Turns out that we ended up conflating the meaning of ""ID"", which is used both to name the consumer nicely so you have readable consumer URLs and it got passed in as the ID for the underlying old Kafka consumer. The latter should really only be happening for debugging purposes since a random UUID (generated automatically for the consumer if the ID is omitted) is way more reliable. Things get a bit more confusing when we consider moving to the new consumer, which has a separate client ID that it uses to make logging on the brokers clearer and traceable to specific applications. A fix for this has to be careful about compatibility. I think the right thing to do is add a new parameter (""name"") which will be used in the URL. If the ""id"" is specified and ""name"" isn't, ""name"" will default to ""id"". The underlying Kafka ""consumer.id"" will only be set if ""id"" was explicitly set. Then, the name is only relevant to the single instance of the REST proxy since consumer URLs are specific to a single process. We can return a 409 if we see a naming conflict during creation, which will prevent the subsequent hang during read. Finally, we'll want to deprecate and deemphasize the ""id"" field in the docs since ""name"" will now be the field they really should be using. documentation-file documentation-file documentation-file source-file source-file source-file test-file test-file test-file test-file test-file",bug,0.9
26,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/26,Add support for SimpleConsumer-like access,"Sometimes apps might not want to be part of a consumer group, or might have trivial consumption patterns such as consuming from a single partition. We can probably expose this pretty cleanly with URLs like GET /topics/foo/partitions/0/messages?offset=100[&count=100], although specifying per-partition offsets at the topic level may get messy.",source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file,"Add support for SimpleConsumer-like access Sometimes apps might not want to be part of a consumer group, or might have trivial consumption patterns such as consuming from a single partition. We can probably expose this pretty cleanly with URLs like GET /topics/foo/partitions/0/messages?offset=100[&count=100], although specifying per-partition offsets at the topic level may get messy. source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file",no-bug,0.8
51,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/51,"Add support for compression, other global producer configs","Settings like compression can probably be configured globally just by exposing the producer config option as one of the REST proxy's config options and passing it along to both the binary and avro producers. The implementation should be trivial; the primary question is how we want to expose these options, e.g. by prefixing all the producer options with ""producer."" and forwarding everything, by allowing options to pass-through if they aren't processed by the KafkaRestConfig, or maybe some other approach.",documentation-file | source-file | source-file | source-file | source-file | test-file | test-file,"Add support for compression, other global producer configs Settings like compression can probably be configured globally just by exposing the producer config option as one of the REST proxy's config options and passing it along to both the binary and avro producers. The implementation should be trivial; the primary question is how we want to expose these options, e.g. by prefixing all the producer options with ""producer."" and forwarding everything, by allowing options to pass-through if they aren't processed by the KafkaRestConfig, or maybe some other approach. documentation-file source-file source-file source-file source-file test-file test-file",no-bug,0.9
95,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/95,Fix log4j issue in kafka-rest-run-class with equivalent fix from schema-registry,See this mailing list thread: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/confluent-platform/zchg-wbYCMU/N_VqMiZ8WroJ Should be a trivial port of changes already made in schema-registry.,other-file,Fix log4j issue in kafka-rest-run-class with equivalent fix from schema-registry See this mailing list thread: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/confluent-platform/zchg-wbYCMU/N_VqMiZ8WroJ Should be a trivial port of changes already made in schema-registry. other-file,no-bug,0.9
62,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/62,Reduce time required to run tests,"The tests take a long time to run. Some of this overhead is unavoidable with the integration tests. However, there's probably some low-hanging fruit in ClusterTestHarness that might make a big difference: - Default number of brokers is 3. This is probably overkill for almost every test. - Might be able to pull up/tear down brokers in parallel, although it would be worth checking how long they take to start up/tear down before trying to make the parallel version work. - Schema registry only needs to be started if we're going to use Avro. Only a couple of the tests need Avro support.",test-file | test-file | test-file | test-file | test-file | test-file,"Reduce time required to run tests The tests take a long time to run. Some of this overhead is unavoidable with the integration tests. However, there's probably some low-hanging fruit in ClusterTestHarness that might make a big difference: - Default number of brokers is 3. This is probably overkill for almost every test. - Might be able to pull up/tear down brokers in parallel, although it would be worth checking how long they take to start up/tear down before trying to make the parallel version work. - Schema registry only needs to be started if we're going to use Avro. Only a couple of the tests need Avro support. test-file test-file test-file test-file test-file test-file",no-bug,0.95
24,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/24,Improve handling of 404s for topics,"Currently all endpoints that will interact with a topic explicitly check for the topics existence. However, for some endpoints, e.g. producing messages, this doesn't produce the expected behavior with auto.topics.create.enable=true. In some cases it probably makes sense to check directly and avoid creation (e.g. GET /topic/foo) whereas others should handle any errors that are generated dynamically (e.g. POST /topic/foo with auto.topics.create.enable=false).",documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file,"Improve handling of 404s for topics Currently all endpoints that will interact with a topic explicitly check for the topics existence. However, for some endpoints, e.g. producing messages, this doesn't produce the expected behavior with auto.topics.create.enable=true. In some cases it probably makes sense to check directly and avoid creation (e.g. GET /topic/foo) whereas others should handle any errors that are generated dynamically (e.g. POST /topic/foo with auto.topics.create.enable=false). documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file",no-bug,0.7
30,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/30,Fix importance levels for configs,"Some of the importance levels for the configs aren't set right. For example, the mediatype is set to high, but should really be low since users generally shouldn't set this value -- it was set to high in rest-utils because any rest-utils application _should_ override it with a value specific for that application. The rest of the settings could also use review (and some in rest-utils may need updates as well).",documentation-file | source-file | documentation-file | source-file,"Fix importance levels for configs Some of the importance levels for the configs aren't set right. For example, the mediatype is set to high, but should really be low since users generally shouldn't set this value -- it was set to high in rest-utils because any rest-utils application _should_ override it with a value specific for that application. The rest of the settings could also use review (and some in rest-utils may need updates as well). documentation-file source-file documentation-file source-file",no-bug,0.8
91,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/91,Handle large number of schemas,"See related issue: confluentinc/schema-registry#172 When many requests come in for the same topic with the same schema and do not use the schema ID (e.g., the first request from many different clients), each one is registered via the serializer (see AvroRestProducer). However, because the serializer uses an IdentityHashMap, any cached schemas that are equivalent are not used, which results in a) an unnecessary HTTP request and b) a _different_ entry in the IdentityHashMap for the same schema. This is a) slower than necessary and b) eventually exhausts the number of entries allowed in the cache (`max.schemas.per.subject`). One solution to this problem if the schema-registry implementation isn't going to detect these identical schemas would be to do deduplication at the kafka-rest level. This is a case where deduplication via the more expensive equals() check is actually valid since the REST proxy is expected to end up with many different instances of the same schema.",source-file | test-file,"Handle large number of schemas See related issue: confluentinc/schema-registry#172 When many requests come in for the same topic with the same schema and do not use the schema ID (e.g., the first request from many different clients), each one is registered via the serializer (see AvroRestProducer). However, because the serializer uses an IdentityHashMap, any cached schemas that are equivalent are not used, which results in a) an unnecessary HTTP request and b) a _different_ entry in the IdentityHashMap for the same schema. This is a) slower than necessary and b) eventually exhausts the number of entries allowed in the cache (`max.schemas.per.subject`). One solution to this problem if the schema-registry implementation isn't going to detect these identical schemas would be to do deduplication at the kafka-rest level. This is a case where deduplication via the more expensive equals() check is actually valid since the REST proxy is expected to end up with many different instances of the same schema. source-file test-file",no-bug,0.85
1,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/1,Documentation,,documentation-file | source-file,Documentation  documentation-file source-file,no-bug,0.9
298,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/298,Map Basic Auth to Kafka client Auth,"For security reason, we are using Kafka with SASL_PLAIN and we did configure ACLs on topics to allow users to perform (or not) read/write operations. Currently our users can publish messages to theirs topics using a Kafka client but we want to give them the ability to publish their messages over HTTPS + Basic Auth. My idea was to create a ""dynamic"" pool. We could start with a one to one mapping (ie. HTTP credentials == Kafka client credentials) or we could implement an interface to have a fine grained mapping. The ""dynamic"" pool will need to create a new producer/consumer for every credentials. Let's take an example: Topic | ACL  |  topic_a | principal: user_a, operations: write<br>principal: admin, operations: write,read topic_b | principal: user_b, operations: write<br>principal: admin, operations: write,read `admin` can publish message on topic_a and topic_b  POST http://localhost:8082/topics/topic_a admin:password 200  (create a new producer with `admin:password` credentials)  POST http://localhost:8082/topics/topic_b admin:password 200  (reuse the producer with `admin:password` credentials) `user_b` cannot publish message on `topic_a`  POST http://localhost:8082/topics/topic_a user_b:password 403  (create a new producer with `user_b:password` credentials) `user_b` can publish message on `topic_b`  POST http://localhost:8082/topics/topic_b user_b:password 200  (reuse the producer with `user_b:password` credentials) This feature requires some changes and before working on it, I want to make sure that this is something you will consider :wink: Let me know what you think!",config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | config-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file,"Map Basic Auth to Kafka client Auth For security reason, we are using Kafka with SASL_PLAIN and we did configure ACLs on topics to allow users to perform (or not) read/write operations. Currently our users can publish messages to theirs topics using a Kafka client but we want to give them the ability to publish their messages over HTTPS + Basic Auth. My idea was to create a ""dynamic"" pool. We could start with a one to one mapping (ie. HTTP credentials == Kafka client credentials) or we could implement an interface to have a fine grained mapping. The ""dynamic"" pool will need to create a new producer/consumer for every credentials. Let's take an example: Topic | ACL  |  topic_a | principal: user_a, operations: write<br>principal: admin, operations: write,read topic_b | principal: user_b, operations: write<br>principal: admin, operations: write,read `admin` can publish message on topic_a and topic_b  POST http://localhost:8082/topics/topic_a admin:password 200  (create a new producer with `admin:password` credentials)  POST http://localhost:8082/topics/topic_b admin:password 200  (reuse the producer with `admin:password` credentials) `user_b` cannot publish message on `topic_a`  POST http://localhost:8082/topics/topic_a user_b:password 403  (create a new producer with `user_b:password` credentials) `user_b` can publish message on `topic_b`  POST http://localhost:8082/topics/topic_b user_b:password 200  (reuse the producer with `user_b:password` credentials) This feature requires some changes and before working on it, I want to make sure that this is something you will consider :wink: Let me know what you think! config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file config-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file config-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file",no-bug,0.9
34,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/34,Fix quickstart,"Reviewing the overall product docs, I think there needs to be a certain continuation to the individual product docs from the global docs. For example, after going through the Confluent quick start [http://y2zjnzc0owi5nmy2m2jkmzfjm2m0mmi1yzq3mwjmnzu2ode0.s3-website-us-west-2.amazonaws.com/docs/quickstart.html] that assumes you've started the service, I expect to just see the steps to try out some features/APIs. Specifically, let's separate the installation from the quick start. The quick start should only go through the APIs and not starting the service. The installation section should have the package install and service start including starting the dependent services.",documentation-file,"Fix quickstart Reviewing the overall product docs, I think there needs to be a certain continuation to the individual product docs from the global docs. For example, after going through the Confluent quick start [http://y2zjnzc0owi5nmy2m2jkmzfjm2m0mmi1yzq3mwjmnzu2ode0.s3-website-us-west-2.amazonaws.com/docs/quickstart.html] that assumes you've started the service, I expect to just see the steps to try out some features/APIs. Specifically, let's separate the installation from the quick start. The quick start should only go through the APIs and not starting the service. The installation section should have the package install and service start including starting the dependent services. documentation-file",no-bug,0.9
118,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/118,Not possible to send keyed Avro messages via Proxy,"We tried different ways, like curl or [kafka-node-rest](https://github.com/confluentinc/kafka-rest-node), to send a Avro message to a topic with a key. The topic is created and schema submitted to schema registry, however, sending the message fails with a HTTP error code 500. Setting like ports etc. are all defaults for the platform. Here an example what we tried with kafka-node-rest: - create the schemata for key and value  javascript var idSchema = new KafkaRest.AvroSchema('string'); var valueSchema = new KafkaRest.AvroSchema( { 'namespace': 'mynamespace', 'type': 'record', 'name': 'myschema', 'fields': [ { 'name': 'key', 'type': 'string' }, { 'name': 'date', 'type': 'string' }, { 'name': 'title', 'type': 'string' } ] });  - send it  javascript topic.produce(idSchema, valueSchema, { 'key': link, 'value': { 'key': link, 'date': date, 'title': title } }, function(err, res) {  });  what throws an error.  javascript { name: 'APIError', status: 500, message: 'Internal Server Error', data: { error_code: 500, message: 'Internal Server Error' } }  - sending without a key works  javascript topic.produce(valueSchema, { 'key': link, 'date': date, 'title': title }, function(err, res) {  });  result:  javascript { offsets: [ { partition: 0, offset: 750, error_code: null, error: null } ], key_schema_id: null, value_schema_id: 121 }  However, with the **kafka-avro-console-producer** works fine  shell ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic rss \ --property parse.key=true \ --property key.separator=, \ --property key.schema='{""type"":""string""}' \ --property value.schema=""{\""type\"": \""record\"", \""name\"": \""rss\"", \""fields\"": [{\""name\"": \""key\"", \""type\"": \""string\""}, {\""name\"": \""title\"", \""type\"": \""string\""}, {\""name\"": \""content\"", \""type\"": \""string\""}, {\""name\"": \""date\"", \""type\"": \""string\""}]}"" ""test"",{""key"": ""http://example.com"", ""title"": ""test title"", ""content"": ""test content"", ""date"": ""2015-10-18'T'20:20:20""}  Any idea what could be wrong? Maybe this issue relates to [#114](https://github.com/confluentinc/kafka-rest/issues/114). Thanks in advance.",source-file | test-file,"Not possible to send keyed Avro messages via Proxy We tried different ways, like curl or [kafka-node-rest](https://github.com/confluentinc/kafka-rest-node), to send a Avro message to a topic with a key. The topic is created and schema submitted to schema registry, however, sending the message fails with a HTTP error code 500. Setting like ports etc. are all defaults for the platform. Here an example what we tried with kafka-node-rest: - create the schemata for key and value  javascript var idSchema = new KafkaRest.AvroSchema('string'); var valueSchema = new KafkaRest.AvroSchema( { 'namespace': 'mynamespace', 'type': 'record', 'name': 'myschema', 'fields': [ { 'name': 'key', 'type': 'string' }, { 'name': 'date', 'type': 'string' }, { 'name': 'title', 'type': 'string' } ] });  - send it  javascript topic.produce(idSchema, valueSchema, { 'key': link, 'value': { 'key': link, 'date': date, 'title': title } }, function(err, res) {  });  what throws an error.  javascript { name: 'APIError', status: 500, message: 'Internal Server Error', data: { error_code: 500, message: 'Internal Server Error' } }  - sending without a key works  javascript topic.produce(valueSchema, { 'key': link, 'date': date, 'title': title }, function(err, res) {  });  result:  javascript { offsets: [ { partition: 0, offset: 750, error_code: null, error: null } ], key_schema_id: null, value_schema_id: 121 }  However, with the **kafka-avro-console-producer** works fine  shell ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic rss \ --property parse.key=true \ --property key.separator=, \ --property key.schema='{""type"":""string""}' \ --property value.schema=""{\""type\"": \""record\"", \""name\"": \""rss\"", \""fields\"": [{\""name\"": \""key\"", \""type\"": \""string\""}, {\""name\"": \""title\"", \""type\"": \""string\""}, {\""name\"": \""content\"", \""type\"": \""string\""}, {\""name\"": \""date\"", \""type\"": \""string\""}]}"" ""test"",{""key"": ""http://example.com"", ""title"": ""test title"", ""content"": ""test content"", ""date"": ""2015-10-18'T'20:20:20""}  Any idea what could be wrong? Maybe this issue relates to [#114](https://github.com/confluentinc/kafka-rest/issues/114). Thanks in advance. source-file test-file",bug,0.9
37,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/37,Catch MessageStreamsExistException from ConsumerConnector.createMessageStreams,"When ConsumersResource.readTopic is invoked, it calls createMessageStreams if there isn't already a stream. However, if the consumer has already invoked the method for a different topic, it will throw a MessageStreamsExistException. Currently this isn't being caught, so it'll be turned into a 500. We should catch it and turn it into a more useful error code. This will also require updating the docs since it'll introduce a new error code.",documentation-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | documentation-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file,"Catch MessageStreamsExistException from ConsumerConnector.createMessageStreams When ConsumersResource.readTopic is invoked, it calls createMessageStreams if there isn't already a stream. However, if the consumer has already invoked the method for a different topic, it will throw a MessageStreamsExistException. Currently this isn't being caught, so it'll be turned into a 500. We should catch it and turn it into a more useful error code. This will also require updating the docs since it'll introduce a new error code. documentation-file source-file source-file source-file source-file source-file test-file test-file test-file documentation-file source-file source-file source-file source-file source-file test-file test-file test-file",bug,0.9
475,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/475,Kafka Rest API incorrect message returned by GET request for specified offset when data is compressed,"Test on a compressed topic vs a non-compressed topic. Should fail on compressed topic. GET /topics/myTopic/partitions/0/messages?offset=10&count=1 Tested using this call on the rest api using docker image version 4.1.0 and 5.0.0 and re-created on both. When requesting an offset between 0-8 it returns offset 0, when requesting from 9-149 it returns 9, when requesting from 150-265 it returns 150 etc Performing a /topics/myTopic/partitions/0/messages?offset=0&count=1000 shows that we have no offset gaps from 0 to 1000 Compressed format: ByteBufferMessageSet( LegacyRecordBatch(offset=8, Record(magic=1, attributes=1, compression=GZIP, crc=3359863988, CreateTime=1537196426232, key=0 bytes, value=703 bytes)), LegacyRecordBatch(offset=149, Record(magic=1, attributes=1, compression=GZIP, crc=712877603, CreateTime=1537196426317, key=0 bytes, value=6199 bytes)), Regular format: ByteBufferMessageSet( LegacyRecordBatch(offset=1, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=2, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=3, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)),",source-file,"Kafka Rest API incorrect message returned by GET request for specified offset when data is compressed Test on a compressed topic vs a non-compressed topic. Should fail on compressed topic. GET /topics/myTopic/partitions/0/messages?offset=10&count=1 Tested using this call on the rest api using docker image version 4.1.0 and 5.0.0 and re-created on both. When requesting an offset between 0-8 it returns offset 0, when requesting from 9-149 it returns 9, when requesting from 150-265 it returns 150 etc Performing a /topics/myTopic/partitions/0/messages?offset=0&count=1000 shows that we have no offset gaps from 0 to 1000 Compressed format: ByteBufferMessageSet( LegacyRecordBatch(offset=8, Record(magic=1, attributes=1, compression=GZIP, crc=3359863988, CreateTime=1537196426232, key=0 bytes, value=703 bytes)), LegacyRecordBatch(offset=149, Record(magic=1, attributes=1, compression=GZIP, crc=712877603, CreateTime=1537196426317, key=0 bytes, value=6199 bytes)), Regular format: ByteBufferMessageSet( LegacyRecordBatch(offset=1, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=2, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), LegacyRecordBatch(offset=3, Record(magic=1, attributes=0, compression=NONE, crc=1309344491, CreateTime=-1, key=3 bytes, value=4 bytes)), source-file",bug,0.95
94,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/94,Rename Main class,"This is an inconvenient name since jps just lists it as Main instead of as kafka-rest, and it gets confusing if you're also running schema-registry. We'll need to maintain the Main class for compatibility, but we should change all usages (e.g. in scripts) to the new class.",other-file | other-file | other-file | documentation-file | config-file | source-file,"Rename Main class This is an inconvenient name since jps just lists it as Main instead of as kafka-rest, and it gets confusing if you're also running schema-registry. We'll need to maintain the Main class for compatibility, but we should change all usages (e.g. in scripts) to the new class. other-file other-file other-file documentation-file config-file source-file",no-bug,0.95
33,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/33,Override default port,"We're currently using the default port from rest-utils, which results in conflicts with other rest-utils projects, e.g. confluentinc/schema-registry#103. We should either override to something different and/or change the rest-utils default. Changing rest-utils might actually be the simpler solution since it means we don't also have to make changes in any repositories that already rely on the 8080 address.",documentation-file | documentation-file | documentation-file | source-file | documentation-file | documentation-file | documentation-file | source-file,"Override default port We're currently using the default port from rest-utils, which results in conflicts with other rest-utils projects, e.g. confluentinc/schema-registry#103. We should either override to something different and/or change the rest-utils default. Changing rest-utils might actually be the simpler solution since it means we don't also have to make changes in any repositories that already rely on the 8080 address. documentation-file documentation-file documentation-file source-file documentation-file documentation-file documentation-file source-file",no-bug,0.8
7,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/7,Clean shutdown,"The proxy should handle SIGTERM properly, waiting for outstanding requests to finish while ignoring any new incoming requests. Jetty has some support for doing this: http://docs.codehaus.org/display/JETTY/How+to+gracefully+shutdown although given the long timeouts for produce and consume requests, this needs to be handled carefully.",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | other-file,"Clean shutdown The proxy should handle SIGTERM properly, waiting for outstanding requests to finish while ignoring any new incoming requests. Jetty has some support for doing this: http://docs.codehaus.org/display/JETTY/How+to+gracefully+shutdown although given the long timeouts for produce and consume requests, this needs to be handled carefully. source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file other-file",no-bug,0.9
15,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/15,Limit consumer responses by # of bytes rather than # of messages,`consumer.request.max.messages` should be `consumer.request.max.bytes`. This matches the underlying interface better and can provide better performance than using a # of messages since it can guarantee large responses as long as there is data available.,documentation-file | documentation-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file,Limit consumer responses by # of bytes rather than # of messages `consumer.request.max.messages` should be `consumer.request.max.bytes`. This matches the underlying interface better and can provide better performance than using a # of messages since it can guarantee large responses as long as there is data available. documentation-file documentation-file source-file source-file source-file source-file source-file test-file test-file,no-bug,0.7
77,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/77,Get java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable when calling the /topics/:topic endpoint,"Whenever I call the topics/:topic endpoint I get the following error  java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable(Ljava/util/Collection;)Lscala/collection/Iterable; at io.confluent.kafkarest.MetadataObserver.getTopic(MetadataObserver.java:91) at io.confluent.kafkarest.resources.TopicsResource.getTopic(TopicsResource.java:70) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) (stack trace continues)  From some preliminary research it appear this could be due to differences between the compile time version of scala and the runtime. http://stackoverflow.com/questions/4393946/nosuchmethoderror-when-attempting-to-implicitly-convert-a-java-to-scala-collecti I'm running on Centos. Here's the output of java -version  java version ""1.8.0_05"" Java(TM) SE Runtime Environment (build 1.8.0_05-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)  I installed the package `confluent-platform-2.11.5`",other-file,"Get java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable when calling the /topics/:topic endpoint Whenever I call the topics/:topic endpoint I get the following error  java.lang.NoSuchMethodError: scala.collection.JavaConversions.asScalaIterable(Ljava/util/Collection;)Lscala/collection/Iterable; at io.confluent.kafkarest.MetadataObserver.getTopic(MetadataObserver.java:91) at io.confluent.kafkarest.resources.TopicsResource.getTopic(TopicsResource.java:70) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) (stack trace continues)  From some preliminary research it appear this could be due to differences between the compile time version of scala and the runtime. http://stackoverflow.com/questions/4393946/nosuchmethoderror-when-attempting-to-implicitly-convert-a-java-to-scala-collecti I'm running on Centos. Here's the output of java -version  java version ""1.8.0_05"" Java(TM) SE Runtime Environment (build 1.8.0_05-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.5-b02, mixed mode)  I installed the package `confluent-platform-2.11.5` other-file",bug,0.8
12,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/12,Get bootstrap servers from Zookeeper instead of from a user setting,,documentation-file | source-file | source-file | source-file | source-file | test-file | test-file | documentation-file | source-file | source-file | source-file | source-file | test-file | test-file,Get bootstrap servers from Zookeeper instead of from a user setting  documentation-file source-file source-file source-file source-file test-file test-file documentation-file source-file source-file source-file source-file test-file test-file,no-bug,0.7
10,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/10,Topic metadata should include everything provided by --describe on kafka-topics.sh,"This should apply to all API endpoints that return a detailed view of a topic instead of just the topic name and should include all the info you can get using different flags to --describe (e.g. per-topic configs, which only show up with certain combinations of flags). Docs will need to be updated to reflect these changes.",config-file | config-file,"Topic metadata should include everything provided by --describe on kafka-topics.sh This should apply to all API endpoints that return a detailed view of a topic instead of just the topic name and should include all the info you can get using different flags to --describe (e.g. per-topic configs, which only show up with certain combinations of flags). Docs will need to be updated to reflect these changes. config-file config-file",no-bug,0.8
341,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/341,Consumer read takes long time with multiple consumers in consumer group,"A fundamental issue is seen with REST Proxy. Suppose if we create two consumer instances under the same consumer group, read call is taking longer time with calls with subsequent instance. Steps to reproduce: 1. Create my_consumer_instance1 and my_consumer_instance2 under my_json_consumer1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance1"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance2"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 2. Subscribe both the instances to topic test1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/subscription curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/subscription 3. Read the message from my_consumer_instance1 several times and we will get responses back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records 4. Read the message from my_consumer_instance2 and the call will hang. It will take around 8 minutes to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/records 5. Again read the message from my_consumer_instance1 and the call will hang. It will take some time to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records",source-file | test-file | source-file | test-file | source-file | test-file,"Consumer read takes long time with multiple consumers in consumer group A fundamental issue is seen with REST Proxy. Suppose if we create two consumer instances under the same consumer group, read call is taking longer time with calls with subsequent instance. Steps to reproduce: 1. Create my_consumer_instance1 and my_consumer_instance2 under my_json_consumer1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance1"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""name"": ""my_consumer_instance2"", ""format"": ""json"", ""auto.offset.reset"": ""earliest""}' http://localhost:8082/consumers/my_json_consumer1 2. Subscribe both the instances to topic test1: curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/subscription curl -H ""Content-Type: application/vnd.kafka.v2+json"" -d '{""topics"":[""test1""]}' http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/subscription 3. Read the message from my_consumer_instance1 several times and we will get responses back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records 4. Read the message from my_consumer_instance2 and the call will hang. It will take around 8 minutes to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance2/records 5. Again read the message from my_consumer_instance1 and the call will hang. It will take some time to get a response back: curl -X GET -H ""Accept: application/vnd.kafka.json.v2+json"" http://localhost:8082/consumers/my_json_consumer1/instances/my_consumer_instance1/records source-file test-file source-file test-file source-file test-file",bug,0.9
42,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/42,review comments on the doc,"The following are my review comments on doc.  1. curl -X POST -H ""Content-Type: application/vnd.kafka.v1+json"" \ --data '{""records"":[{""value"":""S2Fma2E=""}]}' ""http://localhost:8080/topics/test"" It returns the null schema id. Shouldn't that only happen to Avro data? {""offsets"":[{""partition"":0,""offset"":0}],""key_schema_id"":null,""value_schema_id"":null} 2. Installation, should we change the following commands to use bin/kafka-rest-start ? $ java io.confluent.kafkarest.Main [server.properties] mvn exec:java 3. Under Errors, an request entity => a request entity 4. GET /topics/{topic_name}/partitions The example uses GET /topics/test/partitions/1 We should remove /1. Also, the topic test was created with only 1 partition earlier and the example shows 2 partitions. Also, partitions start from 0, instead of 1. 5. To send Avro data, we need to mention that we need to start schema registry server first. 6. Example Avro response. The following response misses the key_schema_id and value_schema_id. { ""value_schema_id"": 32, ""offsets"": [ { ""partition"": 1, ""offset"": 100, }, { ""partition"": 1, ""offset"": 101, } ] } 7. The following should probably be smallest/largest, instead of true. ""auto.offset.reset"": ""true"" 8. The following response format is incorrect for GET /consumers/testgroup/instances/my_consumer/topics/test_topic. [ { ""topic"": ""test"", ""partition"": 1, ""consumed"": 100, ""committed"": 100 }, { ""topic"": ""test"", ""partition"": 2, ""consumed"": 200, ""committed"": 200 }, { ""topic"": ""test2"", ""partition"": 1, ""consumed"": 50, ""committed"": 50 } ] It should be sth like [{""key"":""AAAAAAE="",""value"":""AAAAAAAY"",""partition"":0,""offset"":0},{""key"":""AAAAAAE="",""value"":""AAAAAAAa"",""partition"":0,""offset"":1}]. 9. We need to make it clear that the same consumer instance can only consumer a single topic (i.e., can't change to a different topic later). 10. Producer: should we allow auto topic creation on the producer side? 11. Need to change ""value_schema"": ""{\""name\"":\""int\"",\""type\"": \""int\""}"";"", to ""value_schema"": ""{\""name\"":\""int\"",\""type\"": \""int\""}"", ",documentation-file | documentation-file,"review comments on the doc The following are my review comments on doc.  1. curl -X POST -H ""Content-Type: application/vnd.kafka.v1+json"" \ --data '{""records"":[{""value"":""S2Fma2E=""}]}' ""http://localhost:8080/topics/test"" It returns the null schema id. Shouldn't that only happen to Avro data? {""offsets"":[{""partition"":0,""offset"":0}],""key_schema_id"":null,""value_schema_id"":null} 2. Installation, should we change the following commands to use bin/kafka-rest-start ? $ java io.confluent.kafkarest.Main [server.properties] mvn exec:java 3. Under Errors, an request entity => a request entity 4. GET /topics/{topic_name}/partitions The example uses GET /topics/test/partitions/1 We should remove /1. Also, the topic test was created with only 1 partition earlier and the example shows 2 partitions. Also, partitions start from 0, instead of 1. 5. To send Avro data, we need to mention that we need to start schema registry server first. 6. Example Avro response. The following response misses the key_schema_id and value_schema_id. { ""value_schema_id"": 32, ""offsets"": [ { ""partition"": 1, ""offset"": 100, }, { ""partition"": 1, ""offset"": 101, } ] } 7. The following should probably be smallest/largest, instead of true. ""auto.offset.reset"": ""true"" 8. The following response format is incorrect for GET /consumers/testgroup/instances/my_consumer/topics/test_topic. [ { ""topic"": ""test"", ""partition"": 1, ""consumed"": 100, ""committed"": 100 }, { ""topic"": ""test"", ""partition"": 2, ""consumed"": 200, ""committed"": 200 }, { ""topic"": ""test2"", ""partition"": 1, ""consumed"": 50, ""committed"": 50 } ] It should be sth like [{""key"":""AAAAAAE="",""value"":""AAAAAAAY"",""partition"":0,""offset"":0},{""key"":""AAAAAAE="",""value"":""AAAAAAAa"",""partition"":0,""offset"":1}]. 9. We need to make it clear that the same consumer instance can only consumer a single topic (i.e., can't change to a different topic later). 10. Producer: should we allow auto topic creation on the producer side? 11. Need to change ""value_schema"": ""{\""name\"":\""int\"",\""type\"": \""int\""}"";"", to ""value_schema"": ""{\""name\"":\""int\"",\""type\"": \""int\""}"",  documentation-file documentation-file",no-bug,0.9
229,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/229,Add Documentation on Jetty Metrics,"The rest-utils project now exposes the JMX metrics from the Jetty server (like thread pool management, session usage, servlet handler metrics etc) via https://github.com/confluentinc/rest-utils/issues/46 We should update the rest proxy documentation to reflect that these metrics are now exposed.",documentation-file,"Add Documentation on Jetty Metrics The rest-utils project now exposes the JMX metrics from the Jetty server (like thread pool management, session usage, servlet handler metrics etc) via https://github.com/confluentinc/rest-utils/issues/46 We should update the rest proxy documentation to reflect that these metrics are now exposed. documentation-file",no-bug,0.95
31,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/31,Add advertised host name support,"Consumers return absolute URIs, so we should have a config to control the host name used in this URI since it can't always be determined correctly automatically. To stay consistent with schema-registry, the option should be ""host.name"" rather than ""advertised.host.name"" as used by Kafka.",documentation-file | source-file | source-file | source-file | test-file | documentation-file | source-file | source-file | source-file | test-file,"Add advertised host name support Consumers return absolute URIs, so we should have a config to control the host name used in this URI since it can't always be determined correctly automatically. To stay consistent with schema-registry, the option should be ""host.name"" rather than ""advertised.host.name"" as used by Kafka. documentation-file source-file source-file source-file test-file documentation-file source-file source-file source-file test-file",no-bug,0.8
4,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/4,Multi-topic produce requests,"A single request should be able to produce to multiple topics, just like a single request can post to a topic and publish to specific partitions. Implementing this should be straightforward, but we need to figure out which resource the request should be POSTed to.",source-file,"Multi-topic produce requests A single request should be able to produce to multiple topics, just like a single request can post to a topic and publish to specific partitions. Implementing this should be straightforward, but we need to figure out which resource the request should be POSTed to. source-file",no-bug,0.8
25,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/25,URL for commit offsets should be more intuitive,Probably `/consumers/{group}/instances/{instance}/offsets` instead of `/consumers/{group}/instances/{instance}`,documentation-file | source-file | test-file | test-file | test-file | documentation-file | source-file | test-file | test-file | test-file,URL for commit offsets should be more intuitive Probably `/consumers/{group}/instances/{instance}/offsets` instead of `/consumers/{group}/instances/{instance}` documentation-file source-file test-file test-file test-file documentation-file source-file test-file test-file test-file,no-bug,0.7
6,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/6,Add basic event logging,"The server should, at a minimum, include basic event logging. Especially important are unhandled exceptions (GenericExceptionMapper) and we should probably also be doing simple per-request logging.",config-file,"Add basic event logging The server should, at a minimum, include basic event logging. Especially important are unhandled exceptions (GenericExceptionMapper) and we should probably also be doing simple per-request logging. config-file",no-bug,0.9
748,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/748,Not possible to increase default value consumer max.poll.records on kafkaRestConfig,"Hi All, I'm testing to increase the default value of max.poll.records = 30 to 500 version 2.4.2. when i force on kafka rest proxy config file like this : consumer.max.poll.records=500 at startup server, i hav i WARN : `WARN Property consumer.max.poll.records is not valid (io.confluent.kafka.utils.VerifiableProperties)` all consumer still getting connection with default config : `max.poll.records = 30` How to increase this property for all consumers ? Regards Martin",source-file | source-file,"Not possible to increase default value consumer max.poll.records on kafkaRestConfig Hi All, I'm testing to increase the default value of max.poll.records = 30 to 500 version 2.4.2. when i force on kafka rest proxy config file like this : consumer.max.poll.records=500 at startup server, i hav i WARN : `WARN Property consumer.max.poll.records is not valid (io.confluent.kafka.utils.VerifiableProperties)` all consumer still getting connection with default config : `max.poll.records = 30` How to increase this property for all consumers ? Regards Martin source-file source-file",no-bug,0.9
79,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/79,Support for JSON messages,"I was wondering if there was any possibility of adding support for messages in formats beyond just Avro/Base64. For my use case (and likely many others), only strings are passed back and forth, so encoding to Base64 seems like overkill. Having to encode in Base64 isn't a huge pain point, but just seems unnecessary. Apologies if this is already supported and I just didn't find it in the documentation since I just started working with this today! Looks like it will be quite useful regardless. :)",source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | config-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | documentation-file | documentation-file | documentation-file,"Support for JSON messages I was wondering if there was any possibility of adding support for messages in formats beyond just Avro/Base64. For my use case (and likely many others), only strings are passed back and forth, so encoding to Base64 seems like overkill. Having to encode in Base64 isn't a huge pain point, but just seems unnecessary. Apologies if this is already supported and I just didn't find it in the documentation since I just started working with this today! Looks like it will be quite useful regardless. :) source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file source-file source-file source-file source-file test-file test-file test-file test-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file source-file source-file source-file source-file test-file test-file test-file test-file config-file source-file source-file source-file source-file test-file test-file test-file test-file test-file documentation-file documentation-file documentation-file",no-bug,0.9
92,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/92,Docs: Unclear default partitioning behavior,"When using the POST /topics/{topic} endpoint to produce a message: 1. If record.partition is not explicitly passed, is the record.key used as partitioning key? 2. Which partitioning scheme is used when sending the message to Kafka? Is it the default Kafka partitioner (hash(key)%partitions)? 3. Is there a way to override the partitioning behavior other than specifying record.partition? I'm sorry if these questions are trivial, I've couldn't find a conclusive statement in the docs and I may have some terminology gaps as I'm only getting into Kafka and Confluent.",documentation-file,"Docs: Unclear default partitioning behavior When using the POST /topics/{topic} endpoint to produce a message: 1. If record.partition is not explicitly passed, is the record.key used as partitioning key? 2. Which partitioning scheme is used when sending the message to Kafka? Is it the default Kafka partitioner (hash(key)%partitions)? 3. Is there a way to override the partitioning behavior other than specifying record.partition? I'm sorry if these questions are trivial, I've couldn't find a conclusive statement in the docs and I may have some terminology gaps as I'm only getting into Kafka and Confluent. documentation-file",no-bug,0.9
29,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/29,Handle exceptions and convert to RestExceptions,"We need to handle exceptions and properly convert them to RestExceptions. This includes - Async produce callbacks should check Exception types and convert them. New producer exceptions were modeled well so this should be straightforward - Consumer exceptions, which need to trigger errors in consume task to trigger the response. - All the metadata APIs, which probably mostly means catching ZkClient exceptions",documentation-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | test-file | documentation-file | source-file | source-file | source-file | source-file | source-file | test-file | test-file | source-file | source-file | source-file | test-file,"Handle exceptions and convert to RestExceptions We need to handle exceptions and properly convert them to RestExceptions. This includes - Async produce callbacks should check Exception types and convert them. New producer exceptions were modeled well so this should be straightforward - Consumer exceptions, which need to trigger errors in consume task to trigger the response. - All the metadata APIs, which probably mostly means catching ZkClient exceptions documentation-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file source-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file test-file documentation-file source-file source-file source-file source-file source-file test-file test-file source-file source-file source-file test-file",no-bug,0.8
18,kafka-rest,https://github.com/confluentinc/kafka-rest/issues/18,Replace our AbstractPerformanceTest with the one from confluentinc/common,"confluentinc/common#9 will move this code into common-utils, once that's merged we should get rid of our copy.",test-file | source-file | source-file | source-file | source-file,"Replace our AbstractPerformanceTest with the one from confluentinc/common confluentinc/common#9 will move this code into common-utils, once that's merged we should get rid of our copy. test-file source-file source-file source-file source-file",no-bug,0.9
